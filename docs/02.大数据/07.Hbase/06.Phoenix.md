---
title: Phoenix
date: 2022-05-02 16:18:40
permalink: /pages/6698ab/
categories:
  - 大数据
  - Hbase
tags:
  - 
---
# Phoenix

Phoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。

Phoenix特点:

1. 容易集成：如**Spark**，Hive，Pig，Flume和Map Reduce；
2. 操作简单：DML命令以及通过DDL命令创建和操作表和版本化增量更改；
3. 支持HBase**二级索引**创建。

![image-20211125073505744](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125073505744.png)

## 安装

```sh
cd /opt/software/
tar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module/
cd /opt/module/
mv apache-phoenix-5.0.0-HBase-2.0-bin phoenix
```

复制server包并拷贝到各个节点的hbase/lib

```sh
cd /opt/module/phoenix/
cp phoenix-5.0.0-HBase-2.0-server.jar /opt/module/hbase/lib/
```

复制client包并拷贝到各个节点的hbase/lib

```sh
cp phoenix-5.0.0-HBase-2.0-client.jar /opt/module/hbase/lib/
```

同步hbase lib

```sh
xsync /opt/module/hbase/lib/
```

配置环境变量

```sh
sudo vim /etc/profile.d/my_env.sh
```

```sh
#phoenix
export PHOENIX_HOME=/opt/module/phoenix
export PHOENIX_CLASSPATH=$PHOENIX_HOME
export PATH=$PATH:$PHOENIX_HOME/bin
```

```sh
source /etc/profile.d/my_env.sh
```

**集群启动 hadoop zookeeper和hbase** 

启动Phoenix

```sh
sqlline.py hadoop102,hadoop103,hadoop104:2181
```

## Phoenix Shell操作

- 显示所有表

  - ```sql
    !table
    -- 或者
    !tables
    ```

- 创建表

  - ```sql
    -- 直接指定单个列作为RowKey
    CREATE TABLE IF NOT EXISTS student(
    id VARCHAR primary key,
    name VARCHAR);
    
    -- 在phoenix中，表名等会自动转换为大写，若要小写，使用双引号，如"us_population"。
    -- 指定多个列的联合作为RowKey
    CREATE TABLE IF NOT EXISTS us_population (
    State CHAR(2) NOT NULL,
    City VARCHAR NOT NULL,
    Population BIGINT
    CONSTRAINT my_pk PRIMARY KEY (state, city));
    
    ```

- 插入数据

  - ```sql
    upsert into student values('1001','zhangsan');
    ```

- 查询数据

  - ```sql
    select * from student;
    select * from student where id='1001';
    ```

- 删除数据

  - ```sql
    delete from student where id='1001';
    ```

- 删除表

  - ```sql
    drop table student;
    ```

## Dbeaver连接

Hbase推荐我们使用瘦客户端进行连接 我们先启动服务端

```sh
queryserver.py start #服务端启动
sqlline-thin.py hadoop102:8765 #客户端连接
```

修改类名 URL模块和端口号

```sh
类名: org.apache.phoenix.queryserver.client.Driver
URL模板: jdbc:phoenix:thin:url=http://{host}:{port};serialization=PROTOBUF
端口号: 8765
```

![image-20211125090823871](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125090823871.png)

添加驱动从maven仓库下载jar导入

https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-queryserver-client

![image-20211125090934676](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125090934676.png)

##  表的映射

默认情况下，**直接在HBase中创建的表，通过Phoenix是查看不到的**。如果要在Phoenix中操作直接在HBase中创建的表，则需要在Phoenix中进行表的**映射**。映射方式有两种：**视图映射和表映射**。

### 视图映射

Phoenix创建的视图是**只读**的，所以只能用来做**查询**，无法通过视图对源数据进行**修改**等操作。

在hbase shell建立表

```sql
create 'test','info1','info2'
```

在phoenix中创建关联test表的视图

```sql
create view "test"(id varchar primary key,"info1"."name" varchar, "info2"."address" varchar);
```

删除视图

```sql
drop view "test"
```

### 表映射

以类似创建视图的方式创建关联表，只需要将create view改为create table即可

```sql
create table "test"(id varchar primary key,"info1"."name" varchar, "info2"."address" varchar) column_encoded_bytes=0;
```

## JDBC操作

### 胖客户端

创建工程 导入依赖

```xml
  <dependencies>
        <dependency>
            <groupId>org.apache.phoenix</groupId>
            <artifactId>phoenix-core</artifactId>
            <version>5.0.0-HBase-2.0</version>
        </dependency>

        <dependency>
            <groupId>com.lmax</groupId>
            <artifactId>disruptor</artifactId>
            <version>3.3.6</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.8.4</version>
        </dependency>

    </dependencies>
```

编码

```java
package com.atguigu.phoenix;


import java.sql.*;

public class PhoenixClient {

    public static void main(String[] args)  throws  Exception{
        //1.定义参数
        String driver = "org.apache.phoenix.jdbc.PhoenixDriver";
        String url = "jdbc:phoenix:hadoop102,hadoop103,hadoop104:2181";

        //2.加载驱动
        Class.forName(driver);

        //3.创建连接
        Connection connection = DriverManager.getConnection(url);

        //4.预编译SQL
        PreparedStatement preparedStatement = connection.prepareStatement("SELECT * FROM STUDENT");

        //5.查询获取返回值
        ResultSet resultSet = preparedStatement.executeQuery();

        //6.打印结果
        while (resultSet.next()) {
            System.out.println(resultSet.getString(1) + resultSet.getString(2));
        }

        //7.关闭资源
        resultSet.close();
        preparedStatement.close();
        connection.close();
    }

}
```

### 瘦客户端

依赖

```xml
    <dependencies>
        <dependency>
            <groupId>org.apache.phoenix</groupId>
            <artifactId>phoenix-queryserver-client</artifactId>
            <version>5.0.0-HBase-2.0</version>
        </dependency>

    </dependencies>
```

编码

```java
package com.atguigu.phoenix;

import org.apache.phoenix.queryserver.client.ThinClientUtil;

import java.sql.*;

public class PhoenixClient2 {
    public static void main(String[] args) throws SQLException {
        String url = ThinClientUtil.getConnectionUrl("hadoop102", 8765); //获取jdbc url
        Connection connection = DriverManager.getConnection(url); //获取连接对象
        PreparedStatement preparedStatement = connection.prepareStatement("SELECT * FROM STUDENT");
        ResultSet resultSet = preparedStatement.executeQuery();

        while (resultSet.next()) {
            System.out.println(resultSet.getString(1) + resultSet.getString(2));
        }

        resultSet.close();
        preparedStatement.close();
        connection.close();

    }
}
```



## Phoenix 二级索引

### 二级索引配置文件

添加如下配置到HBase的HRegionserver节点的hbase-site.xml

```xml
<!-- phoenix regionserver 配置参数-->
	<property>
		<name>hbase.regionserver.wal.codec</name>
		<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
	</property>

	<property>
		<name>hbase.region.server.rpc.scheduler.factory.class</name>
		<value>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory</value>
		<description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>
	</property>

	<property>
		<name>hbase.rpc.controllerfactory.class</name>
		<value>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</value>
		<description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>
	</property>

```

添加如下配置到HBase中HMaster节点的hbase-site.xml中

```xml
<!-- phoenix master 配置参数 -->
	<property>
		<name>hbase.master.loadbalancer.class</name>
		<value>org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer</value>
	</property>

	<property>
		<name>hbase.coprocessor.master.classes</name>
		<value>org.apache.phoenix.hbase.index.master.IndexMasterObserver</value>
	</property>

```

重新启动hbase和Phoenix

### 全局二级索引

Global Index是默认的索引格式，创建全局索引时，会在HBase中**建立一张新表**。也就是说**索引数据和数据表是存放在不同的表中**的，因此全局索引**适用于多读少写的业务场景**。

![image-20211125113658425](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125113658425.png)

**写数据的时候会消耗大量开销**，因为索引表也要**更新**，而索引表是**分布在不同的数据节点上**的，跨节点的数据传输带来了较大的性能消耗。

在**读数据**的时候Phoenix会选择**索引表来降低查询消耗的时间**。

创建单个字段的全局索引

```sql
CREATE INDEX student_index ON student (name);
```

索引表查看

```sh
scan 'STUDENT_INDEX'
```

value全部为x空的 而row变成value+rowkey二进制

![image-20211125144210236](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125144210236.png)

二级索引只能针对 指定列**限定符+主键的索引表** 如果查询中查询列带上**其他列限定符** 则**无法从此索引表进行索引查询** 从而进行**全表扫描**。

![image-20211125152657808](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125152657808.png)



### 多级索引

针对上面二级索引只能进行两个列限定符 进索引扫描 我们可以创建索引表时指定多个列限定符

```sql
CREATE INDEX student_name_sex_index ON student (name,sex); -- 创建一个主键 name sex 索引表
```

创建完成后我们就可以进行多个列限定符来进行索引查询

```sql
explain select id,name,sex from student  where name = '6'; -- 从有这三个列限定符的索引表进行索引查询 没有则全表扫描
explain select id,name,sex,address from student  where name = '6'; -- 因为没有包含id,name,sex,address这四个列限定符的索引表 所有进行全表扫描
```

**注意: 创建索引表顺序要与查询时一致 否则无法进行索引查询**



### 删除索引

直接删除指定索引表

```sql
drop index STUDENT_NAME_SEX_INDEX on student;
```

### 携带其他字段的全局索引

```sql
CREATE INDEX student_name_index_all ON student (name) INCLUDE (age,sex,address);
```

![image-20211125153241439](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125153241439.png)

通过携带其他字段 **根据指定字段创建索引 并携带其他字段中的值** 携带字段没有创建索引只是附加为索引字段的结果 但条件查询必须是索引字段 否则也是进行全表扫描

### 全局索引的缺点

1. 占用空间 需要建立多个表
2. 维护成本高 无法在更新和添加频繁的表中使用 需要重新全表更新索引

### Phoenix 本地索引

Local Index适用于写操作频繁的场景。

索引数据和数据表的数据是存放在**同一张表中**（**且是同一个Region**），**避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销**。

```sql
CREATE LOCAL INDEX local_index_student ON student (name);
```

同样有个新的索引表

![image-20211125161848429](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125161848429.png)

但在hbase shell中 查看list时没有查看到此表 索引数据和数据表的数据是存放在**同一张表中**

查看student表数据

![image-20211125162107302](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125162107302.png)

**直接在原表中插入索引数据 索引值+主键 value为空**

![image-20211125161800896](https://cdn.jsdelivr.net/gh/Iekrwh/images/md-images/image-20211125161800896.png)

本地索引不局限查询列 我们可以查询所有列数据 但是条件语句必须是索引列限定

1. 先命中索引列查出主键id
2. 根据主键id命中原表的数据
3. 全局比本地索引快  全局是一次命中  而本地需要命中之后根据主键查询
4. 全局空间牺牲更大 而本地直接在表中增加
5. 全局索引更新要更新全部索引表  而本地索引只需要更新一张表(原数据表中的索引行)

### 协处理器

编写协处理器，实现在往A表插入数据的同时让HBase自身（协处理器）向B表中插入一条数据。

**相当于增强操作** 而我们的全局索引就是基于协处理器进行索引查询

创建项目 导入依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-client</artifactId>
        <version>1.3.1</version>
</dependency>

    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-server</artifactId>
        <version>1.3.1</version>
    </dependency>
</dependencies>
```

定义FruitTableCoprocessor类并继承BaseRegionObserver类

```java
package com.atguigu;

import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;
import org.apache.hadoop.hbase.coprocessor.ObserverContext;
import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
import org.apache.hadoop.hbase.regionserver.wal.WALEdit;

import java.io.IOException;

public class FruitTableCoprocessor extends BaseRegionObserver {

    @Override
    public void postPut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit, Durability durability) throws IOException {

        //获取连接
        Connection connection = ConnectionFactory.createConnection(HBaseConfiguration.create());

        //获取表对象
        Table table = connection.getTable(TableName.valueOf("fruit"));

        //插入数据
        table.put(put);

        //关闭资源
        table.close();
        connection.close();
    }
}
```



