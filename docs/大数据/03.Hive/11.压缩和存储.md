---
title: 压缩和存储
date: 2022-03-17 22:07:28
permalink: /pages/9c39ac/
categories:
  - 大数据
  - Hive
tags:
  - 
---
# 压缩和存储

## 压缩参数设置

要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：

| 参数                                               | 默认值                                                       | 阶段        | 建议                                         |
| -------------------------------------------------- | ------------------------------------------------------------ | ----------- | -------------------------------------------- |
| io.compression.codecs    （在core-site.xml中配置） | org.apache.hadoop.io.compress.DefaultCodec,  org.apache.hadoop.io.compress.GzipCodec,  org.apache.hadoop.io.compress.BZip2Codec,  org.apache.hadoop.io.compress.Lz4Codec | 输入压缩    | Hadoop使用文件扩展名判断是否支持某种编解码器 |
| mapreduce.map.output.compress                      | false                                                        | mapper输出  | 这个参数设为true启用压缩                     |
| mapreduce.map.output.compress.codec                | org.apache.hadoop.io.compress.DefaultCodec                   | mapper输出  | 使用LZO、LZ4或snappy编解码器在此阶段压缩数据 |
| mapreduce.output.fileoutputformat.compress         | false                                                        | reducer输出 | 这个参数设为true启用压缩                     |
| mapreduce.output.fileoutputformat.compress.codec   | org.apache.hadoop.io.compress. DefaultCodec                  | reducer输出 | 使用标准工具或者编解码器，如gzip和bzip2      |
| mapreduce.output.fileoutputformat.compress.type    | RECORD                                                       | reducer输出 | SequenceFile输出使用的压缩类型：NONE和BLOCK  |

## 开启map输出阶段压缩(MR引擎)

临时开启

```sql
set hive.exec.compress.intermediate=true; -- 开启hive中间传送的压缩功能
set mapreduce.map.output.compress=true; -- 开启mapreduce中map输出压缩功能
set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec; -- 设置指定的压缩方式
```

## 开启Reduce输出阶段压缩

```sql
set hive.exec.compress.output=true; -- 开启hive最终输出数据压缩
set mapreduce.output.fileoutputformat.compress=true; -- 开启mapreduce最终输出数据压缩
set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec; -- 设置压缩方式
set mapreduce.output.fileoutputformat.compress.type=BLOCK; -- 设置mapreduce最终输出压缩方式
```



## 文件存储格式

Hive支持的存储数据的格式主要有：

行存储 TEXTFILE 、SEQUENCEFILE

列存储 ORC、PARQUET  **Hive中使用列存储比较好**

![image-20211109185028034](https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211109185028034.png)

## TEXTFILE 

默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。

## Orc格式

 

Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：

![image-20211109185329421](https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211109185329421.png)

1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。

2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。

3）Stripe Footer：存的是各个Stream的类型，长度等信息。

每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。

## Parquet格式

Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。

（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。

（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。

（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。

## 指定文件存储格式

### textfile

在建表时通过stored 关键字指定格式

```sql
create table log_text (
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as textfile;  -- 指定为textfile 默认也是textfile
```

### orc

```sql
create table log_orc(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as orc  -- 指定为orc格式
tblproperties("orc.compress"="NONE"); -- 去掉压缩
```

只能插入数据 不能直接加数据文件

```sql
insert into table log_orc select * from log_text ;
```

### parquet

```sql
create table log_parquet(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as parquet ;
```

插入数据

```sql
insert into table log_parquet select * from log_text ;
```

### 三种存储格式大小比较

![image-20211109190508464](https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211109190508464.png)

## 存储和压缩结合

### orc存储方式的压缩

| Key                      | Default     | Notes                                                        |
| ------------------------ | ----------- | ------------------------------------------------------------ |
| orc.compress             | ZLIB        | high level  compression (one of NONE, ZLIB, SNAPPY)          |
| orc.compress.size        | 262,144     | number of bytes  in each compression chunk                   |
| orc.stripe.size          | 268,435,456 | number of bytes in each stripe                               |
| orc.row.index.stride     | 10,000      | number of rows between index entries (must be >= 1000)       |
| orc.create.index         | true        | whether to create  row indexes                               |
| orc.bloom.filter.columns | ""          | comma separated list of column names for which bloom filter should be  created |
| orc.bloom.filter.fpp     | 0.05        | false positive probability for bloom filter (must >0.0 and <1.0) |

所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现  在建表时指定

#### ZLIB压缩格式

```sql
create table log_zlib(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as orc  
tblproperties("orc.compress"="ZLIB"); -- 指定压缩为ZLIB 默认也为ZLIB
```

插入数据

```sql
insert into log_orc_zlib select * from log_text;
```

#### SNAPPY压缩格式

```sql
create table log_orc_snappy(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as orc
tblproperties("orc.compress"="SNAPPY");
```

插入数据

```sql
insert into log_orc_snappy select * from log_text;
```

### 压缩后文件大小比较

![image-20211109191148903](https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211109191148903.png)

### Parquet存储方式的压缩

```sql
create table log_par_snappy(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as parquet
tblproperties("parquet.compression"="SNAPPY");

insert into log_par_snappy select * from log_text;
```

![image-20211109191516349](https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211109191516349.png)

未压缩和压缩之后文件大小

### 总结

在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。





