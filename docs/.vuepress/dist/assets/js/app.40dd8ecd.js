(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var a,i,s=e[0],l=e[1],c=e[2],p=0,u=[];p<s.length;p++)i=s[p],Object.prototype.hasOwnProperty.call(r,i)&&r[i]&&u.push(r[i][0]),r[i]=0;for(a in l)Object.prototype.hasOwnProperty.call(l,a)&&(n[a]=l[a]);for(d&&d(e);u.length;)u.shift()();return o.push.apply(o,c||[]),t()}function t(){for(var n,e=0;e<o.length;e++){for(var t=o[e],a=!0,s=1;s<t.length;s++){var l=t[s];0!==r[l]&&(a=!1)}a&&(o.splice(e--,1),n=i(i.s=t[0]))}return n}var a={},r={1:0},o=[];function i(e){if(a[e])return a[e].exports;var t=a[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.e=function(n){var e=[],t=r[n];if(0!==t)if(t)e.push(t[2]);else{var a=new Promise((function(e,a){t=r[n]=[e,a]}));e.push(t[2]=a);var o,s=document.createElement("script");s.charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.src=function(n){return i.p+"assets/js/"+({}[n]||n)+"."+{2:"6a9cea3c",3:"6634b688",4:"8c3c0acd",5:"69b21161",6:"6b6896fe",7:"726dbc5f",8:"99cd54ee",9:"c89f984c",10:"2c01b12e",11:"664036c6",12:"f548447f",13:"8d4c6448",14:"b6ff5175",15:"730457a3",16:"f9e71cf5",17:"208779aa",18:"ab9a3bbb",19:"ad906bc0",20:"e27af5ca",21:"799ac5c3",22:"65a08aeb",23:"8536e70e",24:"3d058006",25:"f1c45c8c",26:"f2173fd0",27:"9ccd43d1",28:"da91b7e2",29:"461bce84",30:"8fdd5612",31:"f6fe2381",32:"f5aed9a2",33:"5932f9b2",34:"589697c2",35:"f13cad1b",36:"a27e3982",37:"1eb4eed4",38:"060b5e7e",39:"dc67fc14",40:"498ba062",41:"ea0dc104",42:"fbe5561c",43:"db0e44fd",44:"84603aa3",45:"6906fb0e",46:"c16b17b4",47:"c709a621",48:"bfdd7f0d",49:"504ac581",50:"eae7b421",51:"6f8c192b",52:"fcad9f75",53:"59c1cdf1",54:"c6056518",55:"62b17cc8",56:"28d2d58f",57:"e18df71f",58:"7b004771",59:"4271ceaa",60:"a513cb60",61:"3d8ba833",62:"aafdece6",63:"6d65dc9c",64:"b6732f71",65:"ea7a6b22",66:"d2cffa21",67:"641cd913",68:"19b6fe07",69:"10facc31",70:"14645f41",71:"a0b3696a",72:"e5be5fe1",73:"3f99599e",74:"28993790",75:"1f27d919",76:"5cf38880",77:"dafc6a5e",78:"8e47a6fe",79:"1f75688f",80:"325f1cde",81:"8cca5128",82:"8395fcab",83:"43951c63",84:"ad728d6d",85:"ebcc76cb",86:"d697fdcb",87:"39017ed5",88:"0bf64781",89:"e73af49c",90:"fbb12a35",91:"4839b991",92:"e7ade2e9",93:"3faaeac7",94:"25fb6f2d",95:"9a84e931",96:"e68e0aae",97:"d24a88cc",98:"7175d22e",99:"76456f24",100:"b3f59007",101:"b744e9c9",102:"0e6c35dd",103:"f22fef25",104:"9be911d5",105:"1469eafc",106:"9d267c5a",107:"b728fd5c",108:"4c0db71c",109:"cc8fb748",110:"c147c22c",111:"2b04a883",112:"db53739e",113:"1d5b2eff",114:"856088cb",115:"1074b8af",116:"d88d0a22",117:"1b17ec7c",118:"3f26f81e",119:"b68c191a",120:"49495207",121:"4a8174b0",122:"3d985319",123:"2db02e7a",124:"b26fd0a8",125:"1404e4be",126:"74732852",127:"6e95f432",128:"8056cd04",129:"05eb9883",130:"1e945430",131:"17e8268c",132:"5e12d60c",133:"9d349ee6",134:"836c4bd5",135:"b0a6cae7",136:"9c8151f9",137:"5fe25c85",138:"f7e7fbc9",139:"d56b3843",140:"0585b72d",141:"3dd0e26d",142:"94865158",143:"f5880cb7",144:"51410816",145:"f71d7f5a",146:"cba6467b",147:"340bfbe6",148:"e7f5ad4c",149:"c56c9d48",150:"efff543b",151:"46d42773",152:"b313c215",153:"72694bb0",154:"11fb04c2",155:"3d138baa",156:"3089831f",157:"d0ff3b6c",158:"94feb5c7",159:"a3baa78c",160:"86a99a62",161:"45b8b66b",162:"c64e56ec",163:"483a832b",164:"c19901cd",165:"f48d973d",166:"4aa2f408",167:"5acd78d8",168:"7e540d2c",169:"1a43a9fa",170:"2042e4e4",171:"0b170f15",172:"3f7c4192",173:"9f14fd16",174:"19942088",175:"5d5f9532",176:"cfa74820",177:"7057d212",178:"8b82debd",179:"b1df9ddf",180:"4085708e",181:"55d0a9dd",182:"dfbf6d8c",183:"56ba6c6a",184:"15b6aa3d",185:"d70097e9",186:"655e747b",187:"761a3f9f",188:"2db23a7d",189:"6e4875f8",190:"cd67fc8e",191:"f14772fb",192:"4efaec96",193:"80a5eeb7",194:"fa5ded41",195:"12256e2b",196:"dc77f375",197:"b004aa4b",198:"acf92801",199:"20b22e09",200:"826af643",201:"e5fb447a",202:"48d1b308",203:"84d332e0",204:"cc7d6f19",205:"f7d9cd24",206:"dc5c4afb",207:"5f7972ef",208:"70becdf7",209:"cad1c569",210:"fa527b0e",211:"993b8c0e",212:"d93d494e",213:"4b16adac",214:"6ec78061",215:"13029f81",216:"689d0457",217:"34c99396",218:"15ab7732",219:"75f13837",220:"c56ac0a9",221:"895f3699",222:"27eeeb52",223:"12668c30",224:"05ac3ee3",225:"edd622c1",226:"1c461bc8",227:"ecd8c7d2",228:"27d830bc",229:"e81fb37c",230:"d0ba3c56",231:"14b36b1c",232:"72276e73",233:"e69b011e",234:"09a1a458",235:"608e7252",236:"ff627f0b",237:"07b8be2c",238:"dae66b3b",239:"06c9ecd8",240:"fee390d1",241:"d42db9a1",242:"09b84c17",243:"2c2bc879",244:"9dc51c1c",245:"000b0100",246:"0ac8526d",247:"93bccdfd",248:"1320538e",249:"8830e715",250:"9ffdfca4",251:"1daed062",252:"f3000209",253:"aea926e5",254:"6d0eb14e",255:"8db9d533",256:"a11d12cb",257:"62603a31",258:"dcedf3c8",259:"0518d1c7",260:"842f6899",261:"1460bcaa",262:"acac0383",263:"6ea1fc01",264:"d02d65d5",265:"1ede84db",266:"662695a2",267:"f0620bc3",268:"2da06d21",269:"d63285e1",270:"bc08bf36",271:"272e051b",272:"54adbe1d",273:"99b5dd67",274:"d9a1ff1b",275:"6fa57e84",276:"f8e1117c",277:"f86f7050",278:"18ef4ec9",279:"c10e792b",280:"480236b8",281:"cff870df",282:"677b72f8",283:"8362f4fe",284:"7d8c3007",285:"003d79ff",286:"4beb6df8",287:"f4435cc2",288:"2aeed3e9",289:"c8be086b",290:"ab89b490",291:"9fd0deda",292:"142d04cd",293:"d3e0e817",294:"6580c1eb",295:"799d2b7d",296:"e2279804",297:"df6bc992",298:"2c39fdeb",299:"8d63e833",300:"65620f4b",301:"3284de8c",302:"e3ff1017",303:"bbc7c3b2",304:"dd7907f9",305:"f18fb17b",306:"e82e1d30",307:"dd96a4d6",308:"058f6d88"}[n]+".js"}(n);var l=new Error;o=function(e){s.onerror=s.onload=null,clearTimeout(c);var t=r[n];if(0!==t){if(t){var a=e&&("load"===e.type?"missing":e.type),o=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+a+": "+o+")",l.name="ChunkLoadError",l.type=a,l.request=o,t[1](l)}r[n]=void 0}};var c=setTimeout((function(){o({type:"timeout",target:s})}),12e4);s.onerror=s.onload=o,document.head.appendChild(s)}return Promise.all(e)},i.m=n,i.c=a,i.d=function(n,e,t){i.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},i.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},i.t=function(n,e){if(1&e&&(n=i(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var a in n)i.d(t,a,function(e){return n[e]}.bind(null,a));return t},i.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return i.d(e,"a",e),e},i.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},i.p="/",i.oe=function(n){throw console.error(n),n};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=e,s=s.slice();for(var c=0;c<s.length;c++)e(s[c]);var d=l;o.push([240,0]),t()}([function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e,t){var a=t(0),r=t(38).f,o=t(30),i=t(14),s=t(114),l=t(120),c=t(107);n.exports=function(n,e){var t,d,p,u,m,g=n.target,h=n.global,f=n.stat;if(t=h?a:f?a[g]||s(g,{}):(a[g]||{}).prototype)for(d in e){if(u=e[d],p=n.noTargetGet?(m=r(t,d))&&m.value:t[d],!c(h?d:g+(f?".":"#")+d,n.forced)&&void 0!==p){if(typeof u==typeof p)continue;l(u,p)}(n.sham||p&&p.sham)&&o(u,"sham",!0),i(t,d,u,n)}}},function(n,e,t){var a=t(64),r=Function.prototype,o=r.bind,i=r.call,s=a&&o.bind(i,i);n.exports=a?function(n){return n&&s(n)}:function(n){return n&&function(){return i.apply(n,arguments)}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){var a=t(124),r=t(14),o=t(256);a||r(Object.prototype,"toString",o,{unsafe:!0})},function(n,e){n.exports=function(n){return"function"==typeof n}},function(n,e,t){var a=t(0),r=t(80),o=t(10),i=t(81),s=t(115),l=t(160),c=r("wks"),d=a.Symbol,p=d&&d.for,u=l?d:d&&d.withoutSetter||i;n.exports=function(n){if(!o(c,n)||!s&&"string"!=typeof c[n]){var e="Symbol."+n;s&&o(d,n)?c[n]=d[n]:c[n]=l&&p?p(e):u(e)}return c[n]}},function(n,e,t){var a=t(3);n.exports=!a((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){var a=t(0),r=t(9),o=a.String,i=a.TypeError;n.exports=function(n){if(r(n))return n;throw i(o(n)+" is not an object")}},function(n,e,t){var a=t(5);n.exports=function(n){return"object"==typeof n?null!==n:a(n)}},function(n,e,t){var a=t(2),r=t(17),o=a({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return o(r(n),e)}},function(n,e,t){var a=t(64),r=Function.prototype.call;n.exports=a?r.bind(r):function(){return r.apply(r,arguments)}},function(n,e,t){var a=t(0),r=t(78),o=a.String;n.exports=function(n){if("Symbol"===r(n))throw TypeError("Cannot convert a Symbol value to a string");return o(n)}},function(n,e,t){var a=t(0),r=t(7),o=t(162),i=t(161),s=t(8),l=t(83),c=a.TypeError,d=Object.defineProperty,p=Object.getOwnPropertyDescriptor;e.f=r?i?function(n,e,t){if(s(n),e=l(e),s(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var a=p(n,e);a&&a.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:a.configurable,enumerable:"enumerable"in t?t.enumerable:a.enumerable,writable:!1})}return d(n,e,t)}:d:function(n,e,t){if(s(n),e=l(e),s(t),o)try{return d(n,e,t)}catch(n){}if("get"in t||"set"in t)throw c("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var a=t(0),r=t(5),o=t(10),i=t(30),s=t(114),l=t(88),c=t(40),d=t(67).CONFIGURABLE,p=c.get,u=c.enforce,m=String(String).split("String");(n.exports=function(n,e,t,l){var c,p=!!l&&!!l.unsafe,g=!!l&&!!l.enumerable,h=!!l&&!!l.noTargetGet,f=l&&void 0!==l.name?l.name:e;r(t)&&("Symbol("===String(f).slice(0,7)&&(f="["+String(f).replace(/^Symbol\(([^)]*)\)/,"$1")+"]"),(!o(t,"name")||d&&t.name!==f)&&i(t,"name",f),(c=u(t)).source||(c.source=m.join("string"==typeof f?f:""))),n!==a?(p?!h&&n[e]&&(g=!0):delete n[e],g?n[e]=t:i(n,e,t)):g?n[e]=t:s(e,t)})(Function.prototype,"toString",(function(){return r(this)&&p(this).source||l(this)}))},function(n,e,t){"use strict";function a(n,e,t,a,r,o,i,s){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),a&&(c.functional=!0),o&&(c._scopeId="data-v-"+o),i?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),r&&r.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(i)},c._ssrRegister=l):r&&(l=s?function(){r.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:r),l)if(c.functional){c._injectStyles=l;var d=c.render;c.render=function(n,e){return l.call(e),d(n,e)}}else{var p=c.beforeCreate;c.beforeCreate=p?[].concat(p,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return a}))},function(n,e,t){"use strict";var a=t(1),r=t(94);a({target:"RegExp",proto:!0,forced:/./.exec!==r},{exec:r})},function(n,e,t){var a=t(0),r=t(19),o=a.Object;n.exports=function(n){return o(r(n))}},function(n,e,t){var a=t(0),r=t(5),o=function(n){return r(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?o(a[n]):a[n]&&a[n][e]}},function(n,e,t){var a=t(0).TypeError;n.exports=function(n){if(null==n)throw a("Can't call method on "+n);return n}},function(n,e,t){var a=t(63),r=t(19);n.exports=function(n){return a(r(n))}},function(n,e,t){"use strict";var a=t(178).charAt,r=t(12),o=t(40),i=t(166),s=o.set,l=o.getterFor("String Iterator");i(String,"String",(function(n){s(this,{type:"String Iterator",string:r(n),index:0})}),(function(){var n,e=l(this),t=e.string,r=e.index;return r>=t.length?{value:void 0,done:!0}:(n=a(t,r),e.index+=n.length,{value:n,done:!1})}))},function(n,e,t){"use strict";var a=t(1),r=t(186);a({target:"Array",proto:!0,forced:[].forEach!=r},{forEach:r})},function(n,e,t){var a=t(0),r=t(179),o=t(180),i=t(186),s=t(30),l=function(n){if(n&&n.forEach!==i)try{s(n,"forEach",i)}catch(e){n.forEach=i}};for(var c in r)r[c]&&l(a[c]&&a[c].prototype);l(o)},function(n,e,t){var a=t(53);n.exports=function(n){return a(n.length)}},function(n,e,t){"use strict";var a=t(1),r=t(57).filter;a({target:"Array",proto:!0,forced:!t(92)("filter")},{filter:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(14),r=t(271),o=Error.prototype;o.toString!==r&&a(o,"toString",r)},function(n,e,t){var a=t(0),r=t(179),o=t(180),i=t(149),s=t(30),l=t(6),c=l("iterator"),d=l("toStringTag"),p=i.values,u=function(n,e){if(n){if(n[c]!==p)try{s(n,c,p)}catch(e){n[c]=p}if(n[d]||s(n,d,e),r[e])for(var t in i)if(n[t]!==i[t])try{s(n,t,i[t])}catch(e){n[t]=i[t]}}};for(var m in r)u(a[m]&&a[m].prototype,m);u(o,"DOMTokenList")},function(n,e,t){var a=t(2),r=a({}.toString),o=a("".slice);n.exports=function(n){return o(r(n),8,-1)}},function(n,e){n.exports=!1},function(n,e,t){var a=t(7),r=t(13),o=t(49);n.exports=a?function(n,e,t){return r.f(n,e,o(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var a=t(18);n.exports=a("navigator","userAgent")||""},function(n,e,t){var a=t(194),r="object"==typeof self&&self&&self.Object===Object&&self,o=a||r||Function("return this")();n.exports=o},function(n,e,t){var a,r=t(8),o=t(116),i=t(118),s=t(65),l=t(165),c=t(82),d=t(87),p=d("IE_PROTO"),u=function(){},m=function(n){return"<script>"+n+"<\/script>"},g=function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e},h=function(){try{a=new ActiveXObject("htmlfile")}catch(n){}var n,e;h="undefined"!=typeof document?document.domain&&a?g(a):((e=c("iframe")).style.display="none",l.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F):g(a);for(var t=i.length;t--;)delete h.prototype[i[t]];return h()};s[p]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(u.prototype=r(n),t=new u,u.prototype=null,t[p]=n):t=h(),void 0===e?t:o.f(t,e)}},function(n,e,t){var a=t(2);n.exports=a({}.isPrototypeOf)},function(n,e,t){var a=t(64),r=Function.prototype,o=r.apply,i=r.call;n.exports="object"==typeof Reflect&&Reflect.apply||(a?i.bind(o):function(){return i.apply(o,arguments)})},function(n,e,t){var a=t(0),r=t(5),o=t(85),i=a.TypeError;n.exports=function(n){if(r(n))return n;throw i(o(n)+" is not a function")}},function(n,e,t){var a=t(7),r=t(11),o=t(119),i=t(49),s=t(20),l=t(83),c=t(10),d=t(162),p=Object.getOwnPropertyDescriptor;e.f=a?p:function(n,e){if(n=s(n),e=l(e),d)try{return p(n,e)}catch(n){}if(c(n,e))return i(!r(o.f,n,e),n[e])}},function(n,e,t){"use strict";var a=t(36),r=t(11),o=t(2),i=t(110),s=t(3),l=t(8),c=t(5),d=t(55),p=t(53),u=t(12),m=t(19),g=t(128),h=t(48),f=t(272),v=t(111),b=t(6)("replace"),y=Math.max,x=Math.min,k=o([].concat),w=o([].push),S=o("".indexOf),E=o("".slice),T="$0"==="a".replace(/./,"$0"),_=!!/./[b]&&""===/./[b]("a","$0");i("replace",(function(n,e,t){var o=_?"$":"$0";return[function(n,t){var a=m(this),o=null==n?void 0:h(n,b);return o?r(o,n,a,t):r(e,u(a),n,t)},function(n,r){var i=l(this),s=u(n);if("string"==typeof r&&-1===S(r,o)&&-1===S(r,"$<")){var m=t(e,i,s,r);if(m.done)return m.value}var h=c(r);h||(r=u(r));var b=i.global;if(b){var T=i.unicode;i.lastIndex=0}for(var _=[];;){var I=v(i,s);if(null===I)break;if(w(_,I),!b)break;""===u(I[0])&&(i.lastIndex=g(s,p(i.lastIndex),T))}for(var j,z="",C=0,A=0;A<_.length;A++){for(var P=u((I=_[A])[0]),D=y(x(d(I.index),s.length),0),q=[],L=1;L<I.length;L++)w(q,void 0===(j=I[L])?j:String(j));var B=I.groups;if(h){var O=k([P],q,D,s);void 0!==B&&w(O,B);var F=u(a(r,void 0,O))}else F=f(P,s,D,q,B,r);D>=C&&(z+=E(s,C,D)+F,C=D+P.length)}return z+E(s,C)}]}),!!s((function(){var n=/./;return n.exec=function(){var n=[];return n.groups={a:"7"},n},"7"!=="".replace(n,"$<a>")}))||!T||_)},function(n,e,t){var a,r,o,i=t(242),s=t(0),l=t(2),c=t(9),d=t(30),p=t(10),u=t(113),m=t(87),g=t(65),h=s.TypeError,f=s.WeakMap;if(i||u.state){var v=u.state||(u.state=new f),b=l(v.get),y=l(v.has),x=l(v.set);a=function(n,e){if(y(v,n))throw new h("Object already initialized");return e.facade=n,x(v,n,e),e},r=function(n){return b(v,n)||{}},o=function(n){return y(v,n)}}else{var k=m("state");g[k]=!0,a=function(n,e){if(p(n,k))throw new h("Object already initialized");return e.facade=n,d(n,k,e),e},r=function(n){return p(n,k)?n[k]:{}},o=function(n){return p(n,k)}}n.exports={set:a,get:r,has:o,enforce:function(n){return o(n)?r(n):a(n,{})},getterFor:function(n){return function(e){var t;if(!c(e)||(t=r(e)).type!==n)throw h("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var a=t(1),r=t(0),o=t(36),i=t(267),s=r.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=i(n,e,l),a({global:!0,forced:l},t)},d=function(n,e){if(s&&s[n]){var t={};t[n]=i("WebAssembly."+n,e,l),a({target:"WebAssembly",stat:!0,forced:l},t)}};c("Error",(function(n){return function(e){return o(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return o(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return o(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return o(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return o(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return o(n,this,arguments)}})),c("URIError",(function(n){return function(e){return o(n,this,arguments)}})),d("CompileError",(function(n){return function(e){return o(n,this,arguments)}})),d("LinkError",(function(n){return function(e){return o(n,this,arguments)}})),d("RuntimeError",(function(n){return function(e){return o(n,this,arguments)}}))},function(n,e,t){var a=t(291),r=t(294);n.exports=function(n,e){var t=r(n,e);return a(t)?t:void 0}},function(n,e,t){var a=t(1),r=t(0),o=t(36),i=t(5),s=t(32),l=t(69),c=t(151),d=/MSIE .\./.test(s),p=r.Function,u=function(n){return function(e,t){var a=c(arguments.length,1)>2,r=i(e)?e:p(e),s=a?l(arguments,2):void 0;return n(a?function(){o(r,this,s)}:r,t)}};a({global:!0,bind:!0,forced:d},{setTimeout:u(r.setTimeout),setInterval:u(r.setInterval)})},function(n,e,t){"use strict";var a=t(1),r=t(0),o=t(60),i=t(89),s=t(9),l=t(106),c=t(24),d=t(20),p=t(70),u=t(6),m=t(92),g=t(69),h=m("slice"),f=u("species"),v=r.Array,b=Math.max;a({target:"Array",proto:!0,forced:!h},{slice:function(n,e){var t,a,r,u=d(this),m=c(u),h=l(n,m),y=l(void 0===e?m:e,m);if(o(u)&&(t=u.constructor,(i(t)&&(t===v||o(t.prototype))||s(t)&&null===(t=t[f]))&&(t=void 0),t===v||void 0===t))return g(u,h,y);for(a=new(void 0===t?v:t)(b(y-h,0)),r=0;h<y;h++,r++)h in u&&p(a,r,u[h]);return a.length=r,a}})},function(n,e,t){"use strict";t.d(e,"e",(function(){return a})),t.d(e,"b",(function(){return o})),t.d(e,"j",(function(){return i})),t.d(e,"g",(function(){return l})),t.d(e,"h",(function(){return c})),t.d(e,"i",(function(){return d})),t.d(e,"c",(function(){return p})),t.d(e,"f",(function(){return u})),t.d(e,"l",(function(){return m})),t.d(e,"m",(function(){return g})),t.d(e,"d",(function(){return f})),t.d(e,"k",(function(){return v})),t.d(e,"n",(function(){return b})),t.d(e,"a",(function(){return x}));t(16),t(39),t(147),t(77),t(75),t(112),t(46),t(22),t(4),t(23),t(25),t(79),t(104),t(105),t(54),t(159),t(26),t(143);var a=/#.*$/,r=/\.(md|html)$/,o=/\/$/,i=/^[a-z]+:/i;function s(n){return decodeURI(n).replace(a,"").replace(r,"")}function l(n){return i.test(n)}function c(n){return/^mailto:/.test(n)}function d(n){return/^tel:/.test(n)}function p(n){if(l(n))return n;var e=n.match(a),t=e?e[0]:"",r=s(n);return o.test(r)?n:r+".html"+t}function u(n,e){var t=n.hash,r=function(n){var e=n.match(a);if(e)return e[0]}(e);return(!r||t===r)&&s(n.path)===s(e)}function m(n,e,t){if(l(e))return{type:"external",path:e};t&&(e=function(n,e,t){var a=n.charAt(0);if("/"===a)return n;if("?"===a||"#"===a)return e+n;var r=e.split("/");t&&r[r.length-1]||r.pop();for(var o=n.replace(/^\//,"").split("/"),i=0;i<o.length;i++){var s=o[i];".."===s?r.pop():"."!==s&&r.push(s)}""!==r[0]&&r.unshift("");return r.join("/")}(e,t));for(var a=s(e),r=0;r<n.length;r++)if(s(n[r].regularPath)===a)return Object.assign({},n[r],{type:"page",path:p(n[r].path)});return console.error('[vuepress] No matching page found for sidebar item "'.concat(e,'"')),{}}function g(n,e,t,a){var r=t.pages,o=t.themeConfig,i=a&&o.locales&&o.locales[a]||o;if("auto"===(n.frontmatter.sidebar||i.sidebar||o.sidebar))return h(n);var s=i.sidebar||o.sidebar;if(s){var l=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(var t in e)if(0===(a=n,/(\.html|\/)$/.test(a)?a:a+"/").indexOf(encodeURI(t)))return{base:t,config:e[t]};var a;return{}}(e,s),c=l.base,d=l.config;return"auto"===d?h(n):d?d.map((function(n){return function n(e,t,a){var r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1;if("string"==typeof e)return m(t,e,a);if(Array.isArray(e))return Object.assign(m(t,e[0],a),{title:e[1]});r>3&&console.error("[vuepress] detected a too deep nested sidebar group.");var o=e.children||[];return 0===o.length&&e.path?Object.assign(m(t,e.path,a),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:o.map((function(e){return n(e,t,a,r+1)})),collapsable:!1!==e.collapsable}}(n,r,c)})):[]}return[]}function h(n){var e=f(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map((function(e){return{type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}}))}]}function f(n){var e;return(n=n.map((function(n){return Object.assign({},n)}))).forEach((function(n){2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)})),n.filter((function(n){return 2===n.level}))}function v(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function b(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function y(n){var e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function x(n,e){return y(e)-y(n)}},function(n,e,t){"use strict";var a=t(1),r=t(57).map;a({target:"Array",proto:!0,forced:!t(92)("map")},{map:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var a=t(3);n.exports=function(n,e){var t=[][n];return!!t&&a((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var a=t(37);n.exports=function(n,e){var t=n[e];return null==t?void 0:a(t)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(76),t(71),t(25),t(4),t(383),t(22),t(23),t(181),t(384),t(100);function a(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function r(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(n);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,a)}return t}function o(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?r(Object(t),!0).forEach((function(e){a(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}},function(n,e,t){var a,r,o=t(0),i=t(32),s=o.process,l=o.Deno,c=s&&s.versions||l&&l.version,d=c&&c.v8;d&&(r=(a=d.split("."))[0]>0&&a[0]<4?1:+(a[0]+a[1])),!r&&i&&(!(a=i.match(/Edge\/(\d+)/))||a[1]>=74)&&(a=i.match(/Chrome\/(\d+)/))&&(r=+a[1]),n.exports=r},function(n,e,t){var a=t(55),r=Math.min;n.exports=function(n){return n>0?r(a(n),9007199254740991):0}},function(n,e,t){"use strict";var a=t(1),r=t(0),o=t(3),i=t(60),s=t(9),l=t(17),c=t(24),d=t(70),p=t(152),u=t(92),m=t(6),g=t(52),h=m("isConcatSpreadable"),f=r.TypeError,v=g>=51||!o((function(){var n=[];return n[h]=!1,n.concat()[0]!==n})),b=u("concat"),y=function(n){if(!s(n))return!1;var e=n[h];return void 0!==e?!!e:i(n)};a({target:"Array",proto:!0,forced:!v||!b},{concat:function(n){var e,t,a,r,o,i=l(this),s=p(i,0),u=0;for(e=-1,a=arguments.length;e<a;e++)if(y(o=-1===e?i:arguments[e])){if(u+(r=c(o))>9007199254740991)throw f("Maximum allowed index exceeded");for(t=0;t<r;t++,u++)t in o&&d(s,u,o[t])}else{if(u>=9007199254740991)throw f("Maximum allowed index exceeded");d(s,u++,o)}return s.length=u,s}})},function(n,e){var t=Math.ceil,a=Math.floor;n.exports=function(n){var e=+n;return e!=e||0===e?0:(e>0?a:t)(e)}},function(n,e,t){var a=t(2),r=t(37),o=t(64),i=a(a.bind);n.exports=function(n,e){return r(n),void 0===e?n:o?i(n,e):function(){return n.apply(e,arguments)}}},function(n,e,t){var a=t(56),r=t(2),o=t(63),i=t(17),s=t(24),l=t(152),c=r([].push),d=function(n){var e=1==n,t=2==n,r=3==n,d=4==n,p=6==n,u=7==n,m=5==n||p;return function(g,h,f,v){for(var b,y,x=i(g),k=o(x),w=a(h,f),S=s(k),E=0,T=v||l,_=e?T(g,S):t||u?T(g,0):void 0;S>E;E++)if((m||E in k)&&(y=w(b=k[E],E,x),n))if(e)_[E]=y;else if(y)switch(n){case 3:return!0;case 5:return b;case 6:return E;case 2:c(_,b)}else switch(n){case 4:return!1;case 7:c(_,b)}return p?-1:r||d?d:_}};n.exports={forEach:d(0),map:d(1),filter:d(2),some:d(3),every:d(4),find:d(5),findIndex:d(6),filterReject:d(7)}},function(n,e,t){var a=t(164),r=t(118).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return a(n,r)}},function(n,e,t){var a=t(13).f,r=t(10),o=t(6)("toStringTag");n.exports=function(n,e,t){n&&!t&&(n=n.prototype),n&&!r(n,o)&&a(n,o,{configurable:!0,value:e})}},function(n,e,t){var a=t(28);n.exports=Array.isArray||function(n){return"Array"==a(n)}},function(n,e,t){var a=t(7),r=t(67).EXISTS,o=t(2),i=t(13).f,s=Function.prototype,l=o(s.toString),c=/function\b(?:\s|\/\*[\S\s]*?\*\/|\/\/[^\n\r]*[\n\r]+)*([^\s(/]*)/,d=o(c.exec);a&&!r&&i(s,"name",{configurable:!0,get:function(){try{return d(c,l(this))[1]}catch(n){return""}}})},function(n,e,t){var a=t(72),r=t(276),o=t(277),i=a?a.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":i&&i in Object(n)?r(n):o(n)}},function(n,e,t){var a=t(0),r=t(2),o=t(3),i=t(28),s=a.Object,l=r("".split);n.exports=o((function(){return!s("z").propertyIsEnumerable(0)}))?function(n){return"String"==i(n)?l(n,""):s(n)}:s},function(n,e,t){var a=t(3);n.exports=!a((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e){n.exports={}},function(n,e){n.exports={}},function(n,e,t){var a=t(7),r=t(10),o=Function.prototype,i=a&&Object.getOwnPropertyDescriptor,s=r(o,"name"),l=s&&"something"===function(){}.name,c=s&&(!a||a&&i(o,"name").configurable);n.exports={EXISTS:s,PROPER:l,CONFIGURABLE:c}},function(n,e,t){var a=t(2),r=t(8),o=t(243);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=a(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(t,[]),e=t instanceof Array}catch(n){}return function(t,a){return r(t),o(a),e?n(t,a):t.__proto__=a,t}}():void 0)},function(n,e,t){var a=t(2);n.exports=a([].slice)},function(n,e,t){"use strict";var a=t(83),r=t(13),o=t(49);n.exports=function(n,e,t){var i=a(e);i in n?r.f(n,i,o(0,t)):n[i]=t}},function(n,e,t){"use strict";var a=t(1),r=t(0),o=t(18),i=t(36),s=t(11),l=t(2),c=t(29),d=t(7),p=t(115),u=t(3),m=t(10),g=t(60),h=t(5),f=t(9),v=t(35),b=t(84),y=t(8),x=t(17),k=t(20),w=t(83),S=t(12),E=t(49),T=t(34),_=t(86),I=t(58),j=t(188),z=t(122),C=t(38),A=t(13),P=t(116),D=t(119),q=t(69),L=t(14),B=t(80),O=t(87),F=t(65),R=t(81),M=t(6),H=t(189),N=t(190),U=t(59),J=t(40),$=t(57).forEach,Z=O("hidden"),V=M("toPrimitive"),K=J.set,Q=J.getterFor("Symbol"),W=Object.prototype,G=r.Symbol,X=G&&G.prototype,Y=r.TypeError,nn=r.QObject,en=o("JSON","stringify"),tn=C.f,an=A.f,rn=j.f,on=D.f,sn=l([].push),ln=B("symbols"),cn=B("op-symbols"),dn=B("string-to-symbol-registry"),pn=B("symbol-to-string-registry"),un=B("wks"),mn=!nn||!nn.prototype||!nn.prototype.findChild,gn=d&&u((function(){return 7!=T(an({},"a",{get:function(){return an(this,"a",{value:7}).a}})).a}))?function(n,e,t){var a=tn(W,e);a&&delete W[e],an(n,e,t),a&&n!==W&&an(W,e,a)}:an,hn=function(n,e){var t=ln[n]=T(X);return K(t,{type:"Symbol",tag:n,description:e}),d||(t.description=e),t},fn=function(n,e,t){n===W&&fn(cn,e,t),y(n);var a=w(e);return y(t),m(ln,a)?(t.enumerable?(m(n,Z)&&n[Z][a]&&(n[Z][a]=!1),t=T(t,{enumerable:E(0,!1)})):(m(n,Z)||an(n,Z,E(1,{})),n[Z][a]=!0),gn(n,a,t)):an(n,a,t)},vn=function(n,e){y(n);var t=k(e),a=_(t).concat(kn(t));return $(a,(function(e){d&&!s(bn,t,e)||fn(n,e,t[e])})),n},bn=function(n){var e=w(n),t=s(on,this,e);return!(this===W&&m(ln,e)&&!m(cn,e))&&(!(t||!m(this,e)||!m(ln,e)||m(this,Z)&&this[Z][e])||t)},yn=function(n,e){var t=k(n),a=w(e);if(t!==W||!m(ln,a)||m(cn,a)){var r=tn(t,a);return!r||!m(ln,a)||m(t,Z)&&t[Z][a]||(r.enumerable=!0),r}},xn=function(n){var e=rn(k(n)),t=[];return $(e,(function(n){m(ln,n)||m(F,n)||sn(t,n)})),t},kn=function(n){var e=n===W,t=rn(e?cn:k(n)),a=[];return $(t,(function(n){!m(ln,n)||e&&!m(W,n)||sn(a,ln[n])})),a};(p||(L(X=(G=function(){if(v(X,this))throw Y("Symbol is not a constructor");var n=arguments.length&&void 0!==arguments[0]?S(arguments[0]):void 0,e=R(n),t=function(n){this===W&&s(t,cn,n),m(this,Z)&&m(this[Z],e)&&(this[Z][e]=!1),gn(this,e,E(1,n))};return d&&mn&&gn(W,e,{configurable:!0,set:t}),hn(e,n)}).prototype,"toString",(function(){return Q(this).tag})),L(G,"withoutSetter",(function(n){return hn(R(n),n)})),D.f=bn,A.f=fn,P.f=vn,C.f=yn,I.f=j.f=xn,z.f=kn,H.f=function(n){return hn(M(n),n)},d&&(an(X,"description",{configurable:!0,get:function(){return Q(this).description}}),c||L(W,"propertyIsEnumerable",bn,{unsafe:!0}))),a({global:!0,wrap:!0,forced:!p,sham:!p},{Symbol:G}),$(_(un),(function(n){N(n)})),a({target:"Symbol",stat:!0,forced:!p},{for:function(n){var e=S(n);if(m(dn,e))return dn[e];var t=G(e);return dn[e]=t,pn[t]=e,t},keyFor:function(n){if(!b(n))throw Y(n+" is not a symbol");if(m(pn,n))return pn[n]},useSetter:function(){mn=!0},useSimple:function(){mn=!1}}),a({target:"Object",stat:!0,forced:!p,sham:!d},{create:function(n,e){return void 0===e?T(n):vn(T(n),e)},defineProperty:fn,defineProperties:vn,getOwnPropertyDescriptor:yn}),a({target:"Object",stat:!0,forced:!p},{getOwnPropertyNames:xn,getOwnPropertySymbols:kn}),a({target:"Object",stat:!0,forced:u((function(){z.f(1)}))},{getOwnPropertySymbols:function(n){return z.f(x(n))}}),en)&&a({target:"JSON",stat:!0,forced:!p||u((function(){var n=G();return"[null]"!=en([n])||"{}"!=en({a:n})||"{}"!=en(Object(n))}))},{stringify:function(n,e,t){var a=q(arguments),r=e;if((f(e)||void 0!==n)&&!b(n))return g(e)||(e=function(n,e){if(h(r)&&(e=s(r,this,n,e)),!b(e))return e}),a[1]=e,i(en,null,a)}});if(!X[V]){var wn=X.valueOf;L(X,V,(function(n){return s(wn,this)}))}U(G,"Symbol"),F[Z]=!0},function(n,e,t){var a=t(33).Symbol;n.exports=a},function(n,e,t){"use strict";t.d(e,"a",(function(){return o}));t(79);var a=t(74);t(71),t(93),t(4),t(127),t(21),t(27),t(154);var r=t(101);t(41),t(26);function o(n){return function(n){if(Array.isArray(n))return Object(a.a)(n)}(n)||function(n){if("undefined"!=typeof Symbol&&null!=n[Symbol.iterator]||null!=n["@@iterator"])return Array.from(n)}(n)||Object(r.a)(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";function a(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,a=new Array(e);t<e;t++)a[t]=n[t];return a}t.d(e,"a",(function(){return a}))},function(n,e,t){"use strict";var a=t(36),r=t(11),o=t(2),i=t(110),s=t(153),l=t(8),c=t(19),d=t(125),p=t(128),u=t(53),m=t(12),g=t(48),h=t(126),f=t(111),v=t(94),b=t(109),y=t(3),x=b.UNSUPPORTED_Y,k=Math.min,w=[].push,S=o(/./.exec),E=o(w),T=o("".slice);i("split",(function(n,e,t){var o;return o="c"=="abbc".split(/(b)*/)[1]||4!="test".split(/(?:)/,-1).length||2!="ab".split(/(?:ab)*/).length||4!=".".split(/(.?)(.?)/).length||".".split(/()()/).length>1||"".split(/.?/).length?function(n,t){var o=m(c(this)),i=void 0===t?4294967295:t>>>0;if(0===i)return[];if(void 0===n)return[o];if(!s(n))return r(e,o,n,i);for(var l,d,p,u=[],g=(n.ignoreCase?"i":"")+(n.multiline?"m":"")+(n.unicode?"u":"")+(n.sticky?"y":""),f=0,b=new RegExp(n.source,g+"g");(l=r(v,b,o))&&!((d=b.lastIndex)>f&&(E(u,T(o,f,l.index)),l.length>1&&l.index<o.length&&a(w,u,h(l,1)),p=l[0].length,f=d,u.length>=i));)b.lastIndex===l.index&&b.lastIndex++;return f===o.length?!p&&S(b,"")||E(u,""):E(u,T(o,f)),u.length>i?h(u,0,i):u}:"0".split(void 0,0).length?function(n,t){return void 0===n&&0===t?[]:r(e,this,n,t)}:e,[function(e,t){var a=c(this),i=null==e?void 0:g(e,n);return i?r(i,e,a,t):r(o,m(a),e,t)},function(n,a){var r=l(this),i=m(n),s=t(o,r,i,a,o!==e);if(s.done)return s.value;var c=d(r,RegExp),g=r.unicode,h=(r.ignoreCase?"i":"")+(r.multiline?"m":"")+(r.unicode?"u":"")+(x?"g":"y"),v=new c(x?"^(?:"+r.source+")":r,h),b=void 0===a?4294967295:a>>>0;if(0===b)return[];if(0===i.length)return null===f(v,i)?[i]:[];for(var y=0,w=0,S=[];w<i.length;){v.lastIndex=x?0:w;var _,I=f(v,x?T(i,w):i);if(null===I||(_=k(u(v.lastIndex+(x?w:0)),i.length))===y)w=p(i,w,g);else{if(E(S,T(i,y,w)),S.length===b)return S;for(var j=1;j<=I.length-1;j++)if(E(S,I[j]),S.length===b)return S;w=y=_}}return E(S,T(i,y)),S}]}),!!y((function(){var n=/(?:)/,e=n.exec;n.exec=function(){return e.apply(this,arguments)};var t="ab".split(n);return 2!==t.length||"a"!==t[0]||"b"!==t[1]})),x)},function(n,e,t){var a=t(1),r=t(17),o=t(86);a({target:"Object",stat:!0,forced:t(3)((function(){o(1)}))},{keys:function(n){return o(r(n))}})},function(n,e,t){"use strict";t(16);var a,r,o=t(1),i=t(0),s=t(11),l=t(2),c=t(5),d=t(9),p=(a=!1,(r=/[ac]/).exec=function(){return a=!0,/./.exec.apply(this,arguments)},!0===r.test("abc")&&a),u=i.Error,m=l(/./.test);o({target:"RegExp",proto:!0,forced:!p},{test:function(n){var e=this.exec;if(!c(e))return m(this,n);var t=s(e,this,n);if(null!==t&&!d(t))throw new u("RegExp exec method returned something other than an Object or null");return!!t}})},function(n,e,t){var a=t(0),r=t(124),o=t(5),i=t(28),s=t(6)("toStringTag"),l=a.Object,c="Arguments"==i(function(){return arguments}());n.exports=r?i:function(n){var e,t,a;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=l(n),s))?t:c?i(e):"Object"==(a=i(e))&&o(e.callee)?"Arguments":a}},function(n,e,t){t(1)({target:"Array",stat:!0},{isArray:t(60)})},function(n,e,t){var a=t(29),r=t(113);(n.exports=function(n,e){return r[n]||(r[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.21.1",mode:a?"pure":"global",copyright:"© 2014-2022 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.21.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e,t){var a=t(2),r=0,o=Math.random(),i=a(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+i(++r+o,36)}},function(n,e,t){var a=t(0),r=t(9),o=a.document,i=r(o)&&r(o.createElement);n.exports=function(n){return i?o.createElement(n):{}}},function(n,e,t){var a=t(163),r=t(84);n.exports=function(n){var e=a(n,"string");return r(e)?e:e+""}},function(n,e,t){var a=t(0),r=t(18),o=t(5),i=t(35),s=t(160),l=a.Object;n.exports=s?function(n){return"symbol"==typeof n}:function(n){var e=r("Symbol");return o(e)&&i(e.prototype,l(n))}},function(n,e,t){var a=t(0).String;n.exports=function(n){try{return a(n)}catch(n){return"Object"}}},function(n,e,t){var a=t(164),r=t(118);n.exports=Object.keys||function(n){return a(n,r)}},function(n,e,t){var a=t(80),r=t(81),o=a("keys");n.exports=function(n){return o[n]||(o[n]=r(n))}},function(n,e,t){var a=t(2),r=t(5),o=t(113),i=a(Function.toString);r(o.inspectSource)||(o.inspectSource=function(n){return i(n)}),n.exports=o.inspectSource},function(n,e,t){var a=t(2),r=t(3),o=t(5),i=t(78),s=t(18),l=t(88),c=function(){},d=[],p=s("Reflect","construct"),u=/^\s*(?:class|function)\b/,m=a(u.exec),g=!u.exec(c),h=function(n){if(!o(n))return!1;try{return p(c,d,n),!0}catch(n){return!1}},f=function(n){if(!o(n))return!1;switch(i(n)){case"AsyncFunction":case"GeneratorFunction":case"AsyncGeneratorFunction":return!1}try{return g||!!m(u,l(n))}catch(n){return!0}};f.sham=!0,n.exports=!p||r((function(){var n;return h(h.call)||!h(Object)||!h((function(){n=!0}))||n}))?f:h},function(n,e,t){var a=t(28),r=t(0);n.exports="process"==a(r.process)},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(4);function a(n,e,t,a,r,o,i){try{var s=n[o](i),l=s.value}catch(n){return void t(n)}s.done?e(l):Promise.resolve(l).then(a,r)}function r(n){return function(){var e=this,t=arguments;return new Promise((function(r,o){var i=n.apply(e,t);function s(n){a(i,r,o,s,l,"next",n)}function l(n){a(i,r,o,s,l,"throw",n)}s(void 0)}))}}},function(n,e,t){var a=t(3),r=t(6),o=t(52),i=r("species");n.exports=function(n){return o>=51||!a((function(){var e=[];return(e.constructor={})[i]=function(){return{foo:1}},1!==e[n](Boolean).foo}))}},function(n,e,t){"use strict";var a=t(1),r=t(7),o=t(0),i=t(2),s=t(10),l=t(5),c=t(35),d=t(12),p=t(13).f,u=t(120),m=o.Symbol,g=m&&m.prototype;if(r&&l(m)&&(!("description"in g)||void 0!==m().description)){var h={},f=function(){var n=arguments.length<1||void 0===arguments[0]?void 0:d(arguments[0]),e=c(g,this)?new m(n):void 0===n?m():m(n);return""===n&&(h[e]=!0),e};u(f,m),f.prototype=g,g.constructor=f;var v="Symbol(test)"==String(m("test")),b=i(g.toString),y=i(g.valueOf),x=/^Symbol\((.*)\)[^)]+$/,k=i("".replace),w=i("".slice);p(g,"description",{configurable:!0,get:function(){var n=y(this),e=b(n);if(s(h,n))return"";var t=v?w(e,7,-1):k(e,x,"$1");return""===t?void 0:t}}),a({global:!0,forced:!0},{Symbol:f})}},function(n,e,t){"use strict";var a,r,o=t(11),i=t(2),s=t(12),l=t(155),c=t(109),d=t(80),p=t(34),u=t(40).get,m=t(227),g=t(232),h=d("native-string-replace",String.prototype.replace),f=RegExp.prototype.exec,v=f,b=i("".charAt),y=i("".indexOf),x=i("".replace),k=i("".slice),w=(r=/b*/g,o(f,a=/a/,"a"),o(f,r,"a"),0!==a.lastIndex||0!==r.lastIndex),S=c.BROKEN_CARET,E=void 0!==/()??/.exec("")[1];(w||E||S||m||g)&&(v=function(n){var e,t,a,r,i,c,d,m=this,g=u(m),T=s(n),_=g.raw;if(_)return _.lastIndex=m.lastIndex,e=o(v,_,T),m.lastIndex=_.lastIndex,e;var I=g.groups,j=S&&m.sticky,z=o(l,m),C=m.source,A=0,P=T;if(j&&(z=x(z,"y",""),-1===y(z,"g")&&(z+="g"),P=k(T,m.lastIndex),m.lastIndex>0&&(!m.multiline||m.multiline&&"\n"!==b(T,m.lastIndex-1))&&(C="(?: "+C+")",P=" "+P,A++),t=new RegExp("^(?:"+C+")",z)),E&&(t=new RegExp("^"+C+"$(?!\\s)",z)),w&&(a=m.lastIndex),r=o(f,j?t:m,P),j?r?(r.input=k(r.input,A),r[0]=k(r[0],A),r.index=m.lastIndex,m.lastIndex+=r[0].length):m.lastIndex=0:w&&r&&(m.lastIndex=m.global?r.index+r[0].length:a),E&&r&&r.length>1&&o(h,r[0],t,(function(){for(i=1;i<arguments.length-2;i++)void 0===arguments[i]&&(r[i]=void 0)})),r&&I)for(r.groups=c=p(null),i=0;i<I.length;i++)c[(d=I[i])[0]]=r[d[1]];return r}),n.exports=v},function(n,e,t){var a=t(281),r=t(282),o=t(283),i=t(284),s=t(285);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=r,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var a=t(196);n.exports=function(n,e){for(var t=n.length;t--;)if(a(n[t][0],e))return t;return-1}},function(n,e,t){var a=t(42)(Object,"create");n.exports=a},function(n,e,t){var a=t(303);n.exports=function(n,e){var t=n.__data__;return a(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var a=t(137);n.exports=function(n){if("string"==typeof n||a(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var a=t(1),r=t(7),o=t(13).f;a({target:"Object",stat:!0,forced:Object.defineProperty!==o,sham:!r},{defineProperty:o})},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(44),t(4),t(61),t(154),t(21),t(16),t(77);var a=t(74);function r(n,e){if(n){if("string"==typeof n)return Object(a.a)(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(a.a)(n,e):void 0}}},function(n,e,t){var a,r;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(r="function"==typeof(a=function(){var n,e,t={version:"0.2.0"},a=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function r(n,e,t){return n<e?e:n>t?t:n}function o(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(a[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=r(n,a.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(a.barSelector),d=a.speed,p=a.easing;return l.offsetWidth,i((function(e){""===a.positionUsing&&(a.positionUsing=t.getPositioningCSS()),s(c,function(n,e,t){var r;return(r="translate3d"===a.positionUsing?{transform:"translate3d("+o(n)+"%,0,0)"}:"translate"===a.positionUsing?{transform:"translate("+o(n)+"%,0)"}:{"margin-left":o(n)+"%"}).transition="all "+e+"ms "+t,r}(n,d,p)),1===n?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),d)}),d)):setTimeout(e,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),a.trickleSpeed)};return a.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*r(Math.random()*e,.1,.95)),e=r(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*a.trickleRate)},n=0,e=0,t.promise=function(a){return a&&"resolved"!==a.state()?(0===e&&t.start(),n++,e++,a.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=a.template;var r,i=e.querySelector(a.barSelector),l=n?"-100":o(t.status||0),d=document.querySelector(a.parent);return s(i,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),a.showSpinner||(r=e.querySelector(a.spinnerSelector))&&u(r),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(e),e},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(a.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&u(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var i=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),s=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var a,r=n.length,o=e.charAt(0).toUpperCase()+e.slice(1);r--;)if((a=n[r]+o)in t)return a;return e}(t))}function a(n,e,a){e=t(e),n.style[e]=a}return function(n,e){var t,r,o=arguments;if(2==o.length)for(t in e)void 0!==(r=e[t])&&e.hasOwnProperty(t)&&a(n,t,r);else a(n,o[1],o[2])}}();function l(n,e){return("string"==typeof n?n:p(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=p(n),a=t+e;l(t,e)||(n.className=a.substring(1))}function d(n,e){var t,a=p(n);l(n,e)&&(t=a.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function p(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function u(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?a.call(e,t,e,n):a)||(n.exports=r)},function(n){n.exports=JSON.parse('{"name":"vuepress-plugin-comment","version":"0.7.3","description":"Comment plugin in vuepress, such as Gitalk, Valine...","main":"index.js","scripts":{"test":"echo \\"Error: no test specified\\" && exit 1"},"repository":{"type":"git","url":"git+ssh://git@github.com/dongyuanxin/vuepress-plugin-comment.git"},"keywords":["vuepress","comment","plugin","vue","gitalk","valine"],"author":"dongyuanxin","license":"MIT","bugs":{"url":"https://github.com/dongyuanxin/vuepress-plugin-comment/issues"},"homepage":"https://github.com/dongyuanxin/vuepress-plugin-comment#readme","dependencies":{"ejs":"^2.6.1","gitalk":"^1.5.0","gitalk-fix":"^1.5.2","i":"^0.3.6","npm":"^6.9.0","valine":"^1.3.9"}}')},function(n,e,t){"use strict";var a=t(1),r=t(2),o=t(117).indexOf,i=t(47),s=r([].indexOf),l=!!s&&1/s([1],1,-0)<0,c=i("indexOf");a({target:"Array",proto:!0,forced:l||!c},{indexOf:function(n){var e=arguments.length>1?arguments[1]:void 0;return l?s(this,n,e)||0:o(this,n,e)}})},function(n,e,t){var a=t(2),r=t(14),o=Date.prototype,i=a(o.toString),s=a(o.getTime);"Invalid Date"!=String(new Date(NaN))&&r(o,"toString",(function(){var n=s(this);return n==n?i(this):"Invalid Date"}))},function(n,e,t){var a=t(55),r=Math.max,o=Math.min;n.exports=function(n,e){var t=a(n);return t<0?r(t+e,0):o(t,e)}},function(n,e,t){var a=t(3),r=t(5),o=/#|\.prototype\./,i=function(n,e){var t=l[s(n)];return t==d||t!=c&&(r(e)?a(e):!!e)},s=i.normalize=function(n){return String(n).replace(o,".").toLowerCase()},l=i.data={},c=i.NATIVE="N",d=i.POLYFILL="P";n.exports=i},function(n,e,t){var a=t(78),r=t(48),o=t(66),i=t(6)("iterator");n.exports=function(n){if(null!=n)return r(n,i)||r(n,"@@iterator")||o[a(n)]}},function(n,e,t){var a=t(3),r=t(0).RegExp,o=a((function(){var n=r("a","y");return n.lastIndex=2,null!=n.exec("abcd")})),i=o||a((function(){return!r("a","y").sticky})),s=o||a((function(){var n=r("^r","gy");return n.lastIndex=2,null!=n.exec("str")}));n.exports={BROKEN_CARET:s,MISSED_STICKY:i,UNSUPPORTED_Y:o}},function(n,e,t){"use strict";t(16);var a=t(2),r=t(14),o=t(94),i=t(3),s=t(6),l=t(30),c=s("species"),d=RegExp.prototype;n.exports=function(n,e,t,p){var u=s(n),m=!i((function(){var e={};return e[u]=function(){return 7},7!=""[n](e)})),g=m&&!i((function(){var e=!1,t=/a/;return"split"===n&&((t={}).constructor={},t.constructor[c]=function(){return t},t.flags="",t[u]=/./[u]),t.exec=function(){return e=!0,null},t[u](""),!e}));if(!m||!g||t){var h=a(/./[u]),f=e(u,""[n],(function(n,e,t,r,i){var s=a(n),l=e.exec;return l===o||l===d.exec?m&&!i?{done:!0,value:h(e,t,r)}:{done:!0,value:s(t,e,r)}:{done:!1}}));r(String.prototype,n,f[0]),r(d,u,f[1])}p&&l(d[u],"sham",!0)}},function(n,e,t){var a=t(0),r=t(11),o=t(8),i=t(5),s=t(28),l=t(94),c=a.TypeError;n.exports=function(n,e){var t=n.exec;if(i(t)){var a=r(t,n,e);return null!==a&&o(a),a}if("RegExp"===s(n))return r(l,n,e);throw c("RegExp#exec called on incompatible receiver")}},function(n,e,t){"use strict";var a=t(1),r=t(2),o=t(63),i=t(20),s=t(47),l=r([].join),c=o!=Object,d=s("join",",");a({target:"Array",proto:!0,forced:c||!d},{join:function(n){return l(i(this),void 0===n?",":n)}})},function(n,e,t){var a=t(0),r=t(114),o=a["__core-js_shared__"]||r("__core-js_shared__",{});n.exports=o},function(n,e,t){var a=t(0),r=Object.defineProperty;n.exports=function(n,e){try{r(a,n,{value:e,configurable:!0,writable:!0})}catch(t){a[n]=e}return e}},function(n,e,t){var a=t(52),r=t(3);n.exports=!!Object.getOwnPropertySymbols&&!r((function(){var n=Symbol();return!String(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&a&&a<41}))},function(n,e,t){var a=t(7),r=t(161),o=t(13),i=t(8),s=t(20),l=t(86);e.f=a&&!r?Object.defineProperties:function(n,e){i(n);for(var t,a=s(e),r=l(e),c=r.length,d=0;c>d;)o.f(n,t=r[d++],a[t]);return n}},function(n,e,t){var a=t(20),r=t(106),o=t(24),i=function(n){return function(e,t,i){var s,l=a(e),c=o(l),d=r(i,c);if(n&&t!=t){for(;c>d;)if((s=l[d++])!=s)return!0}else for(;c>d;d++)if((n||d in l)&&l[d]===t)return n||d||0;return!n&&-1}};n.exports={includes:i(!0),indexOf:i(!1)}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){"use strict";var a={}.propertyIsEnumerable,r=Object.getOwnPropertyDescriptor,o=r&&!a.call({1:2},1);e.f=o?function(n){var e=r(this,n);return!!e&&e.enumerable}:a},function(n,e,t){var a=t(10),r=t(121),o=t(38),i=t(13);n.exports=function(n,e,t){for(var s=r(e),l=i.f,c=o.f,d=0;d<s.length;d++){var p=s[d];a(n,p)||t&&a(t,p)||l(n,p,c(e,p))}}},function(n,e,t){var a=t(18),r=t(2),o=t(58),i=t(122),s=t(8),l=r([].concat);n.exports=a("Reflect","ownKeys")||function(n){var e=o.f(s(n)),t=i.f;return t?l(e,t(n)):e}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var a=t(0),r=t(10),o=t(5),i=t(17),s=t(87),l=t(168),c=s("IE_PROTO"),d=a.Object,p=d.prototype;n.exports=l?d.getPrototypeOf:function(n){var e=i(n);if(r(e,c))return e[c];var t=e.constructor;return o(t)&&e instanceof t?t.prototype:e instanceof d?p:null}},function(n,e,t){var a={};a[t(6)("toStringTag")]="z",n.exports="[object z]"===String(a)},function(n,e,t){var a=t(8),r=t(173),o=t(6)("species");n.exports=function(n,e){var t,i=a(n).constructor;return void 0===i||null==(t=a(i)[o])?e:r(t)}},function(n,e,t){var a=t(0),r=t(106),o=t(24),i=t(70),s=a.Array,l=Math.max;n.exports=function(n,e,t){for(var a=o(n),c=r(e,a),d=r(void 0===t?a:t,a),p=s(l(d-c,0)),u=0;c<d;c++,u++)i(p,u,n[c]);return p.length=u,p}},function(n,e,t){t(190)("iterator")},function(n,e,t){"use strict";var a=t(178).charAt;n.exports=function(n,e,t){return e+(t?a(n,e).length:1)}},function(n,e,t){var a=t(1),r=t(0),o=t(18),i=t(36),s=t(2),l=t(3),c=r.Array,d=o("JSON","stringify"),p=s(/./.exec),u=s("".charAt),m=s("".charCodeAt),g=s("".replace),h=s(1..toString),f=/[\uD800-\uDFFF]/g,v=/^[\uD800-\uDBFF]$/,b=/^[\uDC00-\uDFFF]$/,y=function(n,e,t){var a=u(t,e-1),r=u(t,e+1);return p(v,n)&&!p(b,r)||p(b,n)&&!p(v,a)?"\\u"+h(m(n,0),16):n},x=l((function(){return'"\\udf06\\ud834"'!==d("\udf06\ud834")||'"\\udead"'!==d("\udead")}));d&&a({target:"JSON",stat:!0,forced:x},{stringify:function(n,e,t){for(var a=0,r=arguments.length,o=c(r);a<r;a++)o[a]=arguments[a];var s=i(d,null,o);return"string"==typeof s?g(s,f,y):s}})},function(n,e,t){var a=t(275),r=t(50),o=Object.prototype,i=o.hasOwnProperty,s=o.propertyIsEnumerable,l=a(function(){return arguments}())?a:function(n){return r(n)&&i.call(n,"callee")&&!s.call(n,"callee")};n.exports=l},function(n,e,t){var a=t(42)(t(33),"Map");n.exports=a},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var a=t(295),r=t(302),o=t(304),i=t(305),s=t(306);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=r,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var a=t(31),r=t(137),o=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,i=/^\w*$/;n.exports=function(n,e){if(a(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!r(n))||(i.test(n)||!o.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var a=t(62),r=t(50);n.exports=function(n){return"symbol"==typeof n||r(n)&&"[object Symbol]"==a(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){var a=t(1),r=t(0),o=t(59);a({global:!0},{Reflect:{}}),o(r.Reflect,"Reflect",!0)},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(79);t(71),t(93),t(4),t(127),t(21),t(27);var a=t(101);t(41),t(26);function r(n,e){return function(n){if(Array.isArray(n))return n}(n)||function(n,e){var t=null==n?null:"undefined"!=typeof Symbol&&n[Symbol.iterator]||n["@@iterator"];if(null!=t){var a,r,o=[],i=!0,s=!1;try{for(t=t.call(n);!(i=(a=t.next()).done)&&(o.push(a.value),!e||o.length!==e);i=!0);}catch(n){s=!0,r=n}finally{try{i||null==t.return||t.return()}finally{if(s)throw r}}return o}}(n,e)||Object(a.a)(n,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";t.r(e);t(16),t(75),t(43),t(39);var a={name:"LastReadingPopup",data:function(){return{lastReading:null,show:!1}},computed:{popupConfig:function(){var n={"/":{message:"检测到您上一次阅读的位置，是否移至该位置？",sureButtonText:"确定",cancelButtonText:"取消"},"/zh/":{message:"检测到您上一次阅读的位置，是否移至该位置？",sureButtonText:"前往",cancelButtonText:"取消"}},e=this.$lang.split("-")[0];return n["/".concat(e,"/")]||n[this.$localePath]||n},message:function(){var n=this.popupConfig;return n&&n.message||n["/"].message},sureButtonText:function(){var n=this.popupConfig;return n&&n.sureButtonText||n["/"].sureButtonText},cancelButtonText:function(){var n=this.popupConfig;return n&&n.cancelButtonText||n["/"].cancelButtonText}},mounted:function(){var n=this;window.ActiveXObject||"ActiveXObject"in window?setTimeout((function(){window.addEventListener("load",n.init())}),1e3):setTimeout((function(){window.addEventListener("load",n.init)}),1e3)},methods:{init:function(){this.lastReading=JSON.parse(localStorage.getItem("lastReading")),this.lastReading&&(this.$route.path===this.lastReading.path?this.goto():(this.show=!0,setTimeout(this.clean,1e4)))},goto:function(){var n=this;this.$route.path!==this.lastReading.path?this.$router.replace(this.lastReading.path).then((function(){document.documentElement.scrollTop=n.lastReading.scrollTop,n.clean()})):this.$nextTick((function(){document.documentElement.scrollTop=n.lastReading.scrollTop}))},dontgoto:function(){this.clean()},clean:function(){this.show=!1,localStorage.removeItem("lastReading")}}},r=(t(372),t(15)),o=Object(r.a)(a,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("transition",{attrs:{name:"sw-update-popup"}},[n.show?t("div",{staticClass:"sw-update-popup"},[n._v("\n    "+n._s(n.message)+"\n\n    "),t("br"),n._v(" "),t("button",{on:{click:n.goto}},[n._v("\n      "+n._s(n.sureButtonText)+"\n    ")]),n._v(" "),t("button",{on:{click:n.dontgoto}},[n._v("\n      "+n._s(n.cancelButtonText)+"\n    ")])]):n._e()])}),[],!1,null,"181714f2",null);e.default=o.exports},function(n,e,t){"use strict";var a=t(1),r=t(57).some;a({target:"Array",proto:!0,forced:!t(47)("some")},{some:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var a=t(2),r=t(67).PROPER,o=t(14),i=t(8),s=t(35),l=t(12),c=t(3),d=t(155),p=RegExp.prototype,u=p.toString,m=a(d),g=c((function(){return"/a/b"!=u.call({source:"a",flags:"b"})})),h=r&&"toString"!=u.name;(g||h)&&o(RegExp.prototype,"toString",(function(){var n=i(this),e=l(n.source),t=n.flags;return"/"+e+"/"+l(void 0===t&&s(p,n)&&!("flags"in p)?m(n):t)}),{unsafe:!0})},function(n,e,t){var a=t(6),r=t(34),o=t(13),i=a("unscopables"),s=Array.prototype;null==s[i]&&o.f(s,i,{configurable:!0,value:r(null)}),n.exports=function(n){s[i][n]=!0}},function(n,e,t){var a=t(2),r=t(19),o=t(12),i=t(146),s=a("".replace),l="["+i+"]",c=RegExp("^"+l+l+"*"),d=RegExp(l+l+"*$"),p=function(n){return function(e){var t=o(r(e));return 1&n&&(t=s(t,c,"")),2&n&&(t=s(t,d,"")),t}};n.exports={start:p(1),end:p(2),trim:p(3)}},function(n,e){n.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(n,e,t){"use strict";var a=t(11),r=t(110),o=t(8),i=t(53),s=t(12),l=t(19),c=t(48),d=t(128),p=t(111);r("match",(function(n,e,t){return[function(e){var t=l(this),r=null==e?void 0:c(e,n);return r?a(r,e,t):new RegExp(e)[n](s(t))},function(n){var a=o(this),r=s(n),l=t(e,a,r);if(l.done)return l.value;if(!a.global)return p(a,r);var c=a.unicode;a.lastIndex=0;for(var u,m=[],g=0;null!==(u=p(a,r));){var h=s(u[0]);m[g]=h,""===h&&(a.lastIndex=d(r,i(a.lastIndex),c)),g++}return 0===g?null:m}]}))},function(n,e,t){var a=function(n){"use strict";var e=Object.prototype,t=e.hasOwnProperty,a="function"==typeof Symbol?Symbol:{},r=a.iterator||"@@iterator",o=a.asyncIterator||"@@asyncIterator",i=a.toStringTag||"@@toStringTag";function s(n,e,t){return Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}),n[e]}try{s({},"")}catch(n){s=function(n,e,t){return n[e]=t}}function l(n,e,t,a){var r=e&&e.prototype instanceof p?e:p,o=Object.create(r.prototype),i=new S(a||[]);return o._invoke=function(n,e,t){var a="suspendedStart";return function(r,o){if("executing"===a)throw new Error("Generator is already running");if("completed"===a){if("throw"===r)throw o;return T()}for(t.method=r,t.arg=o;;){var i=t.delegate;if(i){var s=x(i,t);if(s){if(s===d)continue;return s}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===a)throw a="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);a="executing";var l=c(n,e,t);if("normal"===l.type){if(a=t.done?"completed":"suspendedYield",l.arg===d)continue;return{value:l.arg,done:t.done}}"throw"===l.type&&(a="completed",t.method="throw",t.arg=l.arg)}}}(n,t,i),o}function c(n,e,t){try{return{type:"normal",arg:n.call(e,t)}}catch(n){return{type:"throw",arg:n}}}n.wrap=l;var d={};function p(){}function u(){}function m(){}var g={};s(g,r,(function(){return this}));var h=Object.getPrototypeOf,f=h&&h(h(E([])));f&&f!==e&&t.call(f,r)&&(g=f);var v=m.prototype=p.prototype=Object.create(g);function b(n){["next","throw","return"].forEach((function(e){s(n,e,(function(n){return this._invoke(e,n)}))}))}function y(n,e){var a;this._invoke=function(r,o){function i(){return new e((function(a,i){!function a(r,o,i,s){var l=c(n[r],n,o);if("throw"!==l.type){var d=l.arg,p=d.value;return p&&"object"==typeof p&&t.call(p,"__await")?e.resolve(p.__await).then((function(n){a("next",n,i,s)}),(function(n){a("throw",n,i,s)})):e.resolve(p).then((function(n){d.value=n,i(d)}),(function(n){return a("throw",n,i,s)}))}s(l.arg)}(r,o,a,i)}))}return a=a?a.then(i,i):i()}}function x(n,e){var t=n.iterator[e.method];if(void 0===t){if(e.delegate=null,"throw"===e.method){if(n.iterator.return&&(e.method="return",e.arg=void 0,x(n,e),"throw"===e.method))return d;e.method="throw",e.arg=new TypeError("The iterator does not provide a 'throw' method")}return d}var a=c(t,n.iterator,e.arg);if("throw"===a.type)return e.method="throw",e.arg=a.arg,e.delegate=null,d;var r=a.arg;return r?r.done?(e[n.resultName]=r.value,e.next=n.nextLoc,"return"!==e.method&&(e.method="next",e.arg=void 0),e.delegate=null,d):r:(e.method="throw",e.arg=new TypeError("iterator result is not an object"),e.delegate=null,d)}function k(n){var e={tryLoc:n[0]};1 in n&&(e.catchLoc=n[1]),2 in n&&(e.finallyLoc=n[2],e.afterLoc=n[3]),this.tryEntries.push(e)}function w(n){var e=n.completion||{};e.type="normal",delete e.arg,n.completion=e}function S(n){this.tryEntries=[{tryLoc:"root"}],n.forEach(k,this),this.reset(!0)}function E(n){if(n){var e=n[r];if(e)return e.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var a=-1,o=function e(){for(;++a<n.length;)if(t.call(n,a))return e.value=n[a],e.done=!1,e;return e.value=void 0,e.done=!0,e};return o.next=o}}return{next:T}}function T(){return{value:void 0,done:!0}}return u.prototype=m,s(v,"constructor",m),s(m,"constructor",u),u.displayName=s(m,i,"GeneratorFunction"),n.isGeneratorFunction=function(n){var e="function"==typeof n&&n.constructor;return!!e&&(e===u||"GeneratorFunction"===(e.displayName||e.name))},n.mark=function(n){return Object.setPrototypeOf?Object.setPrototypeOf(n,m):(n.__proto__=m,s(n,i,"GeneratorFunction")),n.prototype=Object.create(v),n},n.awrap=function(n){return{__await:n}},b(y.prototype),s(y.prototype,o,(function(){return this})),n.AsyncIterator=y,n.async=function(e,t,a,r,o){void 0===o&&(o=Promise);var i=new y(l(e,t,a,r),o);return n.isGeneratorFunction(t)?i:i.next().then((function(n){return n.done?n.value:i.next()}))},b(v),s(v,i,"Generator"),s(v,r,(function(){return this})),s(v,"toString",(function(){return"[object Generator]"})),n.keys=function(n){var e=[];for(var t in n)e.push(t);return e.reverse(),function t(){for(;e.length;){var a=e.pop();if(a in n)return t.value=a,t.done=!1,t}return t.done=!0,t}},n.values=E,S.prototype={constructor:S,reset:function(n){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(w),!n)for(var e in this)"t"===e.charAt(0)&&t.call(this,e)&&!isNaN(+e.slice(1))&&(this[e]=void 0)},stop:function(){this.done=!0;var n=this.tryEntries[0].completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(n){if(this.done)throw n;var e=this;function a(t,a){return i.type="throw",i.arg=n,e.next=t,a&&(e.method="next",e.arg=void 0),!!a}for(var r=this.tryEntries.length-1;r>=0;--r){var o=this.tryEntries[r],i=o.completion;if("root"===o.tryLoc)return a("end");if(o.tryLoc<=this.prev){var s=t.call(o,"catchLoc"),l=t.call(o,"finallyLoc");if(s&&l){if(this.prev<o.catchLoc)return a(o.catchLoc,!0);if(this.prev<o.finallyLoc)return a(o.finallyLoc)}else if(s){if(this.prev<o.catchLoc)return a(o.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<o.finallyLoc)return a(o.finallyLoc)}}}},abrupt:function(n,e){for(var a=this.tryEntries.length-1;a>=0;--a){var r=this.tryEntries[a];if(r.tryLoc<=this.prev&&t.call(r,"finallyLoc")&&this.prev<r.finallyLoc){var o=r;break}}o&&("break"===n||"continue"===n)&&o.tryLoc<=e&&e<=o.finallyLoc&&(o=null);var i=o?o.completion:{};return i.type=n,i.arg=e,o?(this.method="next",this.next=o.finallyLoc,d):this.complete(i)},complete:function(n,e){if("throw"===n.type)throw n.arg;return"break"===n.type||"continue"===n.type?this.next=n.arg:"return"===n.type?(this.rval=this.arg=n.arg,this.method="return",this.next="end"):"normal"===n.type&&e&&(this.next=e),d},finish:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.finallyLoc===n)return this.complete(t.completion,t.afterLoc),w(t),d}},catch:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.tryLoc===n){var a=t.completion;if("throw"===a.type){var r=a.arg;w(t)}return r}}throw new Error("illegal catch attempt")},delegateYield:function(n,e,t){return this.delegate={iterator:E(n),resultName:e,nextLoc:t},"next"===this.method&&(this.arg=void 0),d}},n}(n.exports);try{regeneratorRuntime=a}catch(n){"object"==typeof globalThis?globalThis.regeneratorRuntime=a:Function("r","regeneratorRuntime = r")(a)}},function(n,e,t){"use strict";var a=t(20),r=t(144),o=t(66),i=t(40),s=t(13).f,l=t(166),c=t(29),d=t(7),p=i.set,u=i.getterFor("Array Iterator");n.exports=l(Array,"Array",(function(n,e){p(this,{type:"Array Iterator",target:a(n),index:0,kind:e})}),(function(){var n=u(this),e=n.target,t=n.kind,a=n.index++;return!e||a>=e.length?(n.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:a,done:!1}:"values"==t?{value:e[a],done:!1}:{value:[a,e[a]],done:!1}}),"values");var m=o.Arguments=o.Array;if(r("keys"),r("values"),r("entries"),!c&&d&&"values"!==m.name)try{s(m,"name",{value:"values"})}catch(n){}},function(n,e,t){var a=t(0),r=t(11),o=t(37),i=t(8),s=t(85),l=t(108),c=a.TypeError;n.exports=function(n,e){var t=arguments.length<2?l(n):e;if(o(t))return i(r(t,n));throw c(s(n)+" is not iterable")}},function(n,e,t){var a=t(0).TypeError;n.exports=function(n,e){if(n<e)throw a("Not enough arguments");return n}},function(n,e,t){var a=t(257);n.exports=function(n,e){return new(a(n))(0===e?0:e)}},function(n,e,t){var a=t(9),r=t(28),o=t(6)("match");n.exports=function(n){var e;return a(n)&&(void 0!==(e=n[o])?!!e:"RegExp"==r(n))}},function(n,e,t){var a=t(1),r=t(265);a({target:"Array",stat:!0,forced:!t(172)((function(n){Array.from(n)}))},{from:r})},function(n,e,t){"use strict";var a=t(8);n.exports=function(){var n=a(this),e="";return n.global&&(e+="g"),n.ignoreCase&&(e+="i"),n.multiline&&(e+="m"),n.dotAll&&(e+="s"),n.unicode&&(e+="u"),n.sticky&&(e+="y"),e}},function(n,e,t){var a=t(5),r=t(9),o=t(68);n.exports=function(n,e,t){var i,s;return o&&a(i=e.constructor)&&i!==t&&r(s=i.prototype)&&s!==t.prototype&&o(n,s),n}},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e){var t=/^\s+|\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,r=/^0b[01]+$/i,o=/^0o[0-7]+$/i,i=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),d=Object.prototype.toString,p=Math.max,u=Math.min,m=function(){return c.Date.now()};function g(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function h(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==d.call(n)}(n))return NaN;if(g(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=g(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var s=r.test(n);return s||o.test(n)?i(n.slice(2),s?2:8):a.test(n)?NaN:+n}n.exports=function(n,e,t){var a,r,o,i,s,l,c=0,d=!1,f=!1,v=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function b(e){var t=a,o=r;return a=r=void 0,c=e,i=n.apply(o,t)}function y(n){return c=n,s=setTimeout(k,e),d?b(n):i}function x(n){var t=n-l;return void 0===l||t>=e||t<0||f&&n-c>=o}function k(){var n=m();if(x(n))return w(n);s=setTimeout(k,function(n){var t=e-(n-l);return f?u(t,o-(n-c)):t}(n))}function w(n){return s=void 0,v&&a?b(n):(a=r=void 0,i)}function S(){var n=m(),t=x(n);if(a=arguments,r=this,l=n,t){if(void 0===s)return y(l);if(f)return s=setTimeout(k,e),b(l)}return void 0===s&&(s=setTimeout(k,e)),i}return e=h(e)||0,g(t)&&(d=!!t.leading,o=(f="maxWait"in t)?p(h(t.maxWait)||0,e):o,v="trailing"in t?!!t.trailing:v),S.cancel=function(){void 0!==s&&clearTimeout(s),c=0,a=l=r=s=void 0},S.flush=function(){return void 0===s?i:w(m())},S}},function(n,e,t){"use strict";var a=t(1),r=t(373).start;a({target:"String",proto:!0,forced:t(375)},{padStart:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(115);n.exports=a&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var a=t(7),r=t(3);n.exports=a&&r((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var a=t(7),r=t(3),o=t(82);n.exports=!a&&!r((function(){return 7!=Object.defineProperty(o("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var a=t(0),r=t(11),o=t(9),i=t(84),s=t(48),l=t(241),c=t(6),d=a.TypeError,p=c("toPrimitive");n.exports=function(n,e){if(!o(n)||i(n))return n;var t,a=s(n,p);if(a){if(void 0===e&&(e="default"),t=r(a,n,e),!o(t)||i(t))return t;throw d("Can't convert object to primitive value")}return void 0===e&&(e="number"),l(n,e)}},function(n,e,t){var a=t(2),r=t(10),o=t(20),i=t(117).indexOf,s=t(65),l=a([].push);n.exports=function(n,e){var t,a=o(n),c=0,d=[];for(t in a)!r(s,t)&&r(a,t)&&l(d,t);for(;e.length>c;)r(a,t=e[c++])&&(~i(d,t)||l(d,t));return d}},function(n,e,t){var a=t(18);n.exports=a("document","documentElement")},function(n,e,t){"use strict";var a=t(1),r=t(11),o=t(29),i=t(67),s=t(5),l=t(228),c=t(123),d=t(68),p=t(59),u=t(30),m=t(14),g=t(6),h=t(66),f=t(167),v=i.PROPER,b=i.CONFIGURABLE,y=f.IteratorPrototype,x=f.BUGGY_SAFARI_ITERATORS,k=g("iterator"),w=function(){return this};n.exports=function(n,e,t,i,g,f,S){l(t,e,i);var E,T,_,I=function(n){if(n===g&&P)return P;if(!x&&n in C)return C[n];switch(n){case"keys":case"values":case"entries":return function(){return new t(this,n)}}return function(){return new t(this)}},j=e+" Iterator",z=!1,C=n.prototype,A=C[k]||C["@@iterator"]||g&&C[g],P=!x&&A||I(g),D="Array"==e&&C.entries||A;if(D&&(E=c(D.call(new n)))!==Object.prototype&&E.next&&(o||c(E)===y||(d?d(E,y):s(E[k])||m(E,k,w)),p(E,j,!0,!0),o&&(h[j]=w)),v&&"values"==g&&A&&"values"!==A.name&&(!o&&b?u(C,"name","values"):(z=!0,P=function(){return r(A,this)})),g)if(T={values:I("values"),keys:f?P:I("keys"),entries:I("entries")},S)for(_ in T)(x||z||!(_ in C))&&m(C,_,T[_]);else a({target:e,proto:!0,forced:x||z},T);return o&&!S||C[k]===P||m(C,k,P,{name:g}),h[e]=P,T}},function(n,e,t){"use strict";var a,r,o,i=t(3),s=t(5),l=t(34),c=t(123),d=t(14),p=t(6),u=t(29),m=p("iterator"),g=!1;[].keys&&("next"in(o=[].keys())?(r=c(c(o)))!==Object.prototype&&(a=r):g=!0),null==a||i((function(){var n={};return a[m].call(n)!==n}))?a={}:u&&(a=l(a)),s(a[m])||d(a,m,(function(){return this})),n.exports={IteratorPrototype:a,BUGGY_SAFARI_ITERATORS:g}},function(n,e,t){var a=t(3);n.exports=!a((function(){function n(){}return n.prototype.constructor=null,Object.getPrototypeOf(new n)!==n.prototype}))},function(n,e,t){var a=t(0);n.exports=a.Promise},function(n,e,t){var a=t(6),r=t(66),o=a("iterator"),i=Array.prototype;n.exports=function(n){return void 0!==n&&(r.Array===n||i[o]===n)}},function(n,e,t){var a=t(11),r=t(8),o=t(48);n.exports=function(n,e,t){var i,s;r(n);try{if(!(i=o(n,"return"))){if("throw"===e)throw t;return t}i=a(i,n)}catch(n){s=!0,i=n}if("throw"===e)throw t;if(s)throw i;return r(i),t}},function(n,e,t){var a=t(6)("iterator"),r=!1;try{var o=0,i={next:function(){return{done:!!o++}},return:function(){r=!0}};i[a]=function(){return this},Array.from(i,(function(){throw 2}))}catch(n){}n.exports=function(n,e){if(!e&&!r)return!1;var t=!1;try{var o={};o[a]=function(){return{next:function(){return{done:t=!0}}}},n(o)}catch(n){}return t}},function(n,e,t){var a=t(0),r=t(89),o=t(85),i=a.TypeError;n.exports=function(n){if(r(n))return n;throw i(o(n)+" is not a constructor")}},function(n,e,t){var a,r,o,i,s=t(0),l=t(36),c=t(56),d=t(5),p=t(10),u=t(3),m=t(165),g=t(69),h=t(82),f=t(151),v=t(175),b=t(90),y=s.setImmediate,x=s.clearImmediate,k=s.process,w=s.Dispatch,S=s.Function,E=s.MessageChannel,T=s.String,_=0,I={};try{a=s.location}catch(n){}var j=function(n){if(p(I,n)){var e=I[n];delete I[n],e()}},z=function(n){return function(){j(n)}},C=function(n){j(n.data)},A=function(n){s.postMessage(T(n),a.protocol+"//"+a.host)};y&&x||(y=function(n){f(arguments.length,1);var e=d(n)?n:S(n),t=g(arguments,1);return I[++_]=function(){l(e,void 0,t)},r(_),_},x=function(n){delete I[n]},b?r=function(n){k.nextTick(z(n))}:w&&w.now?r=function(n){w.now(z(n))}:E&&!v?(i=(o=new E).port2,o.port1.onmessage=C,r=c(i.postMessage,i)):s.addEventListener&&d(s.postMessage)&&!s.importScripts&&a&&"file:"!==a.protocol&&!u(A)?(r=A,s.addEventListener("message",C,!1)):r="onreadystatechange"in h("script")?function(n){m.appendChild(h("script")).onreadystatechange=function(){m.removeChild(this),j(n)}}:function(n){setTimeout(z(n),0)}),n.exports={set:y,clear:x}},function(n,e,t){var a=t(32);n.exports=/(?:ipad|iphone|ipod).*applewebkit/i.test(a)},function(n,e,t){var a=t(8),r=t(9),o=t(177);n.exports=function(n,e){if(a(n),r(e)&&e.constructor===n)return e;var t=o.f(n);return(0,t.resolve)(e),t.promise}},function(n,e,t){"use strict";var a=t(37),r=function(n){var e,t;this.promise=new n((function(n,a){if(void 0!==e||void 0!==t)throw TypeError("Bad Promise constructor");e=n,t=a})),this.resolve=a(e),this.reject=a(t)};n.exports.f=function(n){return new r(n)}},function(n,e,t){var a=t(2),r=t(55),o=t(12),i=t(19),s=a("".charAt),l=a("".charCodeAt),c=a("".slice),d=function(n){return function(e,t){var a,d,p=o(i(e)),u=r(t),m=p.length;return u<0||u>=m?n?"":void 0:(a=l(p,u))<55296||a>56319||u+1===m||(d=l(p,u+1))<56320||d>57343?n?s(p,u):a:n?c(p,u,u+2):d-56320+(a-55296<<10)+65536}};n.exports={codeAt:d(!1),charAt:d(!0)}},function(n,e){n.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(n,e,t){var a=t(82)("span").classList,r=a&&a.constructor&&a.constructor.prototype;n.exports=r===Object.prototype?void 0:r},function(n,e,t){var a=t(1),r=t(7),o=t(121),i=t(20),s=t(38),l=t(70);a({target:"Object",stat:!0,sham:!r},{getOwnPropertyDescriptors:function(n){for(var e,t,a=i(n),r=s.f,c=o(a),d={},p=0;c.length>p;)void 0!==(t=r(a,e=c[p++]))&&l(d,e,t);return d}})},function(n,e,t){var a=t(1),r=t(3),o=t(17),i=t(123),s=t(168);a({target:"Object",stat:!0,forced:r((function(){i(1)})),sham:!s},{getPrototypeOf:function(n){return i(o(n))}})},function(n,e,t){"use strict";var a,r=t(1),o=t(2),i=t(38).f,s=t(53),l=t(12),c=t(184),d=t(19),p=t(185),u=t(29),m=o("".startsWith),g=o("".slice),h=Math.min,f=p("startsWith");r({target:"String",proto:!0,forced:!!(u||f||(a=i(String.prototype,"startsWith"),!a||a.writable))&&!f},{startsWith:function(n){var e=l(d(this));c(n);var t=s(h(arguments.length>1?arguments[1]:void 0,e.length)),a=l(n);return m?m(e,a,t):g(e,t,t+a.length)===a}})},function(n,e,t){var a=t(0),r=t(153),o=a.TypeError;n.exports=function(n){if(r(n))throw o("The method doesn't accept regular expressions");return n}},function(n,e,t){var a=t(6)("match");n.exports=function(n){var e=/./;try{"/./"[n](e)}catch(t){try{return e[a]=!1,"/./"[n](e)}catch(n){}}return!1}},function(n,e,t){"use strict";var a=t(57).forEach,r=t(47)("forEach");n.exports=r?[].forEach:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}},function(n,e,t){var a=t(3);n.exports=!a((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(n,e,t){var a=t(28),r=t(20),o=t(58).f,i=t(126),s="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];n.exports.f=function(n){return s&&"Window"==a(n)?function(n){try{return o(n)}catch(n){return i(s)}}(n):o(r(n))}},function(n,e,t){var a=t(6);e.f=a},function(n,e,t){var a=t(264),r=t(10),o=t(189),i=t(13).f;n.exports=function(n){var e=a.Symbol||(a.Symbol={});r(e,n)||i(e,n,{value:o.f(n)})}},function(n,e,t){var a=t(12);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:a(n)}},function(n,e,t){t(1)({target:"Object",stat:!0,sham:!t(7)},{create:t(34)})},function(n,e){n.exports=function(n,e){for(var t=-1,a=e.length,r=n.length;++t<a;)n[r+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var a=t(95),r=t(286),o=t(287),i=t(288),s=t(289),l=t(290);function c(n){var e=this.__data__=new a(n);this.size=e.size}c.prototype.clear=r,c.prototype.delete=o,c.prototype.get=i,c.prototype.has=s,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var a=t(62),r=t(132);n.exports=function(n){if(!r(n))return!1;var e=a(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var a=t(307),r=t(50);n.exports=function n(e,t,o,i,s){return e===t||(null==e||null==t||!r(e)&&!r(t)?e!=e&&t!=t:a(e,t,o,i,n,s))}},function(n,e,t){var a=t(201),r=t(310),o=t(202);n.exports=function(n,e,t,i,s,l){var c=1&t,d=n.length,p=e.length;if(d!=p&&!(c&&p>d))return!1;var u=l.get(n),m=l.get(e);if(u&&m)return u==e&&m==n;var g=-1,h=!0,f=2&t?new a:void 0;for(l.set(n,e),l.set(e,n);++g<d;){var v=n[g],b=e[g];if(i)var y=c?i(b,v,g,e,n,l):i(v,b,g,n,e,l);if(void 0!==y){if(y)continue;h=!1;break}if(f){if(!r(e,(function(n,e){if(!o(f,e)&&(v===n||s(v,n,t,i,l)))return f.push(e)}))){h=!1;break}}else if(v!==b&&!s(v,b,t,i,l)){h=!1;break}}return l.delete(n),l.delete(e),h}},function(n,e,t){var a=t(133),r=t(308),o=t(309);function i(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new a;++e<t;)this.add(n[e])}i.prototype.add=i.prototype.push=r,i.prototype.has=o,n.exports=i},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var a=t(320),r=t(326),o=t(207);n.exports=function(n){return o(n)?a(n):r(n)}},function(n,e,t){(function(n){var a=t(33),r=t(322),o=e&&!e.nodeType&&e,i=o&&"object"==typeof n&&n&&!n.nodeType&&n,s=i&&i.exports===o?a.Buffer:void 0,l=(s?s.isBuffer:void 0)||r;n.exports=l}).call(this,t(157)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var a=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==a||"symbol"!=a&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var a=t(323),r=t(324),o=t(325),i=o&&o.isTypedArray,s=i?r(i):a;n.exports=s},function(n,e,t){var a=t(197),r=t(135);n.exports=function(n){return null!=n&&r(n.length)&&!a(n)}},function(n,e,t){var a=t(42)(t(33),"Set");n.exports=a},function(n,e,t){var a=t(132);n.exports=function(n){return n==n&&!a(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var a=t(212),r=t(99);n.exports=function(n,e){for(var t=0,o=(e=a(e,n)).length;null!=n&&t<o;)n=n[r(e[t++])];return t&&t==o?n:void 0}},function(n,e,t){var a=t(31),r=t(136),o=t(337),i=t(340);n.exports=function(n,e){return a(n)?n:r(n,e)?[n]:o(i(n))}},function(n,e,t){"use strict";var a=t(0),r=t(2),o=t(37),i=t(9),s=t(10),l=t(69),c=t(64),d=a.Function,p=r([].concat),u=r([].join),m={},g=function(n,e,t){if(!s(m,e)){for(var a=[],r=0;r<e;r++)a[r]="a["+r+"]";m[e]=d("C,a","return new C("+u(a,",")+")")}return m[e](n,t)};n.exports=c?d.bind:function(n){var e=o(this),t=e.prototype,a=l(arguments,1),r=function(){var t=p(a,l(arguments));return this instanceof r?g(e,t.length,t):e.apply(n,t)};return i(t)&&(r.prototype=t),r}},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){t(1)({target:"Object",stat:!0},{setPrototypeOf:t(68)})},function(n,e,t){var a=t(1),r=t(18),o=t(36),i=t(213),s=t(173),l=t(8),c=t(9),d=t(34),p=t(3),u=r("Reflect","construct"),m=Object.prototype,g=[].push,h=p((function(){function n(){}return!(u((function(){}),[],n)instanceof n)})),f=!p((function(){u((function(){}))})),v=h||f;a({target:"Reflect",stat:!0,forced:v,sham:v},{construct:function(n,e){s(n),l(e);var t=arguments.length<3?n:s(arguments[2]);if(f&&!h)return u(n,e,t);if(n==t){switch(e.length){case 0:return new n;case 1:return new n(e[0]);case 2:return new n(e[0],e[1]);case 3:return new n(e[0],e[1],e[2]);case 4:return new n(e[0],e[1],e[2],e[3])}var a=[null];return o(g,a,e),new(o(i,n,a))}var r=t.prototype,p=d(c(r)?r:m),v=o(n,p,e);return c(v)?v:p}})},function(n,e,t){},function(n,e,t){},function(n,e,t){var a=t(273),r=t(278),o=t(349),i=t(357),s=t(366),l=t(235),c=o((function(n){var e=l(n);return s(e)&&(e=void 0),i(a(n,1,s,!0),r(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var a=/["'&<>]/;n.exports=function(n){var e,t=""+n,r=a.exec(t);if(!r)return t;var o="",i=0,s=0;for(i=r.index;i<t.length;i++){switch(t.charCodeAt(i)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}s!==i&&(o+=t.substring(s,i)),s=i+1,o+=e}return s!==i?o+t.substring(s,i):o}},function(n,e,t){"use strict";
/**
 * @file Embedded JavaScript templating engine. {@link http://ejs.co}
 * @author Matthew Eernisse <mde@fleegix.org>
 * @author Tiancheng "Timothy" Gu <timothygu99@gmail.com>
 * @project EJS
 * @license {@link http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0}
 */var a=t(386),r=t(387),o=t(388),i=!1,s=t(389).version,l=["delimiter","scope","context","debug","compileDebug","client","_with","rmWhitespace","strict","filename","async"],c=l.concat("cache"),d=/^\uFEFF/;function p(n,t){var r,o,i=t.views,s=/^[A-Za-z]+:\\|^\//.exec(n);if(s&&s.length)r=e.resolveInclude(n.replace(/^\/*/,""),t.root||"/",!0);else if(t.filename&&(o=e.resolveInclude(n,t.filename),a.existsSync(o)&&(r=o)),r||Array.isArray(i)&&i.some((function(t){return o=e.resolveInclude(n,t,!0),a.existsSync(o)}))&&(r=o),!r)throw new Error('Could not find the include file "'+t.escapeFunction(n)+'"');return r}function u(n,t){var a,r=n.filename,o=arguments.length>1;if(n.cache){if(!r)throw new Error("cache option requires a filename");if(a=e.cache.get(r))return a;o||(t=g(r).toString().replace(d,""))}else if(!o){if(!r)throw new Error("Internal EJS error: no file name or template provided");t=g(r).toString().replace(d,"")}return a=e.compile(t,n),n.cache&&e.cache.set(r,a),a}function m(n,t,a){var r;if(!a){if("function"==typeof e.promiseImpl)return new e.promiseImpl((function(e,a){try{e(r=u(n)(t))}catch(n){a(n)}}));throw new Error("Please provide a callback function")}try{r=u(n)(t)}catch(n){return a(n)}a(null,r)}function g(n){return e.fileLoader(n)}function h(n,e,t,a,r){var o=e.split("\n"),i=Math.max(a-3,0),s=Math.min(o.length,a+3),l=r(t),c=o.slice(i,s).map((function(n,e){var t=e+i+1;return(t==a?" >> ":"    ")+t+"| "+n})).join("\n");throw n.path=l,n.message=(l||"ejs")+":"+a+"\n"+c+"\n\n"+n.message,n}function f(n){return n.replace(/;(\s*$)/,"$1")}function v(n,t){t=t||{};var a={};this.templateText=n,this.mode=null,this.truncate=!1,this.currentLine=1,this.source="",this.dependencies=[],a.client=t.client||!1,a.escapeFunction=t.escape||t.escapeFunction||o.escapeXML,a.compileDebug=!1!==t.compileDebug,a.debug=!!t.debug,a.filename=t.filename,a.openDelimiter=t.openDelimiter||e.openDelimiter||"<",a.closeDelimiter=t.closeDelimiter||e.closeDelimiter||">",a.delimiter=t.delimiter||e.delimiter||"%",a.strict=t.strict||!1,a.context=t.context,a.cache=t.cache||!1,a.rmWhitespace=t.rmWhitespace,a.root=t.root,a.outputFunctionName=t.outputFunctionName,a.localsName=t.localsName||e.localsName||"locals",a.views=t.views,a.async=t.async,a.destructuredLocals=t.destructuredLocals,a.legacyInclude=void 0===t.legacyInclude||!!t.legacyInclude,a.strict?a._with=!1:a._with=void 0===t._with||t._with,this.opts=a,this.regex=this.createRegex()}e.cache=o.cache,e.fileLoader=a.readFileSync,e.localsName="locals",e.promiseImpl=new Function("return this;")().Promise,e.resolveInclude=function(n,e,t){var a=r.dirname,o=r.extname,i=(0,r.resolve)(t?e:a(e),n);return o(n)||(i+=".ejs"),i},e.compile=function(n,e){return e&&e.scope&&(i||(console.warn("`scope` option is deprecated and will be removed in EJS 3"),i=!0),e.context||(e.context=e.scope),delete e.scope),new v(n,e).compile()},e.render=function(n,e,t){var a=e||{},r=t||{};return 2==arguments.length&&o.shallowCopyFromList(r,a,l),u(r,n)(a)},e.renderFile=function(){var n,e,t,a=Array.prototype.slice.call(arguments),r=a.shift(),i={filename:r};return"function"==typeof arguments[arguments.length-1]&&(n=a.pop()),a.length?(e=a.shift(),a.length?o.shallowCopy(i,a.pop()):(e.settings&&(e.settings.views&&(i.views=e.settings.views),e.settings["view cache"]&&(i.cache=!0),(t=e.settings["view options"])&&o.shallowCopy(i,t)),o.shallowCopyFromList(i,e,c)),i.filename=r):e={},m(i,e,n)},e.Template=v,e.clearCache=function(){e.cache.reset()},v.modes={EVAL:"eval",ESCAPED:"escaped",RAW:"raw",COMMENT:"comment",LITERAL:"literal"},v.prototype={createRegex:function(){var n="(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)",e=o.escapeRegExpChars(this.opts.delimiter),t=o.escapeRegExpChars(this.opts.openDelimiter),a=o.escapeRegExpChars(this.opts.closeDelimiter);return n=n.replace(/%/g,e).replace(/</g,t).replace(/>/g,a),new RegExp(n)},compile:function(){var n,e,t,a=this.opts,i="",s="",l=a.escapeFunction;if(!this.source){if(this.generateSource(),i+='  var __output = "";\n  function __append(s) { if (s !== undefined && s !== null) __output += s }\n',a.outputFunctionName&&(i+="  var "+a.outputFunctionName+" = __append;\n"),a.destructuredLocals&&a.destructuredLocals.length){for(var c="  var __locals = ("+a.localsName+" || {}),\n",d=0;d<a.destructuredLocals.length;d++){var m=a.destructuredLocals[d];d>0&&(c+=",\n  "),c+=m+" = __locals."+m}i+=c+";\n"}!1!==a._with&&(i+="  with ("+a.localsName+" || {}) {\n",s+="  }\n"),s+="  return __output;\n",this.source=i+this.source+s}n=a.compileDebug?"var __line = 1\n  , __lines = "+JSON.stringify(this.templateText)+"\n  , __filename = "+(a.filename?JSON.stringify(a.filename):"undefined")+";\ntry {\n"+this.source+"} catch (e) {\n  rethrow(e, __lines, __filename, __line, escapeFn);\n}\n":this.source,a.client&&(n="escapeFn = escapeFn || "+l.toString()+";\n"+n,a.compileDebug&&(n="rethrow = rethrow || "+h.toString()+";\n"+n)),a.strict&&(n='"use strict";\n'+n),a.debug&&console.log(n),a.compileDebug&&a.filename&&(n=n+"\n//# sourceURL="+a.filename+"\n");try{if(a.async)try{t=new Function("return (async function(){}).constructor;")()}catch(n){throw n instanceof SyntaxError?new Error("This environment does not support async/await"):n}else t=Function;e=new t(a.localsName+", escapeFn, include, rethrow",n)}catch(n){throw n instanceof SyntaxError&&(a.filename&&(n.message+=" in "+a.filename),n.message+=" while compiling ejs\n\n",n.message+="If the above error is not helpful, you may want to try EJS-Lint:\n",n.message+="https://github.com/RyanZim/EJS-Lint",a.async||(n.message+="\n",n.message+="Or, if you meant to create an async function, pass `async: true` as an option.")),n}var g=a.client?e:function(n){return e.apply(a.context,[n||{},l,function(e,t){var r=o.shallowCopy({},n);return t&&(r=o.shallowCopy(r,t)),function(n,e){var t=o.shallowCopy({},e);return t.filename=p(n,t),u(t)}(e,a)(r)},h])};if(g.dependencies=this.dependencies,a.filename&&"function"==typeof Object.defineProperty){var f=a.filename,v=r.basename(f,r.extname(f));try{Object.defineProperty(g,"name",{value:v,writable:!1,enumerable:!1,configurable:!0})}catch(n){}}return g},generateSource:function(){var n=this.opts;n.rmWhitespace&&(this.templateText=this.templateText.replace(/[\r\n]+/g,"\n").replace(/^\s+|\s+$/gm,"")),this.templateText=this.templateText.replace(/[ \t]*<%_/gm,"<%_").replace(/_%>[ \t]*/gm,"_%>");var t=this,a=this.parseTemplateText(),r=this.opts.delimiter,i=this.opts.openDelimiter,s=this.opts.closeDelimiter;a&&a.length&&a.forEach((function(l,c){var u,m,h,f,b,y;if(0===l.indexOf(i+r)&&0!==l.indexOf(i+r+r)&&(m=a[c+2])!=r+s&&m!="-"+r+s&&m!="_"+r+s)throw new Error('Could not find matching close tag for "'+l+'".');if(n.legacyInclude&&(h=l.match(/^\s*include\s+(\S+)/))&&(u=a[c-1])&&(u==i+r||u==i+r+"-"||u==i+r+"_"))return f=o.shallowCopy({},t.opts),b=function(n,e){var t,a,r=o.shallowCopy({},e);a=g(t=p(n,r)).toString().replace(d,""),r.filename=t;var i=new v(a,r);return i.generateSource(),{source:i.source,filename:t,template:a}}(h[1],f),y=t.opts.compileDebug?"    ; (function(){\n      var __line = 1\n      , __lines = "+JSON.stringify(b.template)+"\n      , __filename = "+JSON.stringify(b.filename)+";\n      try {\n"+b.source+"      } catch (e) {\n        rethrow(e, __lines, __filename, __line, escapeFn);\n      }\n    ; }).call(this)\n":"    ; (function(){\n"+b.source+"    ; }).call(this)\n",t.source+=y,void t.dependencies.push(e.resolveInclude(h[1],f.filename));t.scanLine(l)}))},parseTemplateText:function(){for(var n,e=this.templateText,t=this.regex,a=t.exec(e),r=[];a;)0!==(n=a.index)&&(r.push(e.substring(0,n)),e=e.slice(n)),r.push(a[0]),e=e.slice(a[0].length),a=t.exec(e);return e&&r.push(e),r},_addOutput:function(n){if(this.truncate&&(n=n.replace(/^(?:\r\n|\r|\n)/,""),this.truncate=!1),!n)return n;n=(n=(n=(n=n.replace(/\\/g,"\\\\")).replace(/\n/g,"\\n")).replace(/\r/g,"\\r")).replace(/"/g,'\\"'),this.source+='    ; __append("'+n+'")\n'},scanLine:function(n){var e,t=this.opts.delimiter,a=this.opts.openDelimiter,r=this.opts.closeDelimiter;switch(e=n.split("\n").length-1,n){case a+t:case a+t+"_":this.mode=v.modes.EVAL;break;case a+t+"=":this.mode=v.modes.ESCAPED;break;case a+t+"-":this.mode=v.modes.RAW;break;case a+t+"#":this.mode=v.modes.COMMENT;break;case a+t+t:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+n.replace(a+t+t,a+t)+'")\n';break;case t+t+r:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+n.replace(t+t+r,t+r)+'")\n';break;case t+r:case"-"+t+r:case"_"+t+r:this.mode==v.modes.LITERAL&&this._addOutput(n),this.mode=null,this.truncate=0===n.indexOf("-")||0===n.indexOf("_");break;default:if(this.mode){switch(this.mode){case v.modes.EVAL:case v.modes.ESCAPED:case v.modes.RAW:n.lastIndexOf("//")>n.lastIndexOf("\n")&&(n+="\n")}switch(this.mode){case v.modes.EVAL:this.source+="    ; "+n+"\n";break;case v.modes.ESCAPED:this.source+="    ; __append(escapeFn("+f(n)+"))\n";break;case v.modes.RAW:this.source+="    ; __append("+f(n)+")\n";break;case v.modes.COMMENT:break;case v.modes.LITERAL:this._addOutput(n)}}else this._addOutput(n)}this.opts.compileDebug&&e&&(this.currentLine+=e,this.source+="    ; __line = "+this.currentLine+"\n")}},e.escapeXML=o.escapeXML,e.__express=e.renderFile,e.VERSION=s,e.name="ejs","undefined"!=typeof window&&(window.ejs=e)},function(n,e,t){"use strict";t.r(e);var a={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},r=(t(376),t(15)),o=Object(r.a)(a,(function(){var n=this.$createElement;return(this._self._c||n)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"5a3e6e86",null);e.default=o.exports},function(n,e,t){"use strict";t.r(e);t(22),t(4),t(23),t(46),t(25);var a={name:"CodeGroup",data:function(){return{codeTabs:[],activeCodeTabIndex:-1}},watch:{activeCodeTabIndex:function(n){this.codeTabs.forEach((function(n){n.elm.classList.remove("theme-code-block__active")})),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted:function(){var n=this;this.codeTabs=(this.$slots.default||[]).filter((function(n){return Boolean(n.componentOptions)})).map((function(e,t){return""===e.componentOptions.propsData.active&&(n.activeCodeTabIndex=t),{title:e.componentOptions.propsData.title,elm:e.elm}})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab:function(n){this.activeCodeTabIndex=n}}},r=(t(377),t(15)),o=Object(r.a)(a,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"theme-code-group"},[t("div",{staticClass:"theme-code-group__nav"},[t("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(e,a){return t("li",{key:e.title,staticClass:"theme-code-group__li"},[t("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":a===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(a)}}},[n._v("\n            "+n._s(e.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?t("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"392329f0",null);e.default=o.exports},function(n,e,t){"use strict";var a=t(7),r=t(0),o=t(2),i=t(107),s=t(14),l=t(10),c=t(156),d=t(35),p=t(84),u=t(163),m=t(3),g=t(58).f,h=t(38).f,f=t(13).f,v=t(371),b=t(145).trim,y=r.Number,x=y.prototype,k=r.TypeError,w=o("".slice),S=o("".charCodeAt),E=function(n){var e=u(n,"number");return"bigint"==typeof e?e:T(e)},T=function(n){var e,t,a,r,o,i,s,l,c=u(n,"number");if(p(c))throw k("Cannot convert a Symbol value to a number");if("string"==typeof c&&c.length>2)if(c=b(c),43===(e=S(c,0))||45===e){if(88===(t=S(c,2))||120===t)return NaN}else if(48===e){switch(S(c,1)){case 66:case 98:a=2,r=49;break;case 79:case 111:a=8,r=55;break;default:return+c}for(i=(o=w(c,2)).length,s=0;s<i;s++)if((l=S(o,s))<48||l>r)return NaN;return parseInt(o,a)}return+c};if(i("Number",!y(" 0o1")||!y("0b1")||y("+0x1"))){for(var _,I=function(n){var e=arguments.length<1?0:y(E(n)),t=this;return d(x,t)&&m((function(){v(t)}))?c(Object(e),t,I):e},j=a?g(y):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,isFinite,isInteger,isNaN,isSafeInteger,parseFloat,parseInt,fromString,range".split(","),z=0;j.length>z;z++)l(y,_=j[z])&&!l(I,_)&&f(I,_,h(y,_));I.prototype=x,x.constructor=I,s(r,"Number",I)}},function(n,e,t){var a=t(3),r=t(0).RegExp;n.exports=a((function(){var n=r(".","s");return!(n.dotAll&&n.exec("\n")&&"s"===n.flags)}))},function(n,e,t){"use strict";var a=t(167).IteratorPrototype,r=t(34),o=t(49),i=t(59),s=t(66),l=function(){return this};n.exports=function(n,e,t,c){var d=e+" Iterator";return n.prototype=r(a,{next:o(+!c,t)}),i(n,d,!1,!0),s[d]=l,n}},function(n,e,t){var a=t(14);n.exports=function(n,e,t){for(var r in e)a(n,r,e[r],t);return n}},function(n,e,t){"use strict";var a=t(18),r=t(13),o=t(6),i=t(7),s=o("species");n.exports=function(n){var e=a(n),t=r.f;i&&e&&!e[s]&&t(e,s,{configurable:!0,get:function(){return this}})}},function(n,e,t){var a=t(0),r=t(35),o=a.TypeError;n.exports=function(n,e){if(r(e,n))return n;throw o("Incorrect invocation")}},function(n,e,t){var a=t(3),r=t(0).RegExp;n.exports=a((function(){var n=r("(?<a>b)","g");return"b"!==n.exec("b").groups.a||"bc"!=="b".replace(n,"$<a>c")}))},function(n,e,t){"use strict";var a=t(1),r=t(117).includes,o=t(144);a({target:"Array",proto:!0},{includes:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}}),o("includes")},function(n,e,t){"use strict";var a=t(1),r=t(2),o=t(184),i=t(19),s=t(12),l=t(185),c=r("".indexOf);a({target:"String",proto:!0,forced:!l("includes")},{includes:function(n){return!!~c(s(i(this)),s(o(n)),arguments.length>1?arguments[1]:void 0)}})},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){"use strict";var a=t(1),r=t(145).trim;a({target:"String",proto:!0,forced:t(369)("trim")},{trim:function(){return r(this)}})},function(n,e,t){"use strict";var a=t(1),r=t(2),o=t(37),i=t(17),s=t(24),l=t(12),c=t(3),d=t(238),p=t(47),u=t(378),m=t(379),g=t(52),h=t(380),f=[],v=r(f.sort),b=r(f.push),y=c((function(){f.sort(void 0)})),x=c((function(){f.sort(null)})),k=p("sort"),w=!c((function(){if(g)return g<70;if(!(u&&u>3)){if(m)return!0;if(h)return h<603;var n,e,t,a,r="";for(n=65;n<76;n++){switch(e=String.fromCharCode(n),n){case 66:case 69:case 70:case 72:t=3;break;case 68:case 71:t=4;break;default:t=2}for(a=0;a<47;a++)f.push({k:e+a,v:t})}for(f.sort((function(n,e){return e.v-n.v})),a=0;a<f.length;a++)e=f[a].k.charAt(0),r.charAt(r.length-1)!==e&&(r+=e);return"DGBEFHACIJK"!==r}}));a({target:"Array",proto:!0,forced:y||!x||!k||!w},{sort:function(n){void 0!==n&&o(n);var e=i(this);if(w)return void 0===n?v(e):v(e,n);var t,a,r=[],c=s(e);for(a=0;a<c;a++)a in e&&b(r,e[a]);for(d(r,function(n){return function(e,t){return void 0===t?-1:void 0===e?1:void 0!==n?+n(e,t)||0:l(e)>l(t)?1:-1}}(n)),t=r.length,a=0;a<t;)e[a]=r[a++];for(;a<c;)delete e[a++];return e}})},function(n,e,t){var a=t(126),r=Math.floor,o=function(n,e){var t=n.length,l=r(t/2);return t<8?i(n,e):s(n,o(a(n,0,l),e),o(a(n,l),e),e)},i=function(n,e){for(var t,a,r=n.length,o=1;o<r;){for(a=o,t=n[o];a&&e(n[a-1],t)>0;)n[a]=n[--a];a!==o++&&(n[a]=t)}return n},s=function(n,e,t,a){for(var r=e.length,o=t.length,i=0,s=0;i<r||s<o;)n[i+s]=i<r&&s<o?a(e[i],t[s])<=0?e[i++]:t[s++]:i<r?e[i++]:t[s++];return n};n.exports=o},function(n,e,t){var a=t(0),r=t(7),o=t(109).MISSED_STICKY,i=t(28),s=t(13).f,l=t(40).get,c=RegExp.prototype,d=a.TypeError;r&&o&&s(c,"sticky",{configurable:!0,get:function(){if(this!==c){if("RegExp"===i(this))return!!l(this).sticky;throw d("Incompatible receiver, RegExp required")}}})},function(n,e,t){n.exports=t(392)},function(n,e,t){var a=t(0),r=t(11),o=t(5),i=t(9),s=a.TypeError;n.exports=function(n,e){var t,a;if("string"===e&&o(t=n.toString)&&!i(a=r(t,n)))return a;if(o(t=n.valueOf)&&!i(a=r(t,n)))return a;if("string"!==e&&o(t=n.toString)&&!i(a=r(t,n)))return a;throw s("Can't convert object to primitive value")}},function(n,e,t){var a=t(0),r=t(5),o=t(88),i=a.WeakMap;n.exports=r(i)&&/native code/.test(o(i))},function(n,e,t){var a=t(0),r=t(5),o=a.String,i=a.TypeError;n.exports=function(n){if("object"==typeof n||r(n))return n;throw i("Can't set "+o(n)+" as a prototype")}},function(n,e,t){"use strict";var a,r,o,i,s=t(1),l=t(29),c=t(0),d=t(18),p=t(11),u=t(169),m=t(14),g=t(229),h=t(68),f=t(59),v=t(230),b=t(37),y=t(5),x=t(9),k=t(231),w=t(88),S=t(245),E=t(172),T=t(125),_=t(174).set,I=t(246),j=t(176),z=t(249),C=t(177),A=t(250),P=t(251),D=t(40),q=t(107),L=t(6),B=t(252),O=t(90),F=t(52),R=L("species"),M="Promise",H=D.getterFor(M),N=D.set,U=D.getterFor(M),J=u&&u.prototype,$=u,Z=J,V=c.TypeError,K=c.document,Q=c.process,W=C.f,G=W,X=!!(K&&K.createEvent&&c.dispatchEvent),Y=y(c.PromiseRejectionEvent),nn=!1,en=q(M,(function(){var n=w($),e=n!==String($);if(!e&&66===F)return!0;if(l&&!Z.finally)return!0;if(F>=51&&/native code/.test(n))return!1;var t=new $((function(n){n(1)})),a=function(n){n((function(){}),(function(){}))};return(t.constructor={})[R]=a,!(nn=t.then((function(){}))instanceof a)||!e&&B&&!Y})),tn=en||!E((function(n){$.all(n).catch((function(){}))})),an=function(n){var e;return!(!x(n)||!y(e=n.then))&&e},rn=function(n,e){var t,a,r,o=e.value,i=1==e.state,s=i?n.ok:n.fail,l=n.resolve,c=n.reject,d=n.domain;try{s?(i||(2===e.rejection&&dn(e),e.rejection=1),!0===s?t=o:(d&&d.enter(),t=s(o),d&&(d.exit(),r=!0)),t===n.promise?c(V("Promise-chain cycle")):(a=an(t))?p(a,t,l,c):l(t)):c(o)}catch(n){d&&!r&&d.exit(),c(n)}},on=function(n,e){n.notified||(n.notified=!0,I((function(){for(var t,a=n.reactions;t=a.get();)rn(t,n);n.notified=!1,e&&!n.rejection&&ln(n)})))},sn=function(n,e,t){var a,r;X?((a=K.createEvent("Event")).promise=e,a.reason=t,a.initEvent(n,!1,!0),c.dispatchEvent(a)):a={promise:e,reason:t},!Y&&(r=c["on"+n])?r(a):"unhandledrejection"===n&&z("Unhandled promise rejection",t)},ln=function(n){p(_,c,(function(){var e,t=n.facade,a=n.value;if(cn(n)&&(e=A((function(){O?Q.emit("unhandledRejection",a,t):sn("unhandledrejection",t,a)})),n.rejection=O||cn(n)?2:1,e.error))throw e.value}))},cn=function(n){return 1!==n.rejection&&!n.parent},dn=function(n){p(_,c,(function(){var e=n.facade;O?Q.emit("rejectionHandled",e):sn("rejectionhandled",e,n.value)}))},pn=function(n,e,t){return function(a){n(e,a,t)}},un=function(n,e,t){n.done||(n.done=!0,t&&(n=t),n.value=e,n.state=2,on(n,!0))},mn=function(n,e,t){if(!n.done){n.done=!0,t&&(n=t);try{if(n.facade===e)throw V("Promise can't be resolved itself");var a=an(e);a?I((function(){var t={done:!1};try{p(a,e,pn(mn,t,n),pn(un,t,n))}catch(e){un(t,e,n)}})):(n.value=e,n.state=1,on(n,!1))}catch(e){un({done:!1},e,n)}}};if(en&&(Z=($=function(n){k(this,Z),b(n),p(a,this);var e=H(this);try{n(pn(mn,e),pn(un,e))}catch(n){un(e,n)}}).prototype,(a=function(n){N(this,{type:M,done:!1,notified:!1,parent:!1,reactions:new P,rejection:!1,state:0,value:void 0})}).prototype=g(Z,{then:function(n,e){var t=U(this),a=W(T(this,$));return t.parent=!0,a.ok=!y(n)||n,a.fail=y(e)&&e,a.domain=O?Q.domain:void 0,0==t.state?t.reactions.add(a):I((function(){rn(a,t)})),a.promise},catch:function(n){return this.then(void 0,n)}}),r=function(){var n=new a,e=H(n);this.promise=n,this.resolve=pn(mn,e),this.reject=pn(un,e)},C.f=W=function(n){return n===$||n===o?new r(n):G(n)},!l&&y(u)&&J!==Object.prototype)){i=J.then,nn||(m(J,"then",(function(n,e){var t=this;return new $((function(n,e){p(i,t,n,e)})).then(n,e)}),{unsafe:!0}),m(J,"catch",Z.catch,{unsafe:!0}));try{delete J.constructor}catch(n){}h&&h(J,Z)}s({global:!0,wrap:!0,forced:en},{Promise:$}),f($,M,!1,!0),v(M),o=d(M),s({target:M,stat:!0,forced:en},{reject:function(n){var e=W(this);return p(e.reject,void 0,n),e.promise}}),s({target:M,stat:!0,forced:l||en},{resolve:function(n){return j(l&&this===o?$:this,n)}}),s({target:M,stat:!0,forced:tn},{all:function(n){var e=this,t=W(e),a=t.resolve,r=t.reject,o=A((function(){var t=b(e.resolve),o=[],i=0,s=1;S(n,(function(n){var l=i++,c=!1;s++,p(t,e,n).then((function(n){c||(c=!0,o[l]=n,--s||a(o))}),r)})),--s||a(o)}));return o.error&&r(o.value),t.promise},race:function(n){var e=this,t=W(e),a=t.reject,r=A((function(){var r=b(e.resolve);S(n,(function(n){p(r,e,n).then(t.resolve,a)}))}));return r.error&&a(r.value),t.promise}})},function(n,e,t){var a=t(0),r=t(56),o=t(11),i=t(8),s=t(85),l=t(170),c=t(24),d=t(35),p=t(150),u=t(108),m=t(171),g=a.TypeError,h=function(n,e){this.stopped=n,this.result=e},f=h.prototype;n.exports=function(n,e,t){var a,v,b,y,x,k,w,S=t&&t.that,E=!(!t||!t.AS_ENTRIES),T=!(!t||!t.IS_ITERATOR),_=!(!t||!t.INTERRUPTED),I=r(e,S),j=function(n){return a&&m(a,"normal",n),new h(!0,n)},z=function(n){return E?(i(n),_?I(n[0],n[1],j):I(n[0],n[1])):_?I(n,j):I(n)};if(T)a=n;else{if(!(v=u(n)))throw g(s(n)+" is not iterable");if(l(v)){for(b=0,y=c(n);y>b;b++)if((x=z(n[b]))&&d(f,x))return x;return new h(!1)}a=p(n,v)}for(k=a.next;!(w=o(k,a)).done;){try{x=z(w.value)}catch(n){m(a,"throw",n)}if("object"==typeof x&&x&&d(f,x))return x}return new h(!1)}},function(n,e,t){var a,r,o,i,s,l,c,d,p=t(0),u=t(56),m=t(38).f,g=t(174).set,h=t(175),f=t(247),v=t(248),b=t(90),y=p.MutationObserver||p.WebKitMutationObserver,x=p.document,k=p.process,w=p.Promise,S=m(p,"queueMicrotask"),E=S&&S.value;E||(a=function(){var n,e;for(b&&(n=k.domain)&&n.exit();r;){e=r.fn,r=r.next;try{e()}catch(n){throw r?i():o=void 0,n}}o=void 0,n&&n.enter()},h||b||v||!y||!x?!f&&w&&w.resolve?((c=w.resolve(void 0)).constructor=w,d=u(c.then,c),i=function(){d(a)}):b?i=function(){k.nextTick(a)}:(g=u(g,p),i=function(){g(a)}):(s=!0,l=x.createTextNode(""),new y(a).observe(l,{characterData:!0}),i=function(){l.data=s=!s})),n.exports=E||function(n){var e={fn:n,next:void 0};o&&(o.next=e),r||(r=e,i()),o=e}},function(n,e,t){var a=t(32),r=t(0);n.exports=/ipad|iphone|ipod/i.test(a)&&void 0!==r.Pebble},function(n,e,t){var a=t(32);n.exports=/web0s(?!.*chrome)/i.test(a)},function(n,e,t){var a=t(0);n.exports=function(n,e){var t=a.console;t&&t.error&&(1==arguments.length?t.error(n):t.error(n,e))}},function(n,e){n.exports=function(n){try{return{error:!1,value:n()}}catch(n){return{error:!0,value:n}}}},function(n,e){var t=function(){this.head=null,this.tail=null};t.prototype={add:function(n){var e={item:n,next:null};this.head?this.tail.next=e:this.head=e,this.tail=e},get:function(){var n=this.head;if(n)return this.head=n.next,this.tail===n&&(this.tail=null),n.item}},n.exports=t},function(n,e){n.exports="object"==typeof window},function(n,e,t){var a=t(1),r=t(254);a({target:"Object",stat:!0,forced:Object.assign!==r},{assign:r})},function(n,e,t){"use strict";var a=t(7),r=t(2),o=t(11),i=t(3),s=t(86),l=t(122),c=t(119),d=t(17),p=t(63),u=Object.assign,m=Object.defineProperty,g=r([].concat);n.exports=!u||i((function(){if(a&&1!==u({b:1},u(m({},"a",{enumerable:!0,get:function(){m(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var n={},e={},t=Symbol();return n[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(n){e[n]=n})),7!=u({},n)[t]||"abcdefghijklmnopqrst"!=s(u({},e)).join("")}))?function(n,e){for(var t=d(n),r=arguments.length,i=1,u=l.f,m=c.f;r>i;)for(var h,f=p(arguments[i++]),v=u?g(s(f),u(f)):s(f),b=v.length,y=0;b>y;)h=v[y++],a&&!o(m,f,h)||(t[h]=f[h]);return t}:u},function(n,e,t){"use strict";var a=t(1),r=t(29),o=t(169),i=t(3),s=t(18),l=t(5),c=t(125),d=t(176),p=t(14);if(a({target:"Promise",proto:!0,real:!0,forced:!!o&&i((function(){o.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(n){var e=c(this,s("Promise")),t=l(n);return this.then(t?function(t){return d(e,n()).then((function(){return t}))}:n,t?function(t){return d(e,n()).then((function(){throw t}))}:n)}}),!r&&l(o)){var u=s("Promise").prototype.finally;o.prototype.finally!==u&&p(o.prototype,"finally",u,{unsafe:!0})}},function(n,e,t){"use strict";var a=t(124),r=t(78);n.exports=a?{}.toString:function(){return"[object "+r(this)+"]"}},function(n,e,t){var a=t(0),r=t(60),o=t(89),i=t(9),s=t(6)("species"),l=a.Array;n.exports=function(n){var e;return r(n)&&(e=n.constructor,(o(e)&&(e===l||r(e.prototype))||i(e)&&null===(e=e[s]))&&(e=void 0)),void 0===e?l:e}},function(n,e,t){"use strict";var a=t(1),r=t(259).left,o=t(47),i=t(52),s=t(90);a({target:"Array",proto:!0,forced:!o("reduce")||!s&&i>79&&i<83},{reduce:function(n){var e=arguments.length;return r(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(0),r=t(37),o=t(17),i=t(63),s=t(24),l=a.TypeError,c=function(n){return function(e,t,a,c){r(t);var d=o(e),p=i(d),u=s(d),m=n?u-1:0,g=n?-1:1;if(a<2)for(;;){if(m in p){c=p[m],m+=g;break}if(m+=g,n?m<0:u<=m)throw l("Reduce of empty array with no initial value")}for(;n?m>=0:u>m;m+=g)m in p&&(c=t(c,p[m],m,d));return c}};n.exports={left:c(!1),right:c(!0)}},function(n,e,t){var a=t(1),r=t(187),o=t(3),i=t(9),s=t(261).onFreeze,l=Object.freeze;a({target:"Object",stat:!0,forced:o((function(){l(1)})),sham:!r},{freeze:function(n){return l&&i(n)?l(s(n)):n}})},function(n,e,t){var a=t(1),r=t(2),o=t(65),i=t(9),s=t(10),l=t(13).f,c=t(58),d=t(188),p=t(262),u=t(81),m=t(187),g=!1,h=u("meta"),f=0,v=function(n){l(n,h,{value:{objectID:"O"+f++,weakData:{}}})},b=n.exports={enable:function(){b.enable=function(){},g=!0;var n=c.f,e=r([].splice),t={};t[h]=1,n(t).length&&(c.f=function(t){for(var a=n(t),r=0,o=a.length;r<o;r++)if(a[r]===h){e(a,r,1);break}return a},a({target:"Object",stat:!0,forced:!0},{getOwnPropertyNames:d.f}))},fastKey:function(n,e){if(!i(n))return"symbol"==typeof n?n:("string"==typeof n?"S":"P")+n;if(!s(n,h)){if(!p(n))return"F";if(!e)return"E";v(n)}return n[h].objectID},getWeakData:function(n,e){if(!s(n,h)){if(!p(n))return!0;if(!e)return!1;v(n)}return n[h].weakData},onFreeze:function(n){return m&&g&&p(n)&&!s(n,h)&&v(n),n}};o[h]=!0},function(n,e,t){var a=t(3),r=t(9),o=t(28),i=t(263),s=Object.isExtensible,l=a((function(){s(1)}));n.exports=l||i?function(n){return!!r(n)&&((!i||"ArrayBuffer"!=o(n))&&(!s||s(n)))}:s},function(n,e,t){var a=t(3);n.exports=a((function(){if("function"==typeof ArrayBuffer){var n=new ArrayBuffer(8);Object.isExtensible(n)&&Object.defineProperty(n,"a",{value:8})}}))},function(n,e,t){var a=t(0);n.exports=a},function(n,e,t){"use strict";var a=t(0),r=t(56),o=t(11),i=t(17),s=t(266),l=t(170),c=t(89),d=t(24),p=t(70),u=t(150),m=t(108),g=a.Array;n.exports=function(n){var e=i(n),t=c(this),a=arguments.length,h=a>1?arguments[1]:void 0,f=void 0!==h;f&&(h=r(h,a>2?arguments[2]:void 0));var v,b,y,x,k,w,S=m(e),E=0;if(!S||this==g&&l(S))for(v=d(e),b=t?new this(v):g(v);v>E;E++)w=f?h(e[E],E):e[E],p(b,E,w);else for(k=(x=u(e,S)).next,b=t?new this:[];!(y=o(k,x)).done;E++)w=f?s(x,h,[y.value,E],!0):y.value,p(b,E,w);return b.length=E,b}},function(n,e,t){var a=t(8),r=t(171);n.exports=function(n,e,t,o){try{return o?e(a(t)[0],t[1]):e(t)}catch(e){r(n,"throw",e)}}},function(n,e,t){"use strict";var a=t(18),r=t(10),o=t(30),i=t(35),s=t(68),l=t(120),c=t(156),d=t(191),p=t(268),u=t(269),m=t(270),g=t(29);n.exports=function(n,e,t,h){var f=h?2:1,v=n.split("."),b=v[v.length-1],y=a.apply(null,v);if(y){var x=y.prototype;if(!g&&r(x,"cause")&&delete x.cause,!t)return y;var k=a("Error"),w=e((function(n,e){var t=d(h?e:n,void 0),a=h?new y(n):new y;return void 0!==t&&o(a,"message",t),m&&o(a,"stack",u(a.stack,2)),this&&i(x,this)&&c(a,this,w),arguments.length>f&&p(a,arguments[f]),a}));if(w.prototype=x,"Error"!==b&&(s?s(w,k):l(w,k,{name:!0})),l(w,y),!g)try{x.name!==b&&o(x,"name",b),x.constructor=w}catch(n){}return w}}},function(n,e,t){var a=t(9),r=t(30);n.exports=function(n,e){a(e)&&"cause"in e&&r(n,"cause",e.cause)}},function(n,e,t){var a=t(2)("".replace),r=String(Error("zxcasd").stack),o=/\n\s*at [^:]*:[^\n]*/,i=o.test(r);n.exports=function(n,e){if(i&&"string"==typeof n)for(;e--;)n=a(n,o,"");return n}},function(n,e,t){var a=t(3),r=t(49);n.exports=!a((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",r(1,7)),7!==n.stack)}))},function(n,e,t){"use strict";var a=t(7),r=t(3),o=t(8),i=t(34),s=t(191),l=Error.prototype.toString,c=r((function(){if(a){var n=i(Object.defineProperty({},"name",{get:function(){return this===n}}));if("true"!==l.call(n))return!0}return"2: 1"!==l.call({message:1,name:2})||"Error"!==l.call({})}));n.exports=c?function(){var n=o(this),e=s(n.name,"Error"),t=s(n.message);return e?t?e+": "+t:e:t}:l},function(n,e,t){var a=t(2),r=t(17),o=Math.floor,i=a("".charAt),s=a("".replace),l=a("".slice),c=/\$([$&'`]|\d{1,2}|<[^>]*>)/g,d=/\$([$&'`]|\d{1,2})/g;n.exports=function(n,e,t,a,p,u){var m=t+n.length,g=a.length,h=d;return void 0!==p&&(p=r(p),h=c),s(u,h,(function(r,s){var c;switch(i(s,0)){case"$":return"$";case"&":return n;case"`":return l(e,0,t);case"'":return l(e,m);case"<":c=p[l(s,1,-1)];break;default:var d=+s;if(0===d)return r;if(d>g){var u=o(d/10);return 0===u?r:u<=g?void 0===a[u-1]?i(s,1):a[u-1]+i(s,1):r}c=a[d-1]}return void 0===c?"":c}))}},function(n,e,t){var a=t(193),r=t(274);n.exports=function n(e,t,o,i,s){var l=-1,c=e.length;for(o||(o=r),s||(s=[]);++l<c;){var d=e[l];t>0&&o(d)?t>1?n(d,t-1,o,i,s):a(s,d):i||(s[s.length]=d)}return s}},function(n,e,t){var a=t(72),r=t(130),o=t(31),i=a?a.isConcatSpreadable:void 0;n.exports=function(n){return o(n)||r(n)||!!(i&&n&&n[i])}},function(n,e,t){var a=t(62),r=t(50);n.exports=function(n){return r(n)&&"[object Arguments]"==a(n)}},function(n,e,t){var a=t(72),r=Object.prototype,o=r.hasOwnProperty,i=r.toString,s=a?a.toStringTag:void 0;n.exports=function(n){var e=o.call(n,s),t=n[s];try{n[s]=void 0;var a=!0}catch(n){}var r=i.call(n);return a&&(e?n[s]=t:delete n[s]),r}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var a=t(279),r=t(335),o=t(138),i=t(31),s=t(346);n.exports=function(n){return"function"==typeof n?n:null==n?o:"object"==typeof n?i(n)?r(n[0],n[1]):a(n):s(n)}},function(n,e,t){var a=t(280),r=t(334),o=t(210);n.exports=function(n){var e=r(n);return 1==e.length&&e[0][2]?o(e[0][0],e[0][1]):function(t){return t===n||a(t,n,e)}}},function(n,e,t){var a=t(195),r=t(199);n.exports=function(n,e,t,o){var i=t.length,s=i,l=!o;if(null==n)return!s;for(n=Object(n);i--;){var c=t[i];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++i<s;){var d=(c=t[i])[0],p=n[d],u=c[1];if(l&&c[2]){if(void 0===p&&!(d in n))return!1}else{var m=new a;if(o)var g=o(p,u,d,n,e,m);if(!(void 0===g?r(u,p,3,o,m):g))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var a=t(96),r=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=a(e,n);return!(t<0)&&(t==e.length-1?e.pop():r.call(e,t,1),--this.size,!0)}},function(n,e,t){var a=t(96);n.exports=function(n){var e=this.__data__,t=a(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var a=t(96);n.exports=function(n){return a(this.__data__,n)>-1}},function(n,e,t){var a=t(96);n.exports=function(n,e){var t=this.__data__,r=a(t,n);return r<0?(++this.size,t.push([n,e])):t[r][1]=e,this}},function(n,e,t){var a=t(95);n.exports=function(){this.__data__=new a,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var a=t(95),r=t(131),o=t(133);n.exports=function(n,e){var t=this.__data__;if(t instanceof a){var i=t.__data__;if(!r||i.length<199)return i.push([n,e]),this.size=++t.size,this;t=this.__data__=new o(i)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var a=t(197),r=t(292),o=t(132),i=t(198),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,d=l.toString,p=c.hasOwnProperty,u=RegExp("^"+d.call(p).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!o(n)||r(n))&&(a(n)?u:s).test(i(n))}},function(n,e,t){var a,r=t(293),o=(a=/[^.]+$/.exec(r&&r.keys&&r.keys.IE_PROTO||""))?"Symbol(src)_1."+a:"";n.exports=function(n){return!!o&&o in n}},function(n,e,t){var a=t(33)["__core-js_shared__"];n.exports=a},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var a=t(296),r=t(95),o=t(131);n.exports=function(){this.size=0,this.__data__={hash:new a,map:new(o||r),string:new a}}},function(n,e,t){var a=t(297),r=t(298),o=t(299),i=t(300),s=t(301);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=r,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var a=t(97);n.exports=function(){this.__data__=a?a(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var a=t(97),r=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(a){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return r.call(e,n)?e[n]:void 0}},function(n,e,t){var a=t(97),r=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return a?void 0!==e[n]:r.call(e,n)}},function(n,e,t){var a=t(97);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=a&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var a=t(98);n.exports=function(n){var e=a(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var a=t(98);n.exports=function(n){return a(this,n).get(n)}},function(n,e,t){var a=t(98);n.exports=function(n){return a(this,n).has(n)}},function(n,e,t){var a=t(98);n.exports=function(n,e){var t=a(this,n),r=t.size;return t.set(n,e),this.size+=t.size==r?0:1,this}},function(n,e,t){var a=t(195),r=t(200),o=t(311),i=t(314),s=t(330),l=t(31),c=t(204),d=t(206),p="[object Object]",u=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,g,h){var f=l(n),v=l(e),b=f?"[object Array]":s(n),y=v?"[object Array]":s(e),x=(b="[object Arguments]"==b?p:b)==p,k=(y="[object Arguments]"==y?p:y)==p,w=b==y;if(w&&c(n)){if(!c(e))return!1;f=!0,x=!1}if(w&&!x)return h||(h=new a),f||d(n)?r(n,e,t,m,g,h):o(n,e,b,t,m,g,h);if(!(1&t)){var S=x&&u.call(n,"__wrapped__"),E=k&&u.call(e,"__wrapped__");if(S||E){var T=S?n.value():n,_=E?e.value():e;return h||(h=new a),g(T,_,t,m,h)}}return!!w&&(h||(h=new a),i(n,e,t,m,g,h))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length;++t<a;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var a=t(72),r=t(312),o=t(196),i=t(200),s=t(313),l=t(134),c=a?a.prototype:void 0,d=c?c.valueOf:void 0;n.exports=function(n,e,t,a,c,p,u){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!p(new r(n),new r(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return o(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=s;case"[object Set]":var g=1&a;if(m||(m=l),n.size!=e.size&&!g)return!1;var h=u.get(n);if(h)return h==e;a|=2,u.set(n,e);var f=i(m(n),m(e),a,c,p,u);return u.delete(n),f;case"[object Symbol]":if(d)return d.call(n)==d.call(e)}return!1}},function(n,e,t){var a=t(33).Uint8Array;n.exports=a},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,a){t[++e]=[a,n]})),t}},function(n,e,t){var a=t(315),r=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,o,i,s){var l=1&t,c=a(n),d=c.length;if(d!=a(e).length&&!l)return!1;for(var p=d;p--;){var u=c[p];if(!(l?u in e:r.call(e,u)))return!1}var m=s.get(n),g=s.get(e);if(m&&g)return m==e&&g==n;var h=!0;s.set(n,e),s.set(e,n);for(var f=l;++p<d;){var v=n[u=c[p]],b=e[u];if(o)var y=l?o(b,v,u,e,n,s):o(v,b,u,n,e,s);if(!(void 0===y?v===b||i(v,b,t,o,s):y)){h=!1;break}f||(f="constructor"==u)}if(h&&!f){var x=n.constructor,k=e.constructor;x==k||!("constructor"in n)||!("constructor"in e)||"function"==typeof x&&x instanceof x&&"function"==typeof k&&k instanceof k||(h=!1)}return s.delete(n),s.delete(e),h}},function(n,e,t){var a=t(316),r=t(317),o=t(203);n.exports=function(n){return a(n,o,r)}},function(n,e,t){var a=t(193),r=t(31);n.exports=function(n,e,t){var o=e(n);return r(n)?o:a(o,t(n))}},function(n,e,t){var a=t(318),r=t(319),o=Object.prototype.propertyIsEnumerable,i=Object.getOwnPropertySymbols,s=i?function(n){return null==n?[]:(n=Object(n),a(i(n),(function(e){return o.call(n,e)})))}:r;n.exports=s},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length,r=0,o=[];++t<a;){var i=n[t];e(i,t,n)&&(o[r++]=i)}return o}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var a=t(321),r=t(130),o=t(31),i=t(204),s=t(205),l=t(206),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=o(n),d=!t&&r(n),p=!t&&!d&&i(n),u=!t&&!d&&!p&&l(n),m=t||d||p||u,g=m?a(n.length,String):[],h=g.length;for(var f in n)!e&&!c.call(n,f)||m&&("length"==f||p&&("offset"==f||"parent"==f)||u&&("buffer"==f||"byteLength"==f||"byteOffset"==f)||s(f,h))||g.push(f);return g}},function(n,e){n.exports=function(n,e){for(var t=-1,a=Array(n);++t<n;)a[t]=e(t);return a}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var a=t(62),r=t(135),o=t(50),i={};i["[object Float32Array]"]=i["[object Float64Array]"]=i["[object Int8Array]"]=i["[object Int16Array]"]=i["[object Int32Array]"]=i["[object Uint8Array]"]=i["[object Uint8ClampedArray]"]=i["[object Uint16Array]"]=i["[object Uint32Array]"]=!0,i["[object Arguments]"]=i["[object Array]"]=i["[object ArrayBuffer]"]=i["[object Boolean]"]=i["[object DataView]"]=i["[object Date]"]=i["[object Error]"]=i["[object Function]"]=i["[object Map]"]=i["[object Number]"]=i["[object Object]"]=i["[object RegExp]"]=i["[object Set]"]=i["[object String]"]=i["[object WeakMap]"]=!1,n.exports=function(n){return o(n)&&r(n.length)&&!!i[a(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var a=t(194),r=e&&!e.nodeType&&e,o=r&&"object"==typeof n&&n&&!n.nodeType&&n,i=o&&o.exports===r&&a.process,s=function(){try{var n=o&&o.require&&o.require("util").types;return n||i&&i.binding&&i.binding("util")}catch(n){}}();n.exports=s}).call(this,t(157)(n))},function(n,e,t){var a=t(327),r=t(328),o=Object.prototype.hasOwnProperty;n.exports=function(n){if(!a(n))return r(n);var e=[];for(var t in Object(n))o.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var a=t(329)(Object.keys,Object);n.exports=a},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var a=t(331),r=t(131),o=t(332),i=t(208),s=t(333),l=t(62),c=t(198),d=c(a),p=c(r),u=c(o),m=c(i),g=c(s),h=l;(a&&"[object DataView]"!=h(new a(new ArrayBuffer(1)))||r&&"[object Map]"!=h(new r)||o&&"[object Promise]"!=h(o.resolve())||i&&"[object Set]"!=h(new i)||s&&"[object WeakMap]"!=h(new s))&&(h=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,a=t?c(t):"";if(a)switch(a){case d:return"[object DataView]";case p:return"[object Map]";case u:return"[object Promise]";case m:return"[object Set]";case g:return"[object WeakMap]"}return e}),n.exports=h},function(n,e,t){var a=t(42)(t(33),"DataView");n.exports=a},function(n,e,t){var a=t(42)(t(33),"Promise");n.exports=a},function(n,e,t){var a=t(42)(t(33),"WeakMap");n.exports=a},function(n,e,t){var a=t(209),r=t(203);n.exports=function(n){for(var e=r(n),t=e.length;t--;){var o=e[t],i=n[o];e[t]=[o,i,a(i)]}return e}},function(n,e,t){var a=t(199),r=t(336),o=t(343),i=t(136),s=t(209),l=t(210),c=t(99);n.exports=function(n,e){return i(n)&&s(e)?l(c(n),e):function(t){var i=r(t,n);return void 0===i&&i===e?o(t,n):a(e,i,3)}}},function(n,e,t){var a=t(211);n.exports=function(n,e,t){var r=null==n?void 0:a(n,e);return void 0===r?t:r}},function(n,e,t){var a=t(338),r=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,o=/\\(\\)?/g,i=a((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(r,(function(n,t,a,r){e.push(a?r.replace(o,"$1"):t||n)})),e}));n.exports=i},function(n,e,t){var a=t(339);n.exports=function(n){var e=a(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var a=t(133);function r(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var a=arguments,r=e?e.apply(this,a):a[0],o=t.cache;if(o.has(r))return o.get(r);var i=n.apply(this,a);return t.cache=o.set(r,i)||o,i};return t.cache=new(r.Cache||a),t}r.Cache=a,n.exports=r},function(n,e,t){var a=t(341);n.exports=function(n){return null==n?"":a(n)}},function(n,e,t){var a=t(72),r=t(342),o=t(31),i=t(137),s=a?a.prototype:void 0,l=s?s.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(o(e))return r(e,n)+"";if(i(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length,r=Array(a);++t<a;)r[t]=e(n[t],t,n);return r}},function(n,e,t){var a=t(344),r=t(345);n.exports=function(n,e){return null!=n&&r(n,e,a)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var a=t(212),r=t(130),o=t(31),i=t(205),s=t(135),l=t(99);n.exports=function(n,e,t){for(var c=-1,d=(e=a(e,n)).length,p=!1;++c<d;){var u=l(e[c]);if(!(p=null!=n&&t(n,u)))break;n=n[u]}return p||++c!=d?p:!!(d=null==n?0:n.length)&&s(d)&&i(u,d)&&(o(n)||r(n))}},function(n,e,t){var a=t(347),r=t(348),o=t(136),i=t(99);n.exports=function(n){return o(n)?a(i(n)):r(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var a=t(211);n.exports=function(n){return function(e){return a(e,n)}}},function(n,e,t){var a=t(138),r=t(350),o=t(352);n.exports=function(n,e){return o(r(n,e,a),n+"")}},function(n,e,t){var a=t(351),r=Math.max;n.exports=function(n,e,t){return e=r(void 0===e?n.length-1:e,0),function(){for(var o=arguments,i=-1,s=r(o.length-e,0),l=Array(s);++i<s;)l[i]=o[e+i];i=-1;for(var c=Array(e+1);++i<e;)c[i]=o[i];return c[e]=t(l),a(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var a=t(353),r=t(356)(a);n.exports=r},function(n,e,t){var a=t(354),r=t(355),o=t(138),i=r?function(n,e){return r(n,"toString",{configurable:!0,enumerable:!1,value:a(e),writable:!0})}:o;n.exports=i},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var a=t(42),r=function(){try{var n=a(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=r},function(n,e){var t=Date.now;n.exports=function(n){var e=0,a=0;return function(){var r=t(),o=16-(r-a);if(a=r,o>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var a=t(201),r=t(358),o=t(363),i=t(202),s=t(364),l=t(134);n.exports=function(n,e,t){var c=-1,d=r,p=n.length,u=!0,m=[],g=m;if(t)u=!1,d=o;else if(p>=200){var h=e?null:s(n);if(h)return l(h);u=!1,d=i,g=new a}else g=e?[]:m;n:for(;++c<p;){var f=n[c],v=e?e(f):f;if(f=t||0!==f?f:0,u&&v==v){for(var b=g.length;b--;)if(g[b]===v)continue n;e&&g.push(v),m.push(f)}else d(g,v,t)||(g!==m&&g.push(v),m.push(f))}return m}},function(n,e,t){var a=t(359);n.exports=function(n,e){return!!(null==n?0:n.length)&&a(n,e,0)>-1}},function(n,e,t){var a=t(360),r=t(361),o=t(362);n.exports=function(n,e,t){return e==e?o(n,e,t):a(n,r,t)}},function(n,e){n.exports=function(n,e,t,a){for(var r=n.length,o=t+(a?1:-1);a?o--:++o<r;)if(e(n[o],o,n))return o;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var a=t-1,r=n.length;++a<r;)if(n[a]===e)return a;return-1}},function(n,e){n.exports=function(n,e,t){for(var a=-1,r=null==n?0:n.length;++a<r;)if(t(e,n[a]))return!0;return!1}},function(n,e,t){var a=t(208),r=t(365),o=t(134),i=a&&1/o(new a([,-0]))[1]==1/0?function(n){return new a(n)}:r;n.exports=i},function(n,e){n.exports=function(){}},function(n,e,t){var a=t(207),r=t(50);n.exports=function(n){return r(n)&&a(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){var a=t(67).PROPER,r=t(3),o=t(146);n.exports=function(n){return r((function(){return!!o[n]()||"​᠎"!=="​᠎"[n]()||a&&o[n].name!==n}))}},function(n,e,t){var a=t(1),r=t(213);a({target:"Function",proto:!0,forced:Function.bind!==r},{bind:r})},function(n,e,t){var a=t(2);n.exports=a(1..valueOf)},function(n,e,t){"use strict";t(214)},function(n,e,t){var a=t(2),r=t(53),o=t(12),i=t(374),s=t(19),l=a(i),c=a("".slice),d=Math.ceil,p=function(n){return function(e,t,a){var i,p,u=o(s(e)),m=r(t),g=u.length,h=void 0===a?" ":o(a);return m<=g||""==h?u:((p=l(h,d((i=m-g)/h.length))).length>i&&(p=c(p,0,i)),n?u+p:p+u)}};n.exports={start:p(!1),end:p(!0)}},function(n,e,t){"use strict";var a=t(0),r=t(55),o=t(12),i=t(19),s=a.RangeError;n.exports=function(n){var e=o(i(this)),t="",a=r(n);if(a<0||a==1/0)throw s("Wrong number of repetitions");for(;a>0;(a>>>=1)&&(e+=e))1&a&&(t+=e);return t}},function(n,e,t){var a=t(32);n.exports=/Version\/10(?:\.\d+){1,2}(?: [\w./]+)?(?: Mobile\/\w+)? Safari\//.test(a)},function(n,e,t){"use strict";t(215)},function(n,e,t){"use strict";t(216)},function(n,e,t){var a=t(32).match(/firefox\/(\d+)/i);n.exports=!!a&&+a[1]},function(n,e,t){var a=t(32);n.exports=/MSIE|Trident/.test(a)},function(n,e,t){var a=t(32).match(/AppleWebKit\/(\d+)\./);n.exports=!!a&&+a[1]},function(n,e,t){},function(n,e,t){},function(n,e,t){var a=t(1),r=t(3),o=t(20),i=t(38).f,s=t(7),l=r((function(){i(1)}));a({target:"Object",stat:!0,forced:!s||l,sham:!s},{getOwnPropertyDescriptor:function(n,e){return i(o(n),e)}})},function(n,e,t){var a=t(1),r=t(7),o=t(116).f;a({target:"Object",stat:!0,forced:Object.defineProperties!==o,sham:!r},{defineProperties:o})},function(n,e,t){t(1)({target:"Reflect",stat:!0},{ownKeys:t(121)})},function(n,e){},function(n,e){function t(n,e){for(var t=0,a=n.length-1;a>=0;a--){var r=n[a];"."===r?n.splice(a,1):".."===r?(n.splice(a,1),t++):t&&(n.splice(a,1),t--)}if(e)for(;t--;t)n.unshift("..");return n}function a(n,e){if(n.filter)return n.filter(e);for(var t=[],a=0;a<n.length;a++)e(n[a],a,n)&&t.push(n[a]);return t}e.resolve=function(){for(var n="",e=!1,r=arguments.length-1;r>=-1&&!e;r--){var o=r>=0?arguments[r]:process.cwd();if("string"!=typeof o)throw new TypeError("Arguments to path.resolve must be strings");o&&(n=o+"/"+n,e="/"===o.charAt(0))}return(e?"/":"")+(n=t(a(n.split("/"),(function(n){return!!n})),!e).join("/"))||"."},e.normalize=function(n){var o=e.isAbsolute(n),i="/"===r(n,-1);return(n=t(a(n.split("/"),(function(n){return!!n})),!o).join("/"))||o||(n="."),n&&i&&(n+="/"),(o?"/":"")+n},e.isAbsolute=function(n){return"/"===n.charAt(0)},e.join=function(){var n=Array.prototype.slice.call(arguments,0);return e.normalize(a(n,(function(n,e){if("string"!=typeof n)throw new TypeError("Arguments to path.join must be strings");return n})).join("/"))},e.relative=function(n,t){function a(n){for(var e=0;e<n.length&&""===n[e];e++);for(var t=n.length-1;t>=0&&""===n[t];t--);return e>t?[]:n.slice(e,t-e+1)}n=e.resolve(n).substr(1),t=e.resolve(t).substr(1);for(var r=a(n.split("/")),o=a(t.split("/")),i=Math.min(r.length,o.length),s=i,l=0;l<i;l++)if(r[l]!==o[l]){s=l;break}var c=[];for(l=s;l<r.length;l++)c.push("..");return(c=c.concat(o.slice(s))).join("/")},e.sep="/",e.delimiter=":",e.dirname=function(n){if("string"!=typeof n&&(n+=""),0===n.length)return".";for(var e=n.charCodeAt(0),t=47===e,a=-1,r=!0,o=n.length-1;o>=1;--o)if(47===(e=n.charCodeAt(o))){if(!r){a=o;break}}else r=!1;return-1===a?t?"/":".":t&&1===a?"/":n.slice(0,a)},e.basename=function(n,e){var t=function(n){"string"!=typeof n&&(n+="");var e,t=0,a=-1,r=!0;for(e=n.length-1;e>=0;--e)if(47===n.charCodeAt(e)){if(!r){t=e+1;break}}else-1===a&&(r=!1,a=e+1);return-1===a?"":n.slice(t,a)}(n);return e&&t.substr(-1*e.length)===e&&(t=t.substr(0,t.length-e.length)),t},e.extname=function(n){"string"!=typeof n&&(n+="");for(var e=-1,t=0,a=-1,r=!0,o=0,i=n.length-1;i>=0;--i){var s=n.charCodeAt(i);if(47!==s)-1===a&&(r=!1,a=i+1),46===s?-1===e?e=i:1!==o&&(o=1):-1!==e&&(o=-1);else if(!r){t=i+1;break}}return-1===e||-1===a||0===o||1===o&&e===a-1&&e===t+1?"":n.slice(e,a)};var r="b"==="ab".substr(-1)?function(n,e,t){return n.substr(e,t)}:function(n,e,t){return e<0&&(e=n.length+e),n.substr(e,t)}},function(n,e,t){"use strict";var a=/[|\\{}()[\]^$+*?.]/g;e.escapeRegExpChars=function(n){return n?String(n).replace(a,"\\$&"):""};var r={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&#34;","'":"&#39;"},o=/[&<>'"]/g;function i(n){return r[n]||n}e.escapeXML=function(n){return null==n?"":String(n).replace(o,i)},e.escapeXML.toString=function(){return Function.prototype.toString.call(this)+';\nvar _ENCODE_HTML_RULES = {\n      "&": "&amp;"\n    , "<": "&lt;"\n    , ">": "&gt;"\n    , \'"\': "&#34;"\n    , "\'": "&#39;"\n    }\n  , _MATCH_HTML = /[&<>\'"]/g;\nfunction encode_char(c) {\n  return _ENCODE_HTML_RULES[c] || c;\n};\n'},e.shallowCopy=function(n,e){for(var t in e=e||{})n[t]=e[t];return n},e.shallowCopyFromList=function(n,e,t){for(var a=0;a<t.length;a++){var r=t[a];void 0!==e[r]&&(n[r]=e[r])}return n},e.cache={_data:{},set:function(n,e){this._data[n]=e},get:function(n){return this._data[n]},remove:function(n){delete this._data[n]},reset:function(){this._data={}}}},function(n){n.exports=JSON.parse('{"name":"ejs","description":"Embedded JavaScript templates","keywords":["template","engine","ejs"],"version":"2.7.4","author":"Matthew Eernisse <mde@fleegix.org> (http://fleegix.org)","license":"Apache-2.0","main":"./lib/ejs.js","repository":{"type":"git","url":"git://github.com/mde/ejs.git"},"bugs":"https://github.com/mde/ejs/issues","homepage":"https://github.com/mde/ejs","dependencies":{},"devDependencies":{"browserify":"^13.1.1","eslint":"^4.14.0","git-directory-deploy":"^1.5.1","jake":"^10.3.1","jsdoc":"^3.4.0","lru-cache":"^4.0.1","mocha":"^5.0.5","uglify-js":"^3.3.16"},"engines":{"node":">=0.10.0"},"scripts":{"test":"mocha","postinstall":"node ./postinstall.js"}}')},function(n,e,t){"use strict";t(219)},function(n,e,t){"use strict";t(220)},function(n,e,t){"use strict";t.r(e);t(149),t(244),t(253),t(255);var a=t(91),r=(t(148),t(44),t(4),t(21),t(27),t(46),t(25),Object.freeze({}));function o(n){return null==n}function i(n){return null!=n}function s(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return null!==n&&"object"==typeof n}var d=Object.prototype.toString;function p(n){return"[object Object]"===d.call(n)}function u(n){return"[object RegExp]"===d.call(n)}function m(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function g(n){return i(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function h(n){return null==n?"":Array.isArray(n)||p(n)&&n.toString===d?JSON.stringify(n,null,2):String(n)}function f(n){var e=parseFloat(n);return isNaN(e)?n:e}function v(n,e){for(var t=Object.create(null),a=n.split(","),r=0;r<a.length;r++)t[a[r]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}v("slot,component",!0);var b=v("key,ref,slot,slot-scope,is");function y(n,e){if(n.length){var t=n.indexOf(e);if(t>-1)return n.splice(t,1)}}var x=Object.prototype.hasOwnProperty;function k(n,e){return x.call(n,e)}function w(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var S=/-(\w)/g,E=w((function(n){return n.replace(S,(function(n,e){return e?e.toUpperCase():""}))})),T=w((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),_=/\B([A-Z])/g,I=w((function(n){return n.replace(_,"-$1").toLowerCase()}));var j=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var a=arguments.length;return a?a>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function z(n,e){e=e||0;for(var t=n.length-e,a=new Array(t);t--;)a[t]=n[t+e];return a}function C(n,e){for(var t in e)n[t]=e[t];return n}function A(n){for(var e={},t=0;t<n.length;t++)n[t]&&C(e,n[t]);return e}function P(n,e,t){}var D=function(n,e,t){return!1},q=function(n){return n};function L(n,e){if(n===e)return!0;var t=c(n),a=c(e);if(!t||!a)return!t&&!a&&String(n)===String(e);try{var r=Array.isArray(n),o=Array.isArray(e);if(r&&o)return n.length===e.length&&n.every((function(n,t){return L(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(r||o)return!1;var i=Object.keys(n),s=Object.keys(e);return i.length===s.length&&i.every((function(t){return L(n[t],e[t])}))}catch(n){return!1}}function B(n,e){for(var t=0;t<n.length;t++)if(L(n[t],e))return t;return-1}function O(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}var F=["component","directive","filter"],R=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],M={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:D,isReservedAttr:D,isUnknownElement:D,getTagNamespace:P,parsePlatformTagName:q,mustUseProp:D,async:!0,_lifecycleHooks:R},H=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function N(n,e,t,a){Object.defineProperty(n,e,{value:t,enumerable:!!a,writable:!0,configurable:!0})}var U=new RegExp("[^"+H.source+".$_\\d]");var J,$="__proto__"in{},Z="undefined"!=typeof window,V="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,K=V&&WXEnvironment.platform.toLowerCase(),Q=Z&&window.navigator.userAgent.toLowerCase(),W=Q&&/msie|trident/.test(Q),G=Q&&Q.indexOf("msie 9.0")>0,X=Q&&Q.indexOf("edge/")>0,Y=(Q&&Q.indexOf("android"),Q&&/iphone|ipad|ipod|ios/.test(Q)||"ios"===K),nn=(Q&&/chrome\/\d+/.test(Q),Q&&/phantomjs/.test(Q),Q&&Q.match(/firefox\/(\d+)/)),en={}.watch,tn=!1;if(Z)try{var an={};Object.defineProperty(an,"passive",{get:function(){tn=!0}}),window.addEventListener("test-passive",null,an)}catch(n){}var rn=function(){return void 0===J&&(J=!Z&&!V&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),J},on=Z&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function sn(n){return"function"==typeof n&&/native code/.test(n.toString())}var ln,cn="undefined"!=typeof Symbol&&sn(Symbol)&&"undefined"!=typeof Reflect&&sn(Reflect.ownKeys);ln="undefined"!=typeof Set&&sn(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var dn=P,pn=0,un=function(){this.id=pn++,this.subs=[]};un.prototype.addSub=function(n){this.subs.push(n)},un.prototype.removeSub=function(n){y(this.subs,n)},un.prototype.depend=function(){un.target&&un.target.addDep(this)},un.prototype.notify=function(){var n=this.subs.slice();for(var e=0,t=n.length;e<t;e++)n[e].update()},un.target=null;var mn=[];function gn(n){mn.push(n),un.target=n}function hn(){mn.pop(),un.target=mn[mn.length-1]}var fn=function(n,e,t,a,r,o,i,s){this.tag=n,this.data=e,this.children=t,this.text=a,this.elm=r,this.ns=void 0,this.context=o,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=i,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},vn={child:{configurable:!0}};vn.child.get=function(){return this.componentInstance},Object.defineProperties(fn.prototype,vn);var bn=function(n){void 0===n&&(n="");var e=new fn;return e.text=n,e.isComment=!0,e};function yn(n){return new fn(void 0,void 0,void 0,String(n))}function xn(n){var e=new fn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var kn=Array.prototype,wn=Object.create(kn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=kn[n];N(wn,n,(function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];var r,o=e.apply(this,t),i=this.__ob__;switch(n){case"push":case"unshift":r=t;break;case"splice":r=t.slice(2)}return r&&i.observeArray(r),i.dep.notify(),o}))}));var Sn=Object.getOwnPropertyNames(wn),En=!0;function Tn(n){En=n}var _n=function(n){this.value=n,this.dep=new un,this.vmCount=0,N(n,"__ob__",this),Array.isArray(n)?($?function(n,e){n.__proto__=e}(n,wn):function(n,e,t){for(var a=0,r=t.length;a<r;a++){var o=t[a];N(n,o,e[o])}}(n,wn,Sn),this.observeArray(n)):this.walk(n)};function In(n,e){var t;if(c(n)&&!(n instanceof fn))return k(n,"__ob__")&&n.__ob__ instanceof _n?t=n.__ob__:En&&!rn()&&(Array.isArray(n)||p(n))&&Object.isExtensible(n)&&!n._isVue&&(t=new _n(n)),e&&t&&t.vmCount++,t}function jn(n,e,t,a,r){var o=new un,i=Object.getOwnPropertyDescriptor(n,e);if(!i||!1!==i.configurable){var s=i&&i.get,l=i&&i.set;s&&!l||2!==arguments.length||(t=n[e]);var c=!r&&In(t);Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=s?s.call(n):t;return un.target&&(o.depend(),c&&(c.dep.depend(),Array.isArray(e)&&An(e))),e},set:function(e){var a=s?s.call(n):t;e===a||e!=e&&a!=a||s&&!l||(l?l.call(n,e):t=e,c=!r&&In(e),o.notify())}})}}function zn(n,e,t){if(Array.isArray(n)&&m(e))return n.length=Math.max(n.length,e),n.splice(e,1,t),t;if(e in n&&!(e in Object.prototype))return n[e]=t,t;var a=n.__ob__;return n._isVue||a&&a.vmCount?t:a?(jn(a.value,e,t),a.dep.notify(),t):(n[e]=t,t)}function Cn(n,e){if(Array.isArray(n)&&m(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||k(n,e)&&(delete n[e],t&&t.dep.notify())}}function An(n){for(var e=void 0,t=0,a=n.length;t<a;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),Array.isArray(e)&&An(e)}_n.prototype.walk=function(n){for(var e=Object.keys(n),t=0;t<e.length;t++)jn(n,e[t])},_n.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)In(n[e])};var Pn=M.optionMergeStrategies;function Dn(n,e){if(!e)return n;for(var t,a,r,o=cn?Reflect.ownKeys(e):Object.keys(e),i=0;i<o.length;i++)"__ob__"!==(t=o[i])&&(a=n[t],r=e[t],k(n,t)?a!==r&&p(a)&&p(r)&&Dn(a,r):zn(n,t,r));return n}function qn(n,e,t){return t?function(){var a="function"==typeof e?e.call(t,t):e,r="function"==typeof n?n.call(t,t):n;return a?Dn(a,r):r}:e?n?function(){return Dn("function"==typeof e?e.call(this,this):e,"function"==typeof n?n.call(this,this):n)}:e:n}function Ln(n,e){var t=e?n?n.concat(e):Array.isArray(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Bn(n,e,t,a){var r=Object.create(n||null);return e?C(r,e):r}Pn.data=function(n,e,t){return t?qn(n,e,t):e&&"function"!=typeof e?n:qn(n,e)},R.forEach((function(n){Pn[n]=Ln})),F.forEach((function(n){Pn[n+"s"]=Bn})),Pn.watch=function(n,e,t,a){if(n===en&&(n=void 0),e===en&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var r={};for(var o in C(r,n),e){var i=r[o],s=e[o];i&&!Array.isArray(i)&&(i=[i]),r[o]=i?i.concat(s):Array.isArray(s)?s:[s]}return r},Pn.props=Pn.methods=Pn.inject=Pn.computed=function(n,e,t,a){if(!n)return e;var r=Object.create(null);return C(r,n),e&&C(r,e),r},Pn.provide=qn;var On=function(n,e){return void 0===e?n:e};function Fn(n,e,t){if("function"==typeof e&&(e=e.options),function(n,e){var t=n.props;if(t){var a,r,o={};if(Array.isArray(t))for(a=t.length;a--;)"string"==typeof(r=t[a])&&(o[E(r)]={type:null});else if(p(t))for(var i in t)r=t[i],o[E(i)]=p(r)?r:{type:r};else 0;n.props=o}}(e),function(n,e){var t=n.inject;if(t){var a=n.inject={};if(Array.isArray(t))for(var r=0;r<t.length;r++)a[t[r]]={from:t[r]};else if(p(t))for(var o in t){var i=t[o];a[o]=p(i)?C({from:o},i):{from:i}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var a=e[t];"function"==typeof a&&(e[t]={bind:a,update:a})}}(e),!e._base&&(e.extends&&(n=Fn(n,e.extends,t)),e.mixins))for(var a=0,r=e.mixins.length;a<r;a++)n=Fn(n,e.mixins[a],t);var o,i={};for(o in n)s(o);for(o in e)k(n,o)||s(o);function s(a){var r=Pn[a]||On;i[a]=r(n[a],e[a],t,a)}return i}function Rn(n,e,t,a){if("string"==typeof t){var r=n[e];if(k(r,t))return r[t];var o=E(t);if(k(r,o))return r[o];var i=T(o);return k(r,i)?r[i]:r[t]||r[o]||r[i]}}function Mn(n,e,t,a){var r=e[n],o=!k(t,n),i=t[n],s=Jn(Boolean,r.type);if(s>-1)if(o&&!k(r,"default"))i=!1;else if(""===i||i===I(n)){var l=Jn(String,r.type);(l<0||s<l)&&(i=!0)}if(void 0===i){i=function(n,e,t){if(!k(e,"default"))return;var a=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return"function"==typeof a&&"Function"!==Nn(e.type)?a.call(n):a}(a,r,n);var c=En;Tn(!0),In(i),Tn(c)}return i}var Hn=/^\s*function (\w+)/;function Nn(n){var e=n&&n.toString().match(Hn);return e?e[1]:""}function Un(n,e){return Nn(n)===Nn(e)}function Jn(n,e){if(!Array.isArray(e))return Un(e,n)?0:-1;for(var t=0,a=e.length;t<a;t++)if(Un(e[t],n))return t;return-1}function $n(n,e,t){gn();try{if(e)for(var a=e;a=a.$parent;){var r=a.$options.errorCaptured;if(r)for(var o=0;o<r.length;o++)try{if(!1===r[o].call(a,n,e,t))return}catch(n){Vn(n,a,"errorCaptured hook")}}Vn(n,e,t)}finally{hn()}}function Zn(n,e,t,a,r){var o;try{(o=t?n.apply(e,t):n.call(e))&&!o._isVue&&g(o)&&!o._handled&&(o.catch((function(n){return $n(n,a,r+" (Promise/async)")})),o._handled=!0)}catch(n){$n(n,a,r)}return o}function Vn(n,e,t){if(M.errorHandler)try{return M.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Kn(e,null,"config.errorHandler")}Kn(n,e,t)}function Kn(n,e,t){if(!Z&&!V||"undefined"==typeof console)throw n;console.error(n)}var Qn,Wn=!1,Gn=[],Xn=!1;function Yn(){Xn=!1;var n=Gn.slice(0);Gn.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&sn(Promise)){var ne=Promise.resolve();Qn=function(){ne.then(Yn),Y&&setTimeout(P)},Wn=!0}else if(W||"undefined"==typeof MutationObserver||!sn(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Qn="undefined"!=typeof setImmediate&&sn(setImmediate)?function(){setImmediate(Yn)}:function(){setTimeout(Yn,0)};else{var ee=1,te=new MutationObserver(Yn),ae=document.createTextNode(String(ee));te.observe(ae,{characterData:!0}),Qn=function(){ee=(ee+1)%2,ae.data=String(ee)},Wn=!0}function re(n,e){var t;if(Gn.push((function(){if(n)try{n.call(e)}catch(n){$n(n,e,"nextTick")}else t&&t(e)})),Xn||(Xn=!0,Qn()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}var oe=new ln;function ie(n){!function n(e,t){var a,r,o=Array.isArray(e);if(!o&&!c(e)||Object.isFrozen(e)||e instanceof fn)return;if(e.__ob__){var i=e.__ob__.dep.id;if(t.has(i))return;t.add(i)}if(o)for(a=e.length;a--;)n(e[a],t);else for(r=Object.keys(e),a=r.length;a--;)n(e[r[a]],t)}(n,oe),oe.clear()}var se=w((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),a="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=a?n.slice(1):n,once:t,capture:a,passive:e}}));function le(n,e){function t(){var n=arguments,a=t.fns;if(!Array.isArray(a))return Zn(a,null,arguments,e,"v-on handler");for(var r=a.slice(),o=0;o<r.length;o++)Zn(r[o],null,n,e,"v-on handler")}return t.fns=n,t}function ce(n,e,t,a,r,i){var l,c,d,p;for(l in n)c=n[l],d=e[l],p=se(l),o(c)||(o(d)?(o(c.fns)&&(c=n[l]=le(c,i)),s(p.once)&&(c=n[l]=r(p.name,c,p.capture)),t(p.name,c,p.capture,p.passive,p.params)):c!==d&&(d.fns=c,n[l]=d));for(l in e)o(n[l])&&a((p=se(l)).name,e[l],p.capture)}function de(n,e,t){var a;n instanceof fn&&(n=n.data.hook||(n.data.hook={}));var r=n[e];function l(){t.apply(this,arguments),y(a.fns,l)}o(r)?a=le([l]):i(r.fns)&&s(r.merged)?(a=r).fns.push(l):a=le([r,l]),a.merged=!0,n[e]=a}function pe(n,e,t,a,r){if(i(e)){if(k(e,t))return n[t]=e[t],r||delete e[t],!0;if(k(e,a))return n[t]=e[a],r||delete e[a],!0}return!1}function ue(n){return l(n)?[yn(n)]:Array.isArray(n)?function n(e,t){var a,r,c,d,p=[];for(a=0;a<e.length;a++)o(r=e[a])||"boolean"==typeof r||(c=p.length-1,d=p[c],Array.isArray(r)?r.length>0&&(me((r=n(r,(t||"")+"_"+a))[0])&&me(d)&&(p[c]=yn(d.text+r[0].text),r.shift()),p.push.apply(p,r)):l(r)?me(d)?p[c]=yn(d.text+r):""!==r&&p.push(yn(r)):me(r)&&me(d)?p[c]=yn(d.text+r.text):(s(e._isVList)&&i(r.tag)&&o(r.key)&&i(t)&&(r.key="__vlist"+t+"_"+a+"__"),p.push(r)));return p}(n):void 0}function me(n){return i(n)&&i(n.text)&&!1===n.isComment}function ge(n,e){if(n){for(var t=Object.create(null),a=cn?Reflect.ownKeys(n):Object.keys(n),r=0;r<a.length;r++){var o=a[r];if("__ob__"!==o){for(var i=n[o].from,s=e;s;){if(s._provided&&k(s._provided,i)){t[o]=s._provided[i];break}s=s.$parent}if(!s)if("default"in n[o]){var l=n[o].default;t[o]="function"==typeof l?l.call(e):l}else 0}}return t}}function he(n,e){if(!n||!n.length)return{};for(var t={},a=0,r=n.length;a<r;a++){var o=n[a],i=o.data;if(i&&i.attrs&&i.attrs.slot&&delete i.attrs.slot,o.context!==e&&o.fnContext!==e||!i||null==i.slot)(t.default||(t.default=[])).push(o);else{var s=i.slot,l=t[s]||(t[s]=[]);"template"===o.tag?l.push.apply(l,o.children||[]):l.push(o)}}for(var c in t)t[c].every(fe)&&delete t[c];return t}function fe(n){return n.isComment&&!n.asyncFactory||" "===n.text}function ve(n){return n.isComment&&n.asyncFactory}function be(n,e,t){var a,o=Object.keys(e).length>0,i=n?!!n.$stable:!o,s=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(i&&t&&t!==r&&s===t.$key&&!o&&!t.$hasNormal)return t;for(var l in a={},n)n[l]&&"$"!==l[0]&&(a[l]=ye(e,l,n[l]))}else a={};for(var c in e)c in a||(a[c]=xe(e,c));return n&&Object.isExtensible(n)&&(n._normalized=a),N(a,"$stable",i),N(a,"$key",s),N(a,"$hasNormal",o),a}function ye(n,e,t){var a=function(){var n=arguments.length?t.apply(null,arguments):t({}),e=(n=n&&"object"==typeof n&&!Array.isArray(n)?[n]:ue(n))&&n[0];return n&&(!e||1===n.length&&e.isComment&&!ve(e))?void 0:n};return t.proxy&&Object.defineProperty(n,e,{get:a,enumerable:!0,configurable:!0}),a}function xe(n,e){return function(){return n[e]}}function ke(n,e){var t,a,r,o,s;if(Array.isArray(n)||"string"==typeof n)for(t=new Array(n.length),a=0,r=n.length;a<r;a++)t[a]=e(n[a],a);else if("number"==typeof n)for(t=new Array(n),a=0;a<n;a++)t[a]=e(a+1,a);else if(c(n))if(cn&&n[Symbol.iterator]){t=[];for(var l=n[Symbol.iterator](),d=l.next();!d.done;)t.push(e(d.value,t.length)),d=l.next()}else for(o=Object.keys(n),t=new Array(o.length),a=0,r=o.length;a<r;a++)s=o[a],t[a]=e(n[s],s,a);return i(t)||(t=[]),t._isVList=!0,t}function we(n,e,t,a){var r,o=this.$scopedSlots[n];o?(t=t||{},a&&(t=C(C({},a),t)),r=o(t)||("function"==typeof e?e():e)):r=this.$slots[n]||("function"==typeof e?e():e);var i=t&&t.slot;return i?this.$createElement("template",{slot:i},r):r}function Se(n){return Rn(this.$options,"filters",n)||q}function Ee(n,e){return Array.isArray(n)?-1===n.indexOf(e):n!==e}function Te(n,e,t,a,r){var o=M.keyCodes[e]||t;return r&&a&&!M.keyCodes[e]?Ee(r,a):o?Ee(o,n):a?I(a)!==e:void 0===n}function _e(n,e,t,a,r){if(t)if(c(t)){var o;Array.isArray(t)&&(t=A(t));var i=function(i){if("class"===i||"style"===i||b(i))o=n;else{var s=n.attrs&&n.attrs.type;o=a||M.mustUseProp(e,s,i)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=E(i),c=I(i);l in o||c in o||(o[i]=t[i],r&&((n.on||(n.on={}))["update:"+i]=function(n){t[i]=n}))};for(var s in t)i(s)}else;return n}function Ie(n,e){var t=this._staticTrees||(this._staticTrees=[]),a=t[n];return a&&!e||ze(a=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,null,this),"__static__"+n,!1),a}function je(n,e,t){return ze(n,"__once__"+e+(t?"_"+t:""),!0),n}function ze(n,e,t){if(Array.isArray(n))for(var a=0;a<n.length;a++)n[a]&&"string"!=typeof n[a]&&Ce(n[a],e+"_"+a,t);else Ce(n,e,t)}function Ce(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function Ae(n,e){if(e)if(p(e)){var t=n.on=n.on?C({},n.on):{};for(var a in e){var r=t[a],o=e[a];t[a]=r?[].concat(r,o):o}}else;return n}function Pe(n,e,t,a){e=e||{$stable:!t};for(var r=0;r<n.length;r++){var o=n[r];Array.isArray(o)?Pe(o,e,t):o&&(o.proxy&&(o.fn.proxy=!0),e[o.key]=o.fn)}return a&&(e.$key=a),e}function De(n,e){for(var t=0;t<e.length;t+=2){var a=e[t];"string"==typeof a&&a&&(n[e[t]]=e[t+1])}return n}function qe(n,e){return"string"==typeof n?e+n:n}function Le(n){n._o=je,n._n=f,n._s=h,n._l=ke,n._t=we,n._q=L,n._i=B,n._m=Ie,n._f=Se,n._k=Te,n._b=_e,n._v=yn,n._e=bn,n._u=Pe,n._g=Ae,n._d=De,n._p=qe}function Be(n,e,t,a,o){var i,l=this,c=o.options;k(a,"_uid")?(i=Object.create(a))._original=a:(i=a,a=a._original);var d=s(c._compiled),p=!d;this.data=n,this.props=e,this.children=t,this.parent=a,this.listeners=n.on||r,this.injections=ge(c.inject,a),this.slots=function(){return l.$slots||be(n.scopedSlots,l.$slots=he(t,a)),l.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return be(n.scopedSlots,this.slots())}}),d&&(this.$options=c,this.$slots=this.slots(),this.$scopedSlots=be(n.scopedSlots,this.$slots)),c._scopeId?this._c=function(n,e,t,r){var o=Ue(i,n,e,t,r,p);return o&&!Array.isArray(o)&&(o.fnScopeId=c._scopeId,o.fnContext=a),o}:this._c=function(n,e,t,a){return Ue(i,n,e,t,a,p)}}function Oe(n,e,t,a,r){var o=xn(n);return o.fnContext=t,o.fnOptions=a,e.slot&&((o.data||(o.data={})).slot=e.slot),o}function Fe(n,e){for(var t in e)n[E(t)]=e[t]}Le(Be.prototype);var Re={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;Re.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},a=n.data.inlineTemplate;i(a)&&(t.render=a.render,t.staticRenderFns=a.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Xe)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,a,o){0;var i=a.data.scopedSlots,s=n.$scopedSlots,l=!!(i&&!i.$stable||s!==r&&!s.$stable||i&&n.$scopedSlots.$key!==i.$key||!i&&n.$scopedSlots.$key),c=!!(o||n.$options._renderChildren||l);n.$options._parentVnode=a,n.$vnode=a,n._vnode&&(n._vnode.parent=a);if(n.$options._renderChildren=o,n.$attrs=a.data.attrs||r,n.$listeners=t||r,e&&n.$options.props){Tn(!1);for(var d=n._props,p=n.$options._propKeys||[],u=0;u<p.length;u++){var m=p[u],g=n.$options.props;d[m]=Mn(m,g,e,n)}Tn(!0),n.$options.propsData=e}t=t||r;var h=n.$options._parentListeners;n.$options._parentListeners=t,Ge(n,t,h),c&&(n.$slots=he(o,a.context),n.$forceUpdate());0}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,a=n.componentInstance;a._isMounted||(a._isMounted=!0,tt(a,"mounted")),n.data.keepAlive&&(t._isMounted?((e=a)._inactive=!1,rt.push(e)):et(a,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(t&&(e._directInactive=!0,nt(e)))return;if(!e._inactive){e._inactive=!0;for(var a=0;a<e.$children.length;a++)n(e.$children[a]);tt(e,"deactivated")}}(e,!0):e.$destroy())}},Me=Object.keys(Re);function He(n,e,t,a,l){if(!o(n)){var d=t.$options._base;if(c(n)&&(n=d.extend(n)),"function"==typeof n){var p;if(o(n.cid)&&void 0===(n=function(n,e){if(s(n.error)&&i(n.errorComp))return n.errorComp;if(i(n.resolved))return n.resolved;var t=$e;t&&i(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t);if(s(n.loading)&&i(n.loadingComp))return n.loadingComp;if(t&&!i(n.owners)){var a=n.owners=[t],r=!0,l=null,d=null;t.$on("hook:destroyed",(function(){return y(a,t)}));var p=function(n){for(var e=0,t=a.length;e<t;e++)a[e].$forceUpdate();n&&(a.length=0,null!==l&&(clearTimeout(l),l=null),null!==d&&(clearTimeout(d),d=null))},u=O((function(t){n.resolved=Ze(t,e),r?a.length=0:p(!0)})),m=O((function(e){i(n.errorComp)&&(n.error=!0,p(!0))})),h=n(u,m);return c(h)&&(g(h)?o(n.resolved)&&h.then(u,m):g(h.component)&&(h.component.then(u,m),i(h.error)&&(n.errorComp=Ze(h.error,e)),i(h.loading)&&(n.loadingComp=Ze(h.loading,e),0===h.delay?n.loading=!0:l=setTimeout((function(){l=null,o(n.resolved)&&o(n.error)&&(n.loading=!0,p(!1))}),h.delay||200)),i(h.timeout)&&(d=setTimeout((function(){d=null,o(n.resolved)&&m(null)}),h.timeout)))),r=!1,n.loading?n.loadingComp:n.resolved}}(p=n,d)))return function(n,e,t,a,r){var o=bn();return o.asyncFactory=n,o.asyncMeta={data:e,context:t,children:a,tag:r},o}(p,e,t,a,l);e=e||{},Et(n),i(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",a=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var r=e.on||(e.on={}),o=r[a],s=e.model.callback;i(o)?(Array.isArray(o)?-1===o.indexOf(s):o!==s)&&(r[a]=[s].concat(o)):r[a]=s}(n.options,e);var u=function(n,e,t){var a=e.options.props;if(!o(a)){var r={},s=n.attrs,l=n.props;if(i(s)||i(l))for(var c in a){var d=I(c);pe(r,l,c,d,!0)||pe(r,s,c,d,!1)}return r}}(e,n);if(s(n.options.functional))return function(n,e,t,a,o){var s=n.options,l={},c=s.props;if(i(c))for(var d in c)l[d]=Mn(d,c,e||r);else i(t.attrs)&&Fe(l,t.attrs),i(t.props)&&Fe(l,t.props);var p=new Be(t,l,o,a,n),u=s.render.call(null,p._c,p);if(u instanceof fn)return Oe(u,t,p.parent,s,p);if(Array.isArray(u)){for(var m=ue(u)||[],g=new Array(m.length),h=0;h<m.length;h++)g[h]=Oe(m[h],t,p.parent,s,p);return g}}(n,u,e,t,a);var m=e.on;if(e.on=e.nativeOn,s(n.options.abstract)){var h=e.slot;e={},h&&(e.slot=h)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<Me.length;t++){var a=Me[t],r=e[a],o=Re[a];r===o||r&&r._merged||(e[a]=r?Ne(o,r):o)}}(e);var f=n.options.name||l;return new fn("vue-component-"+n.cid+(f?"-"+f:""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:u,listeners:m,tag:l,children:a},p)}}}function Ne(n,e){var t=function(t,a){n(t,a),e(t,a)};return t._merged=!0,t}function Ue(n,e,t,a,r,d){return(Array.isArray(t)||l(t))&&(r=a,a=t,t=void 0),s(d)&&(r=2),function(n,e,t,a,r){if(i(t)&&i(t.__ob__))return bn();i(t)&&i(t.is)&&(e=t.is);if(!e)return bn();0;Array.isArray(a)&&"function"==typeof a[0]&&((t=t||{}).scopedSlots={default:a[0]},a.length=0);2===r?a=ue(a):1===r&&(a=function(n){for(var e=0;e<n.length;e++)if(Array.isArray(n[e]))return Array.prototype.concat.apply([],n);return n}(a));var l,d;if("string"==typeof e){var p;d=n.$vnode&&n.$vnode.ns||M.getTagNamespace(e),l=M.isReservedTag(e)?new fn(M.parsePlatformTagName(e),t,a,void 0,void 0,n):t&&t.pre||!i(p=Rn(n.$options,"components",e))?new fn(e,t,a,void 0,void 0,n):He(p,t,n,a,e)}else l=He(e,t,n,a);return Array.isArray(l)?l:i(l)?(i(d)&&function n(e,t,a){e.ns=t,"foreignObject"===e.tag&&(t=void 0,a=!0);if(i(e.children))for(var r=0,l=e.children.length;r<l;r++){var c=e.children[r];i(c.tag)&&(o(c.ns)||s(a)&&"svg"!==c.tag)&&n(c,t,a)}}(l,d),i(t)&&function(n){c(n.style)&&ie(n.style);c(n.class)&&ie(n.class)}(t),l):bn()}(n,e,t,a,r)}var Je,$e=null;function Ze(n,e){return(n.__esModule||cn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),c(n)?e.extend(n):n}function Ve(n){if(Array.isArray(n))for(var e=0;e<n.length;e++){var t=n[e];if(i(t)&&(i(t.componentOptions)||ve(t)))return t}}function Ke(n,e){Je.$on(n,e)}function Qe(n,e){Je.$off(n,e)}function We(n,e){var t=Je;return function a(){var r=e.apply(null,arguments);null!==r&&t.$off(n,a)}}function Ge(n,e,t){Je=n,ce(e,t||{},Ke,Qe,We,n),Je=void 0}var Xe=null;function Ye(n){var e=Xe;return Xe=n,function(){Xe=e}}function nt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function et(n,e){if(e){if(n._directInactive=!1,nt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)et(n.$children[t]);tt(n,"activated")}}function tt(n,e){gn();var t=n.$options[e],a=e+" hook";if(t)for(var r=0,o=t.length;r<o;r++)Zn(t[r],n,null,n,a);n._hasHookEvent&&n.$emit("hook:"+e),hn()}var at=[],rt=[],ot={},it=!1,st=!1,lt=0;var ct=0,dt=Date.now;if(Z&&!W){var pt=window.performance;pt&&"function"==typeof pt.now&&dt()>document.createEvent("Event").timeStamp&&(dt=function(){return pt.now()})}function ut(){var n,e;for(ct=dt(),st=!0,at.sort((function(n,e){return n.id-e.id})),lt=0;lt<at.length;lt++)(n=at[lt]).before&&n.before(),e=n.id,ot[e]=null,n.run();var t=rt.slice(),a=at.slice();lt=at.length=rt.length=0,ot={},it=st=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,et(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],a=t.vm;a._watcher===t&&a._isMounted&&!a._isDestroyed&&tt(a,"updated")}}(a),on&&M.devtools&&on.emit("flush")}var mt=0,gt=function(n,e,t,a,r){this.vm=n,r&&(n._watcher=this),n._watchers.push(this),a?(this.deep=!!a.deep,this.user=!!a.user,this.lazy=!!a.lazy,this.sync=!!a.sync,this.before=a.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++mt,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new ln,this.newDepIds=new ln,this.expression="","function"==typeof e?this.getter=e:(this.getter=function(n){if(!U.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=P)),this.value=this.lazy?void 0:this.get()};gt.prototype.get=function(){var n;gn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;$n(n,e,'getter for watcher "'+this.expression+'"')}finally{this.deep&&ie(n),hn(),this.cleanupDeps()}return n},gt.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},gt.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},gt.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(n){var e=n.id;if(null==ot[e]){if(ot[e]=!0,st){for(var t=at.length-1;t>lt&&at[t].id>n.id;)t--;at.splice(t+1,0,n)}else at.push(n);it||(it=!0,re(ut))}}(this)},gt.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||c(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'+this.expression+'"';Zn(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},gt.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},gt.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},gt.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||y(this.vm._watchers,this);for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1}};var ht={enumerable:!0,configurable:!0,get:P,set:P};function ft(n,e,t){ht.get=function(){return this[e][t]},ht.set=function(n){this[e][t]=n},Object.defineProperty(n,t,ht)}function vt(n){n._watchers=[];var e=n.$options;e.props&&function(n,e){var t=n.$options.propsData||{},a=n._props={},r=n.$options._propKeys=[];n.$parent&&Tn(!1);var o=function(o){r.push(o);var i=Mn(o,e,t,n);jn(a,o,i),o in n||ft(n,"_props",o)};for(var i in e)o(i);Tn(!0)}(n,e.props),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?P:j(e[t],n)}(n,e.methods),e.data?function(n){var e=n.$options.data;p(e=n._data="function"==typeof e?function(n,e){gn();try{return n.call(e,e)}catch(n){return $n(n,e,"data()"),{}}finally{hn()}}(e,n):e||{})||(e={});var t=Object.keys(e),a=n.$options.props,r=(n.$options.methods,t.length);for(;r--;){var o=t[r];0,a&&k(a,o)||(i=void 0,36!==(i=(o+"").charCodeAt(0))&&95!==i&&ft(n,"_data",o))}var i;In(e,!0)}(n):In(n._data={},!0),e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),a=rn();for(var r in e){var o=e[r],i="function"==typeof o?o:o.get;0,a||(t[r]=new gt(n,i||P,P,bt)),r in n||yt(n,r,o)}}(n,e.computed),e.watch&&e.watch!==en&&function(n,e){for(var t in e){var a=e[t];if(Array.isArray(a))for(var r=0;r<a.length;r++)wt(n,t,a[r]);else wt(n,t,a)}}(n,e.watch)}var bt={lazy:!0};function yt(n,e,t){var a=!rn();"function"==typeof t?(ht.get=a?xt(e):kt(t),ht.set=P):(ht.get=t.get?a&&!1!==t.cache?xt(e):kt(t.get):P,ht.set=t.set||P),Object.defineProperty(n,e,ht)}function xt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),un.target&&e.depend(),e.value}}function kt(n){return function(){return n.call(this,this)}}function wt(n,e,t,a){return p(t)&&(a=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,a)}var St=0;function Et(n){var e=n.options;if(n.super){var t=Et(n.super);if(t!==n.superOptions){n.superOptions=t;var a=function(n){var e,t=n.options,a=n.sealedOptions;for(var r in t)t[r]!==a[r]&&(e||(e={}),e[r]=t[r]);return e}(n);a&&C(n.extendOptions,a),(e=n.options=Fn(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Tt(n){this._init(n)}function _t(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,a=t.cid,r=n._Ctor||(n._Ctor={});if(r[a])return r[a];var o=n.name||t.options.name;var i=function(n){this._init(n)};return(i.prototype=Object.create(t.prototype)).constructor=i,i.cid=e++,i.options=Fn(t.options,n),i.super=t,i.options.props&&function(n){var e=n.options.props;for(var t in e)ft(n.prototype,"_props",t)}(i),i.options.computed&&function(n){var e=n.options.computed;for(var t in e)yt(n.prototype,t,e[t])}(i),i.extend=t.extend,i.mixin=t.mixin,i.use=t.use,F.forEach((function(n){i[n]=t[n]})),o&&(i.options.components[o]=i),i.superOptions=t.options,i.extendOptions=n,i.sealedOptions=C({},i.options),r[a]=i,i}}function It(n){return n&&(n.Ctor.options.name||n.tag)}function jt(n,e){return Array.isArray(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!u(n)&&n.test(e)}function zt(n,e){var t=n.cache,a=n.keys,r=n._vnode;for(var o in t){var i=t[o];if(i){var s=i.name;s&&!e(s)&&Ct(t,o,a,r)}}}function Ct(n,e,t,a){var r=n[e];!r||a&&r.tag===a.tag||r.componentInstance.$destroy(),n[e]=null,y(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=St++,e._isVue=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),a=e._parentVnode;t.parent=e.parent,t._parentVnode=a;var r=a.componentOptions;t.propsData=r.propsData,t._parentListeners=r.listeners,t._renderChildren=r.children,t._componentTag=r.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Fn(Et(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&Ge(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,a=t&&t.context;n.$slots=he(e._renderChildren,a),n.$scopedSlots=r,n._c=function(e,t,a,r){return Ue(n,e,t,a,r,!1)},n.$createElement=function(e,t,a,r){return Ue(n,e,t,a,r,!0)};var o=t&&t.data;jn(n,"$attrs",o&&o.attrs||r,null,!0),jn(n,"$listeners",e._parentListeners||r,null,!0)}(e),tt(e,"beforeCreate"),function(n){var e=ge(n.$options.inject,n);e&&(Tn(!1),Object.keys(e).forEach((function(t){jn(n,t,e[t])})),Tn(!0))}(e),vt(e),function(n){var e=n.$options.provide;e&&(n._provided="function"==typeof e?e.call(n):e)}(e),tt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(Tt),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=zn,n.prototype.$delete=Cn,n.prototype.$watch=function(n,e,t){if(p(e))return wt(this,n,e,t);(t=t||{}).user=!0;var a=new gt(this,n,e,t);if(t.immediate){var r='callback for immediate watcher "'+a.expression+'"';gn(),Zn(e,this,[a.value],this,r),hn()}return function(){a.teardown()}}}(Tt),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var a=this;if(Array.isArray(n))for(var r=0,o=n.length;r<o;r++)a.$on(n[r],t);else(a._events[n]||(a._events[n]=[])).push(t),e.test(n)&&(a._hasHookEvent=!0);return a},n.prototype.$once=function(n,e){var t=this;function a(){t.$off(n,a),e.apply(t,arguments)}return a.fn=e,t.$on(n,a),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(n)){for(var a=0,r=n.length;a<r;a++)t.$off(n[a],e);return t}var o,i=t._events[n];if(!i)return t;if(!e)return t._events[n]=null,t;for(var s=i.length;s--;)if((o=i[s])===e||o.fn===e){i.splice(s,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?z(t):t;for(var a=z(arguments,1),r='event handler for "'+n+'"',o=0,i=t.length;o<i;o++)Zn(t[o],e,a,e,r)}return e}}(Tt),function(n){n.prototype._update=function(n,e){var t=this,a=t.$el,r=t._vnode,o=Ye(t);t._vnode=n,t.$el=r?t.__patch__(r,n):t.__patch__(t.$el,n,e,!1),o(),a&&(a.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){tt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||y(e.$children,n),n._watcher&&n._watcher.teardown();for(var t=n._watchers.length;t--;)n._watchers[t].teardown();n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),tt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Tt),function(n){Le(n.prototype),n.prototype.$nextTick=function(n){return re(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,a=t.render,r=t._parentVnode;r&&(e.$scopedSlots=be(r.data.scopedSlots,e.$slots,e.$scopedSlots)),e.$vnode=r;try{$e=e,n=a.call(e._renderProxy,e.$createElement)}catch(t){$n(t,e,"render"),n=e._vnode}finally{$e=null}return Array.isArray(n)&&1===n.length&&(n=n[0]),n instanceof fn||(n=bn()),n.parent=r,n}}(Tt);var At=[String,RegExp,Array],Pt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:At,exclude:At,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,a=this.keyToCache;if(t){var r=t.tag,o=t.componentInstance,i=t.componentOptions;n[a]={name:It(i),tag:r,componentInstance:o},e.push(a),this.max&&e.length>parseInt(this.max)&&Ct(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Ct(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){zt(n,(function(n){return jt(e,n)}))})),this.$watch("exclude",(function(e){zt(n,(function(n){return!jt(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=Ve(n),t=e&&e.componentOptions;if(t){var a=It(t),r=this.include,o=this.exclude;if(r&&(!a||!jt(r,a))||o&&a&&jt(o,a))return e;var i=this.cache,s=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):e.key;i[l]?(e.componentInstance=i[l].componentInstance,y(s,l),s.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return M}};Object.defineProperty(n,"config",e),n.util={warn:dn,extend:C,mergeOptions:Fn,defineReactive:jn},n.set=zn,n.delete=Cn,n.nextTick=re,n.observable=function(n){return In(n),n},n.options=Object.create(null),F.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,C(n.options.components,Pt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=z(arguments,1);return t.unshift(this),"function"==typeof n.install?n.install.apply(n,t):"function"==typeof n&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Fn(this.options,n),this}}(n),_t(n),function(n){F.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&p(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&"function"==typeof t&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Tt),Object.defineProperty(Tt.prototype,"$isServer",{get:rn}),Object.defineProperty(Tt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Tt,"FunctionalRenderContext",{value:Be}),Tt.version="2.6.14";var Dt=v("style,class"),qt=v("input,textarea,option,select,progress"),Lt=v("contenteditable,draggable,spellcheck"),Bt=v("events,caret,typing,plaintext-only"),Ot=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),Ft="http://www.w3.org/1999/xlink",Rt=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},Mt=function(n){return Rt(n)?n.slice(6,n.length):""},Ht=function(n){return null==n||!1===n};function Nt(n){for(var e=n.data,t=n,a=n;i(a.componentInstance);)(a=a.componentInstance._vnode)&&a.data&&(e=Ut(a.data,e));for(;i(t=t.parent);)t&&t.data&&(e=Ut(e,t.data));return function(n,e){if(i(n)||i(e))return Jt(n,$t(e));return""}(e.staticClass,e.class)}function Ut(n,e){return{staticClass:Jt(n.staticClass,e.staticClass),class:i(n.class)?[n.class,e.class]:e.class}}function Jt(n,e){return n?e?n+" "+e:n:e||""}function $t(n){return Array.isArray(n)?function(n){for(var e,t="",a=0,r=n.length;a<r;a++)i(e=$t(n[a]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):c(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var Zt={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Vt=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Kt=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Qt=function(n){return Vt(n)||Kt(n)};var Wt=Object.create(null);var Gt=v("text,number,password,search,email,tel,url");var Xt=Object.freeze({createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(Zt[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Yt={create:function(n,e){na(e)},update:function(n,e){n.data.ref!==e.data.ref&&(na(n,!0),na(e))},destroy:function(n){na(n,!0)}};function na(n,e){var t=n.data.ref;if(i(t)){var a=n.context,r=n.componentInstance||n.elm,o=a.$refs;e?Array.isArray(o[t])?y(o[t],r):o[t]===r&&(o[t]=void 0):n.data.refInFor?Array.isArray(o[t])?o[t].indexOf(r)<0&&o[t].push(r):o[t]=[r]:o[t]=r}}var ea=new fn("",{},[]),ta=["create","activate","update","remove","destroy"];function aa(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&i(n.data)===i(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,a=i(t=n.data)&&i(t=t.attrs)&&t.type,r=i(t=e.data)&&i(t=t.attrs)&&t.type;return a===r||Gt(a)&&Gt(r)}(n,e)||s(n.isAsyncPlaceholder)&&o(e.asyncFactory.error))}function ra(n,e,t){var a,r,o={};for(a=e;a<=t;++a)i(r=n[a].key)&&(o[r]=a);return o}var oa={create:ia,update:ia,destroy:function(n){ia(n,ea)}};function ia(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,a,r,o=n===ea,i=e===ea,s=la(n.data.directives,n.context),l=la(e.data.directives,e.context),c=[],d=[];for(t in l)a=s[t],r=l[t],a?(r.oldValue=a.value,r.oldArg=a.arg,da(r,"update",e,n),r.def&&r.def.componentUpdated&&d.push(r)):(da(r,"bind",e,n),r.def&&r.def.inserted&&c.push(r));if(c.length){var p=function(){for(var t=0;t<c.length;t++)da(c[t],"inserted",e,n)};o?de(e,"insert",p):p()}d.length&&de(e,"postpatch",(function(){for(var t=0;t<d.length;t++)da(d[t],"componentUpdated",e,n)}));if(!o)for(t in s)l[t]||da(s[t],"unbind",n,n,i)}(n,e)}var sa=Object.create(null);function la(n,e){var t,a,r=Object.create(null);if(!n)return r;for(t=0;t<n.length;t++)(a=n[t]).modifiers||(a.modifiers=sa),r[ca(a)]=a,a.def=Rn(e.$options,"directives",a.name);return r}function ca(n){return n.rawName||n.name+"."+Object.keys(n.modifiers||{}).join(".")}function da(n,e,t,a,r){var o=n.def&&n.def[e];if(o)try{o(t.elm,n,t,a,r)}catch(a){$n(a,t.context,"directive "+n.name+" "+e+" hook")}}var pa=[Yt,oa];function ua(n,e){var t=e.componentOptions;if(!(i(t)&&!1===t.Ctor.options.inheritAttrs||o(n.data.attrs)&&o(e.data.attrs))){var a,r,s=e.elm,l=n.data.attrs||{},c=e.data.attrs||{};for(a in i(c.__ob__)&&(c=e.data.attrs=C({},c)),c)r=c[a],l[a]!==r&&ma(s,a,r,e.data.pre);for(a in(W||X)&&c.value!==l.value&&ma(s,"value",c.value),l)o(c[a])&&(Rt(a)?s.removeAttributeNS(Ft,Mt(a)):Lt(a)||s.removeAttribute(a))}}function ma(n,e,t,a){a||n.tagName.indexOf("-")>-1?ga(n,e,t):Ot(e)?Ht(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):Lt(e)?n.setAttribute(e,function(n,e){return Ht(e)||"false"===e?"false":"contenteditable"===n&&Bt(e)?e:"true"}(e,t)):Rt(e)?Ht(t)?n.removeAttributeNS(Ft,Mt(e)):n.setAttributeNS(Ft,e,t):ga(n,e,t)}function ga(n,e,t){if(Ht(t))n.removeAttribute(e);else{if(W&&!G&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var a=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",a)};n.addEventListener("input",a),n.__ieph=!0}n.setAttribute(e,t)}}var ha={create:ua,update:ua};function fa(n,e){var t=e.elm,a=e.data,r=n.data;if(!(o(a.staticClass)&&o(a.class)&&(o(r)||o(r.staticClass)&&o(r.class)))){var s=Nt(e),l=t._transitionClasses;i(l)&&(s=Jt(s,$t(l))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var va,ba={create:fa,update:fa};function ya(n,e,t){var a=va;return function r(){var o=e.apply(null,arguments);null!==o&&wa(n,r,t,a)}}var xa=Wn&&!(nn&&Number(nn[1])<=53);function ka(n,e,t,a){if(xa){var r=ct,o=e;e=o._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=r||n.timeStamp<=0||n.target.ownerDocument!==document)return o.apply(this,arguments)}}va.addEventListener(n,e,tn?{capture:t,passive:a}:t)}function wa(n,e,t,a){(a||va).removeEventListener(n,e._wrapper||e,t)}function Sa(n,e){if(!o(n.data.on)||!o(e.data.on)){var t=e.data.on||{},a=n.data.on||{};va=e.elm,function(n){if(i(n.__r)){var e=W?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}i(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),ce(t,a,ka,wa,ya,e.context),va=void 0}}var Ea,Ta={create:Sa,update:Sa};function _a(n,e){if(!o(n.data.domProps)||!o(e.data.domProps)){var t,a,r=e.elm,s=n.data.domProps||{},l=e.data.domProps||{};for(t in i(l.__ob__)&&(l=e.data.domProps=C({},l)),s)t in l||(r[t]="");for(t in l){if(a=l[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),a===s[t])continue;1===r.childNodes.length&&r.removeChild(r.childNodes[0])}if("value"===t&&"PROGRESS"!==r.tagName){r._value=a;var c=o(a)?"":String(a);Ia(r,c)&&(r.value=c)}else if("innerHTML"===t&&Kt(r.tagName)&&o(r.innerHTML)){(Ea=Ea||document.createElement("div")).innerHTML="<svg>"+a+"</svg>";for(var d=Ea.firstChild;r.firstChild;)r.removeChild(r.firstChild);for(;d.firstChild;)r.appendChild(d.firstChild)}else if(a!==s[t])try{r[t]=a}catch(n){}}}}function Ia(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,a=n._vModifiers;if(i(a)){if(a.number)return f(t)!==f(e);if(a.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var ja={create:_a,update:_a},za=w((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var a=n.split(t);a.length>1&&(e[a[0].trim()]=a[1].trim())}})),e}));function Ca(n){var e=Aa(n.style);return n.staticStyle?C(n.staticStyle,e):e}function Aa(n){return Array.isArray(n)?A(n):"string"==typeof n?za(n):n}var Pa,Da=/^--/,qa=/\s*!important$/,La=function(n,e,t){if(Da.test(e))n.style.setProperty(e,t);else if(qa.test(t))n.style.setProperty(I(e),t.replace(qa,""),"important");else{var a=Oa(e);if(Array.isArray(t))for(var r=0,o=t.length;r<o;r++)n.style[a]=t[r];else n.style[a]=t}},Ba=["Webkit","Moz","ms"],Oa=w((function(n){if(Pa=Pa||document.createElement("div").style,"filter"!==(n=E(n))&&n in Pa)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<Ba.length;t++){var a=Ba[t]+e;if(a in Pa)return a}}));function Fa(n,e){var t=e.data,a=n.data;if(!(o(t.staticStyle)&&o(t.style)&&o(a.staticStyle)&&o(a.style))){var r,s,l=e.elm,c=a.staticStyle,d=a.normalizedStyle||a.style||{},p=c||d,u=Aa(e.data.style)||{};e.data.normalizedStyle=i(u.__ob__)?C({},u):u;var m=function(n,e){var t,a={};if(e)for(var r=n;r.componentInstance;)(r=r.componentInstance._vnode)&&r.data&&(t=Ca(r.data))&&C(a,t);(t=Ca(n.data))&&C(a,t);for(var o=n;o=o.parent;)o.data&&(t=Ca(o.data))&&C(a,t);return a}(e,!0);for(s in p)o(m[s])&&La(l,s,"");for(s in m)(r=m[s])!==p[s]&&La(l,s,null==r?"":r)}}var Ra={create:Fa,update:Fa},Ma=/\s+/;function Ha(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Ma).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" "+(n.getAttribute("class")||"")+" ";t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function Na(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Ma).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" "+(n.getAttribute("class")||"")+" ",a=" "+e+" ";t.indexOf(a)>=0;)t=t.replace(a," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Ua(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&C(e,Ja(n.name||"v")),C(e,n),e}return"string"==typeof n?Ja(n):void 0}}var Ja=w((function(n){return{enterClass:n+"-enter",enterToClass:n+"-enter-to",enterActiveClass:n+"-enter-active",leaveClass:n+"-leave",leaveToClass:n+"-leave-to",leaveActiveClass:n+"-leave-active"}})),$a=Z&&!G,Za="transition",Va="transitionend",Ka="animation",Qa="animationend";$a&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Za="WebkitTransition",Va="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Ka="WebkitAnimation",Qa="webkitAnimationEnd"));var Wa=Z?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Ga(n){Wa((function(){Wa(n)}))}function Xa(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),Ha(n,e))}function Ya(n,e){n._transitionClasses&&y(n._transitionClasses,e),Na(n,e)}function nr(n,e,t){var a=tr(n,e),r=a.type,o=a.timeout,i=a.propCount;if(!r)return t();var s="transition"===r?Va:Qa,l=0,c=function(){n.removeEventListener(s,d),t()},d=function(e){e.target===n&&++l>=i&&c()};setTimeout((function(){l<i&&c()}),o+1),n.addEventListener(s,d)}var er=/\b(transform|all)(,|$)/;function tr(n,e){var t,a=window.getComputedStyle(n),r=(a[Za+"Delay"]||"").split(", "),o=(a[Za+"Duration"]||"").split(", "),i=ar(r,o),s=(a[Ka+"Delay"]||"").split(", "),l=(a[Ka+"Duration"]||"").split(", "),c=ar(s,l),d=0,p=0;return"transition"===e?i>0&&(t="transition",d=i,p=o.length):"animation"===e?c>0&&(t="animation",d=c,p=l.length):p=(t=(d=Math.max(i,c))>0?i>c?"transition":"animation":null)?"transition"===t?o.length:l.length:0,{type:t,timeout:d,propCount:p,hasTransform:"transition"===t&&er.test(a[Za+"Property"])}}function ar(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return rr(e)+rr(n[t])})))}function rr(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function or(n,e){var t=n.elm;i(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var a=Ua(n.data.transition);if(!o(a)&&!i(t._enterCb)&&1===t.nodeType){for(var r=a.css,s=a.type,l=a.enterClass,d=a.enterToClass,p=a.enterActiveClass,u=a.appearClass,m=a.appearToClass,g=a.appearActiveClass,h=a.beforeEnter,v=a.enter,b=a.afterEnter,y=a.enterCancelled,x=a.beforeAppear,k=a.appear,w=a.afterAppear,S=a.appearCancelled,E=a.duration,T=Xe,_=Xe.$vnode;_&&_.parent;)T=_.context,_=_.parent;var I=!T._isMounted||!n.isRootInsert;if(!I||k||""===k){var j=I&&u?u:l,z=I&&g?g:p,C=I&&m?m:d,A=I&&x||h,P=I&&"function"==typeof k?k:v,D=I&&w||b,q=I&&S||y,L=f(c(E)?E.enter:E);0;var B=!1!==r&&!G,F=lr(P),R=t._enterCb=O((function(){B&&(Ya(t,C),Ya(t,z)),R.cancelled?(B&&Ya(t,j),q&&q(t)):D&&D(t),t._enterCb=null}));n.data.show||de(n,"insert",(function(){var e=t.parentNode,a=e&&e._pending&&e._pending[n.key];a&&a.tag===n.tag&&a.elm._leaveCb&&a.elm._leaveCb(),P&&P(t,R)})),A&&A(t),B&&(Xa(t,j),Xa(t,z),Ga((function(){Ya(t,j),R.cancelled||(Xa(t,C),F||(sr(L)?setTimeout(R,L):nr(t,s,R)))}))),n.data.show&&(e&&e(),P&&P(t,R)),B||F||R()}}}function ir(n,e){var t=n.elm;i(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var a=Ua(n.data.transition);if(o(a)||1!==t.nodeType)return e();if(!i(t._leaveCb)){var r=a.css,s=a.type,l=a.leaveClass,d=a.leaveToClass,p=a.leaveActiveClass,u=a.beforeLeave,m=a.leave,g=a.afterLeave,h=a.leaveCancelled,v=a.delayLeave,b=a.duration,y=!1!==r&&!G,x=lr(m),k=f(c(b)?b.leave:b);0;var w=t._leaveCb=O((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),y&&(Ya(t,d),Ya(t,p)),w.cancelled?(y&&Ya(t,l),h&&h(t)):(e(),g&&g(t)),t._leaveCb=null}));v?v(S):S()}function S(){w.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),u&&u(t),y&&(Xa(t,l),Xa(t,p),Ga((function(){Ya(t,l),w.cancelled||(Xa(t,d),x||(sr(k)?setTimeout(w,k):nr(t,s,w)))}))),m&&m(t,w),y||x||w())}}function sr(n){return"number"==typeof n&&!isNaN(n)}function lr(n){if(o(n))return!1;var e=n.fns;return i(e)?lr(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function cr(n,e){!0!==e.data.show&&or(e)}var dr=function(n){var e,t,a={},r=n.modules,c=n.nodeOps;for(e=0;e<ta.length;++e)for(a[ta[e]]=[],t=0;t<r.length;++t)i(r[t][ta[e]])&&a[ta[e]].push(r[t][ta[e]]);function d(n){var e=c.parentNode(n);i(e)&&c.removeChild(e,n)}function p(n,e,t,r,o,l,d){if(i(n.elm)&&i(l)&&(n=l[d]=xn(n)),n.isRootInsert=!o,!function(n,e,t,r){var o=n.data;if(i(o)){var l=i(n.componentInstance)&&o.keepAlive;if(i(o=o.hook)&&i(o=o.init)&&o(n,!1),i(n.componentInstance))return u(n,e),m(t,n.elm,r),s(l)&&function(n,e,t,r){var o,s=n;for(;s.componentInstance;)if(s=s.componentInstance._vnode,i(o=s.data)&&i(o=o.transition)){for(o=0;o<a.activate.length;++o)a.activate[o](ea,s);e.push(s);break}m(t,n.elm,r)}(n,e,t,r),!0}}(n,e,t,r)){var p=n.data,h=n.children,v=n.tag;i(v)?(n.elm=n.ns?c.createElementNS(n.ns,v):c.createElement(v,n),b(n),g(n,h,e),i(p)&&f(n,e),m(t,n.elm,r)):s(n.isComment)?(n.elm=c.createComment(n.text),m(t,n.elm,r)):(n.elm=c.createTextNode(n.text),m(t,n.elm,r))}}function u(n,e){i(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,h(n)?(f(n,e),b(n)):(na(n),e.push(n))}function m(n,e,t){i(n)&&(i(t)?c.parentNode(t)===n&&c.insertBefore(n,e,t):c.appendChild(n,e))}function g(n,e,t){if(Array.isArray(e)){0;for(var a=0;a<e.length;++a)p(e[a],t,n.elm,null,!0,e,a)}else l(n.text)&&c.appendChild(n.elm,c.createTextNode(String(n.text)))}function h(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return i(n.tag)}function f(n,t){for(var r=0;r<a.create.length;++r)a.create[r](ea,n);i(e=n.data.hook)&&(i(e.create)&&e.create(ea,n),i(e.insert)&&t.push(n))}function b(n){var e;if(i(e=n.fnScopeId))c.setStyleScope(n.elm,e);else for(var t=n;t;)i(e=t.context)&&i(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e),t=t.parent;i(e=Xe)&&e!==n.context&&e!==n.fnContext&&i(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e)}function y(n,e,t,a,r,o){for(;a<=r;++a)p(t[a],o,n,e,!1,t,a)}function x(n){var e,t,r=n.data;if(i(r))for(i(e=r.hook)&&i(e=e.destroy)&&e(n),e=0;e<a.destroy.length;++e)a.destroy[e](n);if(i(e=n.children))for(t=0;t<n.children.length;++t)x(n.children[t])}function k(n,e,t){for(;e<=t;++e){var a=n[e];i(a)&&(i(a.tag)?(w(a),x(a)):d(a.elm))}}function w(n,e){if(i(e)||i(n.data)){var t,r=a.remove.length+1;for(i(e)?e.listeners+=r:e=function(n,e){function t(){0==--t.listeners&&d(n)}return t.listeners=e,t}(n.elm,r),i(t=n.componentInstance)&&i(t=t._vnode)&&i(t.data)&&w(t,e),t=0;t<a.remove.length;++t)a.remove[t](n,e);i(t=n.data.hook)&&i(t=t.remove)?t(n,e):e()}else d(n.elm)}function S(n,e,t,a){for(var r=t;r<a;r++){var o=e[r];if(i(o)&&aa(n,o))return r}}function E(n,e,t,r,l,d){if(n!==e){i(e.elm)&&i(r)&&(e=r[l]=xn(e));var u=e.elm=n.elm;if(s(n.isAsyncPlaceholder))i(e.asyncFactory.resolved)?I(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(s(e.isStatic)&&s(n.isStatic)&&e.key===n.key&&(s(e.isCloned)||s(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,g=e.data;i(g)&&i(m=g.hook)&&i(m=m.prepatch)&&m(n,e);var f=n.children,v=e.children;if(i(g)&&h(e)){for(m=0;m<a.update.length;++m)a.update[m](n,e);i(m=g.hook)&&i(m=m.update)&&m(n,e)}o(e.text)?i(f)&&i(v)?f!==v&&function(n,e,t,a,r){var s,l,d,u=0,m=0,g=e.length-1,h=e[0],f=e[g],v=t.length-1,b=t[0],x=t[v],w=!r;for(0;u<=g&&m<=v;)o(h)?h=e[++u]:o(f)?f=e[--g]:aa(h,b)?(E(h,b,a,t,m),h=e[++u],b=t[++m]):aa(f,x)?(E(f,x,a,t,v),f=e[--g],x=t[--v]):aa(h,x)?(E(h,x,a,t,v),w&&c.insertBefore(n,h.elm,c.nextSibling(f.elm)),h=e[++u],x=t[--v]):aa(f,b)?(E(f,b,a,t,m),w&&c.insertBefore(n,f.elm,h.elm),f=e[--g],b=t[++m]):(o(s)&&(s=ra(e,u,g)),o(l=i(b.key)?s[b.key]:S(b,e,u,g))?p(b,a,n,h.elm,!1,t,m):aa(d=e[l],b)?(E(d,b,a,t,m),e[l]=void 0,w&&c.insertBefore(n,d.elm,h.elm)):p(b,a,n,h.elm,!1,t,m),b=t[++m]);u>g?y(n,o(t[v+1])?null:t[v+1].elm,t,m,v,a):m>v&&k(e,u,g)}(u,f,v,t,d):i(v)?(i(n.text)&&c.setTextContent(u,""),y(u,null,v,0,v.length-1,t)):i(f)?k(f,0,f.length-1):i(n.text)&&c.setTextContent(u,""):n.text!==e.text&&c.setTextContent(u,e.text),i(g)&&i(m=g.hook)&&i(m=m.postpatch)&&m(n,e)}}}function T(n,e,t){if(s(t)&&i(n.parent))n.parent.data.pendingInsert=e;else for(var a=0;a<e.length;++a)e[a].data.hook.insert(e[a])}var _=v("attrs,class,staticClass,staticStyle,key");function I(n,e,t,a){var r,o=e.tag,l=e.data,c=e.children;if(a=a||l&&l.pre,e.elm=n,s(e.isComment)&&i(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(i(l)&&(i(r=l.hook)&&i(r=r.init)&&r(e,!0),i(r=e.componentInstance)))return u(e,t),!0;if(i(o)){if(i(c))if(n.hasChildNodes())if(i(r=l)&&i(r=r.domProps)&&i(r=r.innerHTML)){if(r!==n.innerHTML)return!1}else{for(var d=!0,p=n.firstChild,m=0;m<c.length;m++){if(!p||!I(p,c[m],t,a)){d=!1;break}p=p.nextSibling}if(!d||p)return!1}else g(e,c,t);if(i(l)){var h=!1;for(var v in l)if(!_(v)){h=!0,f(e,t);break}!h&&l.class&&ie(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,r){if(!o(e)){var l,d=!1,u=[];if(o(n))d=!0,p(e,u);else{var m=i(n.nodeType);if(!m&&aa(n,e))E(n,e,u,null,null,r);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),s(t)&&I(n,e,u))return T(e,u,!0),n;l=n,n=new fn(c.tagName(l).toLowerCase(),{},[],void 0,l)}var g=n.elm,f=c.parentNode(g);if(p(e,u,g._leaveCb?null:f,c.nextSibling(g)),i(e.parent))for(var v=e.parent,b=h(e);v;){for(var y=0;y<a.destroy.length;++y)a.destroy[y](v);if(v.elm=e.elm,b){for(var w=0;w<a.create.length;++w)a.create[w](ea,v);var S=v.data.hook.insert;if(S.merged)for(var _=1;_<S.fns.length;_++)S.fns[_]()}else na(v);v=v.parent}i(f)?k([n],0,0):i(n.tag)&&x(n)}}return T(e,u,d),e.elm}i(n)&&x(n)}}({nodeOps:Xt,modules:[ha,ba,Ta,ja,Ra,Z?{create:cr,activate:cr,remove:function(n,e){!0!==n.data.show?ir(n,e):e()}}:{}].concat(pa)});G&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&br(n,"input")}));var pr={inserted:function(n,e,t,a){"select"===t.tag?(a.elm&&!a.elm._vOptions?de(t,"postpatch",(function(){pr.componentUpdated(n,e,t)})):ur(n,e,t.context),n._vOptions=[].map.call(n.options,hr)):("textarea"===t.tag||Gt(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",fr),n.addEventListener("compositionend",vr),n.addEventListener("change",vr),G&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){ur(n,e,t.context);var a=n._vOptions,r=n._vOptions=[].map.call(n.options,hr);if(r.some((function(n,e){return!L(n,a[e])})))(n.multiple?e.value.some((function(n){return gr(n,r)})):e.value!==e.oldValue&&gr(e.value,r))&&br(n,"change")}}};function ur(n,e,t){mr(n,e,t),(W||X)&&setTimeout((function(){mr(n,e,t)}),0)}function mr(n,e,t){var a=e.value,r=n.multiple;if(!r||Array.isArray(a)){for(var o,i,s=0,l=n.options.length;s<l;s++)if(i=n.options[s],r)o=B(a,hr(i))>-1,i.selected!==o&&(i.selected=o);else if(L(hr(i),a))return void(n.selectedIndex!==s&&(n.selectedIndex=s));r||(n.selectedIndex=-1)}}function gr(n,e){return e.every((function(e){return!L(e,n)}))}function hr(n){return"_value"in n?n._value:n.value}function fr(n){n.target.composing=!0}function vr(n){n.target.composing&&(n.target.composing=!1,br(n.target,"input"))}function br(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function yr(n){return!n.componentInstance||n.data&&n.data.transition?n:yr(n.componentInstance._vnode)}var xr={model:pr,show:{bind:function(n,e,t){var a=e.value,r=(t=yr(t)).data&&t.data.transition,o=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;a&&r?(t.data.show=!0,or(t,(function(){n.style.display=o}))):n.style.display=a?o:"none"},update:function(n,e,t){var a=e.value;!a!=!e.oldValue&&((t=yr(t)).data&&t.data.transition?(t.data.show=!0,a?or(t,(function(){n.style.display=n.__vOriginalDisplay})):ir(t,(function(){n.style.display="none"}))):n.style.display=a?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,a,r){r||(n.style.display=n.__vOriginalDisplay)}}},kr={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function wr(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?wr(Ve(e.children)):n}function Sr(n){var e={},t=n.$options;for(var a in t.propsData)e[a]=n[a];var r=t._parentListeners;for(var o in r)e[E(o)]=r[o];return e}function Er(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Tr=function(n){return n.tag||ve(n)},_r=function(n){return"show"===n.name},Ir={name:"transition",props:kr,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Tr)).length){0;var a=this.mode;0;var r=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return r;var o=wr(r);if(!o)return r;if(this._leaving)return Er(n,r);var i="__transition-"+this._uid+"-";o.key=null==o.key?o.isComment?i+"comment":i+o.tag:l(o.key)?0===String(o.key).indexOf(i)?o.key:i+o.key:o.key;var s=(o.data||(o.data={})).transition=Sr(this),c=this._vnode,d=wr(c);if(o.data.directives&&o.data.directives.some(_r)&&(o.data.show=!0),d&&d.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(o,d)&&!ve(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var p=d.data.transition=C({},s);if("out-in"===a)return this._leaving=!0,de(p,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Er(n,r);if("in-out"===a){if(ve(o))return c;var u,m=function(){u()};de(s,"afterEnter",m),de(s,"enterCancelled",m),de(p,"delayLeave",(function(n){u=n}))}}return r}}},jr=C({tag:String,moveClass:String},kr);function zr(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Cr(n){n.data.newPos=n.elm.getBoundingClientRect()}function Ar(n){var e=n.data.pos,t=n.data.newPos,a=e.left-t.left,r=e.top-t.top;if(a||r){n.data.moved=!0;var o=n.elm.style;o.transform=o.WebkitTransform="translate("+a+"px,"+r+"px)",o.transitionDuration="0s"}}delete jr.mode;var Pr={Transition:Ir,TransitionGroup:{props:jr,beforeMount:function(){var n=this,e=this._update;this._update=function(t,a){var r=Ye(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,r(),e.call(n,t,a)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),a=this.prevChildren=this.children,r=this.$slots.default||[],o=this.children=[],i=Sr(this),s=0;s<r.length;s++){var l=r[s];if(l.tag)if(null!=l.key&&0!==String(l.key).indexOf("__vlist"))o.push(l),t[l.key]=l,(l.data||(l.data={})).transition=i;else;}if(a){for(var c=[],d=[],p=0;p<a.length;p++){var u=a[p];u.data.transition=i,u.data.pos=u.elm.getBoundingClientRect(),t[u.key]?c.push(u):d.push(u)}this.kept=n(e,null,c),this.removed=d}return n(e,null,o)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(zr),n.forEach(Cr),n.forEach(Ar),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,a=t.style;Xa(t,e),a.transform=a.WebkitTransform=a.transitionDuration="",t.addEventListener(Va,t._moveCb=function n(a){a&&a.target!==t||a&&!/transform$/.test(a.propertyName)||(t.removeEventListener(Va,n),t._moveCb=null,Ya(t,e))})}})))},methods:{hasMove:function(n,e){if(!$a)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){Na(t,n)})),Ha(t,e),t.style.display="none",this.$el.appendChild(t);var a=tr(t);return this.$el.removeChild(t),this._hasMove=a.hasTransform}}}};Tt.config.mustUseProp=function(n,e,t){return"value"===t&&qt(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Tt.config.isReservedTag=Qt,Tt.config.isReservedAttr=Dt,Tt.config.getTagNamespace=function(n){return Kt(n)?"svg":"math"===n?"math":void 0},Tt.config.isUnknownElement=function(n){if(!Z)return!0;if(Qt(n))return!1;if(n=n.toLowerCase(),null!=Wt[n])return Wt[n];var e=document.createElement(n);return n.indexOf("-")>-1?Wt[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Wt[n]=/HTMLUnknownElement/.test(e.toString())},C(Tt.options.directives,xr),C(Tt.options.components,Pr),Tt.prototype.__patch__=Z?dr:P,Tt.prototype.$mount=function(n,e){return function(n,e,t){var a;return n.$el=e,n.$options.render||(n.$options.render=bn),tt(n,"beforeMount"),a=function(){n._update(n._render(),t)},new gt(n,a,P,{before:function(){n._isMounted&&!n._isDestroyed&&tt(n,"beforeUpdate")}},!0),t=!1,null==n.$vnode&&(n._isMounted=!0,tt(n,"mounted")),n}(this,n=n&&Z?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},Z&&setTimeout((function(){M.devtools&&on&&on.emit("init",Tt)}),0);var Dr=Tt;
/*!
  * vue-router v3.5.3
  * (c) 2021 Evan You
  * @license MIT
  */function qr(n,e){for(var t in e)n[t]=e[t];return n}var Lr=/[!'()*]/g,Br=function(n){return"%"+n.charCodeAt(0).toString(16)},Or=/%2C/g,Fr=function(n){return encodeURIComponent(n).replace(Lr,Br).replace(Or,",")};function Rr(n){try{return decodeURIComponent(n)}catch(n){0}return n}var Mr=function(n){return null==n||"object"==typeof n?n:String(n)};function Hr(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),a=Rr(t.shift()),r=t.length>0?Rr(t.join("=")):null;void 0===e[a]?e[a]=r:Array.isArray(e[a])?e[a].push(r):e[a]=[e[a],r]})),e):e}function Nr(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return Fr(e);if(Array.isArray(t)){var a=[];return t.forEach((function(n){void 0!==n&&(null===n?a.push(Fr(e)):a.push(Fr(e)+"="+Fr(n)))})),a.join("&")}return Fr(e)+"="+Fr(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var Ur=/\/?$/;function Jr(n,e,t,a){var r=a&&a.options.stringifyQuery,o=e.query||{};try{o=$r(o)}catch(n){}var i={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:o,params:e.params||{},fullPath:Kr(e,r),matched:n?Vr(n):[]};return t&&(i.redirectedFrom=Kr(t,r)),Object.freeze(i)}function $r(n){if(Array.isArray(n))return n.map($r);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=$r(n[t]);return e}return n}var Zr=Jr(null,{path:"/"});function Vr(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function Kr(n,e){var t=n.path,a=n.query;void 0===a&&(a={});var r=n.hash;return void 0===r&&(r=""),(t||"/")+(e||Nr)(a)+r}function Qr(n,e,t){return e===Zr?n===e:!!e&&(n.path&&e.path?n.path.replace(Ur,"")===e.path.replace(Ur,"")&&(t||n.hash===e.hash&&Wr(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&Wr(n.query,e.query)&&Wr(n.params,e.params))))}function Wr(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),a=Object.keys(e).sort();return t.length===a.length&&t.every((function(t,r){var o=n[t];if(a[r]!==t)return!1;var i=e[t];return null==o||null==i?o===i:"object"==typeof o&&"object"==typeof i?Wr(o,i):String(o)===String(i)}))}function Gr(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var a in t.instances){var r=t.instances[a],o=t.enteredCbs[a];if(r&&o){delete t.enteredCbs[a];for(var i=0;i<o.length;i++)r._isBeingDestroyed||o[i](r)}}}}var Xr={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,a=e.children,r=e.parent,o=e.data;o.routerView=!0;for(var i=r.$createElement,s=t.name,l=r.$route,c=r._routerViewCache||(r._routerViewCache={}),d=0,p=!1;r&&r._routerRoot!==r;){var u=r.$vnode?r.$vnode.data:{};u.routerView&&d++,u.keepAlive&&r._directInactive&&r._inactive&&(p=!0),r=r.$parent}if(o.routerViewDepth=d,p){var m=c[s],g=m&&m.component;return g?(m.configProps&&Yr(g,o,m.route,m.configProps),i(g,o,a)):i()}var h=l.matched[d],f=h&&h.components[s];if(!h||!f)return c[s]=null,i();c[s]={component:f},o.registerRouteInstance=function(n,e){var t=h.instances[s];(e&&t!==n||!e&&t===n)&&(h.instances[s]=e)},(o.hook||(o.hook={})).prepatch=function(n,e){h.instances[s]=e.componentInstance},o.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==h.instances[s]&&(h.instances[s]=n.componentInstance),Gr(l)};var v=h.props&&h.props[s];return v&&(qr(c[s],{route:l,configProps:v}),Yr(f,o,l,v)),i(f,o,a)}};function Yr(n,e,t,a){var r=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,a);if(r){r=e.props=qr({},r);var o=e.attrs=e.attrs||{};for(var i in r)n.props&&i in n.props||(o[i]=r[i],delete r[i])}}function no(n,e,t){var a=n.charAt(0);if("/"===a)return n;if("?"===a||"#"===a)return e+n;var r=e.split("/");t&&r[r.length-1]||r.pop();for(var o=n.replace(/^\//,"").split("/"),i=0;i<o.length;i++){var s=o[i];".."===s?r.pop():"."!==s&&r.push(s)}return""!==r[0]&&r.unshift(""),r.join("/")}function eo(n){return n.replace(/\/+/g,"/")}var to=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},ao=bo,ro=co,oo=function(n,e){return uo(co(n,e),e)},io=uo,so=vo,lo=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function co(n,e){for(var t,a=[],r=0,o=0,i="",s=e&&e.delimiter||"/";null!=(t=lo.exec(n));){var l=t[0],c=t[1],d=t.index;if(i+=n.slice(o,d),o=d+l.length,c)i+=c[1];else{var p=n[o],u=t[2],m=t[3],g=t[4],h=t[5],f=t[6],v=t[7];i&&(a.push(i),i="");var b=null!=u&&null!=p&&p!==u,y="+"===f||"*"===f,x="?"===f||"*"===f,k=t[2]||s,w=g||h;a.push({name:m||r++,prefix:u||"",delimiter:k,optional:x,repeat:y,partial:b,asterisk:!!v,pattern:w?go(w):v?".*":"[^"+mo(k)+"]+?"})}}return o<n.length&&(i+=n.substr(o)),i&&a.push(i),a}function po(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function uo(n,e){for(var t=new Array(n.length),a=0;a<n.length;a++)"object"==typeof n[a]&&(t[a]=new RegExp("^(?:"+n[a].pattern+")$",fo(e)));return function(e,a){for(var r="",o=e||{},i=(a||{}).pretty?po:encodeURIComponent,s=0;s<n.length;s++){var l=n[s];if("string"!=typeof l){var c,d=o[l.name];if(null==d){if(l.optional){l.partial&&(r+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(to(d)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var p=0;p<d.length;p++){if(c=i(d[p]),!t[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");r+=(0===p?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(d).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):i(d),!t[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');r+=l.prefix+c}}else r+=l}return r}}function mo(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function go(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function ho(n,e){return n.keys=e,n}function fo(n){return n&&n.sensitive?"":"i"}function vo(n,e,t){to(e)||(t=e||t,e=[]);for(var a=(t=t||{}).strict,r=!1!==t.end,o="",i=0;i<n.length;i++){var s=n[i];if("string"==typeof s)o+=mo(s);else{var l=mo(s.prefix),c="(?:"+s.pattern+")";e.push(s),s.repeat&&(c+="(?:"+l+c+")*"),o+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var d=mo(t.delimiter||"/"),p=o.slice(-d.length)===d;return a||(o=(p?o.slice(0,-d.length):o)+"(?:"+d+"(?=$))?"),o+=r?"$":a&&p?"":"(?="+d+"|$)",ho(new RegExp("^"+o,fo(t)),e)}function bo(n,e,t){return to(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var a=0;a<t.length;a++)e.push({name:a,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return ho(n,e)}(n,e):to(n)?function(n,e,t){for(var a=[],r=0;r<n.length;r++)a.push(bo(n[r],e,t).source);return ho(new RegExp("(?:"+a.join("|")+")",fo(t)),e)}(n,e,t):function(n,e,t){return vo(co(n,t),e,t)}(n,e,t)}ao.parse=ro,ao.compile=oo,ao.tokensToFunction=io,ao.tokensToRegExp=so;var yo=Object.create(null);function xo(n,e,t){e=e||{};try{var a=yo[n]||(yo[n]=ao.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),a(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function ko(n,e,t,a){var r="string"==typeof n?{path:n}:n;if(r._normalized)return r;if(r.name){var o=(r=qr({},n)).params;return o&&"object"==typeof o&&(r.params=qr({},o)),r}if(!r.path&&r.params&&e){(r=qr({},r))._normalized=!0;var i=qr(qr({},e.params),r.params);if(e.name)r.name=e.name,r.params=i;else if(e.matched.length){var s=e.matched[e.matched.length-1].path;r.path=xo(s,i,e.path)}else 0;return r}var l=function(n){var e="",t="",a=n.indexOf("#");a>=0&&(e=n.slice(a),n=n.slice(0,a));var r=n.indexOf("?");return r>=0&&(t=n.slice(r+1),n=n.slice(0,r)),{path:n,query:t,hash:e}}(r.path||""),c=e&&e.path||"/",d=l.path?no(l.path,c,t||r.append):c,p=function(n,e,t){void 0===e&&(e={});var a,r=t||Hr;try{a=r(n||"")}catch(n){a={}}for(var o in e){var i=e[o];a[o]=Array.isArray(i)?i.map(Mr):Mr(i)}return a}(l.query,r.query,a&&a.options.parseQuery),u=r.hash||l.hash;return u&&"#"!==u.charAt(0)&&(u="#"+u),{_normalized:!0,path:d,query:p,hash:u}}var wo,So=function(){},Eo={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,a=this.$route,r=t.resolve(this.to,a,this.append),o=r.location,i=r.route,s=r.href,l={},c=t.options.linkActiveClass,d=t.options.linkExactActiveClass,p=null==c?"router-link-active":c,u=null==d?"router-link-exact-active":d,m=null==this.activeClass?p:this.activeClass,g=null==this.exactActiveClass?u:this.exactActiveClass,h=i.redirectedFrom?Jr(null,ko(i.redirectedFrom),null,t):i;l[g]=Qr(a,h,this.exactPath),l[m]=this.exact||this.exactPath?l[g]:function(n,e){return 0===n.path.replace(Ur,"/").indexOf(e.path.replace(Ur,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(a,h);var f=l[g]?this.ariaCurrentValue:null,v=function(n){To(n)&&(e.replace?t.replace(o,So):t.push(o,So))},b={click:To};Array.isArray(this.event)?this.event.forEach((function(n){b[n]=v})):b[this.event]=v;var y={class:l},x=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:i,navigate:v,isActive:l[m],isExactActive:l[g]});if(x){if(1===x.length)return x[0];if(x.length>1||!x.length)return 0===x.length?n():n("span",{},x)}if("a"===this.tag)y.on=b,y.attrs={href:s,"aria-current":f};else{var k=function n(e){var t;if(e)for(var a=0;a<e.length;a++){if("a"===(t=e[a]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(k){k.isStatic=!1;var w=k.data=qr({},k.data);for(var S in w.on=w.on||{},w.on){var E=w.on[S];S in b&&(w.on[S]=Array.isArray(E)?E:[E])}for(var T in b)T in w.on?w.on[T].push(b[T]):w.on[T]=v;var _=k.data.attrs=qr({},k.data.attrs);_.href=s,_["aria-current"]=f}else y.on=b}return n(this.tag,y,this.$slots.default)}};function To(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var _o="undefined"!=typeof window;function Io(n,e,t,a,r){var o=e||[],i=t||Object.create(null),s=a||Object.create(null);n.forEach((function(n){!function n(e,t,a,r,o,i){var s=r.path,l=r.name;0;var c=r.pathToRegexpOptions||{},d=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return eo(e.path+"/"+n)}(s,o,c.strict);"boolean"==typeof r.caseSensitive&&(c.sensitive=r.caseSensitive);var p={path:d,regex:jo(d,c),components:r.components||{default:r.component},alias:r.alias?"string"==typeof r.alias?[r.alias]:r.alias:[],instances:{},enteredCbs:{},name:l,parent:o,matchAs:i,redirect:r.redirect,beforeEnter:r.beforeEnter,meta:r.meta||{},props:null==r.props?{}:r.components?r.props:{default:r.props}};r.children&&r.children.forEach((function(r){var o=i?eo(i+"/"+r.path):void 0;n(e,t,a,r,p,o)}));t[p.path]||(e.push(p.path),t[p.path]=p);if(void 0!==r.alias)for(var u=Array.isArray(r.alias)?r.alias:[r.alias],m=0;m<u.length;++m){0;var g={path:u[m],children:r.children};n(e,t,a,g,o,p.path||"/")}l&&(a[l]||(a[l]=p))}(o,i,s,n,r)}));for(var l=0,c=o.length;l<c;l++)"*"===o[l]&&(o.push(o.splice(l,1)[0]),c--,l--);return{pathList:o,pathMap:i,nameMap:s}}function jo(n,e){return ao(n,[],e)}function zo(n,e){var t=Io(n),a=t.pathList,r=t.pathMap,o=t.nameMap;function i(n,t,i){var s=ko(n,t,!1,e),c=s.name;if(c){var d=o[c];if(!d)return l(null,s);var p=d.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var u in t.params)!(u in s.params)&&p.indexOf(u)>-1&&(s.params[u]=t.params[u]);return s.path=xo(d.path,s.params),l(d,s,i)}if(s.path){s.params={};for(var m=0;m<a.length;m++){var g=a[m],h=r[g];if(Co(h.regex,s.path,s.params))return l(h,s,i)}}return l(null,s)}function s(n,t){var a=n.redirect,r="function"==typeof a?a(Jr(n,t,null,e)):a;if("string"==typeof r&&(r={path:r}),!r||"object"!=typeof r)return l(null,t);var s=r,c=s.name,d=s.path,p=t.query,u=t.hash,m=t.params;if(p=s.hasOwnProperty("query")?s.query:p,u=s.hasOwnProperty("hash")?s.hash:u,m=s.hasOwnProperty("params")?s.params:m,c){o[c];return i({_normalized:!0,name:c,query:p,hash:u,params:m},void 0,t)}if(d){var g=function(n,e){return no(n,e.parent?e.parent.path:"/",!0)}(d,n);return i({_normalized:!0,path:xo(g,m),query:p,hash:u},void 0,t)}return l(null,t)}function l(n,t,a){return n&&n.redirect?s(n,a||t):n&&n.matchAs?function(n,e,t){var a=i({_normalized:!0,path:xo(t,e.params)});if(a){var r=a.matched,o=r[r.length-1];return e.params=a.params,l(o,e)}return l(null,e)}(0,t,n.matchAs):Jr(n,t,a,e)}return{match:i,addRoute:function(n,e){var t="object"!=typeof n?o[n]:void 0;Io([e||n],a,r,o,t),t&&t.alias.length&&Io(t.alias.map((function(n){return{path:n,children:[e]}})),a,r,o,t)},getRoutes:function(){return a.map((function(n){return r[n]}))},addRoutes:function(n){Io(n,a,r,o)}}}function Co(n,e,t){var a=e.match(n);if(!a)return!1;if(!t)return!0;for(var r=1,o=a.length;r<o;++r){var i=n.keys[r-1];i&&(t[i.name||"pathMatch"]="string"==typeof a[r]?Rr(a[r]):a[r])}return!0}var Ao=_o&&window.performance&&window.performance.now?window.performance:Date;function Po(){return Ao.now().toFixed(3)}var Do=Po();function qo(){return Do}function Lo(n){return Do=n}var Bo=Object.create(null);function Oo(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=qr({},window.history.state);return t.key=qo(),window.history.replaceState(t,"",e),window.addEventListener("popstate",Mo),function(){window.removeEventListener("popstate",Mo)}}function Fo(n,e,t,a){if(n.app){var r=n.options.scrollBehavior;r&&n.app.$nextTick((function(){var o=function(){var n=qo();if(n)return Bo[n]}(),i=r.call(n,e,t,a?o:null);i&&("function"==typeof i.then?i.then((function(n){$o(n,o)})).catch((function(n){0})):$o(i,o))}))}}function Ro(){var n=qo();n&&(Bo[n]={x:window.pageXOffset,y:window.pageYOffset})}function Mo(n){Ro(),n.state&&n.state.key&&Lo(n.state.key)}function Ho(n){return Uo(n.x)||Uo(n.y)}function No(n){return{x:Uo(n.x)?n.x:window.pageXOffset,y:Uo(n.y)?n.y:window.pageYOffset}}function Uo(n){return"number"==typeof n}var Jo=/^#\d/;function $o(n,e){var t,a="object"==typeof n;if(a&&"string"==typeof n.selector){var r=Jo.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(r){var o=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),a=n.getBoundingClientRect();return{x:a.left-t.left-e.x,y:a.top-t.top-e.y}}(r,o={x:Uo((t=o).x)?t.x:0,y:Uo(t.y)?t.y:0})}else Ho(n)&&(e=No(n))}else a&&Ho(n)&&(e=No(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var Zo,Vo=_o&&((-1===(Zo=window.navigator.userAgent).indexOf("Android 2.")&&-1===Zo.indexOf("Android 4.0")||-1===Zo.indexOf("Mobile Safari")||-1!==Zo.indexOf("Chrome")||-1!==Zo.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Ko(n,e){Ro();var t=window.history;try{if(e){var a=qr({},t.state);a.key=qo(),t.replaceState(a,"",n)}else t.pushState({key:Lo(Po())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function Qo(n){Ko(n,!0)}function Wo(n,e,t){var a=function(r){r>=n.length?t():n[r]?e(n[r],(function(){a(r+1)})):a(r+1)};a(0)}var Go={redirected:2,aborted:4,cancelled:8,duplicated:16};function Xo(n,e){return ni(n,e,Go.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return ei.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function Yo(n,e){return ni(n,e,Go.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function ni(n,e,t,a){var r=new Error(a);return r._isRouter=!0,r.from=n,r.to=e,r.type=t,r}var ei=["params","query","hash"];function ti(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function ai(n,e){return ti(n)&&n._isRouter&&(null==e||n.type===e)}function ri(n){return function(e,t,a){var r=!1,o=0,i=null;oi(n,(function(n,e,t,s){if("function"==typeof n&&void 0===n.cid){r=!0,o++;var l,c=li((function(e){var r;((r=e).__esModule||si&&"Module"===r[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:wo.extend(e),t.components[s]=e,--o<=0&&a()})),d=li((function(n){var e="Failed to resolve async component "+s+": "+n;i||(i=ti(n)?n:new Error(e),a(i))}));try{l=n(c,d)}catch(n){d(n)}if(l)if("function"==typeof l.then)l.then(c,d);else{var p=l.component;p&&"function"==typeof p.then&&p.then(c,d)}}})),r||a()}}function oi(n,e){return ii(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function ii(n){return Array.prototype.concat.apply([],n)}var si="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function li(n){var e=!1;return function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];if(!e)return e=!0,n.apply(this,t)}}var ci=function(n,e){this.router=n,this.base=function(n){if(!n)if(_o){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=Zr,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function di(n,e,t,a){var r=oi(n,(function(n,a,r,o){var i=function(n,e){"function"!=typeof n&&(n=wo.extend(n));return n.options[e]}(n,e);if(i)return Array.isArray(i)?i.map((function(n){return t(n,a,r,o)})):t(i,a,r,o)}));return ii(a?r.reverse():r)}function pi(n,e){if(e)return function(){return n.apply(e,arguments)}}ci.prototype.listen=function(n){this.cb=n},ci.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},ci.prototype.onError=function(n){this.errorCbs.push(n)},ci.prototype.transitionTo=function(n,e,t){var a,r=this;try{a=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var o=this.current;this.confirmTransition(a,(function(){r.updateRoute(a),e&&e(a),r.ensureURL(),r.router.afterHooks.forEach((function(n){n&&n(a,o)})),r.ready||(r.ready=!0,r.readyCbs.forEach((function(n){n(a)})))}),(function(n){t&&t(n),n&&!r.ready&&(ai(n,Go.redirected)&&o===Zr||(r.ready=!0,r.readyErrorCbs.forEach((function(e){e(n)}))))}))},ci.prototype.confirmTransition=function(n,e,t){var a=this,r=this.current;this.pending=n;var o,i,s=function(n){!ai(n)&&ti(n)&&(a.errorCbs.length?a.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},l=n.matched.length-1,c=r.matched.length-1;if(Qr(n,r)&&l===c&&n.matched[l]===r.matched[c])return this.ensureURL(),n.hash&&Fo(this.router,r,n,!1),s(((i=ni(o=r,n,Go.duplicated,'Avoided redundant navigation to current location: "'+o.fullPath+'".')).name="NavigationDuplicated",i));var d=function(n,e){var t,a=Math.max(n.length,e.length);for(t=0;t<a&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),p=d.updated,u=d.deactivated,m=d.activated,g=[].concat(function(n){return di(n,"beforeRouteLeave",pi,!0)}(u),this.router.beforeHooks,function(n){return di(n,"beforeRouteUpdate",pi)}(p),m.map((function(n){return n.beforeEnter})),ri(m)),h=function(e,t){if(a.pending!==n)return s(Yo(r,n));try{e(n,r,(function(e){!1===e?(a.ensureURL(!0),s(function(n,e){return ni(n,e,Go.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(r,n))):ti(e)?(a.ensureURL(!0),s(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(s(Xo(r,n)),"object"==typeof e&&e.replace?a.replace(e):a.push(e)):t(e)}))}catch(n){s(n)}};Wo(g,h,(function(){Wo(function(n){return di(n,"beforeRouteEnter",(function(n,e,t,a){return function(n,e,t){return function(a,r,o){return n(a,r,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),o(n)}))}}(n,t,a)}))}(m).concat(a.router.resolveHooks),h,(function(){if(a.pending!==n)return s(Yo(r,n));a.pending=null,e(n),a.router.app&&a.router.app.$nextTick((function(){Gr(n)}))}))}))},ci.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},ci.prototype.setupListeners=function(){},ci.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=Zr,this.pending=null};var ui=function(n){function e(e,t){n.call(this,e,t),this._startLocation=mi(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,a=Vo&&t;a&&this.listeners.push(Oo());var r=function(){var t=n.current,r=mi(n.base);n.current===Zr&&r===n._startLocation||n.transitionTo(r,(function(n){a&&Fo(e,n,t,!0)}))};window.addEventListener("popstate",r),this.listeners.push((function(){window.removeEventListener("popstate",r)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var a=this,r=this.current;this.transitionTo(n,(function(n){Ko(eo(a.base+n.fullPath)),Fo(a.router,n,r,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this,r=this.current;this.transitionTo(n,(function(n){Qo(eo(a.base+n.fullPath)),Fo(a.router,n,r,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(mi(this.base)!==this.current.fullPath){var e=eo(this.base+this.current.fullPath);n?Ko(e):Qo(e)}},e.prototype.getCurrentLocation=function(){return mi(this.base)},e}(ci);function mi(n){var e=window.location.pathname,t=e.toLowerCase(),a=n.toLowerCase();return!n||t!==a&&0!==t.indexOf(eo(a+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var gi=function(n){function e(e,t,a){n.call(this,e,t),a&&function(n){var e=mi(n);if(!/^\/#/.test(e))return window.location.replace(eo(n+"/#"+e)),!0}(this.base)||hi()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=Vo&&e;t&&this.listeners.push(Oo());var a=function(){var e=n.current;hi()&&n.transitionTo(fi(),(function(a){t&&Fo(n.router,a,e,!0),Vo||yi(a.fullPath)}))},r=Vo?"popstate":"hashchange";window.addEventListener(r,a),this.listeners.push((function(){window.removeEventListener(r,a)}))}},e.prototype.push=function(n,e,t){var a=this,r=this.current;this.transitionTo(n,(function(n){bi(n.fullPath),Fo(a.router,n,r,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this,r=this.current;this.transitionTo(n,(function(n){yi(n.fullPath),Fo(a.router,n,r,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;fi()!==e&&(n?bi(e):yi(e))},e.prototype.getCurrentLocation=function(){return fi()},e}(ci);function hi(){var n=fi();return"/"===n.charAt(0)||(yi("/"+n),!1)}function fi(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function vi(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function bi(n){Vo?Ko(vi(n)):window.location.hash=n}function yi(n){Vo?Qo(vi(n)):window.location.replace(vi(n))}var xi=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var a=this;this.transitionTo(n,(function(n){a.stack=a.stack.slice(0,a.index+1).concat(n),a.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this;this.transitionTo(n,(function(n){a.stack=a.stack.slice(0,a.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var a=this.stack[t];this.confirmTransition(a,(function(){var n=e.current;e.index=t,e.updateRoute(a),e.router.afterHooks.forEach((function(e){e&&e(a,n)}))}),(function(n){ai(n,Go.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(ci),ki=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=zo(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!Vo&&!1!==n.fallback,this.fallback&&(e="hash"),_o||(e="abstract"),this.mode=e,e){case"history":this.history=new ui(this,n.base);break;case"hash":this.history=new gi(this,n.base,this.fallback);break;case"abstract":this.history=new xi(this,n.base);break;default:0}},wi={currentRoute:{configurable:!0}};function Si(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}ki.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},wi.currentRoute.get=function(){return this.history&&this.history.current},ki.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof ui||t instanceof gi){var a=function(n){t.setupListeners(),function(n){var a=t.current,r=e.options.scrollBehavior;Vo&&r&&"fullPath"in n&&Fo(e,n,a,!1)}(n)};t.transitionTo(t.getCurrentLocation(),a,a)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},ki.prototype.beforeEach=function(n){return Si(this.beforeHooks,n)},ki.prototype.beforeResolve=function(n){return Si(this.resolveHooks,n)},ki.prototype.afterEach=function(n){return Si(this.afterHooks,n)},ki.prototype.onReady=function(n,e){this.history.onReady(n,e)},ki.prototype.onError=function(n){this.history.onError(n)},ki.prototype.push=function(n,e,t){var a=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){a.history.push(n,e,t)}));this.history.push(n,e,t)},ki.prototype.replace=function(n,e,t){var a=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){a.history.replace(n,e,t)}));this.history.replace(n,e,t)},ki.prototype.go=function(n){this.history.go(n)},ki.prototype.back=function(){this.go(-1)},ki.prototype.forward=function(){this.go(1)},ki.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},ki.prototype.resolve=function(n,e,t){var a=ko(n,e=e||this.history.current,t,this),r=this.match(a,e),o=r.redirectedFrom||r.fullPath;return{location:a,route:r,href:function(n,e,t){var a="hash"===t?"#"+e:e;return n?eo(n+"/"+a):a}(this.history.base,o,this.mode),normalizedTo:a,resolved:r}},ki.prototype.getRoutes=function(){return this.matcher.getRoutes()},ki.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==Zr&&this.history.transitionTo(this.history.getCurrentLocation())},ki.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==Zr&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(ki.prototype,wi),ki.install=function n(e){if(!n.installed||wo!==e){n.installed=!0,wo=e;var t=function(n){return void 0!==n},a=function(n,e){var a=n.$options._parentVnode;t(a)&&t(a=a.data)&&t(a=a.registerRouteInstance)&&a(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,a(this,this)},destroyed:function(){a(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",Xr),e.component("RouterLink",Eo);var r=e.config.optionMergeStrategies;r.beforeRouteEnter=r.beforeRouteLeave=r.beforeRouteUpdate=r.created}},ki.version="3.5.3",ki.isNavigationFailure=ai,ki.NavigationFailureType=Go,ki.START_LOCATION=Zr,_o&&window.Vue&&window.Vue.use(ki);var Ei=ki;t(181),t(182),t(258),t(76),t(183),t(22),t(23),t(260);function Ti(n){n.locales&&Object.keys(n.locales).forEach((function(e){n.locales[e].path=e})),Object.freeze(n)}t(71),t(93),t(127);function _i(n){return(_i="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n})(n)}var Ii=t(73),ji=(t(192),t(16),t(39),t(233),t(234),t(41),t(26),{NotFound:function(){return Promise.all([t.e(0),t.e(8)]).then(t.bind(null,504))},Layout:function(){return Promise.all([t.e(0),t.e(3),t.e(2)]).then(t.bind(null,503))}}),zi={"v-28a7b348":function(){return t.e(10).then(t.bind(null,508))},"v-7ab55fd6":function(){return t.e(12).then(t.bind(null,509))},"v-e58ada02":function(){return t.e(11).then(t.bind(null,510))},"v-89816e08":function(){return t.e(9).then(t.bind(null,511))},"v-16106010":function(){return t.e(13).then(t.bind(null,512))},"v-159f3ccd":function(){return t.e(14).then(t.bind(null,513))},"v-2f6093d7":function(){return t.e(15).then(t.bind(null,514))},"v-c36f6bfc":function(){return t.e(17).then(t.bind(null,515))},"v-11649cf0":function(){return t.e(18).then(t.bind(null,516))},"v-5bd2c1fd":function(){return t.e(20).then(t.bind(null,517))},"v-6441954f":function(){return t.e(16).then(t.bind(null,518))},"v-2e467b68":function(){return t.e(21).then(t.bind(null,519))},"v-a61f04aa":function(){return t.e(19).then(t.bind(null,520))},"v-1c0c6895":function(){return t.e(25).then(t.bind(null,521))},"v-320a16ea":function(){return t.e(22).then(t.bind(null,522))},"v-644b5bf5":function(){return t.e(26).then(t.bind(null,523))},"v-6e67de15":function(){return t.e(27).then(t.bind(null,524))},"v-055ee976":function(){return t.e(24).then(t.bind(null,525))},"v-6ddb2230":function(){return t.e(23).then(t.bind(null,526))},"v-d520fc7e":function(){return t.e(29).then(t.bind(null,527))},"v-0c3da0ac":function(){return t.e(28).then(t.bind(null,528))},"v-60cd2f2a":function(){return t.e(30).then(t.bind(null,529))},"v-2c5e7662":function(){return t.e(31).then(t.bind(null,530))},"v-7c2eb23b":function(){return t.e(32).then(t.bind(null,531))},"v-5cb205ae":function(){return t.e(33).then(t.bind(null,532))},"v-3aba634a":function(){return t.e(34).then(t.bind(null,533))},"v-3ce17358":function(){return t.e(35).then(t.bind(null,534))},"v-2751208f":function(){return t.e(36).then(t.bind(null,535))},"v-0f768c01":function(){return t.e(37).then(t.bind(null,536))},"v-a103715e":function(){return t.e(39).then(t.bind(null,537))},"v-2e5129d9":function(){return t.e(38).then(t.bind(null,538))},"v-64afecb3":function(){return t.e(40).then(t.bind(null,539))},"v-97656b46":function(){return t.e(42).then(t.bind(null,540))},"v-128ad24a":function(){return t.e(43).then(t.bind(null,541))},"v-7e57676a":function(){return t.e(41).then(t.bind(null,542))},"v-c9e2e0d2":function(){return t.e(45).then(t.bind(null,543))},"v-68e1016a":function(){return t.e(44).then(t.bind(null,544))},"v-3e34a8f6":function(){return t.e(46).then(t.bind(null,545))},"v-0b21a7f6":function(){return t.e(48).then(t.bind(null,546))},"v-bb4076c0":function(){return t.e(49).then(t.bind(null,547))},"v-e39cdcd2":function(){return t.e(47).then(t.bind(null,548))},"v-fbf30870":function(){return t.e(50).then(t.bind(null,549))},"v-332cf74c":function(){return t.e(51).then(t.bind(null,550))},"v-56d6aafb":function(){return t.e(52).then(t.bind(null,551))},"v-5e457954":function(){return t.e(53).then(t.bind(null,552))},"v-060fb40c":function(){return t.e(54).then(t.bind(null,553))},"v-c91d0ee8":function(){return t.e(55).then(t.bind(null,554))},"v-6cbdabde":function(){return t.e(56).then(t.bind(null,555))},"v-1b1a2fda":function(){return t.e(58).then(t.bind(null,556))},"v-6d96d148":function(){return t.e(57).then(t.bind(null,557))},"v-49d4e880":function(){return t.e(59).then(t.bind(null,558))},"v-58b13ad2":function(){return t.e(61).then(t.bind(null,559))},"v-4574a8d8":function(){return t.e(60).then(t.bind(null,560))},"v-42fdff6b":function(){return t.e(62).then(t.bind(null,561))},"v-3191e504":function(){return t.e(63).then(t.bind(null,562))},"v-471ff6aa":function(){return t.e(64).then(t.bind(null,563))},"v-7d9a1627":function(){return t.e(65).then(t.bind(null,564))},"v-d7039bcc":function(){return t.e(67).then(t.bind(null,565))},"v-a07ea332":function(){return t.e(66).then(t.bind(null,566))},"v-66e73294":function(){return t.e(68).then(t.bind(null,567))},"v-2474d88f":function(){return t.e(69).then(t.bind(null,568))},"v-951a5208":function(){return t.e(70).then(t.bind(null,569))},"v-ca23bd78":function(){return t.e(71).then(t.bind(null,570))},"v-d8c50dce":function(){return t.e(73).then(t.bind(null,571))},"v-32a3ec34":function(){return t.e(75).then(t.bind(null,572))},"v-49a02e0f":function(){return t.e(74).then(t.bind(null,573))},"v-c93d1656":function(){return t.e(76).then(t.bind(null,574))},"v-bc7f093e":function(){return t.e(77).then(t.bind(null,575))},"v-6d95328a":function(){return t.e(79).then(t.bind(null,576))},"v-1a9db526":function(){return t.e(80).then(t.bind(null,577))},"v-06fb6608":function(){return t.e(81).then(t.bind(null,578))},"v-4f1cfdc2":function(){return t.e(78).then(t.bind(null,579))},"v-631835ec":function(){return t.e(82).then(t.bind(null,580))},"v-5ef65d74":function(){return t.e(72).then(t.bind(null,581))},"v-d2053d44":function(){return t.e(84).then(t.bind(null,582))},"v-3ec3ce6e":function(){return t.e(87).then(t.bind(null,583))},"v-6cdcad60":function(){return t.e(83).then(t.bind(null,584))},"v-9df1bfda":function(){return t.e(88).then(t.bind(null,585))},"v-e78fc5d0":function(){return t.e(86).then(t.bind(null,586))},"v-5aca27e4":function(){return t.e(85).then(t.bind(null,587))},"v-1f373500":function(){return t.e(89).then(t.bind(null,588))},"v-625e23c3":function(){return t.e(90).then(t.bind(null,589))},"v-7189a78a":function(){return t.e(92).then(t.bind(null,590))},"v-5945b915":function(){return t.e(91).then(t.bind(null,591))},"v-71d4b5e7":function(){return t.e(93).then(t.bind(null,592))},"v-78697e58":function(){return t.e(94).then(t.bind(null,593))},"v-5d5bf6b5":function(){return t.e(96).then(t.bind(null,594))},"v-7d483deb":function(){return t.e(97).then(t.bind(null,595))},"v-4e5c8b8d":function(){return t.e(95).then(t.bind(null,596))},"v-38d33902":function(){return t.e(99).then(t.bind(null,597))},"v-45335061":function(){return t.e(98).then(t.bind(null,598))},"v-2f94f67c":function(){return t.e(101).then(t.bind(null,599))},"v-35580351":function(){return t.e(102).then(t.bind(null,600))},"v-4fd19c04":function(){return t.e(103).then(t.bind(null,601))},"v-61452fab":function(){return t.e(104).then(t.bind(null,602))},"v-44cc2c9e":function(){return t.e(100).then(t.bind(null,603))},"v-83d6ea7a":function(){return t.e(106).then(t.bind(null,604))},"v-1bb60fa3":function(){return t.e(107).then(t.bind(null,605))},"v-3336a5d0":function(){return t.e(108).then(t.bind(null,606))},"v-36833f0c":function(){return t.e(109).then(t.bind(null,607))},"v-4a502c6c":function(){return t.e(105).then(t.bind(null,608))},"v-33880d43":function(){return t.e(111).then(t.bind(null,609))},"v-3495bce5":function(){return t.e(110).then(t.bind(null,610))},"v-25108381":function(){return t.e(113).then(t.bind(null,611))},"v-241961c2":function(){return t.e(114).then(t.bind(null,612))},"v-58c7430c":function(){return t.e(112).then(t.bind(null,613))},"v-88879b54":function(){return t.e(115).then(t.bind(null,614))},"v-96b98cee":function(){return t.e(117).then(t.bind(null,615))},"v-3d06de88":function(){return t.e(116).then(t.bind(null,616))},"v-05bbaf37":function(){return t.e(119).then(t.bind(null,617))},"v-97dc2282":function(){return t.e(118).then(t.bind(null,618))},"v-74f764f4":function(){return t.e(120).then(t.bind(null,619))},"v-54b65e0e":function(){return t.e(122).then(t.bind(null,620))},"v-3769da1c":function(){return t.e(121).then(t.bind(null,621))},"v-6611a25e":function(){return t.e(123).then(t.bind(null,622))},"v-642b486a":function(){return t.e(127).then(t.bind(null,623))},"v-29319666":function(){return t.e(124).then(t.bind(null,624))},"v-ea37ced6":function(){return t.e(126).then(t.bind(null,625))},"v-068038d1":function(){return t.e(128).then(t.bind(null,626))},"v-24b2db86":function(){return t.e(131).then(t.bind(null,627))},"v-0c6ac868":function(){return t.e(125).then(t.bind(null,628))},"v-3c977991":function(){return t.e(130).then(t.bind(null,629))},"v-5d9b9aeb":function(){return t.e(132).then(t.bind(null,630))},"v-7703b171":function(){return t.e(129).then(t.bind(null,631))},"v-7a01132d":function(){return t.e(136).then(t.bind(null,632))},"v-47db4091":function(){return t.e(133).then(t.bind(null,633))},"v-60e0d3b1":function(){return t.e(137).then(t.bind(null,634))},"v-6a96a0b1":function(){return t.e(139).then(t.bind(null,635))},"v-33e328d1":function(){return t.e(134).then(t.bind(null,636))},"v-257128de":function(){return t.e(135).then(t.bind(null,637))},"v-2bbcab5e":function(){return t.e(138).then(t.bind(null,638))},"v-73c846de":function(){return t.e(140).then(t.bind(null,639))},"v-e1af82de":function(){return t.e(141).then(t.bind(null,640))},"v-1bb1ec60":function(){return t.e(144).then(t.bind(null,641))},"v-61c16c55":function(){return t.e(142).then(t.bind(null,642))},"v-7d022df1":function(){return t.e(145).then(t.bind(null,643))},"v-14c62766":function(){return t.e(143).then(t.bind(null,644))},"v-758de2ee":function(){return t.e(147).then(t.bind(null,645))},"v-43b009de":function(){return t.e(146).then(t.bind(null,646))},"v-16cba74e":function(){return t.e(148).then(t.bind(null,647))},"v-37078826":function(){return t.e(150).then(t.bind(null,648))},"v-0f5ad0f4":function(){return t.e(151).then(t.bind(null,649))},"v-3859f86a":function(){return t.e(149).then(t.bind(null,650))},"v-01a7b822":function(){return t.e(153).then(t.bind(null,651))},"v-22d9c6ec":function(){return t.e(152).then(t.bind(null,652))},"v-766b181e":function(){return t.e(154).then(t.bind(null,653))},"v-bf6ac024":function(){return t.e(155).then(t.bind(null,654))},"v-44752817":function(){return t.e(158).then(t.bind(null,655))},"v-06144067":function(){return t.e(156).then(t.bind(null,656))},"v-424bdfb7":function(){return t.e(159).then(t.bind(null,657))},"v-73daf8d7":function(){return t.e(160).then(t.bind(null,658))},"v-a6e0fd06":function(){return t.e(161).then(t.bind(null,659))},"v-3d56846d":function(){return t.e(157).then(t.bind(null,660))},"v-3072118a":function(){return t.e(164).then(t.bind(null,661))},"v-31cc3bd2":function(){return t.e(163).then(t.bind(null,662))},"v-b2a04030":function(){return t.e(162).then(t.bind(null,663))},"v-3a8393e0":function(){return t.e(165).then(t.bind(null,664))},"v-4e1ae1c0":function(){return t.e(166).then(t.bind(null,665))},"v-3049bd10":function(){return t.e(168).then(t.bind(null,666))},"v-2b35c500":function(){return t.e(167).then(t.bind(null,667))},"v-64db6885":function(){return t.e(170).then(t.bind(null,668))},"v-16aed12b":function(){return t.e(171).then(t.bind(null,669))},"v-6265be4e":function(){return t.e(169).then(t.bind(null,670))},"v-d9c864d4":function(){return t.e(174).then(t.bind(null,671))},"v-95af4b10":function(){return t.e(173).then(t.bind(null,672))},"v-168103c0":function(){return t.e(172).then(t.bind(null,673))},"v-6dca1220":function(){return t.e(176).then(t.bind(null,674))},"v-36a8fca6":function(){return t.e(177).then(t.bind(null,675))},"v-341ec21e":function(){return t.e(175).then(t.bind(null,676))},"v-1cd8dcd0":function(){return t.e(178).then(t.bind(null,677))},"v-c42b5e22":function(){return t.e(180).then(t.bind(null,678))},"v-b10c11a2":function(){return t.e(181).then(t.bind(null,679))},"v-3a75541f":function(){return t.e(179).then(t.bind(null,680))},"v-47095aee":function(){return t.e(183).then(t.bind(null,681))},"v-309ca6de":function(){return t.e(182).then(t.bind(null,682))},"v-702958dc":function(){return t.e(188).then(t.bind(null,683))},"v-16a2d958":function(){return t.e(186).then(t.bind(null,684))},"v-9c066f90":function(){return t.e(189).then(t.bind(null,685))},"v-0680b9de":function(){return t.e(185).then(t.bind(null,686))},"v-410d528c":function(){return t.e(187).then(t.bind(null,687))},"v-1003aec2":function(){return t.e(184).then(t.bind(null,688))},"v-75630836":function(){return t.e(192).then(t.bind(null,689))},"v-8b08f0b2":function(){return t.e(190).then(t.bind(null,690))},"v-aa36ff9c":function(){return t.e(194).then(t.bind(null,691))},"v-51da2d27":function(){return t.e(191).then(t.bind(null,692))},"v-55473949":function(){return t.e(193).then(t.bind(null,693))},"v-7b8967f0":function(){return t.e(196).then(t.bind(null,694))},"v-f2fe618e":function(){return t.e(195).then(t.bind(null,695))},"v-47dc7e48":function(){return t.e(197).then(t.bind(null,696))},"v-38a8ef5e":function(){return t.e(200).then(t.bind(null,697))},"v-4601280a":function(){return t.e(199).then(t.bind(null,698))},"v-457b85c0":function(){return t.e(201).then(t.bind(null,699))},"v-ceac6f68":function(){return t.e(198).then(t.bind(null,700))},"v-43b7262a":function(){return t.e(202).then(t.bind(null,701))},"v-314e9546":function(){return t.e(203).then(t.bind(null,702))},"v-2e61c2f3":function(){return t.e(205).then(t.bind(null,703))},"v-05d97d43":function(){return t.e(206).then(t.bind(null,704))},"v-54d2601a":function(){return t.e(204).then(t.bind(null,705))},"v-36fd303c":function(){return t.e(209).then(t.bind(null,706))},"v-74d7ff21":function(){return t.e(208).then(t.bind(null,707))},"v-635d2d7a":function(){return t.e(210).then(t.bind(null,708))},"v-1bd5106a":function(){return t.e(207).then(t.bind(null,709))},"v-534da396":function(){return t.e(213).then(t.bind(null,710))},"v-87e6b8fc":function(){return t.e(211).then(t.bind(null,711))},"v-07ac2091":function(){return t.e(212).then(t.bind(null,712))},"v-0c44b074":function(){return t.e(216).then(t.bind(null,713))},"v-50b2e256":function(){return t.e(214).then(t.bind(null,714))},"v-947b9e0a":function(){return t.e(215).then(t.bind(null,715))},"v-6f5fde1c":function(){return t.e(218).then(t.bind(null,716))},"v-41675d98":function(){return t.e(217).then(t.bind(null,717))},"v-f624614c":function(){return t.e(219).then(t.bind(null,718))},"v-3cd48312":function(){return t.e(220).then(t.bind(null,719))},"v-2075c59e":function(){return t.e(222).then(t.bind(null,720))},"v-9421b7d2":function(){return t.e(223).then(t.bind(null,721))},"v-88a93a4e":function(){return t.e(221).then(t.bind(null,722))},"v-c0ebeed8":function(){return t.e(225).then(t.bind(null,723))},"v-689f8518":function(){return t.e(224).then(t.bind(null,724))},"v-94fc8dd8":function(){return t.e(227).then(t.bind(null,725))},"v-30e5d4a2":function(){return t.e(226).then(t.bind(null,726))},"v-4a91f9f8":function(){return t.e(229).then(t.bind(null,727))},"v-5c159d57":function(){return t.e(232).then(t.bind(null,728))},"v-6748a4dc":function(){return t.e(230).then(t.bind(null,729))},"v-16076614":function(){return t.e(231).then(t.bind(null,730))},"v-3e534332":function(){return t.e(228).then(t.bind(null,731))},"v-56a1eebf":function(){return t.e(233).then(t.bind(null,732))},"v-6c324d6f":function(){return t.e(234).then(t.bind(null,733))},"v-c631dd4e":function(){return t.e(235).then(t.bind(null,734))},"v-9c664796":function(){return t.e(239).then(t.bind(null,735))},"v-3b8fa256":function(){return t.e(236).then(t.bind(null,736))},"v-ef3e7878":function(){return t.e(237).then(t.bind(null,737))},"v-91bab1e0":function(){return t.e(241).then(t.bind(null,738))},"v-9c36cd5e":function(){return t.e(240).then(t.bind(null,739))},"v-0cd4d5ee":function(){return t.e(242).then(t.bind(null,740))},"v-4672ef15":function(){return t.e(238).then(t.bind(null,741))},"v-00dc2e8f":function(){return t.e(243).then(t.bind(null,742))},"v-724628f5":function(){return t.e(244).then(t.bind(null,743))},"v-1c17faff":function(){return t.e(245).then(t.bind(null,744))},"v-7f22fd75":function(){return t.e(247).then(t.bind(null,745))},"v-2e69f406":function(){return t.e(248).then(t.bind(null,746))},"v-5c5d9838":function(){return t.e(250).then(t.bind(null,747))},"v-f87607aa":function(){return t.e(251).then(t.bind(null,748))},"v-6594ad68":function(){return t.e(249).then(t.bind(null,749))},"v-4b94238d":function(){return t.e(246).then(t.bind(null,750))},"v-369ccb08":function(){return t.e(252).then(t.bind(null,751))},"v-72b26eaf":function(){return t.e(256).then(t.bind(null,752))},"v-98591bee":function(){return t.e(253).then(t.bind(null,753))},"v-c1183f22":function(){return t.e(255).then(t.bind(null,754))},"v-b7e4068e":function(){return t.e(258).then(t.bind(null,755))},"v-051079fa":function(){return t.e(254).then(t.bind(null,756))},"v-00667d92":function(){return t.e(257).then(t.bind(null,757))},"v-752ff4e6":function(){return t.e(260).then(t.bind(null,758))},"v-79c818bb":function(){return t.e(259).then(t.bind(null,759))},"v-3874a2bf":function(){return t.e(261).then(t.bind(null,760))},"v-7688bc37":function(){return t.e(262).then(t.bind(null,761))},"v-132275ff":function(){return t.e(263).then(t.bind(null,762))},"v-37a0dd10":function(){return t.e(265).then(t.bind(null,763))},"v-7abe8cca":function(){return t.e(266).then(t.bind(null,764))},"v-a20663ee":function(){return t.e(267).then(t.bind(null,765))},"v-17274f0b":function(){return t.e(264).then(t.bind(null,766))},"v-a7b2b846":function(){return t.e(268).then(t.bind(null,767))},"v-07e506ef":function(){return t.e(270).then(t.bind(null,768))},"v-7aff2c77":function(){return t.e(269).then(t.bind(null,769))},"v-9ed6a6cc":function(){return t.e(272).then(t.bind(null,770))},"v-55b2d0b0":function(){return t.e(273).then(t.bind(null,771))},"v-73eab2aa":function(){return t.e(274).then(t.bind(null,772))},"v-6e080d03":function(){return t.e(271).then(t.bind(null,773))},"v-1391410f":function(){return t.e(277).then(t.bind(null,774))},"v-210176a7":function(){return t.e(275).then(t.bind(null,775))},"v-598b6c9d":function(){return t.e(276).then(t.bind(null,776))},"v-8c34a962":function(){return t.e(278).then(t.bind(null,777))},"v-4254887a":function(){return t.e(279).then(t.bind(null,778))},"v-43f57a12":function(){return t.e(280).then(t.bind(null,779))},"v-3f9e4bb2":function(){return t.e(282).then(t.bind(null,780))},"v-3cd13c66":function(){return t.e(281).then(t.bind(null,781))},"v-1e6e69af":function(){return t.e(283).then(t.bind(null,782))},"v-6a373828":function(){return t.e(284).then(t.bind(null,783))},"v-64817218":function(){return t.e(287).then(t.bind(null,784))},"v-55a5d0e4":function(){return t.e(288).then(t.bind(null,785))},"v-266c9e22":function(){return t.e(286).then(t.bind(null,786))},"v-3c82fd62":function(){return t.e(290).then(t.bind(null,787))},"v-c86974ba":function(){return t.e(285).then(t.bind(null,788))},"v-3f1fc75e":function(){return t.e(289).then(t.bind(null,789))},"v-c73f1002":function(){return t.e(292).then(t.bind(null,790))},"v-d5889de6":function(){return t.e(291).then(t.bind(null,791))},"v-13acc28d":function(){return t.e(294).then(t.bind(null,792))},"v-41cba2f2":function(){return t.e(293).then(t.bind(null,793))},"v-283892a2":function(){return t.e(295).then(t.bind(null,794))},"v-5a8af3d8":function(){return t.e(297).then(t.bind(null,795))},"v-2117d120":function(){return t.e(296).then(t.bind(null,796))},"v-d5c2e428":function(){return t.e(299).then(t.bind(null,797))},"v-6a5e0750":function(){return t.e(300).then(t.bind(null,798))},"v-762d1d8f":function(){return t.e(302).then(t.bind(null,799))},"v-a108e506":function(){return t.e(303).then(t.bind(null,800))},"v-fc416092":function(){return t.e(301).then(t.bind(null,801))},"v-dbe517a8":function(){return t.e(298).then(t.bind(null,802))},"v-52219ca2":function(){return t.e(304).then(t.bind(null,803))},"v-12efaf51":function(){return t.e(305).then(t.bind(null,804))}};function Ci(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var Ai=/-(\w)/g,Pi=Ci((function(n){return n.replace(Ai,(function(n,e){return e?e.toUpperCase():""}))})),Di=/\B([A-Z])/g,qi=Ci((function(n){return n.replace(Di,"-$1").toLowerCase()})),Li=Ci((function(n){return n.charAt(0).toUpperCase()+n.slice(1)}));function Bi(n,e){if(e)return n(e)?n(e):e.includes("-")?n(Li(Pi(e))):n(Li(e))||n(qi(e))}var Oi=Object.assign({},ji,zi),Fi=function(n){return Oi[n]},Ri=function(n){return zi[n]},Mi=function(n){return ji[n]},Hi=function(n){return Dr.component(n)};function Ni(n){return Bi(Ri,n)}function Ui(n){return Bi(Mi,n)}function Ji(n){return Bi(Fi,n)}function $i(n){return Bi(Hi,n)}function Zi(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];return Promise.all(e.filter((function(n){return n})).map(function(){var n=Object(a.a)(regeneratorRuntime.mark((function n(e){var t;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if($i(e)||!Ji(e)){n.next=5;break}return n.next=3,Ji(e)();case 3:t=n.sent,Dr.component(e,t.default);case 5:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()))}function Vi(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var Ki=t(140),Qi=(t(129),t(112),t(54),t(221)),Wi=t.n(Qi),Gi=t(222),Xi=t.n(Gi),Yi={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(n){return"meta"===Object(Ki.a)(n,1)[0]})).map((function(n){var e=Object(Ki.a)(n,2);e[0];return e[1]})),this.$ssrContext){var n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map((function(n){var e="<meta";return Object.keys(n).forEach((function(t){e+=" ".concat(t,'="').concat(Xi()(n[t]),'"')})),e+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=es(this.$canonicalUrl)}var e},mounted:function(){this.currentMetaTags=Object(Ii.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var n=this.getMergedMetaTags();this.currentMetaTags=ts(n,this.currentMetaTags)},getMergedMetaTags:function(){var n=this.$page.frontmatter.meta||[];return Wi()([{name:"description",content:this.$description}],n,this.siteMeta,as)},updateCanonicalLink:function(){ns(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",es(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){ts(null,this.currentMetaTags),ns()}};function ns(){var n=document.querySelector("link[rel='canonical']");n&&n.remove()}function es(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return n?'<link href="'.concat(n,'" rel="canonical" />'):""}function ts(n,e){if(e&&Object(Ii.a)(e).filter((function(n){return n.parentNode===document.head})).forEach((function(n){return document.head.removeChild(n)})),n)return n.map((function(n){var e=document.createElement("meta");return Object.keys(n).forEach((function(t){e.setAttribute(t,n[t])})),document.head.appendChild(e),e}))}function as(n){for(var e=0,t=["name","property","itemprop"];e<t.length;e++){var a=t[e];if(n.hasOwnProperty(a))return n[a]+a}return JSON.stringify(n)}t(142);var rs=t(158),os={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(rs)()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var n=this,e=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(n){return e.some((function(e){return e.hash===n.hash}))})),a=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),o=window.innerHeight+a,i=0;i<t.length;i++){var s=t[i],l=t[i+1],c=0===i&&0===a||a>=s.parentElement.offsetTop+10&&(!l||a<l.parentElement.offsetTop-10),d=decodeURIComponent(this.$route.hash);if(c&&d!==decodeURIComponent(s.hash)){var p=s;if(o===r)for(var u=i+1;u<t.length;u++)if(d===decodeURIComponent(t[u].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(p.hash),(function(){n.$nextTick((function(){n.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},is=(t(61),t(102)),ss=t.n(is),ls={mounted:function(){var n=this;ss.a.configure({showSpinner:!1}),this.$router.beforeEach((function(n,e,t){n.path===e.path||Dr.component(n.name)||ss.a.start(),t()})),this.$router.afterEach((function(){ss.a.done(),n.isSidebarOpen=!1}))}};t(77),t(43),t(79),t(367);function cs(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}t(100);function ds(n,e){for(var t=0;t<e.length;t++){var a=e[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(n,a.key,a)}}function ps(n,e,t){return e&&ds(n.prototype,e),t&&ds(n,t),Object.defineProperty(n,"prototype",{writable:!1}),n}t(368);var us=function(){function n(){cs(this,n);this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}return ps(n,[{key:"show",value:function(n){var e=this,t=n.text,a=void 0===t?"":t,r=n.duration,o=void 0===r?3e3:r,i=document.createElement("div");i.className="message move-in",i.innerHTML='\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">'.concat(a,"</div>\n    "),this.containerEl.appendChild(i),o>0&&setTimeout((function(){e.close(i)}),o)}},{key:"close",value:function(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",(function(){n.remove()}))}}]),n}(),ms={mounted:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy:function(){var n=this;setTimeout((function(){(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach((function(e){document.querySelectorAll(e).forEach(n.generateCopyButton)}))}),1e3)},generateCopyButton:function(n){var e=this;if(!n.classList.contains("codecopy-enabled")){var t=document.createElement("i");t.className="code-copy",t.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',t.title="Copy to clipboard",t.addEventListener("click",(function(){e.copyToClipboard(n.innerText)})),n.appendChild(t),n.classList.add("codecopy-enabled")}},copyToClipboard:function(n){var e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);var t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy"),(new us).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};t(236),t(104),t(75),t(147),t(370);!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var a=document.head||document.getElementsByTagName("head")[0],r=document.createElement("style");r.type="text/css","top"===t&&a.firstChild?a.insertBefore(r,a.firstChild):a.appendChild(r),r.styleSheet?r.styleSheet.cssText=n:r.appendChild(document.createTextNode(n))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var gs={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},hs={},fs=function(n){return'<div id="app">\n'.concat(n,"\n</div>")},vs=function(n){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[n]?window.$VUEPRESS_DEMO_BLOCK[n]:gs[n]},bs=function n(e,t,a){var r=document.createElement(e);return t&&Object.keys(t).forEach((function(n){if(n.indexOf("data"))r[n]=t[n];else{var e=n.replace("data","");r.dataset[e]=t[n]}})),a&&a.forEach((function(e){var t=e.tag,a=e.attrs,o=e.children;r.appendChild(n(t,a,o))})),r},ys=function(n,e,t){var a,r=(a=n.querySelectorAll(".".concat(e)),Array.prototype.slice.call(a));return 1!==r.length||t?r:r[0]},xs=function(n,e){var t,a,r=n.match(/<style>([\s\S]+)<\/style>/),o=n.match(/<template>([\s\S]+)<\/template>/),i=n.match(/<script>([\s\S]+)<\/script>/),s={css:r&&r[1].replace(/^\n|\n$/g,""),html:o&&o[1].replace(/^\n|\n$/g,""),js:i&&i[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};s.htmlTpl=fs(s.html),s.jsTpl=(t=s.js,a=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(a,"\n})")),s.script=function(n,e){var t=n.split(/export\s+default/),a="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),r=window.Babel?window.Babel.transform(a,{presets:["es2015"]}).code:a,o=[eval][0](r);return o.template=e,o}(s.js,s.html);var l=vs("vue");return s.jsLib.unshift(l),s},ks=function(n,e){var t,a=n.match(/<style>([\s\S]+)<\/style>/),r=n.match(/<html>([\s\S]+)<\/html>/),o=n.match(/<script>([\s\S]+)<\/script>/),i={css:a&&a[1].replace(/^\n|\n$/g,""),html:r&&r[1].replace(/^\n|\n$/g,""),js:o&&o[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};return i.htmlTpl=i.html,i.jsTpl=i.js,i.script=(t=i.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),i},ws=function(n){return n=n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),n+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Ss(){var n=ys(document,"vuepress-plugin-demo-block__wrapper",!0);n.length?n.forEach((function(n){if("true"!==n.dataset.created){n.style.display="block";var e=ys(n,"vuepress-plugin-demo-block__code"),t=ys(n,"vuepress-plugin-demo-block__display"),a=ys(n,"vuepress-plugin-demo-block__footer"),r=ys(t,"vuepress-plugin-demo-block__app"),o=decodeURIComponent(n.dataset.code),i=decodeURIComponent(n.dataset.config),s=decodeURIComponent(n.dataset.type);i=i?JSON.parse(i):{};var l=e.querySelector("div").clientHeight,c="react"===s?function(n,e){var t=(0,window.Babel.transform)(n,{presets:["es2015","react"]}).code,a="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),r=new Function("return ".concat(a))(),o={js:r,css:r.__style__||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],jsTpl:ws(n),htmlTpl:fs("")},i=vs("react"),s=vs("reactDOM");return o.jsLib.unshift(i,s),o}(o,i):"vanilla"===s?ks(o,i):xs(o,i),d=bs("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(a.appendChild(d),d.addEventListener("click",Es.bind(null,d,l,e,a)),vs("jsfiddle")&&a.appendChild(function(n){var e=n.css,t=n.htmlTpl,a=n.jsTpl,r=n.jsLib,o=n.cssLib,i=r.concat(o).concat(vs("cssLib")).concat(vs("jsLib")).join(",");return bs("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:e}},{tag:"input",attrs:{type:"hidden",name:"html",value:t}},{tag:"input",attrs:{type:"hidden",name:"js",value:a}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:i}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(c)),vs("codepen")&&a.appendChild(function(n){var e=n.css,t=n.htmlTpl,a=n.jsTpl,r=n.jsLib,o=n.cssLib,i=JSON.stringify({css:e,html:t,js:a,js_external:r.concat(vs("jsLib")).join(";"),css_external:o.concat(vs("cssLib")).join(";"),layout:vs("codepenLayout"),js_pre_processor:vs("codepenJsProcessor"),editors:vs("codepenEditors")});return bs("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:i}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(c)),void 0!==i.horizontal?i.horizontal:vs("horizontal")){n.classList.add("vuepress-plugin-demo-block__horizontal");var p=e.firstChild.cloneNode(!0);p.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(p)}if(c.css&&function(n){if(!hs[n]){var e=bs("style",{innerHTML:n});document.body.appendChild(e),hs[n]=!0}}(c.css),"react"===s)ReactDOM.render(React.createElement(c.js),r);else if("vue"===s){var u=(new(Vue.extend(c.script))).$mount();r.appendChild(u.$el)}else"vanilla"===s&&(r.innerHTML=c.html,new Function("return (function(){".concat(c.script,"})()"))());n.dataset.created="true"}})):setTimeout((function(n){Ss()}),300)}function Es(n,e,t,a){var r="1"!==n.dataset.isExpand;t.style.height=r?"".concat(e,"px"):0,r?a.classList.add("vuepress-plugin-demo-block__show-link"):a.classList.remove("vuepress-plugin-demo-block__show-link"),n.dataset.isExpand=r?"1":"0"}var Ts={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Ss()},updated:function(){Ss()}},_s=(t(226),"auto"),Is="zoom-in",js="zoom-out",zs="grab",Cs="move";function As(n,e,t){var a=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],r={passive:!1};a?n.addEventListener(e,t,r):n.removeEventListener(e,t,r)}function Ps(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function Ds(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function qs(n,e,t){!function(n){var e=Ls,t=Bs;if(n.transition){var a=n.transition;delete n.transition,n[e]=a}if(n.transform){var r=n.transform;delete n.transform,n[t]=r}}(e);var a=n.style,r={};for(var o in e)t&&(r[o]=a[o]||""),a[o]=e[o];return r}var Ls="transition",Bs="transform",Os="transform",Fs="transitionend";var Rs=function(){},Ms={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Rs,onClose:Rs,onGrab:Rs,onMove:Rs,onRelease:Rs,onBeforeOpen:Rs,onBeforeClose:Rs,onBeforeGrab:Rs,onBeforeRelease:Rs,onImageLoading:Rs,onImageLoaded:Rs},Hs={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),Us(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var a=this.lastScrollPosition.x-e,r=this.lastScrollPosition.y-t,o=this.options.scrollThreshold;(Math.abs(r)>=o||Math.abs(a)>=o)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(Ns(n)&&!Us(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){Ns(n)&&!Us(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,a=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,a)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,a=e.clientY;this.move(t,a)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function Ns(n){return 0===n.button}function Us(n){return n.metaKey||n.ctrlKey}var Js={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,qs(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),As(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){qs(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},$s="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},Zs=function(){function n(n,e){for(var t=0;t<e.length;t++){var a=e[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(n,a.key,a)}}return function(e,t,a){return t&&n(e.prototype,t),a&&n(e,a),e}}(),Vs=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(n[a]=t[a])}return n},Ks={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Ds(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,a=n.transitionDuration,r=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?zs:js,transition:Os+"\n        "+a+"s\n        "+r,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=qs(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,qs(this.el,{transform:"none"})},grab:function(n,e,t){var a=Qs(),r=a.x-n,o=a.y-e;qs(this.el,{cursor:Cs,transform:"translate3d(\n        "+(this.translate.x+r)+"px, "+(this.translate.y+o)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var a=Qs(),r=a.x-n,o=a.y-e;qs(this.el,{transition:Os,transform:"translate3d(\n        "+(this.translate.x+r)+"px, "+(this.translate.y+o)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){qs(this.el,this.styleClose)},restoreOpenStyle:function(){qs(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=Qs(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,a=this.instance.options,r=a.customSize,o=a.scaleBase;if(!r&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(r&&"object"===(void 0===r?"undefined":$s(r)))return{x:r.width/this.rect.width,y:r.height/this.rect.height};var i=this.rect.width/2,s=this.rect.height/2,l=Qs(),c={x:l.x-i,y:l.y-s},d=c.x/i,p=c.y/s,u=o+Math.min(d,p);if(r&&"string"==typeof r){var m=t||this.el.naturalWidth,g=e||this.el.naturalHeight,h=parseFloat(r)*m/(100*this.rect.width),f=parseFloat(r)*g/(100*this.rect.height);if(u>h||u>f)return{x:h,y:f}}return{x:u,y:u}}};function Qs(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function Ws(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(a){As(n,a,e[a],t)}))}var Gs=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(Ks),this.overlay=Object.create(Js),this.handler=Object.create(Hs),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Vs({},Ms,e),this.overlay.init(this),this.handler.init(this)}return Zs(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=Is,As(n,"click",this.handler.click),this.options.preloadImage&&Ps(Ds(n)));return this}},{key:"config",value:function(n){return n?(Vs(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var a="string"==typeof n?document.querySelector(n):n;if("IMG"===a.tagName){if(this.options.onBeforeOpen(a),this.target.init(a,this),!this.options.preloadImage){var r=this.target.srcOriginal;null!=r&&(this.options.onImageLoading(a),Ps(r,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),As(document,"scroll",this.handler.scroll),As(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&As(window,"resize",this.handler.resizeWindow);var o=function n(){As(a,Fs,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&Ws(document,e.handler,!0),t(a)};return As(a,Fs,o),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=_s,this.overlay.fadeOut(),this.target.zoomOut(),As(document,"scroll",this.handler.scroll,!1),As(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&As(window,"resize",this.handler.resizeWindow,!1);var a=function a(){As(t,Fs,a,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&Ws(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return As(t,Fs,a),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var r=this.target.el;this.options.onBeforeGrab(r),this.released=!1,this.target.grab(n,e,t);var o=function n(){As(r,Fs,n,!1),a(r)};return As(r,Fs,o),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Cs,this.target.move(n,e,t);var r=this.target.el,o=function n(){As(r,Fs,n,!1),a(r)};return As(r,Fs,o),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=_s,this.target.restoreOpenStyle();var a=function a(){As(t,Fs,a,!1),n.lock=!1,n.released=!0,e(t)};return As(t,Fs,a),this}}}]),n}(),Xs=".theme-vdoing-content img:not(.no-zoom)",Ys=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),nl=Number("500"),el=function(){function n(){cs(this,n),this.instance=new Gs(Ys)}return ps(n,[{key:"update",value:function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Xs;"undefined"!=typeof window&&this.instance.listen(n)}},{key:"updateDelay",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Xs,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:nl;setTimeout((function(){return n.update(e)}),t)}}]),n}(),tl=[Yi,os,ls,ms,Ts,{watch:{"$page.path":function(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted:function(){this.$vuepress.zooming=new el,this.$vuepress.zooming.updateDelay()}}],al={name:"GlobalLayout",computed:{layout:function(){var n=this.getLayout();return Vi("layout",n),Dr.component(n)}},methods:{getLayout:function(){if(this.$page.path){var n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},rl=t(15),ol=Object(rl.a)(al,(function(){var n=this.$createElement;return(this._self._c||n)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){var a;switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),(a=n[e]).push.apply(a,Object(Ii.a)(t));break;default:throw new Error("Unknown option name.")}}(ol,"mixins",tl);var il=[{name:"v-28a7b348",path:"/pages/38d7c8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-28a7b348").then(t)}},{path:"/pages/38d7c8/index.html",redirect:"/pages/38d7c8/"},{path:"/408/01.数据结构/02.最基本的数据结构.html",redirect:"/pages/38d7c8/"},{name:"v-7ab55fd6",path:"/pages/64e136/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7ab55fd6").then(t)}},{path:"/pages/64e136/index.html",redirect:"/pages/64e136/"},{path:"/408/01.数据结构/04.random的随机行为.html",redirect:"/pages/64e136/"},{name:"v-e58ada02",path:"/pages/3ad1d4/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-e58ada02").then(t)}},{path:"/pages/3ad1d4/index.html",redirect:"/pages/3ad1d4/"},{path:"/408/01.数据结构/03.前缀和数组.html",redirect:"/pages/3ad1d4/"},{name:"v-89816e08",path:"/pages/8e5251/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-89816e08").then(t)}},{path:"/pages/8e5251/index.html",redirect:"/pages/8e5251/"},{path:"/408/01.数据结构/01.位运算.html",redirect:"/pages/8e5251/"},{name:"v-16106010",path:"/pages/9c9f24/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-16106010").then(t)}},{path:"/pages/9c9f24/index.html",redirect:"/pages/9c9f24/"},{path:"/408/01.数据结构/05.对数器.html",redirect:"/pages/9c9f24/"},{name:"v-159f3ccd",path:"/pages/06b58c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-159f3ccd").then(t)}},{path:"/pages/06b58c/index.html",redirect:"/pages/06b58c/"},{path:"/408/01.数据结构/06.二分法查找.html",redirect:"/pages/06b58c/"},{name:"v-2f6093d7",path:"/pages/7e9265/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2f6093d7").then(t)}},{path:"/pages/7e9265/index.html",redirect:"/pages/7e9265/"},{path:"/408/01.数据结构/07.方法参数传递是值还是引用.html",redirect:"/pages/7e9265/"},{name:"v-c36f6bfc",path:"/pages/d2cd3f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c36f6bfc").then(t)}},{path:"/pages/d2cd3f/index.html",redirect:"/pages/d2cd3f/"},{path:"/408/01.数据结构/09.位图.html",redirect:"/pages/d2cd3f/"},{name:"v-11649cf0",path:"/pages/51bf81/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-11649cf0").then(t)}},{path:"/pages/51bf81/index.html",redirect:"/pages/51bf81/"},{path:"/408/01.数据结构/10.位运算实现四则运算.html",redirect:"/pages/51bf81/"},{name:"v-5bd2c1fd",path:"/pages/0dcdc5/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5bd2c1fd").then(t)}},{path:"/pages/0dcdc5/index.html",redirect:"/pages/0dcdc5/"},{path:"/408/01.数据结构/12.排序.html",redirect:"/pages/0dcdc5/"},{name:"v-6441954f",path:"/pages/d3b8b4/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6441954f").then(t)}},{path:"/pages/d3b8b4/index.html",redirect:"/pages/d3b8b4/"},{path:"/408/01.数据结构/08.链表.html",redirect:"/pages/d3b8b4/"},{name:"v-2e467b68",path:"/pages/ef4d8b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2e467b68").then(t)}},{path:"/pages/ef4d8b/index.html",redirect:"/pages/ef4d8b/"},{path:"/408/01.数据结构/13.时间复杂度.html",redirect:"/pages/ef4d8b/"},{name:"v-a61f04aa",path:"/pages/3e25e1/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-a61f04aa").then(t)}},{path:"/pages/3e25e1/index.html",redirect:"/pages/3e25e1/"},{path:"/408/01.数据结构/11.二叉树.html",redirect:"/pages/3e25e1/"},{name:"v-1c0c6895",path:"/archives/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1c0c6895").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-320a16ea",path:"/pages/3bda9f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-320a16ea").then(t)}},{path:"/pages/3bda9f/index.html",redirect:"/pages/3bda9f/"},{path:"/408/01.数据结构/14.队列和栈.html",redirect:"/pages/3bda9f/"},{name:"v-644b5bf5",path:"/categories/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-644b5bf5").then(t)}},{path:"/categories/index.html",redirect:"/categories/"},{path:"/@pages/categoriesPage.html",redirect:"/categories/"},{name:"v-6e67de15",path:"/tags/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6e67de15").then(t)}},{path:"/tags/index.html",redirect:"/tags/"},{path:"/@pages/tagsPage.html",redirect:"/tags/"},{name:"v-055ee976",path:"/pages/88e08c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-055ee976").then(t)}},{path:"/pages/88e08c/index.html",redirect:"/pages/88e08c/"},{path:"/408/01.数据结构/16.堆(优先级队列).html",redirect:"/pages/88e08c/"},{name:"v-6ddb2230",path:"/pages/fe2eaf/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6ddb2230").then(t)}},{path:"/pages/fe2eaf/index.html",redirect:"/pages/fe2eaf/"},{path:"/408/01.数据结构/15.递归.html",redirect:"/pages/fe2eaf/"},{name:"v-d520fc7e",path:"/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-d520fc7e").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-0c3da0ac",path:"/pages/829589/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0c3da0ac").then(t)}},{path:"/pages/829589/index.html",redirect:"/pages/829589/"},{path:"/_posts/随笔/我做了一个手写春联小网页，祝大家虎年暴富.html",redirect:"/pages/829589/"},{name:"v-60cd2f2a",path:"/pages/23186a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-60cd2f2a").then(t)}},{path:"/pages/23186a/index.html",redirect:"/pages/23186a/"},{path:"/前端/01.html/01.BS架构.html",redirect:"/pages/23186a/"},{name:"v-2c5e7662",path:"/pages/3ba523/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2c5e7662").then(t)}},{path:"/pages/3ba523/index.html",redirect:"/pages/3ba523/"},{path:"/前端/01.html/02.HTML.html",redirect:"/pages/3ba523/"},{name:"v-7c2eb23b",path:"/pages/8600fc/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7c2eb23b").then(t)}},{path:"/pages/8600fc/index.html",redirect:"/pages/8600fc/"},{path:"/前端/01.html/03.HTML5.html",redirect:"/pages/8600fc/"},{name:"v-5cb205ae",path:"/pages/c2f00e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5cb205ae").then(t)}},{path:"/pages/c2f00e/index.html",redirect:"/pages/c2f00e/"},{path:"/前端/01.html/04.表单标签.html",redirect:"/pages/c2f00e/"},{name:"v-3aba634a",path:"/pages/b3f43c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3aba634a").then(t)}},{path:"/pages/b3f43c/index.html",redirect:"/pages/b3f43c/"},{path:"/前端/01.html/05.CSS.html",redirect:"/pages/b3f43c/"},{name:"v-3ce17358",path:"/pages/5bc756/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3ce17358").then(t)}},{path:"/pages/5bc756/index.html",redirect:"/pages/5bc756/"},{path:"/前端/01.html/06.html+css网页.html",redirect:"/pages/5bc756/"},{name:"v-2751208f",path:"/pages/54a5fa/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2751208f").then(t)}},{path:"/pages/54a5fa/index.html",redirect:"/pages/54a5fa/"},{path:"/前端/01.html/07.CSS3.html",redirect:"/pages/54a5fa/"},{name:"v-0f768c01",path:"/pages/dc74bd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0f768c01").then(t)}},{path:"/pages/dc74bd/index.html",redirect:"/pages/dc74bd/"},{path:"/前端/01.html/08.移动布局.html",redirect:"/pages/dc74bd/"},{name:"v-a103715e",path:"/pages/6af871/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-a103715e").then(t)}},{path:"/pages/6af871/index.html",redirect:"/pages/6af871/"},{path:"/前端/02.Vue2/00.vue 补充.html",redirect:"/pages/6af871/"},{name:"v-2e5129d9",path:"/pages/e5e5b8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2e5129d9").then(t)}},{path:"/pages/e5e5b8/index.html",redirect:"/pages/e5e5b8/"},{path:"/前端/01.html/09.JavaScript.html",redirect:"/pages/e5e5b8/"},{name:"v-64afecb3",path:"/pages/758bea/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-64afecb3").then(t)}},{path:"/pages/758bea/index.html",redirect:"/pages/758bea/"},{path:"/前端/02.Vue2/01.安装Vue CLI.html",redirect:"/pages/758bea/"},{name:"v-97656b46",path:"/pages/cb74f3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-97656b46").then(t)}},{path:"/pages/cb74f3/index.html",redirect:"/pages/cb74f3/"},{path:"/前端/02.Vue2/03.Elment.html",redirect:"/pages/cb74f3/"},{name:"v-128ad24a",path:"/pages/e17f8f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-128ad24a").then(t)}},{path:"/pages/e17f8f/index.html",redirect:"/pages/e17f8f/"},{path:"/前端/02.Vue2/04.Axios.html",redirect:"/pages/e17f8f/"},{name:"v-7e57676a",path:"/pages/3b8d98/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7e57676a").then(t)}},{path:"/pages/3b8d98/index.html",redirect:"/pages/3b8d98/"},{path:"/前端/02.Vue2/02.在Vue CLI中导入 Elment.html",redirect:"/pages/3b8d98/"},{name:"v-c9e2e0d2",path:"/pages/56d54a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c9e2e0d2").then(t)}},{path:"/pages/56d54a/index.html",redirect:"/pages/56d54a/"},{path:"/前端/02.Vue2/06.github 推送.html",redirect:"/pages/56d54a/"},{name:"v-68e1016a",path:"/pages/efa853/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-68e1016a").then(t)}},{path:"/pages/efa853/index.html",redirect:"/pages/efa853/"},{path:"/前端/02.Vue2/05.配置Vue路由.html",redirect:"/pages/efa853/"},{name:"v-3e34a8f6",path:"/pages/93608e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3e34a8f6").then(t)}},{path:"/pages/93608e/index.html",redirect:"/pages/93608e/"},{path:"/前端/02.Vue2/07.安装 elment-tiptap.html",redirect:"/pages/93608e/"},{name:"v-0b21a7f6",path:"/pages/e5e482/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0b21a7f6").then(t)}},{path:"/pages/e5e482/index.html",redirect:"/pages/e5e482/"},{path:"/前端/02.Vue2/09.让两个组件之间通讯.html",redirect:"/pages/e5e482/"},{name:"v-bb4076c0",path:"/pages/812e04/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-bb4076c0").then(t)}},{path:"/pages/812e04/index.html",redirect:"/pages/812e04/"},{path:"/前端/02.Vue2/10.安装echart.html",redirect:"/pages/812e04/"},{name:"v-e39cdcd2",path:"/pages/f0d787/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-e39cdcd2").then(t)}},{path:"/pages/f0d787/index.html",redirect:"/pages/f0d787/"},{path:"/前端/02.Vue2/08.安装 cropperjs 图片裁切工具.html",redirect:"/pages/f0d787/"},{name:"v-fbf30870",path:"/pages/2bf2c3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-fbf30870").then(t)}},{path:"/pages/2bf2c3/index.html",redirect:"/pages/2bf2c3/"},{path:"/前端/02.Vue2/11.文件对象.html",redirect:"/pages/2bf2c3/"},{name:"v-332cf74c",path:"/pages/fc097d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-332cf74c").then(t)}},{path:"/pages/fc097d/index.html",redirect:"/pages/fc097d/"},{path:"/前端/02.Vue2/12.打包发布.html",redirect:"/pages/fc097d/"},{name:"v-56d6aafb",path:"/pages/b60752/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-56d6aafb").then(t)}},{path:"/pages/b60752/index.html",redirect:"/pages/b60752/"},{path:"/前端/02.Vue2/13.webpack.html",redirect:"/pages/b60752/"},{name:"v-5e457954",path:"/pages/0a8d91/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5e457954").then(t)}},{path:"/pages/0a8d91/index.html",redirect:"/pages/0a8d91/"},{path:"/前端/02.Vue2/14.vue 版本.html",redirect:"/pages/0a8d91/"},{name:"v-060fb40c",path:"/pages/b8f860/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-060fb40c").then(t)}},{path:"/pages/b8f860/index.html",redirect:"/pages/b8f860/"},{path:"/前端/02.Vue2/15.优化打包.html",redirect:"/pages/b8f860/"},{name:"v-c91d0ee8",path:"/pages/c64d65/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c91d0ee8").then(t)}},{path:"/pages/c64d65/index.html",redirect:"/pages/c64d65/"},{path:"/前端/02.Vue2/16.vue 图形界面.html",redirect:"/pages/c64d65/"},{name:"v-6cbdabde",path:"/pages/27ac6a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6cbdabde").then(t)}},{path:"/pages/27ac6a/index.html",redirect:"/pages/27ac6a/"},{path:"/前端/02.Vue2/17.路由懒加载.html",redirect:"/pages/27ac6a/"},{name:"v-1b1a2fda",path:"/pages/14f647/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1b1a2fda").then(t)}},{path:"/pages/14f647/index.html",redirect:"/pages/14f647/"},{path:"/前端/02.Vue2/19.缓存和并行处理.html",redirect:"/pages/14f647/"},{name:"v-6d96d148",path:"/pages/1bc0dd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6d96d148").then(t)}},{path:"/pages/1bc0dd/index.html",redirect:"/pages/1bc0dd/"},{path:"/前端/02.Vue2/18.element 按需引用.html",redirect:"/pages/1bc0dd/"},{name:"v-49d4e880",path:"/pages/195534/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-49d4e880").then(t)}},{path:"/pages/195534/index.html",redirect:"/pages/195534/"},{path:"/前端/02.Vue2/20.JavaScript 异步编程.html",redirect:"/pages/195534/"},{name:"v-58b13ad2",path:"/pages/0c8879/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-58b13ad2").then(t)}},{path:"/pages/0c8879/index.html",redirect:"/pages/0c8879/"},{path:"/后端/01.JavaSE/02.JRE(Java Runtime Enviroment).html",redirect:"/pages/0c8879/"},{name:"v-4574a8d8",path:"/pages/df8281/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4574a8d8").then(t)}},{path:"/pages/df8281/index.html",redirect:"/pages/df8281/"},{path:"/后端/01.JavaSE/01.java跨平台原理.html",redirect:"/pages/df8281/"},{name:"v-42fdff6b",path:"/pages/d217f4/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-42fdff6b").then(t)}},{path:"/pages/d217f4/index.html",redirect:"/pages/d217f4/"},{path:"/后端/01.JavaSE/03.JDK(Java Development Kit).html",redirect:"/pages/d217f4/"},{name:"v-3191e504",path:"/pages/47da21/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3191e504").then(t)}},{path:"/pages/47da21/index.html",redirect:"/pages/47da21/"},{path:"/后端/01.JavaSE/04.常用dos命令.html",redirect:"/pages/47da21/"},{name:"v-471ff6aa",path:"/pages/3ee21a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-471ff6aa").then(t)}},{path:"/pages/3ee21a/index.html",redirect:"/pages/3ee21a/"},{path:"/后端/01.JavaSE/05.配置系统path环境变量.html",redirect:"/pages/3ee21a/"},{name:"v-7d9a1627",path:"/pages/417c2d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7d9a1627").then(t)}},{path:"/pages/417c2d/index.html",redirect:"/pages/417c2d/"},{path:"/后端/01.JavaSE/06.开发运行流程.html",redirect:"/pages/417c2d/"},{name:"v-d7039bcc",path:"/pages/97d4f2/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-d7039bcc").then(t)}},{path:"/pages/97d4f2/index.html",redirect:"/pages/97d4f2/"},{path:"/后端/01.JavaSE/08.基础语法.html",redirect:"/pages/97d4f2/"},{name:"v-a07ea332",path:"/pages/977c49/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-a07ea332").then(t)}},{path:"/pages/977c49/index.html",redirect:"/pages/977c49/"},{path:"/后端/01.JavaSE/07.编写规范问题.html",redirect:"/pages/977c49/"},{name:"v-66e73294",path:"/pages/d2d164/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-66e73294").then(t)}},{path:"/pages/d2d164/index.html",redirect:"/pages/d2d164/"},{path:"/后端/01.JavaSE/09.关键字.html",redirect:"/pages/d2d164/"},{name:"v-2474d88f",path:"/pages/10baa9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2474d88f").then(t)}},{path:"/pages/10baa9/index.html",redirect:"/pages/10baa9/"},{path:"/后端/01.JavaSE/10.常量.html",redirect:"/pages/10baa9/"},{name:"v-951a5208",path:"/pages/ec81f9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-951a5208").then(t)}},{path:"/pages/ec81f9/index.html",redirect:"/pages/ec81f9/"},{path:"/后端/01.JavaSE/11.数据类型.html",redirect:"/pages/ec81f9/"},{name:"v-ca23bd78",path:"/pages/3f6e03/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-ca23bd78").then(t)}},{path:"/pages/3f6e03/index.html",redirect:"/pages/3f6e03/"},{path:"/后端/01.JavaSE/12.数值型内容占用和取整范围.html",redirect:"/pages/3f6e03/"},{name:"v-d8c50dce",path:"/pages/fcb871/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-d8c50dce").then(t)}},{path:"/pages/fcb871/index.html",redirect:"/pages/fcb871/"},{path:"/后端/01.JavaSE/14.标识符.html",redirect:"/pages/fcb871/"},{name:"v-32a3ec34",path:"/pages/a3cd43/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-32a3ec34").then(t)}},{path:"/pages/a3cd43/index.html",redirect:"/pages/a3cd43/"},{path:"/后端/01.JavaSE/16.算术运算符.html",redirect:"/pages/a3cd43/"},{name:"v-49a02e0f",path:"/pages/b2c2df/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-49a02e0f").then(t)}},{path:"/pages/b2c2df/index.html",redirect:"/pages/b2c2df/"},{path:"/后端/01.JavaSE/15.类型转换.html",redirect:"/pages/b2c2df/"},{name:"v-c93d1656",path:"/pages/a4bf30/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c93d1656").then(t)}},{path:"/pages/a4bf30/index.html",redirect:"/pages/a4bf30/"},{path:"/后端/01.JavaSE/17.自增自减运算符.html",redirect:"/pages/a4bf30/"},{name:"v-bc7f093e",path:"/pages/a5f46a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-bc7f093e").then(t)}},{path:"/pages/a5f46a/index.html",redirect:"/pages/a5f46a/"},{path:"/后端/01.JavaSE/18.关系运算符.html",redirect:"/pages/a5f46a/"},{name:"v-6d95328a",path:"/pages/5cf2b5/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6d95328a").then(t)}},{path:"/pages/5cf2b5/index.html",redirect:"/pages/5cf2b5/"},{path:"/后端/01.JavaSE/20.短路逻辑运算符.html",redirect:"/pages/5cf2b5/"},{name:"v-1a9db526",path:"/pages/ad2097/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1a9db526").then(t)}},{path:"/pages/ad2097/index.html",redirect:"/pages/ad2097/"},{path:"/后端/01.JavaSE/21.三元运算符.html",redirect:"/pages/ad2097/"},{name:"v-06fb6608",path:"/pages/9ca53c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-06fb6608").then(t)}},{path:"/pages/9ca53c/index.html",redirect:"/pages/9ca53c/"},{path:"/后端/01.JavaSE/22.数据输入.html",redirect:"/pages/9ca53c/"},{name:"v-4f1cfdc2",path:"/pages/ae5f03/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4f1cfdc2").then(t)}},{path:"/pages/ae5f03/index.html",redirect:"/pages/ae5f03/"},{path:"/后端/01.JavaSE/19.逻辑运算符.html",redirect:"/pages/ae5f03/"},{name:"v-631835ec",path:"/pages/fc3701/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-631835ec").then(t)}},{path:"/pages/fc3701/index.html",redirect:"/pages/fc3701/"},{path:"/后端/01.JavaSE/23.流程控制.html",redirect:"/pages/fc3701/"},{name:"v-5ef65d74",path:"/pages/558212/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5ef65d74").then(t)}},{path:"/pages/558212/index.html",redirect:"/pages/558212/"},{path:"/后端/01.JavaSE/13.变量.html",redirect:"/pages/558212/"},{name:"v-d2053d44",path:"/pages/3455ab/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-d2053d44").then(t)}},{path:"/pages/3455ab/index.html",redirect:"/pages/3455ab/"},{path:"/后端/01.JavaSE/25.idea中的辅助键.html",redirect:"/pages/3455ab/"},{name:"v-3ec3ce6e",path:"/pages/0a58cf/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3ec3ce6e").then(t)}},{path:"/pages/0a58cf/index.html",redirect:"/pages/0a58cf/"},{path:"/后端/01.JavaSE/28.面向对象基础.html",redirect:"/pages/0a58cf/"},{name:"v-6cdcad60",path:"/pages/db81b2/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6cdcad60").then(t)}},{path:"/pages/db81b2/index.html",redirect:"/pages/db81b2/"},{path:"/后端/01.JavaSE/24.循环语句.html",redirect:"/pages/db81b2/"},{name:"v-9df1bfda",path:"/pages/ab994e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-9df1bfda").then(t)}},{path:"/pages/ab994e/index.html",redirect:"/pages/ab994e/"},{path:"/后端/01.JavaSE/29.标准类.html",redirect:"/pages/ab994e/"},{name:"v-e78fc5d0",path:"/pages/8361e2/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-e78fc5d0").then(t)}},{path:"/pages/8361e2/index.html",redirect:"/pages/8361e2/"},{path:"/后端/01.JavaSE/27.方法.html",redirect:"/pages/8361e2/"},{name:"v-5aca27e4",path:"/pages/add390/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5aca27e4").then(t)}},{path:"/pages/add390/index.html",redirect:"/pages/add390/"},{path:"/后端/01.JavaSE/26.数组.html",redirect:"/pages/add390/"},{name:"v-1f373500",path:"/pages/8a171b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1f373500").then(t)}},{path:"/pages/8a171b/index.html",redirect:"/pages/8a171b/"},{path:"/后端/01.JavaSE/30.字符串.html",redirect:"/pages/8a171b/"},{name:"v-625e23c3",path:"/pages/ab65fb/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-625e23c3").then(t)}},{path:"/pages/ab65fb/index.html",redirect:"/pages/ab65fb/"},{path:"/后端/01.JavaSE/31.ArrayList集合.html",redirect:"/pages/ab65fb/"},{name:"v-7189a78a",path:"/pages/f3b824/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7189a78a").then(t)}},{path:"/pages/f3b824/index.html",redirect:"/pages/f3b824/"},{path:"/后端/01.JavaSE/33.修饰符.html",redirect:"/pages/f3b824/"},{name:"v-5945b915",path:"/pages/c7fa91/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5945b915").then(t)}},{path:"/pages/c7fa91/index.html",redirect:"/pages/c7fa91/"},{path:"/后端/01.JavaSE/32.继承.html",redirect:"/pages/c7fa91/"},{name:"v-71d4b5e7",path:"/pages/047301/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-71d4b5e7").then(t)}},{path:"/pages/047301/index.html",redirect:"/pages/047301/"},{path:"/后端/01.JavaSE/34.权限修饰符.html",redirect:"/pages/047301/"},{name:"v-78697e58",path:"/pages/392033/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-78697e58").then(t)}},{path:"/pages/392033/index.html",redirect:"/pages/392033/"},{path:"/后端/01.JavaSE/35.状态修饰符.html",redirect:"/pages/392033/"},{name:"v-5d5bf6b5",path:"/pages/a12790/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5d5bf6b5").then(t)}},{path:"/pages/a12790/index.html",redirect:"/pages/a12790/"},{path:"/后端/01.JavaSE/37.抽象.html",redirect:"/pages/a12790/"},{name:"v-7d483deb",path:"/pages/db4705/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7d483deb").then(t)}},{path:"/pages/db4705/index.html",redirect:"/pages/db4705/"},{path:"/后端/01.JavaSE/38.接口.html",redirect:"/pages/db4705/"},{name:"v-4e5c8b8d",path:"/pages/6407a8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4e5c8b8d").then(t)}},{path:"/pages/6407a8/index.html",redirect:"/pages/6407a8/"},{path:"/后端/01.JavaSE/36.多态.html",redirect:"/pages/6407a8/"},{name:"v-38d33902",path:"/pages/102423/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-38d33902").then(t)}},{path:"/pages/102423/index.html",redirect:"/pages/102423/"},{path:"/后端/01.JavaSE/40.抽象类与接口的区别.html",redirect:"/pages/102423/"},{name:"v-45335061",path:"/pages/b47784/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-45335061").then(t)}},{path:"/pages/b47784/index.html",redirect:"/pages/b47784/"},{path:"/后端/01.JavaSE/39.类和接口的关系.html",redirect:"/pages/b47784/"},{name:"v-2f94f67c",path:"/pages/86e52c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2f94f67c").then(t)}},{path:"/pages/86e52c/index.html",redirect:"/pages/86e52c/"},{path:"/后端/01.JavaSE/42.内部类.html",redirect:"/pages/86e52c/"},{name:"v-35580351",path:"/pages/a4cd8f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-35580351").then(t)}},{path:"/pages/a4cd8f/index.html",redirect:"/pages/a4cd8f/"},{path:"/后端/01.JavaSE/43.Api.html",redirect:"/pages/a4cd8f/"},{name:"v-4fd19c04",path:"/pages/236d5b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4fd19c04").then(t)}},{path:"/pages/236d5b/index.html",redirect:"/pages/236d5b/"},{path:"/后端/01.JavaSE/44.异常.html",redirect:"/pages/236d5b/"},{name:"v-61452fab",path:"/pages/f58543/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-61452fab").then(t)}},{path:"/pages/f58543/index.html",redirect:"/pages/f58543/"},{path:"/后端/01.JavaSE/45.集合.html",redirect:"/pages/f58543/"},{name:"v-44cc2c9e",path:"/pages/384827/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-44cc2c9e").then(t)}},{path:"/pages/384827/index.html",redirect:"/pages/384827/"},{path:"/后端/01.JavaSE/41.类名作为形参和返回值.html",redirect:"/pages/384827/"},{name:"v-83d6ea7a",path:"/pages/5137f9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-83d6ea7a").then(t)}},{path:"/pages/5137f9/index.html",redirect:"/pages/5137f9/"},{path:"/后端/01.JavaSE/47.Set集合.html",redirect:"/pages/5137f9/"},{name:"v-1bb60fa3",path:"/pages/6e8f69/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1bb60fa3").then(t)}},{path:"/pages/6e8f69/index.html",redirect:"/pages/6e8f69/"},{path:"/后端/01.JavaSE/48.树.html",redirect:"/pages/6e8f69/"},{name:"v-3336a5d0",path:"/pages/8b6f0f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3336a5d0").then(t)}},{path:"/pages/8b6f0f/index.html",redirect:"/pages/8b6f0f/"},{path:"/后端/01.JavaSE/49.TreeSet遍历.html",redirect:"/pages/8b6f0f/"},{name:"v-36833f0c",path:"/pages/1793cd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-36833f0c").then(t)}},{path:"/pages/1793cd/index.html",redirect:"/pages/1793cd/"},{path:"/后端/01.JavaSE/50.哈希值.html",redirect:"/pages/1793cd/"},{name:"v-4a502c6c",path:"/pages/d6e1c4/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4a502c6c").then(t)}},{path:"/pages/d6e1c4/index.html",redirect:"/pages/d6e1c4/"},{path:"/后端/01.JavaSE/46.泛型.html",redirect:"/pages/d6e1c4/"},{name:"v-33880d43",path:"/pages/642d0a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-33880d43").then(t)}},{path:"/pages/642d0a/index.html",redirect:"/pages/642d0a/"},{path:"/后端/01.JavaSE/52.HashSet集合.html",redirect:"/pages/642d0a/"},{name:"v-3495bce5",path:"/pages/7525b1/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3495bce5").then(t)}},{path:"/pages/7525b1/index.html",redirect:"/pages/7525b1/"},{path:"/后端/01.JavaSE/51.哈希表.html",redirect:"/pages/7525b1/"},{name:"v-25108381",path:"/pages/4fb557/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-25108381").then(t)}},{path:"/pages/4fb557/index.html",redirect:"/pages/4fb557/"},{path:"/后端/01.JavaSE/54.可变参数.html",redirect:"/pages/4fb557/"},{name:"v-241961c2",path:"/pages/7db438/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-241961c2").then(t)}},{path:"/pages/7db438/index.html",redirect:"/pages/7db438/"},{path:"/后端/01.JavaSE/55.创建不可变的集合.html",redirect:"/pages/7db438/"},{name:"v-58c7430c",path:"/pages/b8041f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-58c7430c").then(t)}},{path:"/pages/b8041f/index.html",redirect:"/pages/b8041f/"},{path:"/后端/01.JavaSE/53.Map 集合.html",redirect:"/pages/b8041f/"},{name:"v-88879b54",path:"/pages/46103a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-88879b54").then(t)}},{path:"/pages/46103a/index.html",redirect:"/pages/46103a/"},{path:"/后端/01.JavaSE/56.Stream流.html",redirect:"/pages/46103a/"},{name:"v-96b98cee",path:"/pages/48b908/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-96b98cee").then(t)}},{path:"/pages/48b908/index.html",redirect:"/pages/48b908/"},{path:"/后端/01.JavaSE/58.File.html",redirect:"/pages/48b908/"},{name:"v-3d06de88",path:"/pages/566611/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3d06de88").then(t)}},{path:"/pages/566611/index.html",redirect:"/pages/566611/"},{path:"/后端/01.JavaSE/57.方法引用.html",redirect:"/pages/566611/"},{name:"v-05bbaf37",path:"/pages/c73ee4/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-05bbaf37").then(t)}},{path:"/pages/c73ee4/index.html",redirect:"/pages/c73ee4/"},{path:"/后端/01.JavaSE/60.多线程高级.html",redirect:"/pages/c73ee4/"},{name:"v-97dc2282",path:"/pages/606294/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-97dc2282").then(t)}},{path:"/pages/606294/index.html",redirect:"/pages/606294/"},{path:"/后端/01.JavaSE/59.多线程.html",redirect:"/pages/606294/"},{name:"v-74f764f4",path:"/pages/fc7fd4/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-74f764f4").then(t)}},{path:"/pages/fc7fd4/index.html",redirect:"/pages/fc7fd4/"},{path:"/后端/01.JavaSE/61.网络编程.html",redirect:"/pages/fc7fd4/"},{name:"v-54b65e0e",path:"/pages/b3e612/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-54b65e0e").then(t)}},{path:"/pages/b3e612/index.html",redirect:"/pages/b3e612/"},{path:"/后端/01.JavaSE/63.反射.html",redirect:"/pages/b3e612/"},{name:"v-3769da1c",path:"/pages/a33e90/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3769da1c").then(t)}},{path:"/pages/a33e90/index.html",redirect:"/pages/a33e90/"},{path:"/后端/01.JavaSE/62.类加载器.html",redirect:"/pages/a33e90/"},{name:"v-6611a25e",path:"/pages/6bad64/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6611a25e").then(t)}},{path:"/pages/6bad64/index.html",redirect:"/pages/6bad64/"},{path:"/后端/01.JavaSE/64.XML.html",redirect:"/pages/6bad64/"},{name:"v-642b486a",path:"/pages/344d91/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-642b486a").then(t)}},{path:"/pages/344d91/index.html",redirect:"/pages/344d91/"},{path:"/后端/01.JavaSE/68.日志.html",redirect:"/pages/344d91/"},{name:"v-29319666",path:"/pages/19426f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-29319666").then(t)}},{path:"/pages/19426f/index.html",redirect:"/pages/19426f/"},{path:"/后端/01.JavaSE/65.枚举.html",redirect:"/pages/19426f/"},{name:"v-ea37ced6",path:"/pages/7ba344/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-ea37ced6").then(t)}},{path:"/pages/7ba344/index.html",redirect:"/pages/7ba344/"},{path:"/后端/01.JavaSE/67.单元测试.html",redirect:"/pages/7ba344/"},{name:"v-068038d1",path:"/pages/a7a164/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-068038d1").then(t)}},{path:"/pages/a7a164/index.html",redirect:"/pages/a7a164/"},{path:"/后端/01.JavaSE/69.HTTP协议.html",redirect:"/pages/a7a164/"},{name:"v-24b2db86",path:"/pages/d35ebd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-24b2db86").then(t)}},{path:"/pages/d35ebd/index.html",redirect:"/pages/d35ebd/"},{path:"/后端/01.JavaSE/72.响应对象.html",redirect:"/pages/d35ebd/"},{name:"v-0c6ac868",path:"/pages/1bf317/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0c6ac868").then(t)}},{path:"/pages/1bf317/index.html",redirect:"/pages/1bf317/"},{path:"/后端/01.JavaSE/66.注解.html",redirect:"/pages/1bf317/"},{name:"v-3c977991",path:"/pages/83bc98/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3c977991").then(t)}},{path:"/pages/83bc98/index.html",redirect:"/pages/83bc98/"},{path:"/后端/01.JavaSE/71.请求对象.html",redirect:"/pages/83bc98/"},{name:"v-5d9b9aeb",path:"/pages/133dfe/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5d9b9aeb").then(t)}},{path:"/pages/133dfe/index.html",redirect:"/pages/133dfe/"},{path:"/后端/01.JavaSE/73.Cookie.html",redirect:"/pages/133dfe/"},{name:"v-7703b171",path:"/pages/9c9dd6/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7703b171").then(t)}},{path:"/pages/9c9dd6/index.html",redirect:"/pages/9c9dd6/"},{path:"/后端/01.JavaSE/70.Servlet.html",redirect:"/pages/9c9dd6/"},{name:"v-7a01132d",path:"/pages/544a3f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7a01132d").then(t)}},{path:"/pages/544a3f/index.html",redirect:"/pages/544a3f/"},{path:"/后端/02.JavaEE/01.JDBC.html",redirect:"/pages/544a3f/"},{name:"v-47db4091",path:"/pages/83bc22/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-47db4091").then(t)}},{path:"/pages/83bc22/index.html",redirect:"/pages/83bc22/"},{path:"/后端/01.JavaSE/74.Session.html",redirect:"/pages/83bc22/"},{name:"v-60e0d3b1",path:"/pages/c1aa2b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-60e0d3b1").then(t)}},{path:"/pages/c1aa2b/index.html",redirect:"/pages/c1aa2b/"},{path:"/后端/02.JavaEE/02.MyBatis.html",redirect:"/pages/c1aa2b/"},{name:"v-6a96a0b1",path:"/pages/40c5ff/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6a96a0b1").then(t)}},{path:"/pages/40c5ff/index.html",redirect:"/pages/40c5ff/"},{path:"/后端/02.JavaEE/04.Jedis.html",redirect:"/pages/40c5ff/"},{name:"v-33e328d1",path:"/pages/427528/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-33e328d1").then(t)}},{path:"/pages/427528/index.html",redirect:"/pages/427528/"},{path:"/后端/01.JavaSE/75.JSP.html",redirect:"/pages/427528/"},{name:"v-257128de",path:"/pages/70e34e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-257128de").then(t)}},{path:"/pages/70e34e/index.html",redirect:"/pages/70e34e/"},{path:"/后端/01.JavaSE/76.Listener.html",redirect:"/pages/70e34e/"},{name:"v-2bbcab5e",path:"/pages/df2f58/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2bbcab5e").then(t)}},{path:"/pages/df2f58/index.html",redirect:"/pages/df2f58/"},{path:"/后端/02.JavaEE/03.Jackson.html",redirect:"/pages/df2f58/"},{name:"v-73c846de",path:"/pages/682f06/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-73c846de").then(t)}},{path:"/pages/682f06/index.html",redirect:"/pages/682f06/"},{path:"/后端/02.JavaEE/05.Maven.html",redirect:"/pages/682f06/"},{name:"v-e1af82de",path:"/pages/528ce7/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-e1af82de").then(t)}},{path:"/pages/528ce7/index.html",redirect:"/pages/528ce7/"},{path:"/后端/02.JavaEE/06.POI.html",redirect:"/pages/528ce7/"},{name:"v-1bb1ec60",path:"/pages/127f3b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1bb1ec60").then(t)}},{path:"/pages/127f3b/index.html",redirect:"/pages/127f3b/"},{path:"/后端/02.JavaEE/09.Maven 高级.html",redirect:"/pages/127f3b/"},{name:"v-61c16c55",path:"/pages/17e650/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-61c16c55").then(t)}},{path:"/pages/17e650/index.html",redirect:"/pages/17e650/"},{path:"/后端/02.JavaEE/07.Spring.html",redirect:"/pages/17e650/"},{name:"v-7d022df1",path:"/pages/dc966e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7d022df1").then(t)}},{path:"/pages/dc966e/index.html",redirect:"/pages/dc966e/"},{path:"/后端/02.JavaEE/10.Dubbo.html",redirect:"/pages/dc966e/"},{name:"v-14c62766",path:"/pages/8cd1ce/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-14c62766").then(t)}},{path:"/pages/8cd1ce/index.html",redirect:"/pages/8cd1ce/"},{path:"/后端/02.JavaEE/08.Spring MVC.html",redirect:"/pages/8cd1ce/"},{name:"v-758de2ee",path:"/pages/ca6c88/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-758de2ee").then(t)}},{path:"/pages/ca6c88/index.html",redirect:"/pages/ca6c88/"},{path:"/后端/02.JavaEE/12.Spring Security.html",redirect:"/pages/ca6c88/"},{name:"v-43b009de",path:"/pages/7bc195/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-43b009de").then(t)}},{path:"/pages/7bc195/index.html",redirect:"/pages/7bc195/"},{path:"/后端/02.JavaEE/11.Zookeeper.html",redirect:"/pages/7bc195/"},{name:"v-16cba74e",path:"/pages/4f5fb3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-16cba74e").then(t)}},{path:"/pages/4f5fb3/index.html",redirect:"/pages/4f5fb3/"},{path:"/后端/02.JavaEE/13.Spring Boot.html",redirect:"/pages/4f5fb3/"},{name:"v-37078826",path:"/pages/3a5e24/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-37078826").then(t)}},{path:"/pages/3a5e24/index.html",redirect:"/pages/3a5e24/"},{path:"/后端/02.JavaEE/15.RabbitMQ.html",redirect:"/pages/3a5e24/"},{name:"v-0f5ad0f4",path:"/pages/da3871/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0f5ad0f4").then(t)}},{path:"/pages/da3871/index.html",redirect:"/pages/da3871/"},{path:"/后端/02.JavaEE/16.RabbitMQ 高级.html",redirect:"/pages/da3871/"},{name:"v-3859f86a",path:"/pages/207023/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3859f86a").then(t)}},{path:"/pages/207023/index.html",redirect:"/pages/207023/"},{path:"/后端/02.JavaEE/14.Spring Boot 高级.html",redirect:"/pages/207023/"},{name:"v-01a7b822",path:"/pages/c588b5/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-01a7b822").then(t)}},{path:"/pages/c588b5/index.html",redirect:"/pages/c588b5/"},{path:"/后端/02.JavaEE/18.Docker.html",redirect:"/pages/c588b5/"},{name:"v-22d9c6ec",path:"/pages/e886f8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-22d9c6ec").then(t)}},{path:"/pages/e886f8/index.html",redirect:"/pages/e886f8/"},{path:"/后端/02.JavaEE/17.Spring Cloud.html",redirect:"/pages/e886f8/"},{name:"v-766b181e",path:"/pages/95da1a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-766b181e").then(t)}},{path:"/pages/95da1a/index.html",redirect:"/pages/95da1a/"},{path:"/后端/02.JavaEE/19.ElasticSearch.html",redirect:"/pages/95da1a/"},{name:"v-bf6ac024",path:"/pages/cf92bb/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-bf6ac024").then(t)}},{path:"/pages/cf92bb/index.html",redirect:"/pages/cf92bb/"},{path:"/后端/02.JavaEE/20.ElasticSearch 高级.html",redirect:"/pages/cf92bb/"},{name:"v-44752817",path:"/pages/0aeef9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-44752817").then(t)}},{path:"/pages/0aeef9/index.html",redirect:"/pages/0aeef9/"},{path:"/后端/03.Linux/03.Nginx.html",redirect:"/pages/0aeef9/"},{name:"v-06144067",path:"/pages/124a07/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-06144067").then(t)}},{path:"/pages/124a07/index.html",redirect:"/pages/124a07/"},{path:"/后端/03.Linux/01.Linux.html",redirect:"/pages/124a07/"},{name:"v-424bdfb7",path:"/pages/a70c4f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-424bdfb7").then(t)}},{path:"/pages/a70c4f/index.html",redirect:"/pages/a70c4f/"},{path:"/后端/03.Linux/04.java.html",redirect:"/pages/a70c4f/"},{name:"v-73daf8d7",path:"/pages/f323f1/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-73daf8d7").then(t)}},{path:"/pages/f323f1/index.html",redirect:"/pages/f323f1/"},{path:"/后端/03.Linux/05.Tomcat.html",redirect:"/pages/f323f1/"},{name:"v-a6e0fd06",path:"/pages/e051f6/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-a6e0fd06").then(t)}},{path:"/pages/e051f6/index.html",redirect:"/pages/e051f6/"},{path:"/后端/04.SQL/01.常见的数据库产品.html",redirect:"/pages/e051f6/"},{name:"v-3d56846d",path:"/pages/f9e4a1/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3d56846d").then(t)}},{path:"/pages/f9e4a1/index.html",redirect:"/pages/f9e4a1/"},{path:"/后端/03.Linux/02.shell.html",redirect:"/pages/f9e4a1/"},{name:"v-3072118a",path:"/pages/641ffa/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3072118a").then(t)}},{path:"/pages/641ffa/index.html",redirect:"/pages/641ffa/"},{path:"/后端/04.SQL/04.MySQL服务端.html",redirect:"/pages/641ffa/"},{name:"v-31cc3bd2",path:"/pages/d2b3d3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-31cc3bd2").then(t)}},{path:"/pages/d2b3d3/index.html",redirect:"/pages/d2b3d3/"},{path:"/后端/04.SQL/03.数据库存储数据的特点.html",redirect:"/pages/d2b3d3/"},{name:"v-b2a04030",path:"/pages/4b2b60/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-b2a04030").then(t)}},{path:"/pages/4b2b60/index.html",redirect:"/pages/4b2b60/"},{path:"/后端/04.SQL/02.数据库相关概念.html",redirect:"/pages/4b2b60/"},{name:"v-3a8393e0",path:"/pages/a4dffe/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3a8393e0").then(t)}},{path:"/pages/a4dffe/index.html",redirect:"/pages/a4dffe/"},{path:"/后端/04.SQL/05.环境变量.html",redirect:"/pages/a4dffe/"},{name:"v-4e1ae1c0",path:"/pages/8c64bc/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4e1ae1c0").then(t)}},{path:"/pages/8c64bc/index.html",redirect:"/pages/8c64bc/"},{path:"/后端/04.SQL/06.命令符指令.html",redirect:"/pages/8c64bc/"},{name:"v-3049bd10",path:"/pages/04fedb/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3049bd10").then(t)}},{path:"/pages/04fedb/index.html",redirect:"/pages/04fedb/"},{path:"/后端/04.SQL/08.数据库.html",redirect:"/pages/04fedb/"},{name:"v-2b35c500",path:"/pages/4c9417/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2b35c500").then(t)}},{path:"/pages/4c9417/index.html",redirect:"/pages/4c9417/"},{path:"/后端/04.SQL/07.SQL语句.html",redirect:"/pages/4c9417/"},{name:"v-64db6885",path:"/pages/de8702/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-64db6885").then(t)}},{path:"/pages/de8702/index.html",redirect:"/pages/de8702/"},{path:"/后端/04.SQL/10.约束.html",redirect:"/pages/de8702/"},{name:"v-16aed12b",path:"/pages/89e8d8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-16aed12b").then(t)}},{path:"/pages/89e8d8/index.html",redirect:"/pages/89e8d8/"},{path:"/后端/04.SQL/11.多表操作.html",redirect:"/pages/89e8d8/"},{name:"v-6265be4e",path:"/pages/5a2265/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6265be4e").then(t)}},{path:"/pages/5a2265/index.html",redirect:"/pages/5a2265/"},{path:"/后端/04.SQL/09.表.html",redirect:"/pages/5a2265/"},{name:"v-d9c864d4",path:"/pages/336822/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-d9c864d4").then(t)}},{path:"/pages/336822/index.html",redirect:"/pages/336822/"},{path:"/后端/04.SQL/14.MySQL 存储过程和函数.html",redirect:"/pages/336822/"},{name:"v-95af4b10",path:"/pages/f561ab/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-95af4b10").then(t)}},{path:"/pages/f561ab/index.html",redirect:"/pages/f561ab/"},{path:"/后端/04.SQL/13.备份.html",redirect:"/pages/f561ab/"},{name:"v-168103c0",path:"/pages/3293cf/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-168103c0").then(t)}},{path:"/pages/3293cf/index.html",redirect:"/pages/3293cf/"},{path:"/后端/04.SQL/12.视图.html",redirect:"/pages/3293cf/"},{name:"v-6dca1220",path:"/pages/4c8706/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6dca1220").then(t)}},{path:"/pages/4c8706/index.html",redirect:"/pages/4c8706/"},{path:"/后端/04.SQL/16.事务.html",redirect:"/pages/4c8706/"},{name:"v-36a8fca6",path:"/pages/7d59ad/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-36a8fca6").then(t)}},{path:"/pages/7d59ad/index.html",redirect:"/pages/7d59ad/"},{path:"/后端/04.SQL/17.存储引擎.html",redirect:"/pages/7d59ad/"},{name:"v-341ec21e",path:"/pages/108b35/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-341ec21e").then(t)}},{path:"/pages/108b35/index.html",redirect:"/pages/108b35/"},{path:"/后端/04.SQL/15.触发器.html",redirect:"/pages/108b35/"},{name:"v-1cd8dcd0",path:"/pages/acc2dd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1cd8dcd0").then(t)}},{path:"/pages/acc2dd/index.html",redirect:"/pages/acc2dd/"},{path:"/后端/04.SQL/18.索引.html",redirect:"/pages/acc2dd/"},{name:"v-c42b5e22",path:"/pages/3ca264/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c42b5e22").then(t)}},{path:"/pages/3ca264/index.html",redirect:"/pages/3ca264/"},{path:"/后端/04.SQL/20.MyCat 中间件.html",redirect:"/pages/3ca264/"},{name:"v-b10c11a2",path:"/pages/4a12e8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-b10c11a2").then(t)}},{path:"/pages/4a12e8/index.html",redirect:"/pages/4a12e8/"},{path:"/后端/04.SQL/21.Nosql.html",redirect:"/pages/4a12e8/"},{name:"v-3a75541f",path:"/pages/3c034d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3a75541f").then(t)}},{path:"/pages/3c034d/index.html",redirect:"/pages/3c034d/"},{path:"/后端/04.SQL/19.锁.html",redirect:"/pages/3c034d/"},{name:"v-47095aee",path:"/pages/4e4f7f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-47095aee").then(t)}},{path:"/pages/4e4f7f/index.html",redirect:"/pages/4e4f7f/"},{path:"/后端/04.SQL/23.Redis高级.html",redirect:"/pages/4e4f7f/"},{name:"v-309ca6de",path:"/pages/a002c8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-309ca6de").then(t)}},{path:"/pages/a002c8/index.html",redirect:"/pages/a002c8/"},{path:"/后端/04.SQL/22.Redis.html",redirect:"/pages/a002c8/"},{name:"v-702958dc",path:"/pages/d37106/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-702958dc").then(t)}},{path:"/pages/d37106/index.html",redirect:"/pages/d37106/"},{path:"/后端/05.Python/04.注释.html",redirect:"/pages/d37106/"},{name:"v-16a2d958",path:"/pages/0c7ca5/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-16a2d958").then(t)}},{path:"/pages/0c7ca5/index.html",redirect:"/pages/0c7ca5/"},{path:"/后端/05.Python/02.close project.html",redirect:"/pages/0c7ca5/"},{name:"v-9c066f90",path:"/pages/f3c5b0/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-9c066f90").then(t)}},{path:"/pages/f3c5b0/index.html",redirect:"/pages/f3c5b0/"},{path:"/后端/05.Python/05.变量以及数据类型.html",redirect:"/pages/f3c5b0/"},{name:"v-0680b9de",path:"/pages/35c9ab/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0680b9de").then(t)}},{path:"/pages/35c9ab/index.html",redirect:"/pages/35c9ab/"},{path:"/后端/05.Python/01.IDE.html",redirect:"/pages/35c9ab/"},{name:"v-410d528c",path:"/pages/ec9ba3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-410d528c").then(t)}},{path:"/pages/ec9ba3/index.html",redirect:"/pages/ec9ba3/"},{path:"/后端/05.Python/03.交互性编程.html",redirect:"/pages/ec9ba3/"},{name:"v-1003aec2",path:"/pages/e5cb41/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1003aec2").then(t)}},{path:"/pages/e5cb41/index.html",redirect:"/pages/e5cb41/"},{path:"/后端/04.SQL/24.MongoDB.html",redirect:"/pages/e5cb41/"},{name:"v-75630836",path:"/pages/11c241/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-75630836").then(t)}},{path:"/pages/11c241/index.html",redirect:"/pages/11c241/"},{path:"/后端/05.Python/08.集合类型.html",redirect:"/pages/11c241/"},{name:"v-8b08f0b2",path:"/pages/d7e80b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-8b08f0b2").then(t)}},{path:"/pages/d7e80b/index.html",redirect:"/pages/d7e80b/"},{path:"/后端/05.Python/06.列表类型.html",redirect:"/pages/d7e80b/"},{name:"v-aa36ff9c",path:"/pages/1f8618/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-aa36ff9c").then(t)}},{path:"/pages/1f8618/index.html",redirect:"/pages/1f8618/"},{path:"/后端/05.Python/10.查看数据类型.html",redirect:"/pages/1f8618/"},{name:"v-51da2d27",path:"/pages/bbada7/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-51da2d27").then(t)}},{path:"/pages/bbada7/index.html",redirect:"/pages/bbada7/"},{path:"/后端/05.Python/07.字典类型.html",redirect:"/pages/bbada7/"},{name:"v-55473949",path:"/pages/684f07/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-55473949").then(t)}},{path:"/pages/684f07/index.html",redirect:"/pages/684f07/"},{path:"/后端/05.Python/09.元组类型.html",redirect:"/pages/684f07/"},{name:"v-7b8967f0",path:"/pages/1d58f1/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7b8967f0").then(t)}},{path:"/pages/1d58f1/index.html",redirect:"/pages/1d58f1/"},{path:"/后端/05.Python/12.输出.html",redirect:"/pages/1d58f1/"},{name:"v-f2fe618e",path:"/pages/d9c568/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-f2fe618e").then(t)}},{path:"/pages/d9c568/index.html",redirect:"/pages/d9c568/"},{path:"/后端/05.Python/11.标识符.html",redirect:"/pages/d9c568/"},{name:"v-47dc7e48",path:"/pages/c9ca02/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-47dc7e48").then(t)}},{path:"/pages/c9ca02/index.html",redirect:"/pages/c9ca02/"},{path:"/后端/05.Python/13.输入.html",redirect:"/pages/c9ca02/"},{name:"v-38a8ef5e",path:"/pages/1e0ec6/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-38a8ef5e").then(t)}},{path:"/pages/1e0ec6/index.html",redirect:"/pages/1e0ec6/"},{path:"/后端/05.Python/16.bin、oct、hex内置函数.html",redirect:"/pages/1e0ec6/"},{name:"v-4601280a",path:"/pages/e533d0/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4601280a").then(t)}},{path:"/pages/e533d0/index.html",redirect:"/pages/e533d0/"},{path:"/后端/05.Python/15.进制转换.html",redirect:"/pages/e533d0/"},{name:"v-457b85c0",path:"/pages/ebc10c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-457b85c0").then(t)}},{path:"/pages/ebc10c/index.html",redirect:"/pages/ebc10c/"},{path:"/后端/05.Python/17.数据类型的转换.html",redirect:"/pages/ebc10c/"},{name:"v-ceac6f68",path:"/pages/6019e1/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-ceac6f68").then(t)}},{path:"/pages/6019e1/index.html",redirect:"/pages/6019e1/"},{path:"/后端/05.Python/14.二进制 八进制 十进制 十六进制.html",redirect:"/pages/6019e1/"},{name:"v-43b7262a",path:"/pages/acb0af/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-43b7262a").then(t)}},{path:"/pages/acb0af/index.html",redirect:"/pages/acb0af/"},{path:"/后端/05.Python/18.算数运算符.html",redirect:"/pages/acb0af/"},{name:"v-314e9546",path:"/pages/687726/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-314e9546").then(t)}},{path:"/pages/687726/index.html",redirect:"/pages/687726/"},{path:"/后端/05.Python/19.分割.html",redirect:"/pages/687726/"},{name:"v-2e61c2f3",path:"/pages/a15165/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2e61c2f3").then(t)}},{path:"/pages/a15165/index.html",redirect:"/pages/a15165/"},{path:"/后端/05.Python/21.提取4位数的各位数.html",redirect:"/pages/a15165/"},{name:"v-05d97d43",path:"/pages/4d8423/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-05d97d43").then(t)}},{path:"/pages/4d8423/index.html",redirect:"/pages/4d8423/"},{path:"/后端/05.Python/22.鸡兔同笼.html",redirect:"/pages/4d8423/"},{name:"v-54d2601a",path:"/pages/c13570/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-54d2601a").then(t)}},{path:"/pages/c13570/index.html",redirect:"/pages/c13570/"},{path:"/后端/05.Python/20.交换变量的值.html",redirect:"/pages/c13570/"},{name:"v-36fd303c",path:"/pages/70c3fd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-36fd303c").then(t)}},{path:"/pages/70c3fd/index.html",redirect:"/pages/70c3fd/"},{path:"/后端/05.Python/25.逻辑运算符.html",redirect:"/pages/70c3fd/"},{name:"v-74d7ff21",path:"/pages/661f38/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-74d7ff21").then(t)}},{path:"/pages/661f38/index.html",redirect:"/pages/661f38/"},{path:"/后端/05.Python/24.比较运算符.html",redirect:"/pages/661f38/"},{name:"v-635d2d7a",path:"/pages/8f779d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-635d2d7a").then(t)}},{path:"/pages/8f779d/index.html",redirect:"/pages/8f779d/"},{path:"/后端/05.Python/26.位运算符.html",redirect:"/pages/8f779d/"},{name:"v-1bd5106a",path:"/pages/bc04ac/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1bd5106a").then(t)}},{path:"/pages/bc04ac/index.html",redirect:"/pages/bc04ac/"},{path:"/后端/05.Python/23.圆的公式.html",redirect:"/pages/bc04ac/"},{name:"v-534da396",path:"/pages/9cbf41/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-534da396").then(t)}},{path:"/pages/9cbf41/index.html",redirect:"/pages/9cbf41/"},{path:"/后端/05.Python/29.if …elif…elif的使用.html",redirect:"/pages/9cbf41/"},{name:"v-87e6b8fc",path:"/pages/689d0c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-87e6b8fc").then(t)}},{path:"/pages/689d0c/index.html",redirect:"/pages/689d0c/"},{path:"/后端/05.Python/27.运算符的优先级.html",redirect:"/pages/689d0c/"},{name:"v-07ac2091",path:"/pages/0f98fb/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-07ac2091").then(t)}},{path:"/pages/0f98fb/index.html",redirect:"/pages/0f98fb/"},{path:"/后端/05.Python/28.if else  分支 条件判断语句.html",redirect:"/pages/0f98fb/"},{name:"v-0c44b074",path:"/pages/921d6f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0c44b074").then(t)}},{path:"/pages/921d6f/index.html",redirect:"/pages/921d6f/"},{path:"/后端/05.Python/32.调试代码(Debug).html",redirect:"/pages/921d6f/"},{name:"v-50b2e256",path:"/pages/a08b12/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-50b2e256").then(t)}},{path:"/pages/a08b12/index.html",redirect:"/pages/a08b12/"},{path:"/后端/05.Python/30.if中的隐性转化.html",redirect:"/pages/a08b12/"},{name:"v-947b9e0a",path:"/pages/6be9fd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-947b9e0a").then(t)}},{path:"/pages/6be9fd/index.html",redirect:"/pages/6be9fd/"},{path:"/后端/05.Python/31.三元表达式.html",redirect:"/pages/6be9fd/"},{name:"v-6f5fde1c",path:"/pages/5f7ab5/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6f5fde1c").then(t)}},{path:"/pages/5f7ab5/index.html",redirect:"/pages/5f7ab5/"},{path:"/后端/05.Python/34.猜拳游戏.html",redirect:"/pages/5f7ab5/"},{name:"v-41675d98",path:"/pages/1f3eec/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-41675d98").then(t)}},{path:"/pages/1f3eec/index.html",redirect:"/pages/1f3eec/"},{path:"/后端/05.Python/33.pass语句.html",redirect:"/pages/1f3eec/"},{name:"v-f624614c",path:"/pages/559847/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-f624614c").then(t)}},{path:"/pages/559847/index.html",redirect:"/pages/559847/"},{path:"/后端/05.Python/35.随机数.html",redirect:"/pages/559847/"},{name:"v-3cd48312",path:"/pages/aed4d8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3cd48312").then(t)}},{path:"/pages/aed4d8/index.html",redirect:"/pages/aed4d8/"},{path:"/后端/05.Python/36.循环.html",redirect:"/pages/aed4d8/"},{name:"v-2075c59e",path:"/pages/d3bc75/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2075c59e").then(t)}},{path:"/pages/d3bc75/index.html",redirect:"/pages/d3bc75/"},{path:"/后端/05.Python/38.快捷键.html",redirect:"/pages/d3bc75/"},{name:"v-9421b7d2",path:"/pages/2d9020/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-9421b7d2").then(t)}},{path:"/pages/2d9020/index.html",redirect:"/pages/2d9020/"},{path:"/后端/05.Python/39.更改某个变量全部代码.html",redirect:"/pages/2d9020/"},{name:"v-88a93a4e",path:"/pages/6bc87f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-88a93a4e").then(t)}},{path:"/pages/6bc87f/index.html",redirect:"/pages/6bc87f/"},{path:"/后端/05.Python/37.range 的使用.html",redirect:"/pages/6bc87f/"},{name:"v-c0ebeed8",path:"/pages/f14379/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c0ebeed8").then(t)}},{path:"/pages/f14379/index.html",redirect:"/pages/f14379/"},{path:"/后端/07.Python模块/02.Beautifulsoup4.html",redirect:"/pages/f14379/"},{name:"v-689f8518",path:"/pages/cf7131/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-689f8518").then(t)}},{path:"/pages/cf7131/index.html",redirect:"/pages/cf7131/"},{path:"/后端/07.Python模块/01.request.html",redirect:"/pages/cf7131/"},{name:"v-94fc8dd8",path:"/pages/4ab7a0/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-94fc8dd8").then(t)}},{path:"/pages/4ab7a0/index.html",redirect:"/pages/4ab7a0/"},{path:"/后端/07.Python模块/04.jieba.html",redirect:"/pages/4ab7a0/"},{name:"v-30e5d4a2",path:"/pages/a3d900/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-30e5d4a2").then(t)}},{path:"/pages/a3d900/index.html",redirect:"/pages/a3d900/"},{path:"/后端/07.Python模块/03.re 正则表达式.html",redirect:"/pages/a3d900/"},{name:"v-4a91f9f8",path:"/pages/34405e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4a91f9f8").then(t)}},{path:"/pages/34405e/index.html",redirect:"/pages/34405e/"},{path:"/后端/07.Python模块/06.selenium.html",redirect:"/pages/34405e/"},{name:"v-5c159d57",path:"/pages/ddf9fb/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5c159d57").then(t)}},{path:"/pages/ddf9fb/index.html",redirect:"/pages/ddf9fb/"},{path:"/后端/08.机器学习/01.机器学习.html",redirect:"/pages/ddf9fb/"},{name:"v-6748a4dc",path:"/pages/fd5355/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6748a4dc").then(t)}},{path:"/pages/fd5355/index.html",redirect:"/pages/fd5355/"},{path:"/后端/07.Python模块/07.time.html",redirect:"/pages/fd5355/"},{name:"v-16076614",path:"/pages/a6ad03/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-16076614").then(t)}},{path:"/pages/a6ad03/index.html",redirect:"/pages/a6ad03/"},{path:"/后端/07.Python模块/08.Falsk.html",redirect:"/pages/a6ad03/"},{name:"v-3e534332",path:"/pages/f8134b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3e534332").then(t)}},{path:"/pages/f8134b/index.html",redirect:"/pages/f8134b/"},{path:"/后端/07.Python模块/05.pymysql  数据库调用.html",redirect:"/pages/f8134b/"},{name:"v-56a1eebf",path:"/pages/9e1ba1/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-56a1eebf").then(t)}},{path:"/pages/9e1ba1/index.html",redirect:"/pages/9e1ba1/"},{path:"/后端/08.机器学习/02.matplotlib.html",redirect:"/pages/9e1ba1/"},{name:"v-6c324d6f",path:"/pages/a58615/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6c324d6f").then(t)}},{path:"/pages/a58615/index.html",redirect:"/pages/a58615/"},{path:"/后端/08.机器学习/03.Numpy.html",redirect:"/pages/a58615/"},{name:"v-c631dd4e",path:"/pages/3f7274/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c631dd4e").then(t)}},{path:"/pages/3f7274/index.html",redirect:"/pages/3f7274/"},{path:"/后端/08.机器学习/04.Pandas.html",redirect:"/pages/3f7274/"},{name:"v-9c664796",path:"/pages/981612/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-9c664796").then(t)}},{path:"/pages/981612/index.html",redirect:"/pages/981612/"},{path:"/大数据/01.Hadoop/04.winutils.html",redirect:"/pages/981612/"},{name:"v-3b8fa256",path:"/pages/8e0c98/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3b8fa256").then(t)}},{path:"/pages/8e0c98/index.html",redirect:"/pages/8e0c98/"},{path:"/大数据/01.Hadoop/01.Hadoop.html",redirect:"/pages/8e0c98/"},{name:"v-ef3e7878",path:"/pages/083fca/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-ef3e7878").then(t)}},{path:"/pages/083fca/index.html",redirect:"/pages/083fca/"},{path:"/大数据/01.Hadoop/02.环境安装.html",redirect:"/pages/083fca/"},{name:"v-91bab1e0",path:"/pages/9ce9a0/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-91bab1e0").then(t)}},{path:"/pages/9ce9a0/index.html",redirect:"/pages/9ce9a0/"},{path:"/大数据/01.Hadoop/06.java操作.html",redirect:"/pages/9ce9a0/"},{name:"v-9c36cd5e",path:"/pages/7e6b01/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-9c36cd5e").then(t)}},{path:"/pages/7e6b01/index.html",redirect:"/pages/7e6b01/"},{path:"/大数据/01.Hadoop/05.IDEA中创建hadoop项目.html",redirect:"/pages/7e6b01/"},{name:"v-0cd4d5ee",path:"/pages/bbfc59/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-0cd4d5ee").then(t)}},{path:"/pages/bbfc59/index.html",redirect:"/pages/bbfc59/"},{path:"/大数据/01.Hadoop/07.HDFS的数据流.html",redirect:"/pages/bbfc59/"},{name:"v-4672ef15",path:"/pages/7afcbc/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4672ef15").then(t)}},{path:"/pages/7afcbc/index.html",redirect:"/pages/7afcbc/"},{path:"/大数据/01.Hadoop/03.HDFS.html",redirect:"/pages/7afcbc/"},{name:"v-00dc2e8f",path:"/pages/785074/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-00dc2e8f").then(t)}},{path:"/pages/785074/index.html",redirect:"/pages/785074/"},{path:"/大数据/01.Hadoop/08.NameNode 工作机制.html",redirect:"/pages/785074/"},{name:"v-724628f5",path:"/pages/49d0d4/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-724628f5").then(t)}},{path:"/pages/49d0d4/index.html",redirect:"/pages/49d0d4/"},{path:"/大数据/01.Hadoop/09.DataNode.html",redirect:"/pages/49d0d4/"},{name:"v-1c17faff",path:"/pages/070d96/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1c17faff").then(t)}},{path:"/pages/070d96/index.html",redirect:"/pages/070d96/"},{path:"/大数据/01.Hadoop/10.MapReduce.html",redirect:"/pages/070d96/"},{name:"v-7f22fd75",path:"/pages/89b57e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7f22fd75").then(t)}},{path:"/pages/89b57e/index.html",redirect:"/pages/89b57e/"},{path:"/大数据/01.Hadoop/12.Yarn.html",redirect:"/pages/89b57e/"},{name:"v-2e69f406",path:"/pages/ce529c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2e69f406").then(t)}},{path:"/pages/ce529c/index.html",redirect:"/pages/ce529c/"},{path:"/大数据/01.Hadoop/13.Hadoop企业优化.html",redirect:"/pages/ce529c/"},{name:"v-5c5d9838",path:"/pages/1cc354/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5c5d9838").then(t)}},{path:"/pages/1cc354/index.html",redirect:"/pages/1cc354/"},{path:"/大数据/01.Hadoop/15.日志.html",redirect:"/pages/1cc354/"},{name:"v-f87607aa",path:"/pages/0dc6f9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-f87607aa").then(t)}},{path:"/pages/0dc6f9/index.html",redirect:"/pages/0dc6f9/"},{path:"/大数据/01.Hadoop/16.Hadoop HA高可用.html",redirect:"/pages/0dc6f9/"},{name:"v-6594ad68",path:"/pages/8dd6c9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6594ad68").then(t)}},{path:"/pages/8dd6c9/index.html",redirect:"/pages/8dd6c9/"},{path:"/大数据/01.Hadoop/14.Hadoop 新特性.html",redirect:"/pages/8dd6c9/"},{name:"v-4b94238d",path:"/pages/65db7d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4b94238d").then(t)}},{path:"/pages/65db7d/index.html",redirect:"/pages/65db7d/"},{path:"/大数据/01.Hadoop/11.MapReduce原理.html",redirect:"/pages/65db7d/"},{name:"v-369ccb08",path:"/pages/f38fc8/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-369ccb08").then(t)}},{path:"/pages/f38fc8/index.html",redirect:"/pages/f38fc8/"},{path:"/大数据/02.Zookeeper/01.概述.html",redirect:"/pages/f38fc8/"},{name:"v-72b26eaf",path:"/pages/4a9c91/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-72b26eaf").then(t)}},{path:"/pages/4a9c91/index.html",redirect:"/pages/4a9c91/"},{path:"/大数据/02.Zookeeper/05.Api.html",redirect:"/pages/4a9c91/"},{name:"v-98591bee",path:"/pages/1597a2/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-98591bee").then(t)}},{path:"/pages/1597a2/index.html",redirect:"/pages/1597a2/"},{path:"/大数据/02.Zookeeper/02.集群搭建.html",redirect:"/pages/1597a2/"},{name:"v-c1183f22",path:"/pages/75493d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c1183f22").then(t)}},{path:"/pages/75493d/index.html",redirect:"/pages/75493d/"},{path:"/大数据/02.Zookeeper/04.内部原理.html",redirect:"/pages/75493d/"},{name:"v-b7e4068e",path:"/pages/66f25c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-b7e4068e").then(t)}},{path:"/pages/66f25c/index.html",redirect:"/pages/66f25c/"},{path:"/大数据/03.Hive/02.环境.html",redirect:"/pages/66f25c/"},{name:"v-051079fa",path:"/pages/be648b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-051079fa").then(t)}},{path:"/pages/be648b/index.html",redirect:"/pages/be648b/"},{path:"/大数据/02.Zookeeper/03.客户端命令行操作.html",redirect:"/pages/be648b/"},{name:"v-00667d92",path:"/pages/2315d6/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-00667d92").then(t)}},{path:"/pages/2315d6/index.html",redirect:"/pages/2315d6/"},{path:"/大数据/03.Hive/01.介绍.html",redirect:"/pages/2315d6/"},{name:"v-752ff4e6",path:"/pages/f55408/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-752ff4e6").then(t)}},{path:"/pages/f55408/index.html",redirect:"/pages/f55408/"},{path:"/大数据/03.Hive/04.Hive 类型.html",redirect:"/pages/f55408/"},{name:"v-79c818bb",path:"/pages/5a826b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-79c818bb").then(t)}},{path:"/pages/5a826b/index.html",redirect:"/pages/5a826b/"},{path:"/大数据/03.Hive/03.DBeaver.html",redirect:"/pages/5a826b/"},{name:"v-3874a2bf",path:"/pages/6652e3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3874a2bf").then(t)}},{path:"/pages/6652e3/index.html",redirect:"/pages/6652e3/"},{path:"/大数据/03.Hive/05.Hive 客户端命令.html",redirect:"/pages/6652e3/"},{name:"v-7688bc37",path:"/pages/675adb/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7688bc37").then(t)}},{path:"/pages/675adb/index.html",redirect:"/pages/675adb/"},{path:"/大数据/03.Hive/06.DDL数据定义.html",redirect:"/pages/675adb/"},{name:"v-132275ff",path:"/pages/ff1fce/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-132275ff").then(t)}},{path:"/pages/ff1fce/index.html",redirect:"/pages/ff1fce/"},{path:"/大数据/03.Hive/07.DML.html",redirect:"/pages/ff1fce/"},{name:"v-37a0dd10",path:"/pages/8f0a49/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-37a0dd10").then(t)}},{path:"/pages/8f0a49/index.html",redirect:"/pages/8f0a49/"},{path:"/大数据/03.Hive/09.函数.html",redirect:"/pages/8f0a49/"},{name:"v-7abe8cca",path:"/pages/b1e5c6/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7abe8cca").then(t)}},{path:"/pages/b1e5c6/index.html",redirect:"/pages/b1e5c6/"},{path:"/大数据/03.Hive/10.自定义函数.html",redirect:"/pages/b1e5c6/"},{name:"v-a20663ee",path:"/pages/9c39ac/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-a20663ee").then(t)}},{path:"/pages/9c39ac/index.html",redirect:"/pages/9c39ac/"},{path:"/大数据/03.Hive/11.压缩和存储.html",redirect:"/pages/9c39ac/"},{name:"v-17274f0b",path:"/pages/73e227/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-17274f0b").then(t)}},{path:"/pages/73e227/index.html",redirect:"/pages/73e227/"},{path:"/大数据/03.Hive/08.查询.html",redirect:"/pages/73e227/"},{name:"v-a7b2b846",path:"/pages/447a54/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-a7b2b846").then(t)}},{path:"/pages/447a54/index.html",redirect:"/pages/447a54/"},{path:"/大数据/03.Hive/12.企业优化.html",redirect:"/pages/447a54/"},{name:"v-07e506ef",path:"/pages/b084db/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-07e506ef").then(t)}},{path:"/pages/b084db/index.html",redirect:"/pages/b084db/"},{path:"/大数据/04.Flume/01.Flume.html",redirect:"/pages/b084db/"},{name:"v-7aff2c77",path:"/pages/1aa014/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-7aff2c77").then(t)}},{path:"/pages/1aa014/index.html",redirect:"/pages/1aa014/"},{path:"/大数据/03.Hive/13.Hive实战.html",redirect:"/pages/1aa014/"},{name:"v-9ed6a6cc",path:"/pages/e462b7/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-9ed6a6cc").then(t)}},{path:"/pages/e462b7/index.html",redirect:"/pages/e462b7/"},{path:"/大数据/04.Flume/03.Flume安装.html",redirect:"/pages/e462b7/"},{name:"v-55b2d0b0",path:"/pages/c12064/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-55b2d0b0").then(t)}},{path:"/pages/c12064/index.html",redirect:"/pages/c12064/"},{path:"/大数据/04.Flume/04.入门案例.html",redirect:"/pages/c12064/"},{name:"v-73eab2aa",path:"/pages/6a0c06/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-73eab2aa").then(t)}},{path:"/pages/6a0c06/index.html",redirect:"/pages/6a0c06/"},{path:"/大数据/04.Flume/05.Flume 进阶.html",redirect:"/pages/6a0c06/"},{name:"v-6e080d03",path:"/pages/f8bd8e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6e080d03").then(t)}},{path:"/pages/f8bd8e/index.html",redirect:"/pages/f8bd8e/"},{path:"/大数据/04.Flume/02.Flime基础架构.html",redirect:"/pages/f8bd8e/"},{name:"v-1391410f",path:"/pages/872cdc/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1391410f").then(t)}},{path:"/pages/872cdc/index.html",redirect:"/pages/872cdc/"},{path:"/大数据/05.Kafka/01.Kafka.html",redirect:"/pages/872cdc/"},{name:"v-210176a7",path:"/pages/8e2fbd/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-210176a7").then(t)}},{path:"/pages/8e2fbd/index.html",redirect:"/pages/8e2fbd/"},{path:"/大数据/04.Flume/06.自定义组件.html",redirect:"/pages/8e2fbd/"},{name:"v-598b6c9d",path:"/pages/800f9f/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-598b6c9d").then(t)}},{path:"/pages/800f9f/index.html",redirect:"/pages/800f9f/"},{path:"/大数据/04.Flume/07.面试题.html",redirect:"/pages/800f9f/"},{name:"v-8c34a962",path:"/pages/3c04a9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-8c34a962").then(t)}},{path:"/pages/3c04a9/index.html",redirect:"/pages/3c04a9/"},{path:"/大数据/05.Kafka/02.架构.html",redirect:"/pages/3c04a9/"},{name:"v-4254887a",path:"/pages/6bd0d7/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-4254887a").then(t)}},{path:"/pages/6bd0d7/index.html",redirect:"/pages/6bd0d7/"},{path:"/大数据/05.Kafka/03.Kafka 安装.html",redirect:"/pages/6bd0d7/"},{name:"v-43f57a12",path:"/pages/ab6220/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-43f57a12").then(t)}},{path:"/pages/ab6220/index.html",redirect:"/pages/ab6220/"},{path:"/大数据/05.Kafka/04.命令操作.html",redirect:"/pages/ab6220/"},{name:"v-3f9e4bb2",path:"/pages/939509/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3f9e4bb2").then(t)}},{path:"/pages/939509/index.html",redirect:"/pages/939509/"},{path:"/大数据/05.Kafka/06.Kafka API.html",redirect:"/pages/939509/"},{name:"v-3cd13c66",path:"/pages/6c830a/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3cd13c66").then(t)}},{path:"/pages/6c830a/index.html",redirect:"/pages/6c830a/"},{path:"/大数据/05.Kafka/05.Kafka原理.html",redirect:"/pages/6c830a/"},{name:"v-1e6e69af",path:"/pages/cf0e70/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-1e6e69af").then(t)}},{path:"/pages/cf0e70/index.html",redirect:"/pages/cf0e70/"},{path:"/大数据/05.Kafka/07.Flume 对接 Kafka.html",redirect:"/pages/cf0e70/"},{name:"v-6a373828",path:"/pages/f5b30c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6a373828").then(t)}},{path:"/pages/f5b30c/index.html",redirect:"/pages/f5b30c/"},{path:"/大数据/05.Kafka/08.Kafka监控.html",redirect:"/pages/f5b30c/"},{name:"v-64817218",path:"/pages/c7cfc3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-64817218").then(t)}},{path:"/pages/c7cfc3/index.html",redirect:"/pages/c7cfc3/"},{path:"/大数据/06.Azkaban/02.任务调度.html",redirect:"/pages/c7cfc3/"},{name:"v-55a5d0e4",path:"/pages/394c5d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-55a5d0e4").then(t)}},{path:"/pages/394c5d/index.html",redirect:"/pages/394c5d/"},{path:"/大数据/06.Azkaban/03.安装.html",redirect:"/pages/394c5d/"},{name:"v-266c9e22",path:"/pages/bac882/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-266c9e22").then(t)}},{path:"/pages/bac882/index.html",redirect:"/pages/bac882/"},{path:"/大数据/06.Azkaban/01.Azkaban.html",redirect:"/pages/bac882/"},{name:"v-3c82fd62",path:"/pages/310c39/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3c82fd62").then(t)}},{path:"/pages/310c39/index.html",redirect:"/pages/310c39/"},{path:"/大数据/07.Hbase/01.Hbase.html",redirect:"/pages/310c39/"},{name:"v-c86974ba",path:"/pages/cf96cc/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c86974ba").then(t)}},{path:"/pages/cf96cc/index.html",redirect:"/pages/cf96cc/"},{path:"/大数据/05.Kafka/09.Kafka面试题.html",redirect:"/pages/cf96cc/"},{name:"v-3f1fc75e",path:"/pages/4ff79d/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-3f1fc75e").then(t)}},{path:"/pages/4ff79d/index.html",redirect:"/pages/4ff79d/"},{path:"/大数据/06.Azkaban/04.Azkaban实战.html",redirect:"/pages/4ff79d/"},{name:"v-c73f1002",path:"/pages/a8d091/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-c73f1002").then(t)}},{path:"/pages/a8d091/index.html",redirect:"/pages/a8d091/"},{path:"/大数据/07.Hbase/03.Hbase 安装.html",redirect:"/pages/a8d091/"},{name:"v-d5889de6",path:"/pages/817c32/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-d5889de6").then(t)}},{path:"/pages/817c32/index.html",redirect:"/pages/817c32/"},{path:"/大数据/07.Hbase/02.Hbase数据模型.html",redirect:"/pages/817c32/"},{name:"v-13acc28d",path:"/pages/ec28f2/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-13acc28d").then(t)}},{path:"/pages/ec28f2/index.html",redirect:"/pages/ec28f2/"},{path:"/大数据/07.Hbase/05.Hbase原理.html",redirect:"/pages/ec28f2/"},{name:"v-41cba2f2",path:"/pages/73c948/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-41cba2f2").then(t)}},{path:"/pages/73c948/index.html",redirect:"/pages/73c948/"},{path:"/大数据/07.Hbase/04.Hbase shell.html",redirect:"/pages/73c948/"},{name:"v-283892a2",path:"/pages/6dd88b/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-283892a2").then(t)}},{path:"/pages/6dd88b/index.html",redirect:"/pages/6dd88b/"},{path:"/大数据/07.Hbase/06.Phoenix.html",redirect:"/pages/6dd88b/"},{name:"v-5a8af3d8",path:"/pages/ce507c/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-5a8af3d8").then(t)}},{path:"/pages/ce507c/index.html",redirect:"/pages/ce507c/"},{path:"/大数据/07.Hbase/08.HBase优化.html",redirect:"/pages/ce507c/"},{name:"v-2117d120",path:"/pages/e654ab/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-2117d120").then(t)}},{path:"/pages/e654ab/index.html",redirect:"/pages/e654ab/"},{path:"/大数据/07.Hbase/07.Hbase与Hive的集成.html",redirect:"/pages/e654ab/"},{name:"v-d5c2e428",path:"/pages/a93b77/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-d5c2e428").then(t)}},{path:"/pages/a93b77/index.html",redirect:"/pages/a93b77/"},{path:"/大数据/08.Scala/02.Scala入门.html",redirect:"/pages/a93b77/"},{name:"v-6a5e0750",path:"/pages/8b0acc/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-6a5e0750").then(t)}},{path:"/pages/8b0acc/index.html",redirect:"/pages/8b0acc/"},{path:"/大数据/08.Scala/03.流程控制.html",redirect:"/pages/8b0acc/"},{name:"v-762d1d8f",path:"/pages/cc783e/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-762d1d8f").then(t)}},{path:"/pages/cc783e/index.html",redirect:"/pages/cc783e/"},{path:"/大数据/09.Spark/01.Spark.html",redirect:"/pages/cc783e/"},{name:"v-a108e506",path:"/pages/3223e3/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-a108e506").then(t)}},{path:"/pages/3223e3/index.html",redirect:"/pages/3223e3/"},{path:"/大数据/09.Spark/02.Spark 入门.html",redirect:"/pages/3223e3/"},{name:"v-fc416092",path:"/pages/b7cb84/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-fc416092").then(t)}},{path:"/pages/b7cb84/index.html",redirect:"/pages/b7cb84/"},{path:"/大数据/08.Scala/04.函数式编程.html",redirect:"/pages/b7cb84/"},{name:"v-dbe517a8",path:"/pages/d2afbe/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-dbe517a8").then(t)}},{path:"/pages/d2afbe/index.html",redirect:"/pages/d2afbe/"},{path:"/大数据/08.Scala/01.Scala介绍.html",redirect:"/pages/d2afbe/"},{name:"v-52219ca2",path:"/pages/a0eb57/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-52219ca2").then(t)}},{path:"/pages/a0eb57/index.html",redirect:"/pages/a0eb57/"},{path:"/大数据/09.Spark/03.SprakCore.html",redirect:"/pages/a0eb57/"},{name:"v-12efaf51",path:"/pages/0de4e9/",component:ol,beforeEnter:function(n,e,t){Zi("Layout","v-12efaf51").then(t)}},{path:"/pages/0de4e9/index.html",redirect:"/pages/0de4e9/"},{path:"/大数据/09.Spark/04.SparkSQL.html",redirect:"/pages/0de4e9/"},{path:"*",component:ol}],sl={title:"",description:"",base:"/",headTags:[["link",{rel:"icon",href:"/img/favicon.ico"}],["link",{rel:"stylesheet",href:"//at.alicdn.com/t/font_3114978_qe0b39no76.css"}],["link",{rel:"stylesheet",href:"//at.alicdn.com/t/font_3114978_qe0b39no76.css"}],["meta",{name:"referrer",content:"no-referrer-when-downgrade"}],["link",{rel:"stylesheet",href:"https://at.alicdn.com/t/font_3077305_pt8umhrn4k9.css"}],["meta",{name:"keywords",content:"后端博客,个人技术博客,后端/前端开发,后端/前端框架,web前端,后端面试题,技术文档,学习,面试,Java,Scala,Hadoop,Spring,Spring Boot,Spring Cloud,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown"}],["meta",{name:"baidu-site-verification",content:"7F55weZDDc"}],["meta",{name:"theme-color",content:"#11a8cd"}]],pages:[{title:"最基本的数据结构",frontmatter:{title:"最基本的数据结构",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/38d7c8/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/02.%E6%9C%80%E5%9F%BA%E6%9C%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html",relativePath:"408/01.数据结构/02.最基本的数据结构.md",key:"v-28a7b348",path:"/pages/38d7c8/",headersStr:null,content:"# 最基本的数据结构\n\n 1. 数组\n    \n    便于寻址 不便于增删数据\n\n 2. 链表\n    \n    便于增删数据 不便于寻址",normalizedContent:"# 最基本的数据结构\n\n 1. 数组\n    \n    便于寻址 不便于增删数据\n\n 2. 链表\n    \n    便于增删数据 不便于寻址",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"random的随机行为",frontmatter:{title:"random的随机行为",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/64e136/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/04.random%E7%9A%84%E9%9A%8F%E6%9C%BA%E8%A1%8C%E4%B8%BA.html",relativePath:"408/01.数据结构/04.random的随机行为.md",key:"v-7ab55fd6",path:"/pages/64e136/",headers:[{level:2,title:"给定一个1~5等概率随机的函数 返回一个7~15等概率的新函数",slug:"给定一个1-5等概率随机的函数-返回一个7-15等概率的新函数",normalizedTitle:"给定一个1~5等概率随机的函数 返回一个7~15等概率的新函数",charIndex:394},{level:2,title:"给定一个0~1不等概率的随机函数 返回0~1等概率随机函数",slug:"给定一个0-1不等概率的随机函数-返回0-1等概率随机函数",normalizedTitle:"给定一个0~1不等概率的随机函数 返回0~1等概率随机函数",charIndex:808}],headersStr:"给定一个1~5等概率随机的函数 返回一个7~15等概率的新函数 给定一个0~1不等概率的随机函数 返回0~1等概率随机函数",content:"# random的随机行为\n\nMath.random中的随机数 为伪随机数 产生一个 [0,1) 浮点型数\n\n并且每个数产生的概率 为等概率 生成的数字是均匀的，也就是说该区间内部的每个数字生成的几率是相同的\n\nint sum = 0;\nfor (int i = 0; i <= 100000; i++) {\n    double ans =  Math.random();\n    if(ans < 0.3) {\n        sum++;\n    }\n}\n//产生 100000 次 [0,1)的随机数 如果小于0.3则计数\n//查询小于0.3出现的概率  为30%左右  所以我们可以判断MJath.random的随机行为为等概率的 \nSystem.out.println((double)sum /100000.0 );\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 给定一个1~5等概率随机的函数 返回一个7~15等概率的新函数\n\n首先1~5是等概率的 我们需要将此函数改为为只返回 0 和 1 的等概率函数\n\n即如果给定的随机函数返回结果为3则重新生成随机数 直到生成不为3的结果 这样我们就可以获得一个 0 ~ 1 等概率随机生成的函数\n\n7 ~ 15 为9个数 则对应的二进制为 1 0 1 需要3个二进制位 我们需要调用3次改造后的 0~1等概率发生器\n\n第一次结果左位移2位 第二次结果左位移1位 第三次结果不位移, 最后将3次结果相加,这样我们获得0~13的等概率函数, 并且我们进行截取 超过9的数重新调用 0~13等概率函数\n\ndo{\n    int ans =  f4();//0~13等概率函数\n}while(ans > 9);\nreturn ans;\n\n\n1\n2\n3\n4\n\n\n这样我们获得一个0~9的等概率发生器 最后调用0~9等概率发生器 + 7则完成此新函数\n\n\n# 给定一个0~1不等概率的随机函数 返回0~1等概率随机函数\n\n首先0~1是不等概率 我们通过调用两次此函数\n\n0 0 概率则为 p*p\n\n1 1 概率则为 (1-p)*(1-p)\n\n0 1 概率为 p*(1-p)\n\n1 0 概率为 (1-p)*p\n\n通过筛选 01 和 10 结果即可以返回一个等概率的函数\n\n01则返回0\n\n10则返回1\n\n// 给定一个 0和1不等概率的函数\npublic static int x() {\n\treturn Math.random() < 0.8 ? 0 : 1;\n}\n\n// 返回一个等概率0和1的新函数\npublic static int x2() {\n\tint ans = 0;\n\tdo {\n\t\tans = x();\n\t} while (ans == x()); //如果两次都结果为0或1则重新调用x函数\n    //只有 ans = 0 x()返回1\n    //和 ans = 1 x()返回0 才返回ans\n\treturn ans;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n",normalizedContent:"# random的随机行为\n\nmath.random中的随机数 为伪随机数 产生一个 [0,1) 浮点型数\n\n并且每个数产生的概率 为等概率 生成的数字是均匀的，也就是说该区间内部的每个数字生成的几率是相同的\n\nint sum = 0;\nfor (int i = 0; i <= 100000; i++) {\n    double ans =  math.random();\n    if(ans < 0.3) {\n        sum++;\n    }\n}\n//产生 100000 次 [0,1)的随机数 如果小于0.3则计数\n//查询小于0.3出现的概率  为30%左右  所以我们可以判断mjath.random的随机行为为等概率的 \nsystem.out.println((double)sum /100000.0 );\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 给定一个1~5等概率随机的函数 返回一个7~15等概率的新函数\n\n首先1~5是等概率的 我们需要将此函数改为为只返回 0 和 1 的等概率函数\n\n即如果给定的随机函数返回结果为3则重新生成随机数 直到生成不为3的结果 这样我们就可以获得一个 0 ~ 1 等概率随机生成的函数\n\n7 ~ 15 为9个数 则对应的二进制为 1 0 1 需要3个二进制位 我们需要调用3次改造后的 0~1等概率发生器\n\n第一次结果左位移2位 第二次结果左位移1位 第三次结果不位移, 最后将3次结果相加,这样我们获得0~13的等概率函数, 并且我们进行截取 超过9的数重新调用 0~13等概率函数\n\ndo{\n    int ans =  f4();//0~13等概率函数\n}while(ans > 9);\nreturn ans;\n\n\n1\n2\n3\n4\n\n\n这样我们获得一个0~9的等概率发生器 最后调用0~9等概率发生器 + 7则完成此新函数\n\n\n# 给定一个0~1不等概率的随机函数 返回0~1等概率随机函数\n\n首先0~1是不等概率 我们通过调用两次此函数\n\n0 0 概率则为 p*p\n\n1 1 概率则为 (1-p)*(1-p)\n\n0 1 概率为 p*(1-p)\n\n1 0 概率为 (1-p)*p\n\n通过筛选 01 和 10 结果即可以返回一个等概率的函数\n\n01则返回0\n\n10则返回1\n\n// 给定一个 0和1不等概率的函数\npublic static int x() {\n\treturn math.random() < 0.8 ? 0 : 1;\n}\n\n// 返回一个等概率0和1的新函数\npublic static int x2() {\n\tint ans = 0;\n\tdo {\n\t\tans = x();\n\t} while (ans == x()); //如果两次都结果为0或1则重新调用x函数\n    //只有 ans = 0 x()返回1\n    //和 ans = 1 x()返回0 才返回ans\n\treturn ans;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"前缀和数组",frontmatter:{title:"前缀和数组",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/3ad1d4/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/03.%E5%89%8D%E7%BC%80%E5%92%8C%E6%95%B0%E7%BB%84.html",relativePath:"408/01.数据结构/03.前缀和数组.md",key:"v-e58ada02",path:"/pages/3ad1d4/",headersStr:null,content:"# 前缀和数组\n\n假设我们需要求指定数组中 l ~ r 范围元素的和\n\n方法一:我们使用一个二维数组存储所有l-r的元素和的结果 (需要n*n/2的空间)\n\n方法二:使用前缀和数组 求出 每个元素 0~元素本身的和(需要n的空间存储结果)\n\n当我们求 3 ~ 7 范围的和 我们返回 ans[7]即0~7的结果 减去 ans[2]即0~2的结果 即可以快速取得指定范围内的合\n\nsum[i···j] = arr[0···j] - arr[0···i-1]\n\n此方法我们称为 前缀和数组",normalizedContent:"# 前缀和数组\n\n假设我们需要求指定数组中 l ~ r 范围元素的和\n\n方法一:我们使用一个二维数组存储所有l-r的元素和的结果 (需要n*n/2的空间)\n\n方法二:使用前缀和数组 求出 每个元素 0~元素本身的和(需要n的空间存储结果)\n\n当我们求 3 ~ 7 范围的和 我们返回 ans[7]即0~7的结果 减去 ans[2]即0~2的结果 即可以快速取得指定范围内的合\n\nsum[i···j] = arr[0···j] - arr[0···i-1]\n\n此方法我们称为 前缀和数组",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"位运算",frontmatter:{title:"位运算",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/8e5251/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/01.%E4%BD%8D%E8%BF%90%E7%AE%97.html",relativePath:"408/01.数据结构/01.位运算.md",key:"v-89816e08",path:"/pages/8e5251/",headers:[{level:2,title:"原码",slug:"原码",normalizedTitle:"原码",charIndex:315},{level:2,title:"反码",slug:"反码",normalizedTitle:"反码",charIndex:390},{level:2,title:"补码",slug:"补码",normalizedTitle:"补码",charIndex:485},{level:2,title:"负数的最小值取负为多少",slug:"负数的最小值取负为多少",normalizedTitle:"负数的最小值取负为多少",charIndex:628},{level:2,title:"打印二进制",slug:"打印二进制",normalizedTitle:"打印二进制",charIndex:1031}],headersStr:"原码 反码 补码 负数的最小值取负为多少 打印二进制",content:'# 位运算\n\n符号    描述     运算规则\n&     与      两个位都为1时，结果才为1\n|     或      两个位都为0时，结果才为0\n^     异或     两个位相同为0，相异为1\n~     取反     0变1，1变0\n<<    左移     各二进位全部左移若干位，高位丢弃，低位补0\n>>    右移     各二进位全部右移若干位，对无符号数，高位补0，有符号数，各编译器处理方法不一样，有的补符号位（算术右移），有的补0（逻辑右移），Java中为算术右移根据符合位来补（如为负数则补1，正数补0）\n>>>   逻辑右移   各二进位全部右移若干位，对无符号数，高位补0，有符号数\n\n\n# 原码\n\n原码就是符号位加上真值 以下以8位来实例\n\n+1的原码为\n0000 0001\n-1的原码为\n1000 0001\n\n\n1\n2\n3\n4\n\n\n\n# 反码\n\n正数的反码是其本身\n\n负数的反码是在其原码的基础上, 符号位不变，其余各个位取反\n\n+1的反码为\n0000 0001\n-1的反码为\n1111 1110\n\n\n1\n2\n3\n4\n\n\n\n# 补码\n\n通常一个数的二进制是以补码的形式展现\n\n 1. 正数的补码就是其本身\n 2. 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)\n\n+1的补码为\n0000 0001\n-1的补码为\n1111 1111\n\n\n1\n2\n3\n4\n\n\n\n# 负数的最小值取负为多少\n\n即负数的最小值 取反 再+1\n\npublic static void main(String[] args) {\n\tint c = Integer.MIN_VALUE; \n\tint d = -c;\n\tint e = ~c + 1;\n\tSystem.out.println(c); //-2147483648\n\tSystem.out.println(d); //-2147483648\n\tSystem.out.println(e); //-2147483648\n    c = 0; \n\td = -c;\n\te = ~c + 1;\n\tSystem.out.println(c); //0\n\tSystem.out.println(d); //0\n\tSystem.out.println(e); //0\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 打印二进制\n\npublic static void print(int num) {\n\tfor (int i = 31; i >=0; i--) {\n        //把1左移n位 和原数做与运算 两者为1时输出1否则输出\n\t\tSystem.out.print((num & (1 << i)) == 0 ? "0" : "1");\n\t}\n\tSystem.out.println();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n',normalizedContent:'# 位运算\n\n符号    描述     运算规则\n&     与      两个位都为1时，结果才为1\n|     或      两个位都为0时，结果才为0\n^     异或     两个位相同为0，相异为1\n~     取反     0变1，1变0\n<<    左移     各二进位全部左移若干位，高位丢弃，低位补0\n>>    右移     各二进位全部右移若干位，对无符号数，高位补0，有符号数，各编译器处理方法不一样，有的补符号位（算术右移），有的补0（逻辑右移），java中为算术右移根据符合位来补（如为负数则补1，正数补0）\n>>>   逻辑右移   各二进位全部右移若干位，对无符号数，高位补0，有符号数\n\n\n# 原码\n\n原码就是符号位加上真值 以下以8位来实例\n\n+1的原码为\n0000 0001\n-1的原码为\n1000 0001\n\n\n1\n2\n3\n4\n\n\n\n# 反码\n\n正数的反码是其本身\n\n负数的反码是在其原码的基础上, 符号位不变，其余各个位取反\n\n+1的反码为\n0000 0001\n-1的反码为\n1111 1110\n\n\n1\n2\n3\n4\n\n\n\n# 补码\n\n通常一个数的二进制是以补码的形式展现\n\n 1. 正数的补码就是其本身\n 2. 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)\n\n+1的补码为\n0000 0001\n-1的补码为\n1111 1111\n\n\n1\n2\n3\n4\n\n\n\n# 负数的最小值取负为多少\n\n即负数的最小值 取反 再+1\n\npublic static void main(string[] args) {\n\tint c = integer.min_value; \n\tint d = -c;\n\tint e = ~c + 1;\n\tsystem.out.println(c); //-2147483648\n\tsystem.out.println(d); //-2147483648\n\tsystem.out.println(e); //-2147483648\n    c = 0; \n\td = -c;\n\te = ~c + 1;\n\tsystem.out.println(c); //0\n\tsystem.out.println(d); //0\n\tsystem.out.println(e); //0\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 打印二进制\n\npublic static void print(int num) {\n\tfor (int i = 31; i >=0; i--) {\n        //把1左移n位 和原数做与运算 两者为1时输出1否则输出\n\t\tsystem.out.print((num & (1 << i)) == 0 ? "0" : "1");\n\t}\n\tsystem.out.println();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"对数器",frontmatter:{title:"对数器",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/9c9f24/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/05.%E5%AF%B9%E6%95%B0%E5%99%A8.html",relativePath:"408/01.数据结构/05.对数器.md",key:"v-16106010",path:"/pages/9c9f24/",headersStr:null,content:"# 对数器\n\n1.有一个你想要测的方法a； 2.实现一个绝对正确但是复杂度不好的方法b； 3.实现一个随机样本产生器； 4.实现对比算法a和b的方法； 5.把方法a和方法b比对多次来验证方法a是否正确； 6.如果有一个样本使得比对出错，打印样本分析是哪个方法出错； 7.当样本数量很多时比对测试依然正确，可以确定方法a已经正确。",normalizedContent:"# 对数器\n\n1.有一个你想要测的方法a； 2.实现一个绝对正确但是复杂度不好的方法b； 3.实现一个随机样本产生器； 4.实现对比算法a和b的方法； 5.把方法a和方法b比对多次来验证方法a是否正确； 6.如果有一个样本使得比对出错，打印样本分析是哪个方法出错； 7.当样本数量很多时比对测试依然正确，可以确定方法a已经正确。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"二分法查找",frontmatter:{title:"二分法查找",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/06b58c/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/06.%E4%BA%8C%E5%88%86%E6%B3%95%E6%9F%A5%E6%89%BE.html",relativePath:"408/01.数据结构/06.二分法查找.md",key:"v-159f3ccd",path:"/pages/06b58c/",headersStr:null,content:"# 二分法查找\n\n二分法查找，也称为折半法，是一种在有序数组中查找特定元素的搜索算法。\n\n（1）首先，从数组的中间元素开始搜索，如果该元素正好是目标元素，则搜索过程结束，否则执行下一步。\n\n（2）如果目标元素大于/小于中间元素，则在数组大于/小于中间元素的那一半区域查找，然后重复步骤（1）的操作。\n\n（3）如果某一步数组为空，则表示找不到目标元素。\n\n二分法查找的时间复杂度O(logn)。",normalizedContent:"# 二分法查找\n\n二分法查找，也称为折半法，是一种在有序数组中查找特定元素的搜索算法。\n\n（1）首先，从数组的中间元素开始搜索，如果该元素正好是目标元素，则搜索过程结束，否则执行下一步。\n\n（2）如果目标元素大于/小于中间元素，则在数组大于/小于中间元素的那一半区域查找，然后重复步骤（1）的操作。\n\n（3）如果某一步数组为空，则表示找不到目标元素。\n\n二分法查找的时间复杂度o(logn)。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"方法参数传递是值还是引用",frontmatter:{title:"方法参数传递是值还是引用",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/7e9265/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/07.%E6%96%B9%E6%B3%95%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92%E6%98%AF%E5%80%BC%E8%BF%98%E6%98%AF%E5%BC%95%E7%94%A8.html",relativePath:"408/01.数据结构/07.方法参数传递是值还是引用.md",key:"v-2f6093d7",path:"/pages/7e9265/",headersStr:null,content:"# 方法参数传递是值还是引用\n\n 1. 如果为基本数据类型（8种）则为值传递\n 2. 引用类型可分为类引用类型（类）、接口引用类型（接口）和数组引用类型（数组）(其实引用传递是特殊值传递,传递为内存的地址)\n 3. 包装类比较特殊 java会自动拆箱 虽然实参传递是引用地址 但形参已经是拆箱后的内容 两者不相同无法作修改\n\n结论:\n\n 1. Java基本数据类型传递参数时是值传递；\n    \n    引用类型传递参数时是引用传递。\n\n 2. 值传递时，将实参的值传递一份给形参；\n    \n    引用传递时，将实参的地址值传递一份给形参。\n\n 3. 值传递时，实参把它的值传递给对应的形参，函数接收的是原始值的一个拷贝，此时内存中存在两个相等的基本类型，即实参和形参，后面方法中的操作都是对形参这个值的修改，不影响实参的值。\n    \n    引用传递时，实参的引用(地址，而不是参数的值)被传递给方法中相对应的形参，函数接收的是原始值的内存地址；在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。",normalizedContent:"# 方法参数传递是值还是引用\n\n 1. 如果为基本数据类型（8种）则为值传递\n 2. 引用类型可分为类引用类型（类）、接口引用类型（接口）和数组引用类型（数组）(其实引用传递是特殊值传递,传递为内存的地址)\n 3. 包装类比较特殊 java会自动拆箱 虽然实参传递是引用地址 但形参已经是拆箱后的内容 两者不相同无法作修改\n\n结论:\n\n 1. java基本数据类型传递参数时是值传递；\n    \n    引用类型传递参数时是引用传递。\n\n 2. 值传递时，将实参的值传递一份给形参；\n    \n    引用传递时，将实参的地址值传递一份给形参。\n\n 3. 值传递时，实参把它的值传递给对应的形参，函数接收的是原始值的一个拷贝，此时内存中存在两个相等的基本类型，即实参和形参，后面方法中的操作都是对形参这个值的修改，不影响实参的值。\n    \n    引用传递时，实参的引用(地址，而不是参数的值)被传递给方法中相对应的形参，函数接收的是原始值的内存地址；在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"位图",frontmatter:{title:"位图",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/d2cd3f/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/09.%E4%BD%8D%E5%9B%BE.html",relativePath:"408/01.数据结构/09.位图.md",key:"v-c36f6bfc",path:"/pages/d2cd3f/",headersStr:null,content:"# 位图\n\n一个int是4字节 32位,一个long类型是8字节 64位，我们可以使用1个long每一位表示一个数字 可以存储0~63 64个数字，开辟一个long类型数组就可以存储多个数字\n\n\n\n\tpublic static class BitMap {\n\t\tprivate long[] bits;\n\n\t\tpublic BitMap(int max) { //最大范围 用于开辟数组空间\n\t\t\tbits = new long[(max + 64) >> 6]; // 相当(x+64) / 64\n\t\t}\n\n\t\tpublic void add(int num) {\n\t\t\t//num & 63 相对应 num % 64 只适用于2的阶乘\n\t\t\tbits[num >> 6] |= (1L << (num & 63));\n\t\t}\n\n\t\tpublic void delete(int num) {\n\t\t\t//1L << (num & 63) 的结果假设为 1000000 取反后则为 01111111\n\t\t\tbits[num >> 6] &= ~(1L << (num & 63));\n\t\t}\n\n\t\tpublic boolean contains(int num) {\n\t\t\treturn (bits[num >> 6] & (1L << (num & 63))) != 0;\n\t\t}\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n",normalizedContent:"# 位图\n\n一个int是4字节 32位,一个long类型是8字节 64位，我们可以使用1个long每一位表示一个数字 可以存储0~63 64个数字，开辟一个long类型数组就可以存储多个数字\n\n\n\n\tpublic static class bitmap {\n\t\tprivate long[] bits;\n\n\t\tpublic bitmap(int max) { //最大范围 用于开辟数组空间\n\t\t\tbits = new long[(max + 64) >> 6]; // 相当(x+64) / 64\n\t\t}\n\n\t\tpublic void add(int num) {\n\t\t\t//num & 63 相对应 num % 64 只适用于2的阶乘\n\t\t\tbits[num >> 6] |= (1l << (num & 63));\n\t\t}\n\n\t\tpublic void delete(int num) {\n\t\t\t//1l << (num & 63) 的结果假设为 1000000 取反后则为 01111111\n\t\t\tbits[num >> 6] &= ~(1l << (num & 63));\n\t\t}\n\n\t\tpublic boolean contains(int num) {\n\t\t\treturn (bits[num >> 6] & (1l << (num & 63))) != 0;\n\t\t}\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"位运算实现四则运算",frontmatter:{title:"位运算实现四则运算",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/51bf81/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/10.%E4%BD%8D%E8%BF%90%E7%AE%97%E5%AE%9E%E7%8E%B0%E5%9B%9B%E5%88%99%E8%BF%90%E7%AE%97.html",relativePath:"408/01.数据结构/10.位运算实现四则运算.md",key:"v-11649cf0",path:"/pages/51bf81/",headers:[{level:2,title:"加法",slug:"加法",normalizedTitle:"加法",charIndex:16},{level:2,title:"减法",slug:"减法",normalizedTitle:"减法",charIndex:492},{level:2,title:"乘法",slug:"乘法",normalizedTitle:"乘法",charIndex:628},{level:2,title:"除法",slug:"除法",normalizedTitle:"除法",charIndex:942},{level:2,title:"29. 两数相除",slug:"_29-两数相除",normalizedTitle:"29. 两数相除",charIndex:1418}],headersStr:"加法 减法 乘法 除法 29. 两数相除",content:"# 位运算实现四则运算\n\n\n# 加法\n\n异或运算 是一个无进位加法\n\nint a = 46; //0101110\nint b = 20; //0010100\nSystem.out.println(a^b); //58 ==> 0111010\nSystem.out.println(a&b); //4 ==> 0000100  进位位置\nSystem.out.println((a&b) << 1);//8 ==>0001000   需要左移一位才是进位后的结果\n//不断的无进位加法 + 进位信息 直到进行信息为空则为 加法\n\n\n1\n2\n3\n4\n5\n6\n\n\n完整写法\n\n\tpublic static int add(int a,int b) {\n\t\tint sum = a;\n\t\twhile (b != 0) { //直接进位信息为空\n\t\t\tsum = a ^ b; //无进位相加\n\t\t\tb = (a&b)<<1; // 进位信息\n\t\t\ta = sum; //a变成无进位信息 加法的结果\n\t\t}\n\t\treturn sum;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 减法\n\na - b = a + (-b) = a + (~b + 1)\n\n//减法\n\tpublic static int sub(int a,int b) {\n\t\tb = add(~b,1);\n\t\treturn add(a,b);\n\t}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 乘法\n\n\n\n如果乘数当前位为1，则取被乘数左移一位的结果加到最终结果中；如果当前位为0，则取0加到乘积中\n\n\t//乘法\n\tpublic static int multiply(int a, int b) {\n\t\tint sum = 0;\n\t\twhile (b != 0) {\n\t\t\tif ((b & 1) != 0) { //如果b 与 1 不等于0则需要相加\n\t\t\t\tsum = add(sum, a);\n\t\t\t}\n\t\t\ta = a << 1; //a左移一位0补全\n\t\t\tb = b >>> 1; //b右移一位\n\t\t}\n\t\treturn sum;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 除法\n\n\tpublic static int div(int a, int b) {\n\t\tint x = a < 0 ? add(~a, 1) : a; // 判断是否为负数 如是则转为绝对值计算\n\t\tint y = b < 0 ? add(~b, 1) : b;\n\t\tint res = 0;\n\t\tfor (int i = 30; i >= 0; i = add(i, -1)) { // i-- ==> add(i,-1)\n\t\t\tif ((x >> i) >= y) { // 如果x右移i位 >= 被除数 则进入分支\n\t\t\t\tres |= (1 << i); // 将倍数(位数)结果增加到ans中\n\t\t\t\tx = add(x, add(~y, 1) << i); // 减掉n倍的被除数\n\t\t\t\t\n\t\t\t}\n\t\t}\n\t\treturn (a < 0 == b < 0) ? res : add(~res, 1); // 判断ab两数符号是否相等 不相等则返回相反数\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 29. 两数相除\n\n\t// 加法\n\tpublic static int add(int a, int b) {\n\t\tint sum = a;\n\t\twhile (b != 0) { // 直接进位信息为空\n\t\t\tsum = a ^ b; // 无进位相加\n\t\t\tb = (a & b) << 1; // 进位信息\n\t\t\ta = sum; // a变成无进位信息 加法的结果\n\t\t}\n\t\treturn sum;\n\t}\n\n\t// 减法\n\tpublic static int sub(int a, int b) {\n\t\tb = add(~b, 1); // 取反+1\n\t\treturn add(a, b);\n\t}\n\n\t// 乘法\n\tpublic static int multiply(int a, int b) {\n\t\tint sum = 0;\n\t\twhile (b != 0) {\n\t\t\tif ((b & 1) != 0) {\n\t\t\t\tsum = add(sum, a);\n\t\t\t}\n\t\t\ta = a << 1;\n\t\t\tb = b >>> 1;\n\t\t}\n\t\treturn sum;\n\t}\n\n\t// 除法\n\tpublic static int div(int a, int b) {\n\t\tint x = a < 0 ? add(~a, 1) : a; // 判断是否为负数 如是则转为绝对值计算\n\t\tint y = b < 0 ? add(~b, 1) : b;\n\t\tint res = 0;\n\t\tfor (int i = 30; i >= 0; i = add(i, -1)) { // i-- ==> add(i,-1)\n\t\t\tif ((x >> i) >= y) { // 如果x右移i位 >= 被除数 则进入分支\n\t\t\t\tres |= (1 << i); // 将倍数(位数)结果增加到ans中\n\t\t\t\tx = add(x, add(~y, 1) << i); // 减掉n倍的被除数\n\t\t\t\t\n\t\t\t}\n\t\t}\n\t\treturn (a < 0 == b < 0) ? res : add(~res, 1); // 判断ab两数符号是否相等 不相等则返回相反数\n\t}\n\n\tpublic static int divide(int a, int b) {\n\t\tif (a == Integer.MIN_VALUE && b == Integer.MIN_VALUE) { // a和b都为最小值 返回1\n\t\t\treturn 1;\n\t\t} else if (b == Integer.MIN_VALUE) { // 被除数为最小值 返回0\n\t\t\treturn 0;\n\t\t} else if (a == Integer.MIN_VALUE) { // 除数为最小值\n\t\t\tif (b == add(~1, 1)) { // 被除数为-1 返回最大值\n\t\t\t\treturn Integer.MAX_VALUE;\n\t\t\t} else {\n\t\t\t\tint c = div(add(a, 1), b); // (a+1)/b = c\n\t\t\t\tint e = multiply(b, c); // b*c = e\n\t\t\t\tint f = add(a, add(~e, 1)); // a-(b*c) = f\n\t\t\t\tint q = div(f, b); // f/b = q\n\t\t\t\treturn add(c, q); // c+q\n\t\t\t}\n\t\t} else {\n\t\t\treturn div(a, b);\n\t\t}\n\t}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n",normalizedContent:"# 位运算实现四则运算\n\n\n# 加法\n\n异或运算 是一个无进位加法\n\nint a = 46; //0101110\nint b = 20; //0010100\nsystem.out.println(a^b); //58 ==> 0111010\nsystem.out.println(a&b); //4 ==> 0000100  进位位置\nsystem.out.println((a&b) << 1);//8 ==>0001000   需要左移一位才是进位后的结果\n//不断的无进位加法 + 进位信息 直到进行信息为空则为 加法\n\n\n1\n2\n3\n4\n5\n6\n\n\n完整写法\n\n\tpublic static int add(int a,int b) {\n\t\tint sum = a;\n\t\twhile (b != 0) { //直接进位信息为空\n\t\t\tsum = a ^ b; //无进位相加\n\t\t\tb = (a&b)<<1; // 进位信息\n\t\t\ta = sum; //a变成无进位信息 加法的结果\n\t\t}\n\t\treturn sum;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 减法\n\na - b = a + (-b) = a + (~b + 1)\n\n//减法\n\tpublic static int sub(int a,int b) {\n\t\tb = add(~b,1);\n\t\treturn add(a,b);\n\t}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 乘法\n\n\n\n如果乘数当前位为1，则取被乘数左移一位的结果加到最终结果中；如果当前位为0，则取0加到乘积中\n\n\t//乘法\n\tpublic static int multiply(int a, int b) {\n\t\tint sum = 0;\n\t\twhile (b != 0) {\n\t\t\tif ((b & 1) != 0) { //如果b 与 1 不等于0则需要相加\n\t\t\t\tsum = add(sum, a);\n\t\t\t}\n\t\t\ta = a << 1; //a左移一位0补全\n\t\t\tb = b >>> 1; //b右移一位\n\t\t}\n\t\treturn sum;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 除法\n\n\tpublic static int div(int a, int b) {\n\t\tint x = a < 0 ? add(~a, 1) : a; // 判断是否为负数 如是则转为绝对值计算\n\t\tint y = b < 0 ? add(~b, 1) : b;\n\t\tint res = 0;\n\t\tfor (int i = 30; i >= 0; i = add(i, -1)) { // i-- ==> add(i,-1)\n\t\t\tif ((x >> i) >= y) { // 如果x右移i位 >= 被除数 则进入分支\n\t\t\t\tres |= (1 << i); // 将倍数(位数)结果增加到ans中\n\t\t\t\tx = add(x, add(~y, 1) << i); // 减掉n倍的被除数\n\t\t\t\t\n\t\t\t}\n\t\t}\n\t\treturn (a < 0 == b < 0) ? res : add(~res, 1); // 判断ab两数符号是否相等 不相等则返回相反数\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 29. 两数相除\n\n\t// 加法\n\tpublic static int add(int a, int b) {\n\t\tint sum = a;\n\t\twhile (b != 0) { // 直接进位信息为空\n\t\t\tsum = a ^ b; // 无进位相加\n\t\t\tb = (a & b) << 1; // 进位信息\n\t\t\ta = sum; // a变成无进位信息 加法的结果\n\t\t}\n\t\treturn sum;\n\t}\n\n\t// 减法\n\tpublic static int sub(int a, int b) {\n\t\tb = add(~b, 1); // 取反+1\n\t\treturn add(a, b);\n\t}\n\n\t// 乘法\n\tpublic static int multiply(int a, int b) {\n\t\tint sum = 0;\n\t\twhile (b != 0) {\n\t\t\tif ((b & 1) != 0) {\n\t\t\t\tsum = add(sum, a);\n\t\t\t}\n\t\t\ta = a << 1;\n\t\t\tb = b >>> 1;\n\t\t}\n\t\treturn sum;\n\t}\n\n\t// 除法\n\tpublic static int div(int a, int b) {\n\t\tint x = a < 0 ? add(~a, 1) : a; // 判断是否为负数 如是则转为绝对值计算\n\t\tint y = b < 0 ? add(~b, 1) : b;\n\t\tint res = 0;\n\t\tfor (int i = 30; i >= 0; i = add(i, -1)) { // i-- ==> add(i,-1)\n\t\t\tif ((x >> i) >= y) { // 如果x右移i位 >= 被除数 则进入分支\n\t\t\t\tres |= (1 << i); // 将倍数(位数)结果增加到ans中\n\t\t\t\tx = add(x, add(~y, 1) << i); // 减掉n倍的被除数\n\t\t\t\t\n\t\t\t}\n\t\t}\n\t\treturn (a < 0 == b < 0) ? res : add(~res, 1); // 判断ab两数符号是否相等 不相等则返回相反数\n\t}\n\n\tpublic static int divide(int a, int b) {\n\t\tif (a == integer.min_value && b == integer.min_value) { // a和b都为最小值 返回1\n\t\t\treturn 1;\n\t\t} else if (b == integer.min_value) { // 被除数为最小值 返回0\n\t\t\treturn 0;\n\t\t} else if (a == integer.min_value) { // 除数为最小值\n\t\t\tif (b == add(~1, 1)) { // 被除数为-1 返回最大值\n\t\t\t\treturn integer.max_value;\n\t\t\t} else {\n\t\t\t\tint c = div(add(a, 1), b); // (a+1)/b = c\n\t\t\t\tint e = multiply(b, c); // b*c = e\n\t\t\t\tint f = add(a, add(~e, 1)); // a-(b*c) = f\n\t\t\t\tint q = div(f, b); // f/b = q\n\t\t\t\treturn add(c, q); // c+q\n\t\t\t}\n\t\t} else {\n\t\t\treturn div(a, b);\n\t\t}\n\t}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"排序",frontmatter:{title:"排序",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/0dcdc5/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/12.%E6%8E%92%E5%BA%8F.html",relativePath:"408/01.数据结构/12.排序.md",key:"v-5bd2c1fd",path:"/pages/0dcdc5/",headers:[{level:3,title:"归并排序",slug:"归并排序",normalizedTitle:"归并排序",charIndex:9},{level:3,title:"快速排序",slug:"快速排序",normalizedTitle:"快速排序",charIndex:9057}],headersStr:"归并排序 快速排序",content:'# 排序\n\n\n# 归并排序\n\n该算法是采用分治法的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。\n\npublic static void mergeSort1(int[] arr) {\n\t\tif (arr == null || arr.length < 2) { // 为空 或 小于两个元素无法排序\n\t\t\treturn;\n\t\t}\n\t\tprocess(arr, 0, arr.length - 1);\n\t}\n\n\tpublic static void process(int[] arr, int l, int r) {\n\t\tif (l == r) { // 只有一个元素\n\t\t\treturn;\n\t\t}\n\t\tint mid = l + ((r - l) / 2); // 为了防止越界 相当于(l+r)/2\n\t\tprocess(arr, l, mid); // 递归分治\n\t\tprocess(arr, mid + 1, r); // 递归\n\t\tmerge(arr, l, mid, r); // 合并子序列\n\n\t}\n\n\tpublic static void merge(int[] arr, int l, int m, int r) {\n\t\tint[] prearr = new int[r - l + 1]; // 开辟存储排序好的数组 左右组有多少个元素就开辟多少个位置\n\t\tint index = 0; // 存储数组指针\n\t\tint p1 = l; // 左组指针\n\t\tint p2 = m + 1; // 右组指针\n\t\twhile (p1 <= m && p2 <= r) { // 防止左右指针越界\n\t\t\tprearr[index++] = arr[p1] <= arr[p2] ? arr[p1++] : arr[p2++]; // 比较大小\n\t\t}\n\t\twhile (p1 <= m) { // 左指针未越界\n\t\t\tprearr[index++] = arr[p1++];\n\t\t}\n\t\twhile (p2 <= r) { // 右指针未越界\n\t\t\tprearr[index++] = arr[p2++];\n\t\t}\n\t\t// 归并到原始数组\n\t\tfor (int i = 0; i < prearr.length; i++) {\n\t\t\tarr[l + i] = prearr[i];\n\t\t}\n\t}\n\n\t//非递归\n\tpublic static void mergeSort2(int[] arr) {\n\t\tif (arr == null || arr.length < 2) { // 为空或小于2个元素\n\t\t\treturn;\n\t\t}\n\t\tint step = 1; // 步长为 1 2 4 8 16 32 2的次方(即多少个为一组)\n\t\tint N = arr.length; // 长度\n\t\twhile (step < N) { // 如果步长 超过 长度\n\t\t\tint L = 0; // 左指针\n\t\t\twhile (L < N) {\n\t\t\t\tint M = 0; // 右指针重置\n\t\t\t\tif (N - L >= step) { // 右指针到左指针 大于等于步长\n\t\t\t\t\tM = L + step - 1; // 右指针赋值\n\t\t\t\t} else {\n\t\t\t\t\tM = N - 1; // 否则右指针为 数组长度-1 为了赋值数值溢出\n\t\t\t\t}\n\t\t\t\tif (M == N - 1) { // 如果左组的 右指针为数组中最后一个则 无右组比较 直接break\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tint R = 0; // 右组右指针\n\t\t\t\tif (N - 1 - M >= step) { // 右组是否能凑齐step个元素\n\t\t\t\t\tR = M + step;\n\t\t\t\t} else { // 无法凑齐 右指针到数组尾元素\n\t\t\t\t\tR = N - 1;\n\t\t\t\t}\n\t\t\t\tmerge(arr, L, M, R); // 合并区间 L为左组左指针 M为左组右指针 M+1为右组左指针 R为右组右指针\n\t\t\t\tif (R == N - 1) { // 右组右指针到数组尾 结束本次步长\n\t\t\t\t\tbreak;\n\t\t\t\t} else { // 否则重新赋值左组左指针 进行一个大组的区间合并\n\t\t\t\t\tL = R + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (step > N / 2) { // 当前步长不能凑齐左右两组 进行合并区间\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstep *= 2; // 步长增加\n\t\t}\n\n\t}\n\t\n\t// 非递归方法实现\n\tpublic static void mergeSort3(int[] arr) {\n\t\tif (arr == null || arr.length < 2) {\n\t\t\treturn;\n\t\t}\n\t\tint N = arr.length;\n\t\tint mergeSize = 1;\n\t\twhile (mergeSize < N) {\n\t\t\tint L = 0;\n\t\t\twhile (L < N) {\n\t\t\t\tif (mergeSize >= N - L) { //当前步长大于剩下为合并的元素\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tint M = L + mergeSize - 1; //左组右指针\n\t\t\t\tint R = M + Math.min(mergeSize, N - M - 1); //右组右指针\n\t\t\t\tmerge(arr, L, M, R);\n\t\t\t\tL = R + 1;\n\t\t\t}\n\t\t\tif (mergeSize > N / 2) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmergeSize <<= 1;\n\t\t}\n\t}\n\n\t// for test\n\tpublic static int[] generateRandomArray(int maxSize, int maxValue) {\n\t\tint[] arr = new int[(int) ((maxSize + 1) * Math.random())];\n\t\tfor (int i = 0; i < arr.length; i++) {\n\t\t\tarr[i] = (int) ((maxValue + 1) * Math.random()) - (int) (maxValue * Math.random());\n\t\t}\n\t\treturn arr;\n\t}\n\n\t// for test\n\tpublic static int[] copyArray(int[] arr) {\n\t\tif (arr == null) {\n\t\t\treturn null;\n\t\t}\n\t\tint[] res = new int[arr.length];\n\t\tfor (int i = 0; i < arr.length; i++) {\n\t\t\tres[i] = arr[i];\n\t\t}\n\t\treturn res;\n\t}\n\n\t// for test\n\tpublic static boolean isEqual(int[] arr1, int[] arr2) {\n\t\tif ((arr1 == null && arr2 != null) || (arr1 != null && arr2 == null)) {\n\t\t\treturn false;\n\t\t}\n\t\tif (arr1 == null && arr2 == null) {\n\t\t\treturn true;\n\t\t}\n\t\tif (arr1.length != arr2.length) {\n\t\t\treturn false;\n\t\t}\n\t\tfor (int i = 0; i < arr1.length; i++) {\n\t\t\tif (arr1[i] != arr2[i]) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\treturn true;\n\t}\n\n\t// for test\n\tpublic static void printArray(int[] arr) {\n\t\tif (arr == null) {\n\t\t\treturn;\n\t\t}\n\t\tfor (int i = 0; i < arr.length; i++) {\n\t\t\tSystem.out.print(arr[i] + " ");\n\t\t}\n\t\tSystem.out.println();\n\t}\n\n\t// 对数器\n\tpublic static void main(String[] args) {\n\t\tint testTime = 500000;\n\t\tint maxSize = 100;\n\t\tint maxValue = 100;\n\t\tSystem.out.println("测试开始");\n\t\tfor (int i = 0; i < testTime; i++) {\n\t\t\tint[] arr1 = generateRandomArray(maxSize, maxValue);\n\t\t\tint[] arr2 = copyArray(arr1);\n\t\t\tmergeSort1(arr1);\n\t\t\tmergeSort2(arr2);\n\t\t\tif (!isEqual(arr1, arr2)) {\n\t\t\t\tSystem.out.println("出错了！");\n\t\t\t\tprintArray(arr1);\n\t\t\t\tprintArray(arr2);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tSystem.out.println("测试结束");\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n\n\n# 求数组小和\n\n给一个数组arr[L-R]范围既要排好序,也要返回每个元素在排序前比当前元素小的和\n\npublic static int smallSum(int[] arr) {\n\t\tif (arr == null || arr.length < 2) {\n\t\t\treturn 0;\n\t\t}\n\t\treturn process(arr, 0, arr.length - 1);\n\t}\n\n\tprivate static int process(int[] arr, int l, int r) {\n\t\tif (l == r) {\n\t\t\treturn 0;\n\t\t}\n\t\tint mid = l + ((r - l) >> 1);\n\t\treturn process(arr, l, mid) + process(arr, mid + 1, r) + merge(arr, l, mid, r);\n\t}\n\n\tprivate static int merge(int[] arr, int l, int mid, int r) {\n\t\tint[] arr2 = new int[r - l + 1];\n\t\tint i = 0;\n\t\tint p1 = l;\n\t\tint p2 = mid + 1;\n\t\tint ans = 0; // 当前归并最小和\n\t\twhile (p1 <= mid && p2 <= r) {\n\t\t\tans += arr[p1] < arr[p2] ? arr[p1] * (r - p2 + 1) : 0; // 如果先放入左组 则代表有r-p2+1个元素大于当前左组的元素 否则没有最小\n\t\t\tarr2[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++]; // 比较大小归并 如等于先放右组避免将相同值元素统计到r-p2+1中\n\t\t}\n\n\t\twhile (p1 <= mid) {\n\t\t\tarr2[i++] = arr[p1++];\n\t\t}\n\n\t\twhile (p2 <= r) {\n\t\t\tarr2[i++] = arr[p2++];\n\t\t}\n\n\t\tfor (i = 0; i < arr2.length; i++) {\n\t\t\tarr[l + i] = arr2[i];\n\t\t}\n\n\t\treturn ans;\n\t}\n\n\t// for test\n\tpublic static int comparator(int[] arr) {\n\t\tif (arr == null || arr.length < 2) {\n\t\t\treturn 0;\n\t\t}\n\t\tint res = 0;\n\t\tfor (int i = 1; i < arr.length; i++) {\n\t\t\tfor (int j = 0; j < i; j++) {\n\t\t\t\tres += arr[j] < arr[i] ? arr[j] : 0;\n\t\t\t}\n\t\t}\n\t\treturn res;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n# 求数组中的逆序对数量\n\n给定一个数组arr，求数组的降序对总数量\n\n在一个数组中，任何一个前面的数a，和任何一个后面的数b，如果(a,b)是降序的，就称为降序对\n\npublic static int reverPairNumber(int[] arr) {\n\t\tif (arr == null || arr.length == 0) {\n\t\t\treturn 0;\n\t\t}\n\t\treturn prcess(arr, 0, arr.length - 1);\n\t}\n\n\tpublic static int prcess(int[] arr, int l, int r) {\n\t\tif (l == r) {\n\t\t\treturn 0;\n\t\t}\n\t\tint m = l + ((r - l) >> 1);\n\t\treturn prcess(arr, l, m) + prcess(arr, m + 1, r) + merge(arr, l, m, r);\n\t}\n\n\tpublic static int merge(int[] arr, int l, int m, int r) {\n\t\tint ans = 0;\n\t\tint[] help = new int[r - l + 1];\n\t\t// 倒着遍历\n\t\tint i = help.length - 1; // 从尾部开始\n\t\tint p1 = m; // 左边边界\n\t\tint p2 = r; // 右组边界\n\t\twhile (p1 >= l && p2 > m) {\n\t\t\tans += arr[p1] > arr[p2] ? (p2 - m) : 0; // 如果左边指针数大于右边指针数则为降序对\n\t\t\thelp[i--] = arr[p1] > arr[p2] ? arr[p1--] : arr[p2--];\n\t\t}\n\t\twhile (p1 >= l) {\n\t\t\thelp[i--] = arr[p1--];\n\t\t}\n\t\twhile (p2 > m) { // 注意到m就停止\n\t\t\thelp[i--] = arr[p2--];\n\t\t}\n\t\tfor (int j = 0; j < help.length; j++) {\n\t\t\tarr[l + j] = help[j];\n\t\t}\n\t\treturn ans;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n# 493. 翻转对\n\n在一个数组中，对于任何一个数num，求有多少个(后面的数*2)依然<num，返回总个数\n\n比如：[3,1,7,0,2]\n3的后面有：1，0\n1的后面有：0\n7的后面有：0，2\n0的后面没有\n2的后面没有\n所以总共有5个\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 327. 区间和的个数\n\npublic int countRangeSum(int[] nums, int lower, int upper) {\n\t\tif (nums == null || nums.length == 0) {\n\t\t\treturn 0;\n\t\t}\n\t\t// 前缀和数组\n\t\tlong[] sum = new long[nums.length];\n\t\tsum[0] = nums[0];\n\t\tfor (int i = 1; i < nums.length; i++) {\n\t\t\tsum[i] = sum[i - 1] + nums[i];\n\n\t\t}\n\t\t// 原数组 已无用 传递前缀和数组\n\t\treturn process(sum, 0, nums.length - 1, lower, upper);\n\t}\n\n\tprivate int process(long[] sum, int l, int r, int lower, int upper) {\n\t\tif (l == r) {\n\t\t\t// 只有一个数时 判断是否在lower和upper范围内 是则res+1\n\t\t\treturn sum[l] >= lower && sum[l] <= upper ? 1 : 0;\n\n\t\t}\n\t\tint m = l + ((r - l) >> 1);\n\t\t// 递归+合并\n\t\treturn process(sum, l, m, lower, upper) + process(sum, m + 1, r, lower, upper)\n\t\t\t\t+ merge(sum, l, m, r, lower, upper);\n\t}\n\n\tprivate int merge(long[] sum, int l, int m, int r, int lower, int upper) {\n\t\tint ans = 0;\n\t\tint windowL = l;\n\t\tint windowR = l; // [windowL, windowR)\n\t\tfor (int i = m + 1; i <= r; i++) { // 从右组开始遍历\n\t\t\tlong min = sum[i] - upper;\n\t\t\tlong max = sum[i] - lower;\n\t\t\twhile (windowR <= m && sum[windowR] <= max) {\n\t\t\t\twindowR++; // 找到最大值下标边界\n\t\t\t}\n\t\t\twhile (windowL <= m && sum[windowL] < min) {\n\t\t\t\twindowL++; // 最小值下标边界\n\t\t\t}\n\t\t\tans += windowR - windowL; // 右组当前元素有多少个在范围内的\n\t\t}\n\n\t\t//归并\n\t\tlong[] help = new long[r - l + 1];\n\t\tint i = 0;\n\t\tint p1 = l;\n\t\tint p2 = m + 1;\n\t\twhile (p1 <= m && p2 <= r) {\n\t\t\thelp[i++] = sum[p1] <= sum[p2] ? sum[p1++] : sum[p2++];\n\t\t}\n\t\twhile (p1 <= m) {\n\t\t\thelp[i++] = sum[p1++];\n\t\t}\n\t\twhile (p2 <= r) {\n\t\t\thelp[i++] = sum[p2++];\n\t\t}\n\t\tfor (int j = 0; j < help.length; j++) {\n\t\t\tsum[l + j] = help[j];\n\t\t}\n\n\t\treturn ans;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n\n# 快速排序\n\n\t// 递归经典写法\n\tpublic static void QuickSort(int[] arr, int l, int r) {\n\t\tif (l >= r) {\n\t\t\treturn;\n\t\t}\n\t\tint left = l;\n\t\tint right = r;\n\t\tint base = arr[r]; // 以最后一个为基准 如果以头为基准则应该先找大于基准 再找小于基准的\n\t\twhile (left < right) {\n\t\t\t// 将小于等于base的放到左边 则应该left++\n\t\t\twhile (arr[left] <= base && left < right) {\n\t\t\t\tleft++;\n\t\t\t}\n\t\t\t// 将大于等于base的放到右边 则right--\n\t\t\twhile (arr[right] >= base && left < right) {\n\t\t\t\tright--;\n\t\t\t}\n\t\t\tif (left < right) {\n\t\t\t\t// 交换\n\t\t\t\tint temp = arr[left];\n\t\t\t\tarr[left] = arr[right];\n\t\t\t\tarr[right] = temp;\n\t\t\t}\n\n\t\t}\n\t\tarr[r] = arr[left]; // 将小于基准的最后一个数 交换到base位置\n\t\tarr[left] = base; // 将base值 交换到小于基准的位置\n\t\t// 拆分大问题 递归\n\t\tQuickSort(arr, l, left - 1);\n\t\tQuickSort(arr, right + 1, r);\n\n\t}\n\n\tpublic static void swap(int[] arr, int i, int j) {\n\t\tint tmp = arr[i];\n\t\tarr[i] = arr[j];\n\t\tarr[j] = tmp;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n',normalizedContent:'# 排序\n\n\n# 归并排序\n\n该算法是采用分治法的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。\n\npublic static void mergesort1(int[] arr) {\n\t\tif (arr == null || arr.length < 2) { // 为空 或 小于两个元素无法排序\n\t\t\treturn;\n\t\t}\n\t\tprocess(arr, 0, arr.length - 1);\n\t}\n\n\tpublic static void process(int[] arr, int l, int r) {\n\t\tif (l == r) { // 只有一个元素\n\t\t\treturn;\n\t\t}\n\t\tint mid = l + ((r - l) / 2); // 为了防止越界 相当于(l+r)/2\n\t\tprocess(arr, l, mid); // 递归分治\n\t\tprocess(arr, mid + 1, r); // 递归\n\t\tmerge(arr, l, mid, r); // 合并子序列\n\n\t}\n\n\tpublic static void merge(int[] arr, int l, int m, int r) {\n\t\tint[] prearr = new int[r - l + 1]; // 开辟存储排序好的数组 左右组有多少个元素就开辟多少个位置\n\t\tint index = 0; // 存储数组指针\n\t\tint p1 = l; // 左组指针\n\t\tint p2 = m + 1; // 右组指针\n\t\twhile (p1 <= m && p2 <= r) { // 防止左右指针越界\n\t\t\tprearr[index++] = arr[p1] <= arr[p2] ? arr[p1++] : arr[p2++]; // 比较大小\n\t\t}\n\t\twhile (p1 <= m) { // 左指针未越界\n\t\t\tprearr[index++] = arr[p1++];\n\t\t}\n\t\twhile (p2 <= r) { // 右指针未越界\n\t\t\tprearr[index++] = arr[p2++];\n\t\t}\n\t\t// 归并到原始数组\n\t\tfor (int i = 0; i < prearr.length; i++) {\n\t\t\tarr[l + i] = prearr[i];\n\t\t}\n\t}\n\n\t//非递归\n\tpublic static void mergesort2(int[] arr) {\n\t\tif (arr == null || arr.length < 2) { // 为空或小于2个元素\n\t\t\treturn;\n\t\t}\n\t\tint step = 1; // 步长为 1 2 4 8 16 32 2的次方(即多少个为一组)\n\t\tint n = arr.length; // 长度\n\t\twhile (step < n) { // 如果步长 超过 长度\n\t\t\tint l = 0; // 左指针\n\t\t\twhile (l < n) {\n\t\t\t\tint m = 0; // 右指针重置\n\t\t\t\tif (n - l >= step) { // 右指针到左指针 大于等于步长\n\t\t\t\t\tm = l + step - 1; // 右指针赋值\n\t\t\t\t} else {\n\t\t\t\t\tm = n - 1; // 否则右指针为 数组长度-1 为了赋值数值溢出\n\t\t\t\t}\n\t\t\t\tif (m == n - 1) { // 如果左组的 右指针为数组中最后一个则 无右组比较 直接break\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tint r = 0; // 右组右指针\n\t\t\t\tif (n - 1 - m >= step) { // 右组是否能凑齐step个元素\n\t\t\t\t\tr = m + step;\n\t\t\t\t} else { // 无法凑齐 右指针到数组尾元素\n\t\t\t\t\tr = n - 1;\n\t\t\t\t}\n\t\t\t\tmerge(arr, l, m, r); // 合并区间 l为左组左指针 m为左组右指针 m+1为右组左指针 r为右组右指针\n\t\t\t\tif (r == n - 1) { // 右组右指针到数组尾 结束本次步长\n\t\t\t\t\tbreak;\n\t\t\t\t} else { // 否则重新赋值左组左指针 进行一个大组的区间合并\n\t\t\t\t\tl = r + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (step > n / 2) { // 当前步长不能凑齐左右两组 进行合并区间\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstep *= 2; // 步长增加\n\t\t}\n\n\t}\n\t\n\t// 非递归方法实现\n\tpublic static void mergesort3(int[] arr) {\n\t\tif (arr == null || arr.length < 2) {\n\t\t\treturn;\n\t\t}\n\t\tint n = arr.length;\n\t\tint mergesize = 1;\n\t\twhile (mergesize < n) {\n\t\t\tint l = 0;\n\t\t\twhile (l < n) {\n\t\t\t\tif (mergesize >= n - l) { //当前步长大于剩下为合并的元素\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tint m = l + mergesize - 1; //左组右指针\n\t\t\t\tint r = m + math.min(mergesize, n - m - 1); //右组右指针\n\t\t\t\tmerge(arr, l, m, r);\n\t\t\t\tl = r + 1;\n\t\t\t}\n\t\t\tif (mergesize > n / 2) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmergesize <<= 1;\n\t\t}\n\t}\n\n\t// for test\n\tpublic static int[] generaterandomarray(int maxsize, int maxvalue) {\n\t\tint[] arr = new int[(int) ((maxsize + 1) * math.random())];\n\t\tfor (int i = 0; i < arr.length; i++) {\n\t\t\tarr[i] = (int) ((maxvalue + 1) * math.random()) - (int) (maxvalue * math.random());\n\t\t}\n\t\treturn arr;\n\t}\n\n\t// for test\n\tpublic static int[] copyarray(int[] arr) {\n\t\tif (arr == null) {\n\t\t\treturn null;\n\t\t}\n\t\tint[] res = new int[arr.length];\n\t\tfor (int i = 0; i < arr.length; i++) {\n\t\t\tres[i] = arr[i];\n\t\t}\n\t\treturn res;\n\t}\n\n\t// for test\n\tpublic static boolean isequal(int[] arr1, int[] arr2) {\n\t\tif ((arr1 == null && arr2 != null) || (arr1 != null && arr2 == null)) {\n\t\t\treturn false;\n\t\t}\n\t\tif (arr1 == null && arr2 == null) {\n\t\t\treturn true;\n\t\t}\n\t\tif (arr1.length != arr2.length) {\n\t\t\treturn false;\n\t\t}\n\t\tfor (int i = 0; i < arr1.length; i++) {\n\t\t\tif (arr1[i] != arr2[i]) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\treturn true;\n\t}\n\n\t// for test\n\tpublic static void printarray(int[] arr) {\n\t\tif (arr == null) {\n\t\t\treturn;\n\t\t}\n\t\tfor (int i = 0; i < arr.length; i++) {\n\t\t\tsystem.out.print(arr[i] + " ");\n\t\t}\n\t\tsystem.out.println();\n\t}\n\n\t// 对数器\n\tpublic static void main(string[] args) {\n\t\tint testtime = 500000;\n\t\tint maxsize = 100;\n\t\tint maxvalue = 100;\n\t\tsystem.out.println("测试开始");\n\t\tfor (int i = 0; i < testtime; i++) {\n\t\t\tint[] arr1 = generaterandomarray(maxsize, maxvalue);\n\t\t\tint[] arr2 = copyarray(arr1);\n\t\t\tmergesort1(arr1);\n\t\t\tmergesort2(arr2);\n\t\t\tif (!isequal(arr1, arr2)) {\n\t\t\t\tsystem.out.println("出错了！");\n\t\t\t\tprintarray(arr1);\n\t\t\t\tprintarray(arr2);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tsystem.out.println("测试结束");\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n\n\n# 求数组小和\n\n给一个数组arr[l-r]范围既要排好序,也要返回每个元素在排序前比当前元素小的和\n\npublic static int smallsum(int[] arr) {\n\t\tif (arr == null || arr.length < 2) {\n\t\t\treturn 0;\n\t\t}\n\t\treturn process(arr, 0, arr.length - 1);\n\t}\n\n\tprivate static int process(int[] arr, int l, int r) {\n\t\tif (l == r) {\n\t\t\treturn 0;\n\t\t}\n\t\tint mid = l + ((r - l) >> 1);\n\t\treturn process(arr, l, mid) + process(arr, mid + 1, r) + merge(arr, l, mid, r);\n\t}\n\n\tprivate static int merge(int[] arr, int l, int mid, int r) {\n\t\tint[] arr2 = new int[r - l + 1];\n\t\tint i = 0;\n\t\tint p1 = l;\n\t\tint p2 = mid + 1;\n\t\tint ans = 0; // 当前归并最小和\n\t\twhile (p1 <= mid && p2 <= r) {\n\t\t\tans += arr[p1] < arr[p2] ? arr[p1] * (r - p2 + 1) : 0; // 如果先放入左组 则代表有r-p2+1个元素大于当前左组的元素 否则没有最小\n\t\t\tarr2[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++]; // 比较大小归并 如等于先放右组避免将相同值元素统计到r-p2+1中\n\t\t}\n\n\t\twhile (p1 <= mid) {\n\t\t\tarr2[i++] = arr[p1++];\n\t\t}\n\n\t\twhile (p2 <= r) {\n\t\t\tarr2[i++] = arr[p2++];\n\t\t}\n\n\t\tfor (i = 0; i < arr2.length; i++) {\n\t\t\tarr[l + i] = arr2[i];\n\t\t}\n\n\t\treturn ans;\n\t}\n\n\t// for test\n\tpublic static int comparator(int[] arr) {\n\t\tif (arr == null || arr.length < 2) {\n\t\t\treturn 0;\n\t\t}\n\t\tint res = 0;\n\t\tfor (int i = 1; i < arr.length; i++) {\n\t\t\tfor (int j = 0; j < i; j++) {\n\t\t\t\tres += arr[j] < arr[i] ? arr[j] : 0;\n\t\t\t}\n\t\t}\n\t\treturn res;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n# 求数组中的逆序对数量\n\n给定一个数组arr，求数组的降序对总数量\n\n在一个数组中，任何一个前面的数a，和任何一个后面的数b，如果(a,b)是降序的，就称为降序对\n\npublic static int reverpairnumber(int[] arr) {\n\t\tif (arr == null || arr.length == 0) {\n\t\t\treturn 0;\n\t\t}\n\t\treturn prcess(arr, 0, arr.length - 1);\n\t}\n\n\tpublic static int prcess(int[] arr, int l, int r) {\n\t\tif (l == r) {\n\t\t\treturn 0;\n\t\t}\n\t\tint m = l + ((r - l) >> 1);\n\t\treturn prcess(arr, l, m) + prcess(arr, m + 1, r) + merge(arr, l, m, r);\n\t}\n\n\tpublic static int merge(int[] arr, int l, int m, int r) {\n\t\tint ans = 0;\n\t\tint[] help = new int[r - l + 1];\n\t\t// 倒着遍历\n\t\tint i = help.length - 1; // 从尾部开始\n\t\tint p1 = m; // 左边边界\n\t\tint p2 = r; // 右组边界\n\t\twhile (p1 >= l && p2 > m) {\n\t\t\tans += arr[p1] > arr[p2] ? (p2 - m) : 0; // 如果左边指针数大于右边指针数则为降序对\n\t\t\thelp[i--] = arr[p1] > arr[p2] ? arr[p1--] : arr[p2--];\n\t\t}\n\t\twhile (p1 >= l) {\n\t\t\thelp[i--] = arr[p1--];\n\t\t}\n\t\twhile (p2 > m) { // 注意到m就停止\n\t\t\thelp[i--] = arr[p2--];\n\t\t}\n\t\tfor (int j = 0; j < help.length; j++) {\n\t\t\tarr[l + j] = help[j];\n\t\t}\n\t\treturn ans;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n# 493. 翻转对\n\n在一个数组中，对于任何一个数num，求有多少个(后面的数*2)依然<num，返回总个数\n\n比如：[3,1,7,0,2]\n3的后面有：1，0\n1的后面有：0\n7的后面有：0，2\n0的后面没有\n2的后面没有\n所以总共有5个\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 327. 区间和的个数\n\npublic int countrangesum(int[] nums, int lower, int upper) {\n\t\tif (nums == null || nums.length == 0) {\n\t\t\treturn 0;\n\t\t}\n\t\t// 前缀和数组\n\t\tlong[] sum = new long[nums.length];\n\t\tsum[0] = nums[0];\n\t\tfor (int i = 1; i < nums.length; i++) {\n\t\t\tsum[i] = sum[i - 1] + nums[i];\n\n\t\t}\n\t\t// 原数组 已无用 传递前缀和数组\n\t\treturn process(sum, 0, nums.length - 1, lower, upper);\n\t}\n\n\tprivate int process(long[] sum, int l, int r, int lower, int upper) {\n\t\tif (l == r) {\n\t\t\t// 只有一个数时 判断是否在lower和upper范围内 是则res+1\n\t\t\treturn sum[l] >= lower && sum[l] <= upper ? 1 : 0;\n\n\t\t}\n\t\tint m = l + ((r - l) >> 1);\n\t\t// 递归+合并\n\t\treturn process(sum, l, m, lower, upper) + process(sum, m + 1, r, lower, upper)\n\t\t\t\t+ merge(sum, l, m, r, lower, upper);\n\t}\n\n\tprivate int merge(long[] sum, int l, int m, int r, int lower, int upper) {\n\t\tint ans = 0;\n\t\tint windowl = l;\n\t\tint windowr = l; // [windowl, windowr)\n\t\tfor (int i = m + 1; i <= r; i++) { // 从右组开始遍历\n\t\t\tlong min = sum[i] - upper;\n\t\t\tlong max = sum[i] - lower;\n\t\t\twhile (windowr <= m && sum[windowr] <= max) {\n\t\t\t\twindowr++; // 找到最大值下标边界\n\t\t\t}\n\t\t\twhile (windowl <= m && sum[windowl] < min) {\n\t\t\t\twindowl++; // 最小值下标边界\n\t\t\t}\n\t\t\tans += windowr - windowl; // 右组当前元素有多少个在范围内的\n\t\t}\n\n\t\t//归并\n\t\tlong[] help = new long[r - l + 1];\n\t\tint i = 0;\n\t\tint p1 = l;\n\t\tint p2 = m + 1;\n\t\twhile (p1 <= m && p2 <= r) {\n\t\t\thelp[i++] = sum[p1] <= sum[p2] ? sum[p1++] : sum[p2++];\n\t\t}\n\t\twhile (p1 <= m) {\n\t\t\thelp[i++] = sum[p1++];\n\t\t}\n\t\twhile (p2 <= r) {\n\t\t\thelp[i++] = sum[p2++];\n\t\t}\n\t\tfor (int j = 0; j < help.length; j++) {\n\t\t\tsum[l + j] = help[j];\n\t\t}\n\n\t\treturn ans;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n\n# 快速排序\n\n\t// 递归经典写法\n\tpublic static void quicksort(int[] arr, int l, int r) {\n\t\tif (l >= r) {\n\t\t\treturn;\n\t\t}\n\t\tint left = l;\n\t\tint right = r;\n\t\tint base = arr[r]; // 以最后一个为基准 如果以头为基准则应该先找大于基准 再找小于基准的\n\t\twhile (left < right) {\n\t\t\t// 将小于等于base的放到左边 则应该left++\n\t\t\twhile (arr[left] <= base && left < right) {\n\t\t\t\tleft++;\n\t\t\t}\n\t\t\t// 将大于等于base的放到右边 则right--\n\t\t\twhile (arr[right] >= base && left < right) {\n\t\t\t\tright--;\n\t\t\t}\n\t\t\tif (left < right) {\n\t\t\t\t// 交换\n\t\t\t\tint temp = arr[left];\n\t\t\t\tarr[left] = arr[right];\n\t\t\t\tarr[right] = temp;\n\t\t\t}\n\n\t\t}\n\t\tarr[r] = arr[left]; // 将小于基准的最后一个数 交换到base位置\n\t\tarr[left] = base; // 将base值 交换到小于基准的位置\n\t\t// 拆分大问题 递归\n\t\tquicksort(arr, l, left - 1);\n\t\tquicksort(arr, right + 1, r);\n\n\t}\n\n\tpublic static void swap(int[] arr, int i, int j) {\n\t\tint tmp = arr[i];\n\t\tarr[i] = arr[j];\n\t\tarr[j] = tmp;\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"链表",frontmatter:{title:"链表",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/d3b8b4/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/08.%E9%93%BE%E8%A1%A8.html",relativePath:"408/01.数据结构/08.链表.md",key:"v-6441954f",path:"/pages/d3b8b4/",headers:[{level:2,title:"单链表和双链表的反转",slug:"单链表和双链表的反转",normalizedTitle:"单链表和双链表的反转",charIndex:9},{level:3,title:"单链表",slug:"单链表",normalizedTitle:"单链表",charIndex:9},{level:3,title:"双链表",slug:"双链表",normalizedTitle:"双链表",charIndex:13},{level:2,title:"单链表实现栈和队列",slug:"单链表实现栈和队列",normalizedTitle:"单链表实现栈和队列",charIndex:2290},{level:3,title:"队列",slug:"队列",normalizedTitle:"队列",charIndex:2297},{level:3,title:"栈",slug:"栈",normalizedTitle:"栈",charIndex:2295},{level:2,title:"双链表实现双端队列",slug:"双链表实现双端队列",normalizedTitle:"双链表实现双端队列",charIndex:4652},{level:2,title:"leecode 链表题",slug:"leecode-链表题",normalizedTitle:"leecode 链表题",charIndex:7012},{level:3,title:"K 个一组翻转链表",slug:"k-个一组翻转链表",normalizedTitle:"k 个一组翻转链表",charIndex:7028},{level:3,title:"两数相加",slug:"两数相加",normalizedTitle:"两数相加",charIndex:8685},{level:3,title:"合并两个有序链表",slug:"合并两个有序链表",normalizedTitle:"合并两个有序链表",charIndex:10139}],headersStr:"单链表和双链表的反转 单链表 双链表 单链表实现栈和队列 队列 栈 双链表实现双端队列 leecode 链表题 K 个一组翻转链表 两数相加 合并两个有序链表",content:'# 链表\n\n\n# 单链表和双链表的反转\n\n\n# 单链表\n\n\t// 单链表\n\tpublic static class Node {\n\t\tint value;\n\t\tNode next;\n\n\t\tpublic Node(int value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t// 反转单链表\n\tpublic static Node reversalNode(Node head) {\n\t\tNode next = null;\n\t\tNode pre = null;\n\t\twhile (head != null) {\n\t\t\tnext = head.next; // 缓存下一个节点\n\t\t\thead.next = pre; // 更改当前节点的下一节点为反向节点\n\t\t\tpre = head; // 缓存当前节点为下一节点\n\t\t\thead = next; // 更新新的当前节点\n\t\t}\n\t\treturn pre;\n\t}\n\n    public static void main(String[] args) {\n            // 单链表反转\n            Node n1 = new Node(1);\n            Node n2 = new Node(2);\n            Node n3 = new Node(3);\n            n1.next = n2;\n            n2.next = n3;\n            System.out.println(n1.value);\n            System.out.println(n1.next.value);\n            System.out.println(n1.next.next.value);\n            System.out.println("=============");\n            n1 = reversalNode(n1);\n            System.out.println(n1.value);\n            System.out.println(n1.next.value);\n            System.out.println(n1.next.next.value);\n            System.out.println("=============");\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# 双链表\n\n\t// 双链表\n\tpublic static class Node1 {\n\t\tint value;\n\t\tNode1 last;\n\t\tNode1 next;\n\n\t\tpublic Node1(int value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t// 双链表反转\n\tpublic static Node1 reversalNode1(Node1 head) {\n\t\tNode1 next = null;\n\t\tNode1 pre = null;\n\t\twhile (head != null) {\n\t\t\tnext = head.next; //记录下一个节点\n\t\t\thead.next = pre; //修改当前节点的右节点 为上次记录的head节点\n\t\t\thead.last = next; //修改当前节点的左节点\n\t\t\tpre = head; //记录当前节点\n\t\t\thead = next; //修改当前节点位置\n\t\t}\n\t\t\n\t\treturn pre; //返回当前节点 即头节点\n\t}\n\tpublic static void main(String[] args) {\n\t\t// 双链表反转\n\t\tNode1 nn1 = new Node1(1);\n\t\tNode1 nn2 = new Node1(2);\n\t\tNode1 nn3 = new Node1(3);\n\t\tnn1.next = nn2;\n\t\tnn2.last = nn1;\n\t\tnn2.next = nn3;\n\t\tnn3.last = nn2;\n\t\tSystem.out.println(nn1.value);\n\t\tSystem.out.println(nn1.next.value);\n\t\tSystem.out.println(nn1.next.next.value);\n\t\tnn1 = reversalNode1(nn1);\n\t\tSystem.out.println("=============");\n\t\tSystem.out.println(nn1.value);\n\t\tSystem.out.println(nn1.next.value);\n\t\tSystem.out.println(nn1.next.next.value);\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# 单链表实现栈和队列\n\n\n# 队列\n\n\tpublic static class Node<V> {\n\t\tV value;\n\t\tNode<V> next;\n\n\t\tpublic Node(V value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t// 队列\n\tpublic static class MyQueue<V> {\n\t\tNode<V> head;\n\t\tNode<V> tail;\n\t\tint size;\n\n\t\tpublic MyQueue() {\n\t\t\thead = null;\n\t\t\ttail = null;\n\t\t\tsize = 0;\n\t\t}\n\n\t\tpublic boolean isEmpty() {\n\t\t\treturn size == 0;\n\t\t}\n\n\t\tpublic int size() {\n\t\t\treturn size;\n\t\t}\n\n\t\t// 添加元素到队列中\n\t\tpublic void offer(V value) {\n\t\t\tNode<V> v = new Node<V>(value);\n\t\t\tif (tail == null) { // tail为null 则当前队列中没有元素 直接添加\n\t\t\t\thead = v; // 头尾同时指向 新添加元素\n\t\t\t\ttail = v;\n\t\t\t} else { // 当前队列有元素\n\t\t\t\ttail.next = v; // 更新尾元素的下一跳为添加的新元素地址\n\t\t\t\ttail = v; // 然后再更新尾元素 为添加元素地址\n\t\t\t}\n\t\t\tsize++; // 长度+1\n\t\t}\n\n\t\t// 从队列中删除元素\n\t\tpublic V poll() {\n\t\t\tV ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value; // 获取头部元素值\n\t\t\t\thead = head.next; // 将头部指向下一个元素\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\tif (head == null) { // 边界情况 当头部当前为null时 尾部也无元素\n\t\t\t\ttail = null;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 查看队头元素\n\t\tpublic V peek() {\n\t\t\tV ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n在上面删除的操作 java中有自己的回收器 因此我们无需手动回收无用对象 只需要把此对象改成不可达(即无法通过任何途径访问到此对象,则jvm会自动释放该对象)\n\n\n# 栈\n\n\tpublic static class Node<V> {\n\t\tV value;\n\t\tNode<V> next;\n\n\t\tpublic Node(V value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t//栈\n\tpublic static class MyStack<V>{\n\t\tint size;\n\t\tNode<V> head;\n\t\t\n\t\tpublic MyStack() {\n\t\t\thead = null;\n\t\t\tsize =0;\n\t\t}\n\t\t\n\t\tpublic boolean isEmpty() {\n\t\t\treturn size ==0;\n\t\t}\n\t\t\n\t\tpublic int size() {\n\t\t\treturn size;\n\t\t}\n\t\t\n\t\t//入栈\n\t\tpublic void pusu(V value) {\n\t\t\tNode<V> v = new Node<V>(value);\n\t\t\tif(head == null) { //栈中无元素\n\t\t\t\thead = v; //添加元素\n\t\t\t} else { //栈中有元素\n\t\t\t\thead.next = v; //压栈底\n\t\t\t\thead = v;//更新栈顶\n\t\t\t}\n\t\t\tsize++;\n\t\t}\n\t\t\n\t\t//出栈\n\t\tpublic V pop() {\n\t\t\tV ans = null;\n\t\t\tif (head !=null) {\n\t\t\t\tans = head.value; //获取栈顶值\n\t\t\t\thead = head.next; //更新栈顶元素为下个元素\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t\t\n\t\t//查询栈顶元素\n\t\tpublic V peek() {\n\t\t\tV ans = null;\n\t\t\tif(head !=null) {\n\t\t\t\tans = head.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 双链表实现双端队列\n\n为什么无法使用单链表实现双端队列？\n\n 1. 单链表支持从队头中增删查元素\n 2. 单链表只支持从队尾增查元素，无法删除元素 因为无法从当前队尾中快速查找到上一跳的地址，只能遍历一次单链表。无法做到O(1)，因为删除操作要遍历一次链表时间复杂程度为O(n)\n 3. 所以我们可以使用双链表可以完成O(1)的双端队列的增删查\n\n\tpublic static class Node<V> {\n\t\tV value;\n\t\tNode<V> fist;\n\t\tNode<V> last;\n\n\t\tpublic Node(V value) {\n\t\t\tthis.value = value;\n\t\t\tfist = null; // 上一跳\n\t\t\tlast = null; // 下一跳\n\t\t}\n\t}\n\n\tpublic static class DoubleQueue<V> {\n\t\tNode<V> head;\n\t\tNode<V> tail;\n\t\tint size;\n\n\t\tpublic DoubleQueue() {\n\t\t\thead = null;\n\t\t\ttail = null;\n\t\t\tsize = 0;\n\t\t}\n\n\t\tpublic boolean isEmpty() {\n\t\t\treturn size == 0;\n\t\t}\n\n\t\tpublic int size() {\n\t\t\treturn size;\n\t\t}\n\n\t\t// 从队列头添加指定元素\n\t\tpublic void addFist(V value) {\n\t\t\tNode<V> v = new Node<V>(value);\n\t\t\tif (head == null) { // 当前队列为空 直接添加\n\t\t\t\thead = v;\n\t\t\t\ttail = v;\n\t\t\t} else { // 从队头添加\n\t\t\t\thead.fist = v; // 原队头上一跳 标记为当前元素\n\t\t\t\tv.last = head; // 当前元素 下一跳为旧队头\n\t\t\t\thead = v; // 队头为当前元素\n\t\t\t}\n\t\t\tsize++;\n\t\t}\n\n\t\t// 从队列尾添加指定元素\n\t\tpublic void addLast(V value) {\n\t\t\tNode<V> v = new Node<V>(value);\n\t\t\tif (head == null) {\n\t\t\t\thead = v;\n\t\t\t\ttail = v;\n\t\t\t} else {\n\t\t\t\ttail.last = v; // 旧队尾下一跳为 添加元素\n\t\t\t\tv.fist = tail; // 添加元素的上一跳 为旧队尾\n\t\t\t\ttail = v; // 更新队尾\n\t\t\t}\n\t\t\tsize++;\n\t\t}\n\n\t\t// 删除队头元素\n\t\tpublic V pollFirst() {\n\t\t\tV ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value;\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\tif (head == tail) { // 边界情况 只有一个元素\n\t\t\t\thead = null;\n\t\t\t\ttail = null;\n\t\t\t} else { \n\t\t\t\thead = head.fist;\n\t\t\t\thead.fist = null; // 将head上一跳标记为null\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 删除队尾元素\n\t\tpublic V pollLast() {\n\t\t\tV ans = null;\n\t\t\tif (head != null) { // 有元素\n\t\t\t\tans = tail.value; // 取当前队尾出值\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\tif (tail == tail) { // 边界情况 只有一个元素\n\t\t\t\thead = null;\n\t\t\t\ttail = null;\n\t\t\t} else { \n\t\t\t\ttail = tail.fist; // 更新队尾\n\t\t\t\ttail.last = null; // 当tail不为null 才将tall下一跳标记为null\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 查看队头\n\t\tpublic V peekFist() {\n\t\t\tV ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 查看队尾\n\t\tpublic V peekLast() {\n\t\t\tV ans = null;\n\t\t\tif (tail != null) {\n\t\t\t\tans = tail.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n\n\n\n# leecode 链表题\n\n\n# K 个一组翻转链表\n\npublic class ListNode {\n\t\tint val;\n\t\tListNode next;\n\n\t\tListNode() {\n\t\t}\n\n\t\tListNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tListNode(int val, ListNode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tpublic class Solution {\n\t\tpublic ListNode reverseKGroup(ListNode head, int k) {\n\t\t\tListNode start = head; // 缓存当前链表头\n\t\t\tListNode end = getKEnd(start, k); // 获取第一组链表尾\n\t\t\tif (end == null) { // 如果k个元素内没有链尾 说明第一组链表＜=k个元素 无法进行反转 直接返回当前链表即可\n\t\t\t\treturn head;\n\t\t\t}\n\t\t\t// 第一组满足k个元素进行反转\n\t\t\thead = end; // 将当前链表头指向链表尾的元素\n\t\t\treverse(start, end); // 进行反转\n\t\t\tListNode lastEnd = start; // 上一组节点尾\n\t\t\twhile (lastEnd.next != null) {\n\t\t\t\tstart = lastEnd.next; // 缓存当前组的链表头\n\t\t\t\tend = getKEnd(start, k); // 获得当前组的链表尾\n\t\t\t\tif (end == null) {\n\t\t\t\t\treturn head;\n\t\t\t\t}\n\t\t\t\treverse(start, end); // 满足k个 反转当前组\n\t\t\t\tlastEnd.next = end; // 将上组的链尾 指向 当前组反转后的链表头\n\t\t\t\tlastEnd = start; // 更新lastEnd为当前组的链尾\n\t\t\t}\n\t\t\treturn head;\n\n\t\t}\n\n\t\tpublic ListNode getKEnd(ListNode pre, int k) {\n\t\t\twhile (--k != 0 && pre != null) { // 返回当前组的最后一个元素(即新的链头)\n\t\t\t\tpre = pre.next;\n\t\t\t}\n\t\t\treturn pre;\n\t\t}\n\n\t\tpublic void reverse(ListNode start, ListNode end) {\n\t\t\tend = end.next; // 将当前end向后移一位(即下一组的链头,下一组未反转)\n\t\t\tListNode cur = start; // 当前元素\n\t\t\tListNode pre = null; // 缓存上次的修改过元素\n\t\t\tListNode next = null; // 下一跳元素\n\t\t\twhile (cur != end) {\n\t\t\t\tnext = cur.next; // 缓存下一跳\n\t\t\t\tcur.next = pre; // 指向上次修改后的元素\n\t\t\t\tpre = cur; // 更新修改后的元素\n\t\t\t\tcur = next; // 更新当前元素\n\t\t\t}\n\t\t\tstart.next = end; // 将当前组链尾 指向下一组的链头\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n\n# 两数相加\n\n\tpublic class ListNode {\n\t\tint val;\n\t\tListNode next;\n\n\t\tListNode() {\n\t\t}\n\n\t\tListNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tListNode(int val, ListNode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic ListNode addTwoNumbers(ListNode l1, ListNode l2) {\n\n\t\t\tListNode l = getSize(l1) > getSize(l2) ? l1 : l2; // 最长的链表\n\t\t\tListNode s = l == l1 ? l2 : l1; // 第二个链表 <= l的长度\n\t\t\tint temp = 0; // 进位标记 如果为1则需要进1 0则不进1\n\t\t\tint cou = 0; // 存储每次计算结果\n\t\t\tListNode tl = l; // 缓存当前长节点 元素\n\t\t\tListNode ts = s; // 缓存当前短节点 元素\n\t\t\tListNode pre = null;\n\t\t\t// 处理短的链表\n\t\t\twhile (ts != null) {\n\t\t\t\tcou = ts.val + tl.val + temp; // 相加结果\n\t\t\t\ttemp = cou / 10; // 是否需要进1\n\t\t\t\ttl.val = cou % 10; // 存储到节点中\n\t\t\t\tpre = tl; // 缓存当前长的节点元素 因为最后进1 需要使用该缓存节点下一跳 指向新的节点\n\t\t\t\t// 为何要在长的和短的链表 中记录当前节点呢 因为有可能短和长的链表长度一致 短的跑完长也跑完 又需要进1 会发生空指针异常\n\t\t\t\ttl = tl.next; // 下一个节点\n\t\t\t\tts = ts.next;\n\t\t\t}\n\t\t\t// 处理长的链表\n\t\t\twhile (tl != null) {\n\t\t\t\tcou = tl.val + temp;\n\t\t\t\ttemp = cou / 10;\n\t\t\t\ttl.val = cou % 10;\n\t\t\t\tpre = tl;\n\t\t\t\ttl = tl.next;\n\t\t\t}\n\t\t\t// 如果进位是1 则需要新创建节点\n\t\t\tif (temp != 0) {\n\t\t\t\tpre.next = new ListNode(1);\n\t\t\t}\n\t\t\treturn l;\n\n\t\t}\n\n\t\tpublic int getSize(ListNode head) {\n\t\t\tint size = 0;\n\t\t\twhile (head.next != null) {\n\t\t\t\tsize++;\n\t\t\t\thead = head.next;\n\t\t\t}\n\t\t\treturn size;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\n\n# 合并两个有序链表\n\npublic class ListNode {\n\t\tint val;\n\t\tListNode next;\n\n\t\tListNode() {\n\t\t}\n\n\t\tListNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tListNode(int val, ListNode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic ListNode mergeTwoLists(ListNode list1, ListNode list2) {\n\t\t\tif (list1 == null || list2 == null) { // 另外一个链表为空直接返回非空链表\n\t\t\t\treturn list1 == null ? list2 : list1;\n\t\t\t}\n\t\t\tListNode head = list1.val <= list2.val ? list1 : list2; // 头节点\n\t\t\tListNode l = head.next; // 下一条\n\t\t\tListNode s = head == list1 ? list2 : list1; // 另外一个链表\n\t\t\tListNode pre = head;\n\t\t\twhile (l != null && s != null) {\n\t\t\t\tif (l.val <= s.val) {\n\t\t\t\t\tpre.next = l;\n\t\t\t\t\tl = l.next;\n\t\t\t\t} else {\n\t\t\t\t\tpre.next = s;\n\t\t\t\t\ts = s.next;\n\t\t\t\t}\n\t\t\t\tpre = pre.next;\n\t\t\t}\n\t\t\tpre.next = l == null ? s : l;\n\t\t\treturn head;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n',normalizedContent:'# 链表\n\n\n# 单链表和双链表的反转\n\n\n# 单链表\n\n\t// 单链表\n\tpublic static class node {\n\t\tint value;\n\t\tnode next;\n\n\t\tpublic node(int value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t// 反转单链表\n\tpublic static node reversalnode(node head) {\n\t\tnode next = null;\n\t\tnode pre = null;\n\t\twhile (head != null) {\n\t\t\tnext = head.next; // 缓存下一个节点\n\t\t\thead.next = pre; // 更改当前节点的下一节点为反向节点\n\t\t\tpre = head; // 缓存当前节点为下一节点\n\t\t\thead = next; // 更新新的当前节点\n\t\t}\n\t\treturn pre;\n\t}\n\n    public static void main(string[] args) {\n            // 单链表反转\n            node n1 = new node(1);\n            node n2 = new node(2);\n            node n3 = new node(3);\n            n1.next = n2;\n            n2.next = n3;\n            system.out.println(n1.value);\n            system.out.println(n1.next.value);\n            system.out.println(n1.next.next.value);\n            system.out.println("=============");\n            n1 = reversalnode(n1);\n            system.out.println(n1.value);\n            system.out.println(n1.next.value);\n            system.out.println(n1.next.next.value);\n            system.out.println("=============");\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# 双链表\n\n\t// 双链表\n\tpublic static class node1 {\n\t\tint value;\n\t\tnode1 last;\n\t\tnode1 next;\n\n\t\tpublic node1(int value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t// 双链表反转\n\tpublic static node1 reversalnode1(node1 head) {\n\t\tnode1 next = null;\n\t\tnode1 pre = null;\n\t\twhile (head != null) {\n\t\t\tnext = head.next; //记录下一个节点\n\t\t\thead.next = pre; //修改当前节点的右节点 为上次记录的head节点\n\t\t\thead.last = next; //修改当前节点的左节点\n\t\t\tpre = head; //记录当前节点\n\t\t\thead = next; //修改当前节点位置\n\t\t}\n\t\t\n\t\treturn pre; //返回当前节点 即头节点\n\t}\n\tpublic static void main(string[] args) {\n\t\t// 双链表反转\n\t\tnode1 nn1 = new node1(1);\n\t\tnode1 nn2 = new node1(2);\n\t\tnode1 nn3 = new node1(3);\n\t\tnn1.next = nn2;\n\t\tnn2.last = nn1;\n\t\tnn2.next = nn3;\n\t\tnn3.last = nn2;\n\t\tsystem.out.println(nn1.value);\n\t\tsystem.out.println(nn1.next.value);\n\t\tsystem.out.println(nn1.next.next.value);\n\t\tnn1 = reversalnode1(nn1);\n\t\tsystem.out.println("=============");\n\t\tsystem.out.println(nn1.value);\n\t\tsystem.out.println(nn1.next.value);\n\t\tsystem.out.println(nn1.next.next.value);\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# 单链表实现栈和队列\n\n\n# 队列\n\n\tpublic static class node<v> {\n\t\tv value;\n\t\tnode<v> next;\n\n\t\tpublic node(v value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t// 队列\n\tpublic static class myqueue<v> {\n\t\tnode<v> head;\n\t\tnode<v> tail;\n\t\tint size;\n\n\t\tpublic myqueue() {\n\t\t\thead = null;\n\t\t\ttail = null;\n\t\t\tsize = 0;\n\t\t}\n\n\t\tpublic boolean isempty() {\n\t\t\treturn size == 0;\n\t\t}\n\n\t\tpublic int size() {\n\t\t\treturn size;\n\t\t}\n\n\t\t// 添加元素到队列中\n\t\tpublic void offer(v value) {\n\t\t\tnode<v> v = new node<v>(value);\n\t\t\tif (tail == null) { // tail为null 则当前队列中没有元素 直接添加\n\t\t\t\thead = v; // 头尾同时指向 新添加元素\n\t\t\t\ttail = v;\n\t\t\t} else { // 当前队列有元素\n\t\t\t\ttail.next = v; // 更新尾元素的下一跳为添加的新元素地址\n\t\t\t\ttail = v; // 然后再更新尾元素 为添加元素地址\n\t\t\t}\n\t\t\tsize++; // 长度+1\n\t\t}\n\n\t\t// 从队列中删除元素\n\t\tpublic v poll() {\n\t\t\tv ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value; // 获取头部元素值\n\t\t\t\thead = head.next; // 将头部指向下一个元素\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\tif (head == null) { // 边界情况 当头部当前为null时 尾部也无元素\n\t\t\t\ttail = null;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 查看队头元素\n\t\tpublic v peek() {\n\t\t\tv ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n在上面删除的操作 java中有自己的回收器 因此我们无需手动回收无用对象 只需要把此对象改成不可达(即无法通过任何途径访问到此对象,则jvm会自动释放该对象)\n\n\n# 栈\n\n\tpublic static class node<v> {\n\t\tv value;\n\t\tnode<v> next;\n\n\t\tpublic node(v value) {\n\t\t\tthis.value = value;\n\t\t}\n\t}\n\n\t//栈\n\tpublic static class mystack<v>{\n\t\tint size;\n\t\tnode<v> head;\n\t\t\n\t\tpublic mystack() {\n\t\t\thead = null;\n\t\t\tsize =0;\n\t\t}\n\t\t\n\t\tpublic boolean isempty() {\n\t\t\treturn size ==0;\n\t\t}\n\t\t\n\t\tpublic int size() {\n\t\t\treturn size;\n\t\t}\n\t\t\n\t\t//入栈\n\t\tpublic void pusu(v value) {\n\t\t\tnode<v> v = new node<v>(value);\n\t\t\tif(head == null) { //栈中无元素\n\t\t\t\thead = v; //添加元素\n\t\t\t} else { //栈中有元素\n\t\t\t\thead.next = v; //压栈底\n\t\t\t\thead = v;//更新栈顶\n\t\t\t}\n\t\t\tsize++;\n\t\t}\n\t\t\n\t\t//出栈\n\t\tpublic v pop() {\n\t\t\tv ans = null;\n\t\t\tif (head !=null) {\n\t\t\t\tans = head.value; //获取栈顶值\n\t\t\t\thead = head.next; //更新栈顶元素为下个元素\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t\t\n\t\t//查询栈顶元素\n\t\tpublic v peek() {\n\t\t\tv ans = null;\n\t\t\tif(head !=null) {\n\t\t\t\tans = head.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 双链表实现双端队列\n\n为什么无法使用单链表实现双端队列？\n\n 1. 单链表支持从队头中增删查元素\n 2. 单链表只支持从队尾增查元素，无法删除元素 因为无法从当前队尾中快速查找到上一跳的地址，只能遍历一次单链表。无法做到o(1)，因为删除操作要遍历一次链表时间复杂程度为o(n)\n 3. 所以我们可以使用双链表可以完成o(1)的双端队列的增删查\n\n\tpublic static class node<v> {\n\t\tv value;\n\t\tnode<v> fist;\n\t\tnode<v> last;\n\n\t\tpublic node(v value) {\n\t\t\tthis.value = value;\n\t\t\tfist = null; // 上一跳\n\t\t\tlast = null; // 下一跳\n\t\t}\n\t}\n\n\tpublic static class doublequeue<v> {\n\t\tnode<v> head;\n\t\tnode<v> tail;\n\t\tint size;\n\n\t\tpublic doublequeue() {\n\t\t\thead = null;\n\t\t\ttail = null;\n\t\t\tsize = 0;\n\t\t}\n\n\t\tpublic boolean isempty() {\n\t\t\treturn size == 0;\n\t\t}\n\n\t\tpublic int size() {\n\t\t\treturn size;\n\t\t}\n\n\t\t// 从队列头添加指定元素\n\t\tpublic void addfist(v value) {\n\t\t\tnode<v> v = new node<v>(value);\n\t\t\tif (head == null) { // 当前队列为空 直接添加\n\t\t\t\thead = v;\n\t\t\t\ttail = v;\n\t\t\t} else { // 从队头添加\n\t\t\t\thead.fist = v; // 原队头上一跳 标记为当前元素\n\t\t\t\tv.last = head; // 当前元素 下一跳为旧队头\n\t\t\t\thead = v; // 队头为当前元素\n\t\t\t}\n\t\t\tsize++;\n\t\t}\n\n\t\t// 从队列尾添加指定元素\n\t\tpublic void addlast(v value) {\n\t\t\tnode<v> v = new node<v>(value);\n\t\t\tif (head == null) {\n\t\t\t\thead = v;\n\t\t\t\ttail = v;\n\t\t\t} else {\n\t\t\t\ttail.last = v; // 旧队尾下一跳为 添加元素\n\t\t\t\tv.fist = tail; // 添加元素的上一跳 为旧队尾\n\t\t\t\ttail = v; // 更新队尾\n\t\t\t}\n\t\t\tsize++;\n\t\t}\n\n\t\t// 删除队头元素\n\t\tpublic v pollfirst() {\n\t\t\tv ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value;\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\tif (head == tail) { // 边界情况 只有一个元素\n\t\t\t\thead = null;\n\t\t\t\ttail = null;\n\t\t\t} else { \n\t\t\t\thead = head.fist;\n\t\t\t\thead.fist = null; // 将head上一跳标记为null\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 删除队尾元素\n\t\tpublic v polllast() {\n\t\t\tv ans = null;\n\t\t\tif (head != null) { // 有元素\n\t\t\t\tans = tail.value; // 取当前队尾出值\n\t\t\t\tsize--;\n\t\t\t}\n\t\t\tif (tail == tail) { // 边界情况 只有一个元素\n\t\t\t\thead = null;\n\t\t\t\ttail = null;\n\t\t\t} else { \n\t\t\t\ttail = tail.fist; // 更新队尾\n\t\t\t\ttail.last = null; // 当tail不为null 才将tall下一跳标记为null\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 查看队头\n\t\tpublic v peekfist() {\n\t\t\tv ans = null;\n\t\t\tif (head != null) {\n\t\t\t\tans = head.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\n\t\t// 查看队尾\n\t\tpublic v peeklast() {\n\t\t\tv ans = null;\n\t\t\tif (tail != null) {\n\t\t\t\tans = tail.value;\n\t\t\t}\n\t\t\treturn ans;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n\n\n\n# leecode 链表题\n\n\n# k 个一组翻转链表\n\npublic class listnode {\n\t\tint val;\n\t\tlistnode next;\n\n\t\tlistnode() {\n\t\t}\n\n\t\tlistnode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tlistnode(int val, listnode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tpublic class solution {\n\t\tpublic listnode reversekgroup(listnode head, int k) {\n\t\t\tlistnode start = head; // 缓存当前链表头\n\t\t\tlistnode end = getkend(start, k); // 获取第一组链表尾\n\t\t\tif (end == null) { // 如果k个元素内没有链尾 说明第一组链表＜=k个元素 无法进行反转 直接返回当前链表即可\n\t\t\t\treturn head;\n\t\t\t}\n\t\t\t// 第一组满足k个元素进行反转\n\t\t\thead = end; // 将当前链表头指向链表尾的元素\n\t\t\treverse(start, end); // 进行反转\n\t\t\tlistnode lastend = start; // 上一组节点尾\n\t\t\twhile (lastend.next != null) {\n\t\t\t\tstart = lastend.next; // 缓存当前组的链表头\n\t\t\t\tend = getkend(start, k); // 获得当前组的链表尾\n\t\t\t\tif (end == null) {\n\t\t\t\t\treturn head;\n\t\t\t\t}\n\t\t\t\treverse(start, end); // 满足k个 反转当前组\n\t\t\t\tlastend.next = end; // 将上组的链尾 指向 当前组反转后的链表头\n\t\t\t\tlastend = start; // 更新lastend为当前组的链尾\n\t\t\t}\n\t\t\treturn head;\n\n\t\t}\n\n\t\tpublic listnode getkend(listnode pre, int k) {\n\t\t\twhile (--k != 0 && pre != null) { // 返回当前组的最后一个元素(即新的链头)\n\t\t\t\tpre = pre.next;\n\t\t\t}\n\t\t\treturn pre;\n\t\t}\n\n\t\tpublic void reverse(listnode start, listnode end) {\n\t\t\tend = end.next; // 将当前end向后移一位(即下一组的链头,下一组未反转)\n\t\t\tlistnode cur = start; // 当前元素\n\t\t\tlistnode pre = null; // 缓存上次的修改过元素\n\t\t\tlistnode next = null; // 下一跳元素\n\t\t\twhile (cur != end) {\n\t\t\t\tnext = cur.next; // 缓存下一跳\n\t\t\t\tcur.next = pre; // 指向上次修改后的元素\n\t\t\t\tpre = cur; // 更新修改后的元素\n\t\t\t\tcur = next; // 更新当前元素\n\t\t\t}\n\t\t\tstart.next = end; // 将当前组链尾 指向下一组的链头\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n\n# 两数相加\n\n\tpublic class listnode {\n\t\tint val;\n\t\tlistnode next;\n\n\t\tlistnode() {\n\t\t}\n\n\t\tlistnode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tlistnode(int val, listnode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic listnode addtwonumbers(listnode l1, listnode l2) {\n\n\t\t\tlistnode l = getsize(l1) > getsize(l2) ? l1 : l2; // 最长的链表\n\t\t\tlistnode s = l == l1 ? l2 : l1; // 第二个链表 <= l的长度\n\t\t\tint temp = 0; // 进位标记 如果为1则需要进1 0则不进1\n\t\t\tint cou = 0; // 存储每次计算结果\n\t\t\tlistnode tl = l; // 缓存当前长节点 元素\n\t\t\tlistnode ts = s; // 缓存当前短节点 元素\n\t\t\tlistnode pre = null;\n\t\t\t// 处理短的链表\n\t\t\twhile (ts != null) {\n\t\t\t\tcou = ts.val + tl.val + temp; // 相加结果\n\t\t\t\ttemp = cou / 10; // 是否需要进1\n\t\t\t\ttl.val = cou % 10; // 存储到节点中\n\t\t\t\tpre = tl; // 缓存当前长的节点元素 因为最后进1 需要使用该缓存节点下一跳 指向新的节点\n\t\t\t\t// 为何要在长的和短的链表 中记录当前节点呢 因为有可能短和长的链表长度一致 短的跑完长也跑完 又需要进1 会发生空指针异常\n\t\t\t\ttl = tl.next; // 下一个节点\n\t\t\t\tts = ts.next;\n\t\t\t}\n\t\t\t// 处理长的链表\n\t\t\twhile (tl != null) {\n\t\t\t\tcou = tl.val + temp;\n\t\t\t\ttemp = cou / 10;\n\t\t\t\ttl.val = cou % 10;\n\t\t\t\tpre = tl;\n\t\t\t\ttl = tl.next;\n\t\t\t}\n\t\t\t// 如果进位是1 则需要新创建节点\n\t\t\tif (temp != 0) {\n\t\t\t\tpre.next = new listnode(1);\n\t\t\t}\n\t\t\treturn l;\n\n\t\t}\n\n\t\tpublic int getsize(listnode head) {\n\t\t\tint size = 0;\n\t\t\twhile (head.next != null) {\n\t\t\t\tsize++;\n\t\t\t\thead = head.next;\n\t\t\t}\n\t\t\treturn size;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n\n\n\n# 合并两个有序链表\n\npublic class listnode {\n\t\tint val;\n\t\tlistnode next;\n\n\t\tlistnode() {\n\t\t}\n\n\t\tlistnode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tlistnode(int val, listnode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic listnode mergetwolists(listnode list1, listnode list2) {\n\t\t\tif (list1 == null || list2 == null) { // 另外一个链表为空直接返回非空链表\n\t\t\t\treturn list1 == null ? list2 : list1;\n\t\t\t}\n\t\t\tlistnode head = list1.val <= list2.val ? list1 : list2; // 头节点\n\t\t\tlistnode l = head.next; // 下一条\n\t\t\tlistnode s = head == list1 ? list2 : list1; // 另外一个链表\n\t\t\tlistnode pre = head;\n\t\t\twhile (l != null && s != null) {\n\t\t\t\tif (l.val <= s.val) {\n\t\t\t\t\tpre.next = l;\n\t\t\t\t\tl = l.next;\n\t\t\t\t} else {\n\t\t\t\t\tpre.next = s;\n\t\t\t\t\ts = s.next;\n\t\t\t\t}\n\t\t\t\tpre = pre.next;\n\t\t\t}\n\t\t\tpre.next = l == null ? s : l;\n\t\t\treturn head;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"时间复杂度",frontmatter:{title:"时间复杂度",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/ef4d8b/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/13.%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6.html",relativePath:"408/01.数据结构/13.时间复杂度.md",key:"v-2e467b68",path:"/pages/ef4d8b/",headersStr:null,content:"# 时间复杂度\n\n 1. 忽略常数时间的操作\n 2. 确定算法总操作数和样本数量之间的表达式关系\n 3. 只看表达式最高阶部分\n\n如一个选择排序算法，每次都要遍历n-1-n个数查找出当前子数组中最小值做交换，0~n-1、1~n-1、···，而交换最小值到当前下标index为常数操作每次固定时间，遍历操作为等差数列和公式 化简为aN^2*bN+c，只看最高阶为2次方，粗略估计为O(N^2)\n\n常见的常数操作:\n\n * 算术运算\n * 位运算\n * 赋值、比较、自增、自减等\n * 数组寻址\n\n固定时间为常数操作，时间不固定则不是常数操作\n\n如果某个算法流程的复杂程度会根据数据状况不同而不同，那么必须按照最差情况估计\n\n如插入排序 在最差的情况下 数组中每个数都要进行交换\n\n在插入排序中，当待排序数组是有序时，是最优的情况，只需当前数跟前一个数比较一下就可以了，这时一共需要比较N- 1次，时间复杂度为 O(N)\n\n最坏的情况是待排序数组是逆序的，此时需要比较次数最多，总次数记为：1+2+3+…+N-1，所以，插入排序最坏情况下的时间复杂度为O(N^2)",normalizedContent:"# 时间复杂度\n\n 1. 忽略常数时间的操作\n 2. 确定算法总操作数和样本数量之间的表达式关系\n 3. 只看表达式最高阶部分\n\n如一个选择排序算法，每次都要遍历n-1-n个数查找出当前子数组中最小值做交换，0~n-1、1~n-1、···，而交换最小值到当前下标index为常数操作每次固定时间，遍历操作为等差数列和公式 化简为an^2*bn+c，只看最高阶为2次方，粗略估计为o(n^2)\n\n常见的常数操作:\n\n * 算术运算\n * 位运算\n * 赋值、比较、自增、自减等\n * 数组寻址\n\n固定时间为常数操作，时间不固定则不是常数操作\n\n如果某个算法流程的复杂程度会根据数据状况不同而不同，那么必须按照最差情况估计\n\n如插入排序 在最差的情况下 数组中每个数都要进行交换\n\n在插入排序中，当待排序数组是有序时，是最优的情况，只需当前数跟前一个数比较一下就可以了，这时一共需要比较n- 1次，时间复杂度为 o(n)\n\n最坏的情况是待排序数组是逆序的，此时需要比较次数最多，总次数记为：1+2+3+…+n-1，所以，插入排序最坏情况下的时间复杂度为o(n^2)",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"二叉树",frontmatter:{title:"二叉树",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/3e25e1/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/11.%E4%BA%8C%E5%8F%89%E6%A0%91.html",relativePath:"408/01.数据结构/11.二叉树.md",key:"v-a61f04aa",path:"/pages/3e25e1/",headers:[{level:2,title:"递归序",slug:"递归序",normalizedTitle:"递归序",charIndex:10},{level:2,title:"100. 相同的树",slug:"_100-相同的树",normalizedTitle:"100. 相同的树",charIndex:873},{level:2,title:"101. 对称二叉树",slug:"_101-对称二叉树",normalizedTitle:"101. 对称二叉树",charIndex:1568},{level:2,title:"104. 二叉树的最大深度",slug:"_104-二叉树的最大深度",normalizedTitle:"104. 二叉树的最大深度",charIndex:2405},{level:2,title:"105. 从前序与中序遍历序列构造二叉树",slug:"_105-从前序与中序遍历序列构造二叉树",normalizedTitle:"105. 从前序与中序遍历序列构造二叉树",charIndex:2934},{level:2,title:"107. 二叉树的层序遍历 II",slug:"_107-二叉树的层序遍历-ii",normalizedTitle:"107. 二叉树的层序遍历 ii",charIndex:4481},{level:2,title:"110. 平衡二叉树",slug:"_110-平衡二叉树",normalizedTitle:"110. 平衡二叉树",charIndex:5654},{level:2,title:"98. 验证二叉搜索树",slug:"_98-验证二叉搜索树",normalizedTitle:"98. 验证二叉搜索树",charIndex:6937},{level:2,title:"112. 路径总和",slug:"_112-路径总和",normalizedTitle:"112. 路径总和",charIndex:8834},{level:2,title:"113. 路径总和 II",slug:"_113-路径总和-ii",normalizedTitle:"113. 路径总和 ii",charIndex:9911}],headersStr:"递归序 100. 相同的树 101. 对称二叉树 104. 二叉树的最大深度 105. 从前序与中序遍历序列构造二叉树 107. 二叉树的层序遍历 II 110. 平衡二叉树 98. 验证二叉搜索树 112. 路径总和 113. 路径总和 II",content:'# 二叉树\n\n\n# 递归序\n\n先序遍历 : 头节点 左节点 右节点\n\n中序遍历 : 左节点 头节点 右节点\n\n后序遍历 : 左节点 右节点 头节点\n\n\tpublic static class Node {\n\t\tpublic int val;\n\t\tpublic Node left;\n\t\tpublic Node right;\n\t\t\n\t\t\n\t\tpublic Node(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t}\n\t\n\tpublic static void f(Node head) {\n\t\tif(head == null) {\n\t\t\treturn;\n\t\t}\n\t\tSystem.out.println("先序遍历"+head.val); //头节点 左节点 右节点\n\t\tf(head.left);\n//\t\tSystem.out.println("中序遍历"+head.val); //左节点 头节点 右节点\n\t\tf(head.right);\n//\t\tSystem.out.println("后序遍历"+head.val); //左节点 右节点 头节点\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\tNode head = new Node(1);\n\t\thead.left = new Node(2);\n\t\thead.right = new Node(3);\n\t\thead.left.left = new Node(4);\n\t\thead.left.right = new Node(5);\n\t\thead.right.left = new Node(6);\n\t\thead.right.right = new Node(8);\n\t\t\n\t\tf(head);\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 100. 相同的树\n\n\tpublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic boolean isSameTree(TreeNode p, TreeNode q) {\n\t\t\tif (p == null ^ q == null) { //有一个为空 另外一个不为空 返回\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tif (p == null && q == null) { //两者都为空\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\t//两者值相等 && 左树相等 && 右树相等\n\t\t\treturn p.val == q.val && isSameTree(p.left, q.left) && isSameTree(p.right, q.right);\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# 101. 对称二叉树\n\n\tpublic class TreeNode {\n\t    int val;\n\t    TreeNode left;\n\t    TreeNode right;\n\t    TreeNode() {}\n\t    TreeNode(int val) { this.val = val; }\n\t    TreeNode(int val, TreeNode left, TreeNode right) {\n\t        this.val = val;\n\t        this.left = left;\n\t        this.right = right;\n\t    }\n\t}\n\t\n\tclass Solution {\n\t    public boolean isSymmetric(TreeNode root) {\n\t    \treturn isSameTree(root,root);//自己与自己比较\n\t    }\n\t    \n\t    public boolean isSameTree(TreeNode p, TreeNode q) {\n\t\t\tif (p == null ^ q == null) { //有一个为空 另外一个不为空 返回\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tif (p == null && q == null) { //两者都为空\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\t//两者值相等 && 左树与`右树相等 && 右树与`左树相等\n\t\t\treturn p.val == q.val && isSameTree(p.left, q.right) && isSameTree(p.right, q.left);\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 104. 二叉树的最大深度\n\n\tpublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic int maxDepth(TreeNode root) {\n\t\t\tif (root == null) {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\treturn Math.max(maxDepth(root.left), maxDepth(root.right)) + 1;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 105. 从前序与中序遍历序列构造二叉树\n\n\tpublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\n\t\tpublic TreeNode buildTree(int[] preorder, int[] inorder) {\n\t\t\tif (preorder == null || inorder == null || preorder.length != inorder.length) {\n\t\t\t\treturn null;\n\t\t\t}\n             Map<Integer, Integer> IndexMap = new HashMap<>();\n\t\t\tfor (int i = 0; i < inorder.length; i++) {\n\t\t\t\tIndexMap.put(inorder[i], i);\n\t\t\t}\n\n\t\t\treturn f(preorder, 0, preorder.length - 1, inorder, 0, inorder.length - 1,IndexMap);\n\t\t}\n\n\t\tprivate TreeNode f(int[] preorder, int l1, int r1, int[] inorder, int l2, int r2,Map<Integer, Integer> IndexMap) {\n\t\t\tif (l1 > r1) { //\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\tTreeNode head = new TreeNode(preorder[l1]); // 头节点\n\t\t\tif (l1 == r1) { // 只有一个元素时\n\t\t\t\treturn head;\n\t\t\t}\n\t\t\tint index = IndexMap.get(preorder[l1]); // 查找当前头节点在中序遍历数组的位置\n\t\t\t// 左树构建 l1+1到 l1 + index - l2 为左树的先序遍历范围 \t\tl2到index-1为中序遍历范围\n\t\t\thead.left = f(preorder, l1 + 1, l1 + index - l2, inorder, l2, index - 1,IndexMap); // 注意左树和右树构建采用的遍历范围均相同长度\n\t\t\t// 右树构建 l1 + index - l2(即左树的结束范围) +1 到 r1为右树先序遍历范围 \t\tindex+1到r2为右树中序遍历的范围\n\t\t\thead.right = f(preorder, l1 + index - l2 + 1, r1, inorder, index + 1, r2,IndexMap);\n\t\t\treturn head;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# 107. 二叉树的层序遍历 II\n\npublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic List<List<Integer>> levelOrderBottom(TreeNode root) {\n\t\t\tList<List<Integer>> ans = new ArrayList<>();\n\t\t\tif (root == null) { // 根节点为空直接返回空列表\n\t\t\t\treturn ans;\n\t\t\t}\n\t\t\tLinkedList<TreeNode> queue = new LinkedList<>();\n\t\t\tqueue.add(root); // 将头节点加入队列中\n\t\t\twhile (!queue.isEmpty()) {\n\t\t\t\tint size = queue.size(); // 获取队列当前长度\n\t\t\t\tList<Integer> tans = new LinkedList<>();\n\t\t\t\tfor (int i = 0; i < size; i++) {\n\t\t\t\t\tTreeNode temp = queue.poll();\n\t\t\t\t\ttans.add(temp.val); // 获取头节点的值 放入局部队列中\n\t\t\t\t\tif (temp.left != null) {\n\t\t\t\t\t\tqueue.add(temp.left); // 左节点非空 加入到队列中\n\t\t\t\t\t}\n\t\t\t\t\tif (temp.right != null) {\n\t\t\t\t\t\tqueue.add(temp.right); // 右节点非空 加入队列中\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\t\tans.add(0, tans); // 将上次队列缓存头节点清空后的节点值加入到结果集合中\n\t\t\t}\n\t\t\treturn ans;\n\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 110. 平衡二叉树\n\n平衡二叉树:一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1 。\n\n\tpublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic boolean isBalanced(TreeNode root) {\n\t\t\treturn process(root).isBalanced; //直接返回root节点的信息\n\t\t}\n\n\t\tpublic class info {\n\t\t\tpublic boolean isBalanced; // 是否为平衡二叉树\n\t\t\tpublic int height; // 高度\n\n\t\t\tpublic info(boolean isBalanced, int height) {\n\t\t\t\tthis.isBalanced = isBalanced;\n\t\t\t\tthis.height = height;\n\t\t\t}\n\n\t\t}\n\n\t\tpublic info process(TreeNode root) {\n\t\t\tif (root == null) { // 空节点\n\t\t\t\treturn new info(true, 0);\n\t\t\t}\n\t\t\tinfo leftInfo = process(root.left); // 左树信息\n\t\t\tinfo rightInfo = process(root.right); // 右树信息\n\n\t\t\tint height = Math.max(leftInfo.height, rightInfo.height) + 1; // 当前节点高度为 左和右树最大值 +1\n\t\t\tboolean isBalanced = leftInfo.isBalanced && rightInfo.isBalanced\n\t\t\t\t\t&& Math.abs(leftInfo.height - rightInfo.height) <= 1; //左树和右树为平衡二叉树 并且 左右树高度差不大于1\n\t\t\treturn new info(isBalanced, height);\n\t\t}\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# 98. 验证二叉搜索树\n\n搜索二叉树:左树的值比根节点的值小，右树的值比根节点的值大\n\n一颗搜索二叉树中序遍历是递增顺序\n\n\tpublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic boolean isValidBST(TreeNode root) {\n\t\t\treturn isBST(root).isBst;\n\t\t}\n\t\t\n\t\tpublic class info{\n\t\t\tboolean isBst;\n\t\t\tint max;\n\t\t\tint min;\n\t\t\tpublic info(boolean isBst, int max, int min) {\n\t\t\t\tthis.isBst = isBst;\n\t\t\t\tthis.max = max;\n\t\t\t\tthis.min = min;\n\t\t\t}\n\t\t\t\n\t\t}\n\t\t\n\t\tpublic info isBST(TreeNode root) {\n\t\t\tif(root == null) { //头节点为空则返回空\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\tinfo leftinfo = isBST(root.left); //递归进入左树\n\t\t\tinfo rightinfo = isBST(root.right); //递归进入右树\n\t\t\t\n\t\t\tint max = root.val; //假设最大值为自身\n\t\t\tint min = root.val;//假设最小值为自身\n\t\t\tif(leftinfo != null) { //左树非空\n\t\t\t\tmax = Math.max(max, leftinfo.max); //提取左树信息\n\t\t\t\tmin = Math.min(min, leftinfo.min);\n\t\t\t}\n\t\t\tif(rightinfo != null) {\n\t\t\t\tmax = Math.max(max, rightinfo.max); //提取右树信息\n\t\t\t\tmin = Math.min(min, rightinfo.min);\n\t\t\t}\n\t\t\t\n\t\t\tboolean bst = true; //先假设为搜索二叉树\n\t\t\tif(leftinfo !=null && !leftinfo.isBst) { //如果左树不满足则 头节点标记为不是二叉树\n\t\t\t\tbst =false;\n\t\t\t}\n\t\t\tif(rightinfo !=null && !rightinfo.isBst) { //如果右树不满足则 头节点标记为不是二叉树\n\t\t\t\tbst =false;\n\t\t\t}\n\t\t\t\n\t\t\t\n\t\t\tboolean lefiBst = leftinfo == null ? true : (leftinfo.max < root.val); //如果左树信息为空返回ture  否则左树最大值必须小于当前节点值\n\t\t\tboolean rightBst = rightinfo == null ? true : (rightinfo.min > root.val);//如果右树信息为空返回ture  否则右树最小值必须大于当前节点值\n\t\t\tif(!lefiBst || !rightBst) { //不满足搜索二叉树规则\n\t\t\t\tbst = false;\n\t\t\t}\n\t\t\t\n\t\t\treturn new info(bst, max, min);\n\t\t}\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n\n# 112. 路径总和\n\npublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\n\t\tpublic boolean isSum = false;\n\n\t\tpublic boolean hasPathSum(TreeNode root, int targetSum) {\n\t\t\tif(root ==null) { //根节点为空 直接返回false\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\ttree(root, 0, targetSum);\n\t\t\treturn isSum;\n\n\t\t}\n\n\t\tpublic void tree(TreeNode root, int preSum, int sum) {\n\t\t\tif (root.left == null && root.right == null) { //当前头节点左右子节点都为空\n\t\t\t\tif (preSum + root.val == sum) { //当前头节点值与上次sum 为target则找到\n\t\t\t\t\tisSum = true;\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tpreSum += root.val; //累加当前sum\n\t\t\tif (root.left != null) { //左节点有叶节点 递归进去\n\t\t\t\ttree(root.left, preSum, sum);\n\t\t\t}\n\t\t\tif (root.right != null) { //右节点有叶节点 递归进去\n\t\t\t\ttree(root.right, preSum, sum);\n\t\t\t}\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# 113. 路径总和 II\n\n\tpublic class TreeNode {\n\t\tint val;\n\t\tTreeNode left;\n\t\tTreeNode right;\n\n\t\tTreeNode() {\n\t\t}\n\n\t\tTreeNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tTreeNode(int val, TreeNode left, TreeNode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass Solution {\n\t\tpublic List<List<Integer>> pathSum(TreeNode root, int targetSum) {\n\t\t\tList<List<Integer>> ans = new ArrayList<>();\n\t\t\tif (root == null) {\n\t\t\t\treturn ans;\n\t\t\t}\n\t\t\tArrayList<Integer> path = new ArrayList<>();\n\t\t\tprocess(root, path, 0, targetSum, ans);\n\t\t\treturn ans;\n\t\t}\n\n\t\tpublic void process(TreeNode root, ArrayList<Integer> path, int preSum, int sum, List<List<Integer>> ans) {\n\t\t\tif (root.left == null & root.right == null) {\n\t\t\t\tif (root.val + preSum == sum) {\n\t\t\t\t\tpath.add(root.val); // 添加当前节点路径\n//\t\t\t\t\tList<Integer> newPath = copy(path); // 拷贝\n\t\t\t\t\tList<Integer> newPath = (List<Integer>) path.clone(); // 本质上是浅拷贝 但里面存储的是值 所以复制了里面值 如里面存储的是引用不应用深拷贝\n\t\t\t\t\t\n\t\t\t\t\tans.add(newPath); // 添加到结果集合中\n\t\t\t\t\tpath.remove(path.size() - 1); // 删除当前路径\n\t\t\t\t}\n\t\t\t}\n\t\t\tpreSum += root.val; // 累加 无需回溯 因为是值传递 不是引用传递\n\t\t\tpath.add(root.val); // 记录节点\n\t\t\tif (root.left != null) { // 递归\n\t\t\t\tprocess(root.left, path, preSum, sum, ans);\n\t\t\t}\n\t\t\tif (root.right != null) { // 递归\n\t\t\t\tprocess(root.right, path, preSum, sum, ans);\n\t\t\t}\n\t\t\tpath.remove(path.size() - 1); // 恢复现场\n\t\t}\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n',normalizedContent:'# 二叉树\n\n\n# 递归序\n\n先序遍历 : 头节点 左节点 右节点\n\n中序遍历 : 左节点 头节点 右节点\n\n后序遍历 : 左节点 右节点 头节点\n\n\tpublic static class node {\n\t\tpublic int val;\n\t\tpublic node left;\n\t\tpublic node right;\n\t\t\n\t\t\n\t\tpublic node(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t}\n\t\n\tpublic static void f(node head) {\n\t\tif(head == null) {\n\t\t\treturn;\n\t\t}\n\t\tsystem.out.println("先序遍历"+head.val); //头节点 左节点 右节点\n\t\tf(head.left);\n//\t\tsystem.out.println("中序遍历"+head.val); //左节点 头节点 右节点\n\t\tf(head.right);\n//\t\tsystem.out.println("后序遍历"+head.val); //左节点 右节点 头节点\n\t}\n\t\n\tpublic static void main(string[] args) {\n\t\tnode head = new node(1);\n\t\thead.left = new node(2);\n\t\thead.right = new node(3);\n\t\thead.left.left = new node(4);\n\t\thead.left.right = new node(5);\n\t\thead.right.left = new node(6);\n\t\thead.right.right = new node(8);\n\t\t\n\t\tf(head);\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 100. 相同的树\n\n\tpublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic boolean issametree(treenode p, treenode q) {\n\t\t\tif (p == null ^ q == null) { //有一个为空 另外一个不为空 返回\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tif (p == null && q == null) { //两者都为空\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\t//两者值相等 && 左树相等 && 右树相等\n\t\t\treturn p.val == q.val && issametree(p.left, q.left) && issametree(p.right, q.right);\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# 101. 对称二叉树\n\n\tpublic class treenode {\n\t    int val;\n\t    treenode left;\n\t    treenode right;\n\t    treenode() {}\n\t    treenode(int val) { this.val = val; }\n\t    treenode(int val, treenode left, treenode right) {\n\t        this.val = val;\n\t        this.left = left;\n\t        this.right = right;\n\t    }\n\t}\n\t\n\tclass solution {\n\t    public boolean issymmetric(treenode root) {\n\t    \treturn issametree(root,root);//自己与自己比较\n\t    }\n\t    \n\t    public boolean issametree(treenode p, treenode q) {\n\t\t\tif (p == null ^ q == null) { //有一个为空 另外一个不为空 返回\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tif (p == null && q == null) { //两者都为空\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\t//两者值相等 && 左树与`右树相等 && 右树与`左树相等\n\t\t\treturn p.val == q.val && issametree(p.left, q.right) && issametree(p.right, q.left);\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 104. 二叉树的最大深度\n\n\tpublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic int maxdepth(treenode root) {\n\t\t\tif (root == null) {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\treturn math.max(maxdepth(root.left), maxdepth(root.right)) + 1;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 105. 从前序与中序遍历序列构造二叉树\n\n\tpublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\n\t\tpublic treenode buildtree(int[] preorder, int[] inorder) {\n\t\t\tif (preorder == null || inorder == null || preorder.length != inorder.length) {\n\t\t\t\treturn null;\n\t\t\t}\n             map<integer, integer> indexmap = new hashmap<>();\n\t\t\tfor (int i = 0; i < inorder.length; i++) {\n\t\t\t\tindexmap.put(inorder[i], i);\n\t\t\t}\n\n\t\t\treturn f(preorder, 0, preorder.length - 1, inorder, 0, inorder.length - 1,indexmap);\n\t\t}\n\n\t\tprivate treenode f(int[] preorder, int l1, int r1, int[] inorder, int l2, int r2,map<integer, integer> indexmap) {\n\t\t\tif (l1 > r1) { //\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\ttreenode head = new treenode(preorder[l1]); // 头节点\n\t\t\tif (l1 == r1) { // 只有一个元素时\n\t\t\t\treturn head;\n\t\t\t}\n\t\t\tint index = indexmap.get(preorder[l1]); // 查找当前头节点在中序遍历数组的位置\n\t\t\t// 左树构建 l1+1到 l1 + index - l2 为左树的先序遍历范围 \t\tl2到index-1为中序遍历范围\n\t\t\thead.left = f(preorder, l1 + 1, l1 + index - l2, inorder, l2, index - 1,indexmap); // 注意左树和右树构建采用的遍历范围均相同长度\n\t\t\t// 右树构建 l1 + index - l2(即左树的结束范围) +1 到 r1为右树先序遍历范围 \t\tindex+1到r2为右树中序遍历的范围\n\t\t\thead.right = f(preorder, l1 + index - l2 + 1, r1, inorder, index + 1, r2,indexmap);\n\t\t\treturn head;\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# 107. 二叉树的层序遍历 ii\n\npublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic list<list<integer>> levelorderbottom(treenode root) {\n\t\t\tlist<list<integer>> ans = new arraylist<>();\n\t\t\tif (root == null) { // 根节点为空直接返回空列表\n\t\t\t\treturn ans;\n\t\t\t}\n\t\t\tlinkedlist<treenode> queue = new linkedlist<>();\n\t\t\tqueue.add(root); // 将头节点加入队列中\n\t\t\twhile (!queue.isempty()) {\n\t\t\t\tint size = queue.size(); // 获取队列当前长度\n\t\t\t\tlist<integer> tans = new linkedlist<>();\n\t\t\t\tfor (int i = 0; i < size; i++) {\n\t\t\t\t\ttreenode temp = queue.poll();\n\t\t\t\t\ttans.add(temp.val); // 获取头节点的值 放入局部队列中\n\t\t\t\t\tif (temp.left != null) {\n\t\t\t\t\t\tqueue.add(temp.left); // 左节点非空 加入到队列中\n\t\t\t\t\t}\n\t\t\t\t\tif (temp.right != null) {\n\t\t\t\t\t\tqueue.add(temp.right); // 右节点非空 加入队列中\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\t\tans.add(0, tans); // 将上次队列缓存头节点清空后的节点值加入到结果集合中\n\t\t\t}\n\t\t\treturn ans;\n\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 110. 平衡二叉树\n\n平衡二叉树:一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1 。\n\n\tpublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic boolean isbalanced(treenode root) {\n\t\t\treturn process(root).isbalanced; //直接返回root节点的信息\n\t\t}\n\n\t\tpublic class info {\n\t\t\tpublic boolean isbalanced; // 是否为平衡二叉树\n\t\t\tpublic int height; // 高度\n\n\t\t\tpublic info(boolean isbalanced, int height) {\n\t\t\t\tthis.isbalanced = isbalanced;\n\t\t\t\tthis.height = height;\n\t\t\t}\n\n\t\t}\n\n\t\tpublic info process(treenode root) {\n\t\t\tif (root == null) { // 空节点\n\t\t\t\treturn new info(true, 0);\n\t\t\t}\n\t\t\tinfo leftinfo = process(root.left); // 左树信息\n\t\t\tinfo rightinfo = process(root.right); // 右树信息\n\n\t\t\tint height = math.max(leftinfo.height, rightinfo.height) + 1; // 当前节点高度为 左和右树最大值 +1\n\t\t\tboolean isbalanced = leftinfo.isbalanced && rightinfo.isbalanced\n\t\t\t\t\t&& math.abs(leftinfo.height - rightinfo.height) <= 1; //左树和右树为平衡二叉树 并且 左右树高度差不大于1\n\t\t\treturn new info(isbalanced, height);\n\t\t}\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# 98. 验证二叉搜索树\n\n搜索二叉树:左树的值比根节点的值小，右树的值比根节点的值大\n\n一颗搜索二叉树中序遍历是递增顺序\n\n\tpublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic boolean isvalidbst(treenode root) {\n\t\t\treturn isbst(root).isbst;\n\t\t}\n\t\t\n\t\tpublic class info{\n\t\t\tboolean isbst;\n\t\t\tint max;\n\t\t\tint min;\n\t\t\tpublic info(boolean isbst, int max, int min) {\n\t\t\t\tthis.isbst = isbst;\n\t\t\t\tthis.max = max;\n\t\t\t\tthis.min = min;\n\t\t\t}\n\t\t\t\n\t\t}\n\t\t\n\t\tpublic info isbst(treenode root) {\n\t\t\tif(root == null) { //头节点为空则返回空\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\tinfo leftinfo = isbst(root.left); //递归进入左树\n\t\t\tinfo rightinfo = isbst(root.right); //递归进入右树\n\t\t\t\n\t\t\tint max = root.val; //假设最大值为自身\n\t\t\tint min = root.val;//假设最小值为自身\n\t\t\tif(leftinfo != null) { //左树非空\n\t\t\t\tmax = math.max(max, leftinfo.max); //提取左树信息\n\t\t\t\tmin = math.min(min, leftinfo.min);\n\t\t\t}\n\t\t\tif(rightinfo != null) {\n\t\t\t\tmax = math.max(max, rightinfo.max); //提取右树信息\n\t\t\t\tmin = math.min(min, rightinfo.min);\n\t\t\t}\n\t\t\t\n\t\t\tboolean bst = true; //先假设为搜索二叉树\n\t\t\tif(leftinfo !=null && !leftinfo.isbst) { //如果左树不满足则 头节点标记为不是二叉树\n\t\t\t\tbst =false;\n\t\t\t}\n\t\t\tif(rightinfo !=null && !rightinfo.isbst) { //如果右树不满足则 头节点标记为不是二叉树\n\t\t\t\tbst =false;\n\t\t\t}\n\t\t\t\n\t\t\t\n\t\t\tboolean lefibst = leftinfo == null ? true : (leftinfo.max < root.val); //如果左树信息为空返回ture  否则左树最大值必须小于当前节点值\n\t\t\tboolean rightbst = rightinfo == null ? true : (rightinfo.min > root.val);//如果右树信息为空返回ture  否则右树最小值必须大于当前节点值\n\t\t\tif(!lefibst || !rightbst) { //不满足搜索二叉树规则\n\t\t\t\tbst = false;\n\t\t\t}\n\t\t\t\n\t\t\treturn new info(bst, max, min);\n\t\t}\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n\n# 112. 路径总和\n\npublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\n\t\tpublic boolean issum = false;\n\n\t\tpublic boolean haspathsum(treenode root, int targetsum) {\n\t\t\tif(root ==null) { //根节点为空 直接返回false\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\ttree(root, 0, targetsum);\n\t\t\treturn issum;\n\n\t\t}\n\n\t\tpublic void tree(treenode root, int presum, int sum) {\n\t\t\tif (root.left == null && root.right == null) { //当前头节点左右子节点都为空\n\t\t\t\tif (presum + root.val == sum) { //当前头节点值与上次sum 为target则找到\n\t\t\t\t\tissum = true;\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tpresum += root.val; //累加当前sum\n\t\t\tif (root.left != null) { //左节点有叶节点 递归进去\n\t\t\t\ttree(root.left, presum, sum);\n\t\t\t}\n\t\t\tif (root.right != null) { //右节点有叶节点 递归进去\n\t\t\t\ttree(root.right, presum, sum);\n\t\t\t}\n\t\t}\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# 113. 路径总和 ii\n\n\tpublic class treenode {\n\t\tint val;\n\t\ttreenode left;\n\t\ttreenode right;\n\n\t\ttreenode() {\n\t\t}\n\n\t\ttreenode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\ttreenode(int val, treenode left, treenode right) {\n\t\t\tthis.val = val;\n\t\t\tthis.left = left;\n\t\t\tthis.right = right;\n\t\t}\n\t}\n\n\tclass solution {\n\t\tpublic list<list<integer>> pathsum(treenode root, int targetsum) {\n\t\t\tlist<list<integer>> ans = new arraylist<>();\n\t\t\tif (root == null) {\n\t\t\t\treturn ans;\n\t\t\t}\n\t\t\tarraylist<integer> path = new arraylist<>();\n\t\t\tprocess(root, path, 0, targetsum, ans);\n\t\t\treturn ans;\n\t\t}\n\n\t\tpublic void process(treenode root, arraylist<integer> path, int presum, int sum, list<list<integer>> ans) {\n\t\t\tif (root.left == null & root.right == null) {\n\t\t\t\tif (root.val + presum == sum) {\n\t\t\t\t\tpath.add(root.val); // 添加当前节点路径\n//\t\t\t\t\tlist<integer> newpath = copy(path); // 拷贝\n\t\t\t\t\tlist<integer> newpath = (list<integer>) path.clone(); // 本质上是浅拷贝 但里面存储的是值 所以复制了里面值 如里面存储的是引用不应用深拷贝\n\t\t\t\t\t\n\t\t\t\t\tans.add(newpath); // 添加到结果集合中\n\t\t\t\t\tpath.remove(path.size() - 1); // 删除当前路径\n\t\t\t\t}\n\t\t\t}\n\t\t\tpresum += root.val; // 累加 无需回溯 因为是值传递 不是引用传递\n\t\t\tpath.add(root.val); // 记录节点\n\t\t\tif (root.left != null) { // 递归\n\t\t\t\tprocess(root.left, path, presum, sum, ans);\n\t\t\t}\n\t\t\tif (root.right != null) { // 递归\n\t\t\t\tprocess(root.right, path, presum, sum, ans);\n\t\t\t}\n\t\t\tpath.remove(path.size() - 1); // 恢复现场\n\t\t}\n\t\t\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-1c0c6895",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"队列和栈",frontmatter:{title:"队列和栈",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/3bda9f/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/14.%E9%98%9F%E5%88%97%E5%92%8C%E6%A0%88.html",relativePath:"408/01.数据结构/14.队列和栈.md",key:"v-320a16ea",path:"/pages/3bda9f/",headers:[{level:2,title:"数组实现队列",slug:"数组实现队列",normalizedTitle:"数组实现队列",charIndex:11},{level:2,title:"栈实现队列",slug:"栈实现队列",normalizedTitle:"栈实现队列",charIndex:920},{level:2,title:"队列实现栈",slug:"队列实现栈",normalizedTitle:"队列实现栈",charIndex:2179}],headersStr:"数组实现队列 栈实现队列 队列实现栈",content:'# 队列和栈\n\n\n# 数组实现队列\n\npublic class MyQueue {\n\tprivate int[] arr;\n\tprivate int pushi;\n\tprivate int polli;\n\tprivate int size;\n\tprivate final int limit;\n\n\tpublic MyQueue(int limit) {\n\t\tthis.limit = limit;\n\t\tpushi = 0;// 头指针\n\t\tpolli = 0;// 尾指针\n\t\tsize = 0;\n\t\tarr = new int[limit];\n\t}\n\n\tpublic void push(int value) {\n\t\tif (size == limit) {\n\t\t\tthrow new RuntimeException("队列已满 无法继续添加");\n\t\t}\n\t\tsize++;\n\t\tarr[pushi] = value;\n\t\tpushi = nextIndex(pushi);\n\t}\n\n\tpublic int pop(int value) {\n\t\tif (size == 0) {\n\t\t\tthrow new RuntimeException("队列为空");\n\t\t}\n\t\tsize--;\n\t\tint ans = arr[polli];\n\t\tpolli = nextIndex(polli);\n\t\treturn ans;\n\n\t}\n\n\tpublic boolean isEmpty() {\n\t\treturn size == 0;\n\t}\n\n\tprivate int nextIndex(int index) {\n\t\t// index = (index+1) % limit;\n\t\treturn index < limit - 1 ? index + 1 : 0;\n\t}\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# 栈实现队列\n\n// 使用两个栈实现\n\tpublic static class Myqueue {\n\t\tprivate Stack<Integer> StackPull;\n\t\tprivate Stack<Integer> StackPop;\n       \n\tpublic Myqueue() {\n\t\tStackPull = new Stack<Integer>();\n\t\tStackPop = new Stack<Integer>();\n\t}\n\n\tpublic void push(int value) {\n\t\tStackPull.push(value);\n\t\tpushToPop();\n\t}\n\n\tprivate void pushToPop() {\n\t\tif (StackPop.isEmpty()) {\n\t\t\twhile (!StackPull.isEmpty()) {\n\t\t\t\tStackPop.push(StackPull.pop()); // 将栈顶 压到 pop栈中\n\t\t\t}\n\t\t}\n\n\t}\n\n\tpublic int pop() {\n\t\tif (StackPull.isEmpty() && StackPop.isEmpty()) {\n\t\t\tthrow new RuntimeException("队列中无元素");\n\t\t}\n\t\tpushToPop();\n\t\treturn StackPop.pop();\n\t}\n\n\tpublic int peek() {\n\t\tif (StackPull.isEmpty() && StackPop.isEmpty()) {\n\t\t\tthrow new RuntimeException("队列中无元素");\n\t\t}\n\t\tpushToPop();\n\t\treturn StackPop.peek();\n\t}\n\n}\n\npublic static void main(String[] args) {\n\tMyqueue test = new Myqueue();\n\ttest.push(1);\n\ttest.push(2);\n\ttest.push(5);\n\tSystem.out.println(test.peek());\n\tSystem.out.println(test.pop());\n\tSystem.out.println(test.peek());\n\tSystem.out.println(test.pop());\n\tSystem.out.println(test.peek());\n\tSystem.out.println(test.pop());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# 队列实现栈\n\n// 两个队列实现\n\tpublic static class TwoQueueStack<T> {\n\t\tpublic Queue<T> queue;\n\t\tpublic Queue<T> help;\n\n\t\tpublic TwoQueueStack() {\n\t\t\tqueue = new LinkedList<>();\n\t\t\thelp = new LinkedList<>();\n\t\t}\n\n\t\tpublic void push(T value) {\n\t\t\tqueue.offer(value); // 加入到队尾\n\t\t}\n\n\t\tpublic T pop() {\n\t\t\twhile (queue.size() > 1) {\n\t\t\t\thelp.offer(queue.poll()); // 除了队尾的元素 全部转移到另外一个队列中\n\t\t\t}\n\t\t\tT ans = queue.poll(); // 当前队尾元素为栈顶\n\t\t\tQueue<T> tep = queue; // 缓存当前空队列\n\t\t\tqueue = help; // 重新转移到queue中\n\t\t\thelp = tep;\n\t\t\treturn ans;\n\t\t}\n\n\t\tpublic T peek() {\n\t\t\twhile (queue.size() > 1) {\n\t\t\t\thelp.offer(queue.poll());\n\t\t\t}\n\t\t\tT ans = queue.poll();\n\t\t\thelp.offer(ans); // 由于是查看栈顶 所以获取到值后 重新添加到另外队列中\n\t\t\tQueue<T> tep = queue;\n\t\t\tqueue = help;\n\t\t\thelp = tep;\n\t\t\treturn ans;\n\t\t}\n\n\t\tpublic boolean isEmpty() {\n\t\t\treturn queue.isEmpty();\n\t\t}\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n',normalizedContent:'# 队列和栈\n\n\n# 数组实现队列\n\npublic class myqueue {\n\tprivate int[] arr;\n\tprivate int pushi;\n\tprivate int polli;\n\tprivate int size;\n\tprivate final int limit;\n\n\tpublic myqueue(int limit) {\n\t\tthis.limit = limit;\n\t\tpushi = 0;// 头指针\n\t\tpolli = 0;// 尾指针\n\t\tsize = 0;\n\t\tarr = new int[limit];\n\t}\n\n\tpublic void push(int value) {\n\t\tif (size == limit) {\n\t\t\tthrow new runtimeexception("队列已满 无法继续添加");\n\t\t}\n\t\tsize++;\n\t\tarr[pushi] = value;\n\t\tpushi = nextindex(pushi);\n\t}\n\n\tpublic int pop(int value) {\n\t\tif (size == 0) {\n\t\t\tthrow new runtimeexception("队列为空");\n\t\t}\n\t\tsize--;\n\t\tint ans = arr[polli];\n\t\tpolli = nextindex(polli);\n\t\treturn ans;\n\n\t}\n\n\tpublic boolean isempty() {\n\t\treturn size == 0;\n\t}\n\n\tprivate int nextindex(int index) {\n\t\t// index = (index+1) % limit;\n\t\treturn index < limit - 1 ? index + 1 : 0;\n\t}\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# 栈实现队列\n\n// 使用两个栈实现\n\tpublic static class myqueue {\n\t\tprivate stack<integer> stackpull;\n\t\tprivate stack<integer> stackpop;\n       \n\tpublic myqueue() {\n\t\tstackpull = new stack<integer>();\n\t\tstackpop = new stack<integer>();\n\t}\n\n\tpublic void push(int value) {\n\t\tstackpull.push(value);\n\t\tpushtopop();\n\t}\n\n\tprivate void pushtopop() {\n\t\tif (stackpop.isempty()) {\n\t\t\twhile (!stackpull.isempty()) {\n\t\t\t\tstackpop.push(stackpull.pop()); // 将栈顶 压到 pop栈中\n\t\t\t}\n\t\t}\n\n\t}\n\n\tpublic int pop() {\n\t\tif (stackpull.isempty() && stackpop.isempty()) {\n\t\t\tthrow new runtimeexception("队列中无元素");\n\t\t}\n\t\tpushtopop();\n\t\treturn stackpop.pop();\n\t}\n\n\tpublic int peek() {\n\t\tif (stackpull.isempty() && stackpop.isempty()) {\n\t\t\tthrow new runtimeexception("队列中无元素");\n\t\t}\n\t\tpushtopop();\n\t\treturn stackpop.peek();\n\t}\n\n}\n\npublic static void main(string[] args) {\n\tmyqueue test = new myqueue();\n\ttest.push(1);\n\ttest.push(2);\n\ttest.push(5);\n\tsystem.out.println(test.peek());\n\tsystem.out.println(test.pop());\n\tsystem.out.println(test.peek());\n\tsystem.out.println(test.pop());\n\tsystem.out.println(test.peek());\n\tsystem.out.println(test.pop());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# 队列实现栈\n\n// 两个队列实现\n\tpublic static class twoqueuestack<t> {\n\t\tpublic queue<t> queue;\n\t\tpublic queue<t> help;\n\n\t\tpublic twoqueuestack() {\n\t\t\tqueue = new linkedlist<>();\n\t\t\thelp = new linkedlist<>();\n\t\t}\n\n\t\tpublic void push(t value) {\n\t\t\tqueue.offer(value); // 加入到队尾\n\t\t}\n\n\t\tpublic t pop() {\n\t\t\twhile (queue.size() > 1) {\n\t\t\t\thelp.offer(queue.poll()); // 除了队尾的元素 全部转移到另外一个队列中\n\t\t\t}\n\t\t\tt ans = queue.poll(); // 当前队尾元素为栈顶\n\t\t\tqueue<t> tep = queue; // 缓存当前空队列\n\t\t\tqueue = help; // 重新转移到queue中\n\t\t\thelp = tep;\n\t\t\treturn ans;\n\t\t}\n\n\t\tpublic t peek() {\n\t\t\twhile (queue.size() > 1) {\n\t\t\t\thelp.offer(queue.poll());\n\t\t\t}\n\t\t\tt ans = queue.poll();\n\t\t\thelp.offer(ans); // 由于是查看栈顶 所以获取到值后 重新添加到另外队列中\n\t\t\tqueue<t> tep = queue;\n\t\t\tqueue = help;\n\t\t\thelp = tep;\n\t\t\treturn ans;\n\t\t}\n\n\t\tpublic boolean isempty() {\n\t\t\treturn queue.isempty();\n\t\t}\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"分类",frontmatter:{categoriesPage:!0,title:"分类",permalink:"/categories/",article:!1},regularPath:"/@pages/categoriesPage.html",relativePath:"@pages/categoriesPage.md",key:"v-644b5bf5",path:"/categories/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"标签",frontmatter:{tagsPage:!0,title:"标签",permalink:"/tags/",article:!1},regularPath:"/@pages/tagsPage.html",relativePath:"@pages/tagsPage.md",key:"v-6e67de15",path:"/tags/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"堆(优先级队列)",frontmatter:{title:"堆(优先级队列)",date:"2022-03-18T16:03:56.000Z",permalink:"/pages/88e08c/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/16.%E5%A0%86(%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97).html",relativePath:"408/01.数据结构/16.堆(优先级队列).md",key:"v-055ee976",path:"/pages/88e08c/",headers:[{level:2,title:"优先级队列(PriorityQueue)与比较器",slug:"优先级队列-priorityqueue-与比较器",normalizedTitle:"优先级队列(priorityqueue)与比较器",charIndex:199},{level:2,title:"数组实现堆",slug:"数组实现堆",normalizedTitle:"数组实现堆",charIndex:1350},{level:2,title:"堆排序",slug:"堆排序",normalizedTitle:"堆排序",charIndex:1360},{level:2,title:"23. 合并K个升序链表",slug:"_23-合并k个升序链表",normalizedTitle:"23. 合并k个升序链表",charIndex:1368}],headersStr:"优先级队列(PriorityQueue)与比较器 数组实现堆 堆排序 23. 合并K个升序链表",content:'# 堆(优先级队列)\n\n堆(Heap)是计算机科学中一类特殊的数据结构的统称。堆通常是一个可以被看做一棵完全二叉树的数组对象。\n\n完全二叉树：叶子结点只能出现在最下层和次下层，且最下层的叶子结点集中在树的左部。需要注意的是，满二叉树肯定是完全二叉树，而完全二叉树不一定是满二叉树。\n\n每一棵子树的根结点最大的堆(即根节点大于等于子节点)叫做最大堆或大根堆，根结点最小的堆叫做最小堆或小根堆\n\n\n# 优先级队列(PriorityQueue)与比较器\n\n\n\tpublic static class MyComparator implements Comparator<Integer> {\n\n\t\t// 负数,第一个参数在前\n\t\t// 正数,第二个参数在前\n\t\t// 0,不作交换\n\t\t@Override\n\t\tpublic int compare(Integer o1, Integer o2) {\n\t\t\tif (o1 > o2) {\n\t\t\t\treturn -1;\n\t\t\t} else if (o2 > o1) {\n\t\t\t\treturn 1;\n\t\t\t} else {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) {\n\t\tPriorityQueue<Integer> heap = new PriorityQueue<>(); // 默认小根堆的优先队列\n\t\theap.add(5);\n\t\theap.add(3);\n\t\theap.add(8);\n\t\theap.add(1);\n\t\tSystem.out.println(heap.peek()); // 查看队头\n\t\tfor (Integer integer : heap) {\n\t\t\tSystem.out.print(integer + " ");\n\t\t}\n\t\tSystem.out.println();\n\n//\t\tPriorityQueue<Integer> heap2 = new PriorityQueue<>(new MyComparator()); // 大根堆的优先队列 构造时传入比较器\n\t\tPriorityQueue<Integer> heap2 = new PriorityQueue<>((o1,o2)-> o2-o1); // 大根堆的优先队列 构造时传入比较器\n\t\theap2.add(5);\n\t\theap2.add(3);\n\t\theap2.add(8);\n\t\theap2.add(1);\n\t\tSystem.out.println(heap2.peek()); // 查看队头\n\t\tfor (Integer integer : heap2) {\n\t\t\tSystem.out.print(integer + " ");\n\t\t}\n\t\tSystem.out.println();\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# 数组实现堆\n\n\n# 堆排序\n\n\n# 23. 合并K个升序链表\n\npublic class ListNode {\n\t\tint val;\n\t\tListNode next;\n\n\t\tListNode() {\n\t\t}\n\n\t\tListNode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tListNode(int val, ListNode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tpublic ListNode mergeKLists(ListNode[] lists) {\n\t\tPriorityQueue<ListNode> heap = new PriorityQueue<ListNode>((o1, o2) -> o1.val - o2.val); //自定义比较器的最小堆\n\t\tfor (ListNode listNode : lists) { //遍历链表数组\n\t\t\tif (listNode != null) { \n\t\t\t\theap.add(listNode); //非空则push到最小堆\n\t\t\t}\n\t\t}\n\t\tif (heap.isEmpty()) { //堆中没有\n\t\t\treturn null;\n\t\t}\n\t\tListNode head = heap.poll(); // 弹出最小堆头\n\t\tListNode pre = head;\n\t\tif (pre.next != null) {\n\t\t\theap.add(pre.next); // 如果pre下一跳 不为空 直接push到最小堆中\n\t\t}\n\t\twhile (!heap.isEmpty()) { //最小堆不为空\n\t\t\tListNode cur = heap.poll(); //弹出栈顶 即最小值\n\t\t\tpre.next = cur; //下一跳 连接\n\t\t\tpre = cur; //更新当前节点\n\t\t\tif (cur.next != null) { //当前节点 下一跳不为空则添加到最小堆中\n\t\t\t\theap.add(cur.next);\n\t\t\t}\n\t\t}\n\t\treturn head;\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n',normalizedContent:'# 堆(优先级队列)\n\n堆(heap)是计算机科学中一类特殊的数据结构的统称。堆通常是一个可以被看做一棵完全二叉树的数组对象。\n\n完全二叉树：叶子结点只能出现在最下层和次下层，且最下层的叶子结点集中在树的左部。需要注意的是，满二叉树肯定是完全二叉树，而完全二叉树不一定是满二叉树。\n\n每一棵子树的根结点最大的堆(即根节点大于等于子节点)叫做最大堆或大根堆，根结点最小的堆叫做最小堆或小根堆\n\n\n# 优先级队列(priorityqueue)与比较器\n\n\n\tpublic static class mycomparator implements comparator<integer> {\n\n\t\t// 负数,第一个参数在前\n\t\t// 正数,第二个参数在前\n\t\t// 0,不作交换\n\t\t@override\n\t\tpublic int compare(integer o1, integer o2) {\n\t\t\tif (o1 > o2) {\n\t\t\t\treturn -1;\n\t\t\t} else if (o2 > o1) {\n\t\t\t\treturn 1;\n\t\t\t} else {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t}\n\n\tpublic static void main(string[] args) {\n\t\tpriorityqueue<integer> heap = new priorityqueue<>(); // 默认小根堆的优先队列\n\t\theap.add(5);\n\t\theap.add(3);\n\t\theap.add(8);\n\t\theap.add(1);\n\t\tsystem.out.println(heap.peek()); // 查看队头\n\t\tfor (integer integer : heap) {\n\t\t\tsystem.out.print(integer + " ");\n\t\t}\n\t\tsystem.out.println();\n\n//\t\tpriorityqueue<integer> heap2 = new priorityqueue<>(new mycomparator()); // 大根堆的优先队列 构造时传入比较器\n\t\tpriorityqueue<integer> heap2 = new priorityqueue<>((o1,o2)-> o2-o1); // 大根堆的优先队列 构造时传入比较器\n\t\theap2.add(5);\n\t\theap2.add(3);\n\t\theap2.add(8);\n\t\theap2.add(1);\n\t\tsystem.out.println(heap2.peek()); // 查看队头\n\t\tfor (integer integer : heap2) {\n\t\t\tsystem.out.print(integer + " ");\n\t\t}\n\t\tsystem.out.println();\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# 数组实现堆\n\n\n# 堆排序\n\n\n# 23. 合并k个升序链表\n\npublic class listnode {\n\t\tint val;\n\t\tlistnode next;\n\n\t\tlistnode() {\n\t\t}\n\n\t\tlistnode(int val) {\n\t\t\tthis.val = val;\n\t\t}\n\n\t\tlistnode(int val, listnode next) {\n\t\t\tthis.val = val;\n\t\t\tthis.next = next;\n\t\t}\n\t}\n\n\tpublic listnode mergeklists(listnode[] lists) {\n\t\tpriorityqueue<listnode> heap = new priorityqueue<listnode>((o1, o2) -> o1.val - o2.val); //自定义比较器的最小堆\n\t\tfor (listnode listnode : lists) { //遍历链表数组\n\t\t\tif (listnode != null) { \n\t\t\t\theap.add(listnode); //非空则push到最小堆\n\t\t\t}\n\t\t}\n\t\tif (heap.isempty()) { //堆中没有\n\t\t\treturn null;\n\t\t}\n\t\tlistnode head = heap.poll(); // 弹出最小堆头\n\t\tlistnode pre = head;\n\t\tif (pre.next != null) {\n\t\t\theap.add(pre.next); // 如果pre下一跳 不为空 直接push到最小堆中\n\t\t}\n\t\twhile (!heap.isempty()) { //最小堆不为空\n\t\t\tlistnode cur = heap.poll(); //弹出栈顶 即最小值\n\t\t\tpre.next = cur; //下一跳 连接\n\t\t\tpre = cur; //更新当前节点\n\t\t\tif (cur.next != null) { //当前节点 下一跳不为空则添加到最小堆中\n\t\t\t\theap.add(cur.next);\n\t\t\t}\n\t\t}\n\t\treturn head;\n\n\t}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"递归",frontmatter:{title:"递归",date:"2022-03-18T16:03:56.000Z",permalink:"/pages/fe2eaf/",categories:[408,"数据结构"],tags:[null]},regularPath:"/408/01.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/15.%E9%80%92%E5%BD%92.html",relativePath:"408/01.数据结构/15.递归.md",key:"v-6ddb2230",path:"/pages/fe2eaf/",headers:[{level:2,title:"Master公式",slug:"master公式",normalizedTitle:"master公式",charIndex:9}],headersStr:"Master公式",content:"# 递归\n\n\n# Master公式\n\n适用范围为子过程规模相等的情况，否则不适用。\n\nT(N) = a*T(N/b) + O(N^d)\n\n 1. log(b,a) > d ->复杂度为O（N^log(b,a)）\n 2. log(b,a) < d ->复杂度为O（N^d）\n 3. log(b,a) = d ->复杂度为O（N^d*logN）",normalizedContent:"# 递归\n\n\n# master公式\n\n适用范围为子过程规模相等的情况，否则不适用。\n\nt(n) = a*t(n/b) + o(n^d)\n\n 1. log(b,a) > d ->复杂度为o（n^log(b,a)）\n 2. log(b,a) < d ->复杂度为o（n^d）\n 3. log(b,a) = d ->复杂度为o（n^d*logn）",charsets:{cjk:!0},lastUpdated:"2022/03/18, 16:09:17",lastUpdatedTimestamp:1647590957e3},{title:"Home",frontmatter:{home:!0,heroText:"Chiriri's blog",tagline:"Iekr个人技术博客，卷使劲卷。",pageClass:"vdoing-index-class"},regularPath:"/",relativePath:"index.md",key:"v-d520fc7e",path:"/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/03/18, 15:32:54",lastUpdatedTimestamp:1647588774e3},{title:"我做了一个手写春联小网页，祝大家虎年暴富",frontmatter:{title:"我做了一个手写春联小网页，祝大家虎年暴富",date:"2022-01-28T14:59:51.000Z",permalink:"/pages/829589/",titleTag:"原创",sidebar:"auto",categories:["随笔"],tags:[null]},regularPath:"/_posts/%E9%9A%8F%E7%AC%94/%E6%88%91%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E6%89%8B%E5%86%99%E6%98%A5%E8%81%94%E5%B0%8F%E7%BD%91%E9%A1%B5%EF%BC%8C%E7%A5%9D%E5%A4%A7%E5%AE%B6%E8%99%8E%E5%B9%B4%E6%9A%B4%E5%AF%8C.html",relativePath:"_posts/随笔/我做了一个手写春联小网页，祝大家虎年暴富.md",key:"v-0c3da0ac",path:"/pages/829589/",headers:[{level:3,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:33},{level:3,title:"产品构思",slug:"产品构思",normalizedTitle:"产品构思",charIndex:195},{level:3,title:"设计",slug:"设计",normalizedTitle:"设计",charIndex:478},{level:3,title:"开发",slug:"开发",normalizedTitle:"开发",charIndex:487}],excerpt:'<p>手写春联：<a href="https://cl.xugaoyi.com/" target="_blank" rel="noopener noreferrer">https://cl.xugaoyi.com/<OutboundLink/></a></p>\n<h3 id="前言"><a class="header-anchor" href="#前言">#</a> 前言</h3>\n<p>虎年春节快到了，首先祝大家新年快乐，轻松暴富。\n最近在网上经常看到生成春联的文章，不过这些小demo要么功能简陋,要么UI特别‘程序员’，满足不了我挑剔的眼光。干脆我自己做一个吧，顺便简单体验一下vite+vue3。（因为页面相对简单，vue组件风格还是使用选项式api，重点还是想把产品快速做出来。）</p>\n',headersStr:"前言 产品构思 设计 开发",content:'手写春联：https://cl.xugaoyi.com/\n\n\n# 前言\n\n虎年春节快到了，首先祝大家新年快乐，轻松暴富。 最近在网上经常看到生成春联的文章，不过这些小demo要么功能简陋,要么UI特别‘程序员’，满足不了我挑剔的眼光。干脆我自己做一个吧，顺便简单体验一下vite+vue3。（因为页面相对简单，vue组件风格还是使用选项式api，重点还是想把产品快速做出来。）\n\n\n\n\n# 产品构思\n\n包含手写春节和生成春联两大功能：\n\n * 手写春联\n   \n   * 模拟用笔写字的字迹\n   * 选择画笔颜色\n   * 调整画笔大小\n   * 清空画布\n   * 撤回笔画\n   * 切换上、下联、横批、福字\n   * 随机切换对联提示\n   * 预览图片和下载\n   * 贴春联海报和下载\n\n * 生成模式\n   \n   * 选择画笔颜色\n   * 挑选生成的对联\n   * 输入对联\n   * 随机切换对联\n   * 贴春联海报和下载\n\n * 其他\n   \n   * 快速切换模式按钮\n   * 可控制的背景音乐\n   * 微信分享网页\n\n\n# 设计\n\n\n\n\n# 开发\n\n * 技术栈\n   * vite (打包&构建)\n   * vue3 (页面开发)\n   * vant（ui）\n   * sass (css)\n   * smooth-signature.js (带笔锋手写库)\n\n<template>\n  <div class="wrap" :class="\'mode-\' + mode" @touchstart="handleTouchstart">\n    \x3c!-- 切换模式按钮 --\x3e\n    <div class="toggle-mode-btn" @click="toggleMode">\n      {{ mode === 1 ? \'手写\' : \'生成\' }}\n      <i class="iconfont icon-qiehuan"></i>\n    </div>\n\n    \x3c!-- 工具栏 --\x3e\n    <div\n      class="actions"\n      :style="{ borderTopRightRadius: colorListVisibility ? \'0\' : \'5px\' }"\n    >\n      \x3c!-- 手写模式显示 --\x3e\n      <template v-if="mode === 1">\n        \x3c!-- 调色板 --\x3e\n        <div class="palette btn-block">\n          <div\n            class="cur-color"\n            @click="togglePalette"\n            :style="{ background: colorList[curColorIndex] }"\n          ></div>\n          <ul class="colorList" v-show="colorListVisibility">\n            <li\n              v-for="(item, index) in colorList"\n              :key="item"\n              :style="{ background: item }"\n              @click="selectColor(index)"\n            ></li>\n          </ul>\n        </div>\n\n        \x3c!-- 滑块 --\x3e\n        <div class="slider-box btn-block">\n          <van-slider\n            v-model="progress"\n            vertical\n            @change="changeProgress"\n            bar-height="28"\n            active-color="transparent"\n            :min="50"\n            :max="150"\n          >\n            <template #button>\n              <div class="custom-button"></div>\n            </template>\n          </van-slider>\n        </div>\n\n        \x3c!-- 清空 --\x3e\n        <div class="btn" @click="handleClear">\n          <i class="iconfont icon-lajitong"></i>\n        </div>\n\n        \x3c!-- 撤销 --\x3e\n        <div class="btn" @click="handleUndo">\n          <i class="iconfont icon-fanhui"></i>\n        </div>\n\n        <div class="line"></div>\n\n        \x3c!-- 切换画布的按钮 --\x3e\n        <div\n          class="btn"\n          :class="{ \'cur-active\': curCanvasIndex === index }"\n          v-for="(item, index) in canvasList"\n          :key="item.name"\n          @click="changeCanvas(index)"\n        >\n          {{ item.name }}\n        </div>\n\n        <div class="line"></div>\n\n        <div class="btn prominent" @click="handlePreview">预览</div>\n        <div class="btn prominent" @click="openPosters">贴联</div>\n      </template>\n\n      \x3c!-- 生成模式显示 --\x3e\n      <template v-else>\n        \x3c!-- 选颜色 --\x3e\n        <div\n          class="color-list-quick"\n          :class="{ active: curColorIndex === index }"\n          v-for="(item, index) in colorList"\n          :key="item"\n          :style="{ background: item }"\n          @click="selectColor(index)"\n        ></div>\n        <div class="line"></div>\n        <div class="btn" @click="showPickBox = true">挑选</div>\n        <div class="btn" @click="showInputBox = true">输入</div>\n\n        \x3c!-- 挑选对联弹窗 --\x3e\n        <van-action-sheet v-model:show="showPickBox" title="请挑选对联">\n          <ul class="duilian-list">\n            <li\n              v-for="(item, index) in duilianList"\n              :key="index"\n              @click="handlePickDuilian(item)"\n            >\n              <span>{{ item.shang }}</span\n              >， <span>{{ item.xia }}</span\n              >。\n              <span>{{ item.heng }}</span>\n            </li>\n          </ul>\n        </van-action-sheet>\n\n        \x3c!-- 输入对联弹窗 --\x3e\n        <van-action-sheet v-model:show="showInputBox" title="请输入对联">\n          <van-form @submit="handleSubmitInput">\n            <van-cell-group inset>\n              <van-field\n                v-model="shanglian"\n                name="shang"\n                label="上联"\n                placeholder="上联"\n                :rules="[\n                  {\n                    required: true,\n                    message: \'请输入7位汉字上联\',\n                    pattern: /^[\\u4e00-\\u9fa5]{7}$/\n                  }\n                ]"\n                clearable\n              />\n              <van-field\n                v-model="xialian"\n                name="xia"\n                label="下联"\n                placeholder="下联"\n                :rules="[\n                  {\n                    required: true,\n                    message: \'请输入7位汉字下联\',\n                    pattern: /^[\\u4e00-\\u9fa5]{7}$/\n                  }\n                ]"\n                clearable\n              />\n              <van-field\n                v-model="hengpi"\n                name="heng"\n                label="横批"\n                placeholder="横批"\n                :rules="[\n                  {\n                    required: true,\n                    message: \'请输入4位汉字横批\',\n                    pattern: /^[\\u4e00-\\u9fa5]{4}$/\n                  }\n                ]"\n                clearable\n              />\n            </van-cell-group>\n            <div style="margin: 16px">\n              <van-button\n                round\n                block\n                type="primary"\n                native-type="submit"\n                color="linear-gradient(to right, #ff6034, #c33825)"\n              >\n                完成\n              </van-button>\n            </div>\n          </van-form>\n        </van-action-sheet>\n      </template>\n    </div>\n\n    \x3c!-- 模式1-春联画布 --\x3e\n    <div\n      v-show="mode === 1"\n      v-for="(item, index) in canvasList"\n      :key="item.name"\n    >\n      <canvas\n        class="canvas"\n        :class="item.className"\n        v-show="curCanvasIndex === index"\n        :style="{\n          marginTop:\n            item.height < clientHeight\n              ? `${(clientHeight - item.height) / 2}px`\n              : 0,\n          marginLeft:\n            item.width < clientWidth ? `${(clientWidth - item.width) / 2}px` : 0\n        }"\n      />\n    </div>\n\n    \x3c!-- 模式2-春联画布 --\x3e\n    <div v-show="mode === 2" class="canvas-mode-2">\n      <div class="row">\n        <canvas id="canvas-top" :width="200 * scale" :height="60 * scale" />\n      </div>\n      <div class="row">\n        <canvas id="canvas-left" :width="60 * scale" :height="364 * scale" />\n        <canvas id="canvas-right" :width="60 * scale" :height="364 * scale" />\n      </div>\n    </div>\n\n    \x3c!-- 贴春联按钮 --\x3e\n    <Button class="btn-posters" @click="openPosters" />\n\n    \x3c!-- footer-当前对联提示 --\x3e\n    <footer v-if="duilian.shang">\n      <div class="refresh-btn" @click="handleRefresh(true)">\n        <i class="iconfont icon-shuaxin" :class="{ rotate: isRotate }"></i>\n      </div>\n      <dl class="duilian">\n        <dt>对联</dt>\n        <dd>\n          <div>{{ duilian.shang }}</div>\n          <div>{{ duilian.xia }}</div>\n        </dd>\n      </dl>\n      <dl>\n        <dt>横批</dt>\n        <dd>{{ duilian.heng }}</dd>\n      </dl>\n    </footer>\n\n    \x3c!-- 分享按钮 --\x3e\n    <div class="share-btn" v-if="isShowShareBtn" @click="isShowShareTip = true">\n      <i class="iconfont icon-fenxiang"></i>\n    </div>\n    \x3c!-- 微信分享提示语 --\x3e\n    <div\n      class="share-tip"\n      v-if="isShowShareTip"\n      @click="isShowShareTip = false"\n    >\n      点击右上角把这个工具分享给朋友\n      <div class="hand">👆</div>\n    </div>\n\n    \x3c!-- 保存tip --\x3e\n    <p v-if="isShowTip" class="download-tip">*长按图片保存或转发</p>\n\n    \x3c!-- 版权 --\x3e\n    <div class="copyright">公众号「有趣研究社」 ©版权所有</div>\n\n    \x3c!-- 载入图片元素，用于快速贴图使用, 注意设置crossorigin="anonymous"解决跨域 --\x3e\n    <div v-if="isReadImages">\n      <img\n        crossorigin="anonymous"\n        v-for="(item, index) in bgList"\n        :src="item"\n        :key="item"\n        class="hide-img"\n        :id="`bg-img-` + index"\n      />\n      <img\n        crossorigin="anonymous"\n        class="hide-img"\n        id="qrcode"\n        src="https://cdn.jsdelivr.net/gh/xugaoyi/image_store2@master/img/qrcode.zul0pldsuao.png"\n      />\n    </div>\n\n    \x3c!-- 背景音乐 --\x3e\n    <audio\n      src="https://cdn.jsdelivr.net/gh/xugaoyi/image_store2@master/cjxq.mp3"\n      id="bgm"\n      ref="bgm"\n      loop\n    />\n    <div\n      class="play-btn"\n      :class="{ paused: !isPlay }"\n      ref="playBtn"\n      @click="handlePlay"\n    >\n      <i class="iconfont icon-yinle"></i>\n    </div>\n  </div>\n\n  <div class="body-bg-img"></div>\n</template>\n\n<script>\nimport { ImagePreview, Notify } from \'vant\'\nimport { isWX, isMobile } from \'@/utils\'\nimport Button from \'@/components/Button.vue\'\nimport dl from \'@/assets/img/yh/dl.jpeg\'\nimport hp from \'@/assets/img/yh/hp.jpeg\'\nimport fz from \'@/assets/img/yh/fz.png\'\n\n// 对联数据\nimport duilianList from \'@/mock/duilian\'\n\nconst PROPORTION = 0.37 // 图片缩小比例\nconst INSTANTIATE_NAME = \'signature\' // 实例名称\nconst MIN_WIDTH = 3 // 画笔最小宽\nconst MAX_WIDTH = 12 // 画笔最大宽\n\n// 海报背景图大小\nconst BG_WIDTH = 750\nconst BG_HEIGHT = 1448\n\n// 贴图定位和大小\nconst POSITION = [\n  { left: 57, top: 510, width: 90, height: 546 }, // 上联\n  { left: 600, top: 510, width: 90, height: 546 }, // 下联\n  { left: 225, top: 345, width: 300, height: 90 }, // 横幅\n  { left: 460, top: 450, width: 130, height: 130 }, // 福字\n]\n\nexport default {\n  name: "Home",\n  components: {\n    Button\n  },\n  data() {\n    return {\n      duilianList,\n      mode: Number(localStorage.getItem(\'mode\')) || 1, // 1 手写，2 生成\n      curCanvasIndex: 0, // 显示哪个画布\n      progress: 100, // 画笔大小的刻度\n      clientWidth: document.documentElement.clientWidth,\n      clientHeight: document.documentElement.clientHeight,\n      canvasList: [\n        {\n          name: \'上联\',\n          className: \'canvas-a\',\n          bgImage: dl,\n          width: 600 * PROPORTION,\n          height: 3640 * PROPORTION,\n        },\n        {\n          name: \'下联\',\n          className: \'canvas-b\',\n          bgImage: dl,\n          width: 600 * PROPORTION,\n          height: 3640 * PROPORTION,\n        },\n        {\n          name: \'横批\',\n          className: \'canvas-c\',\n          bgImage: hp,\n          width: 2000 * PROPORTION,\n          height: 600 * PROPORTION,\n        },\n        {\n          name: \'福字\',\n          className: \'canvas-d\',\n          bgImage: fz,\n          width: 366,\n          height: 366,\n        }\n      ],\n      colorList: [\'#000000\', \'#ffd800\', \'#e8bd48\', \'#ddc08c\',],\n      curColorIndex: 0,\n      colorListVisibility: false, // 画布颜色选择列表可见性\n      isShowTip: false, // 是否显示底部提示语\n      duilian: {}, // 当前对联文本对象\n      isRotate: false, // 刷新icon旋转\n      bgList: [\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/1.4j8qpdnq80i0.jpeg\',\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/4.4460an8ag5o0.jpeg\',\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/5.3axtl4xpvy00.jpeg\',\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/6.2lnbphdqjaq0.jpeg\',\n      ],\n      isReadImages: false, // 延迟加载图片用\n      isShowShareBtn: false, // 是否显示分享按钮\n      isShowShareTip: false, // 是否显示分享提示语\n      isPlay: false, // 是否在播放\n\n      // 模式2\n      canvasTop: null, // 横批\n      canvasLeft: null, // 上联\n      canvasRight: null, // 下联\n      imgObj1: null, // 横批图片对象\n      imgObj2: null, // 上下联图片对象\n      scale: Math.max(window.devicePixelRatio || 1, 2), // 用于增加画布清晰度\n      showPickBox: false, // 挑选对联的弹框\n      showInputBox: false, // 输入对联的弹框\n      shanglian: \'\', // 输入的上联\n      xialian: \'\', // 输入的下联\n      hengpi: \'\', // 输入的横批\n    };\n  },\n  computed: {\n    // 模式1-当前画布实例\n    curCanvasInstantiate() {\n      return this[INSTANTIATE_NAME + this.curCanvasIndex]\n    }\n  },\n  created() {\n    // 微信浏览器显示分享按钮\n    this.isShowShareBtn = isWX()\n  },\n  mounted() {\n    if (!isMobile()) {\n      Notify({ type: \'warning\', message: \'请用移动端打开获得最佳体验\', duration: 6000, });\n    }\n\n    this.initMode1();\n\n    // 初始化对联提示\n    this.handleRefresh();\n\n    this.initMode2();\n\n    // 按钮添加激活时发光效果class\n    const btnEl = document.querySelectorAll(\'.btn,.btn-block\');\n    btnEl.forEach((item) => {\n      item.addEventListener(\'touchstart\', () => {\n        item.classList.add(\'btn-active\')\n      })\n      item.addEventListener(\'touchend\', () => {\n        setTimeout(() => {\n          item.classList.remove(\'btn-active\')\n        }, 100)\n      })\n    })\n\n    // 延迟加载贴图背景\n    setTimeout(() => {\n      this.isReadImages = true\n    }, 1000)\n  },\n\n  watch: {\n    // 切换画笔颜色\n    curColorIndex() {\n      this.curCanvasInstantiate.color = this.colorList[this.curColorIndex]\n      if (this.mode === 2) {\n        this.refreshDuilian()\n      }\n    },\n    // 切换画布时应用当前画笔颜色和大小\n    curCanvasIndex() {\n      this.curCanvasInstantiate.color = this.colorList[this.curColorIndex]\n      this.handleChangeSize()\n      window.scrollTo(0, 0)\n    }\n  },\n\n  methods: {\n    initMode1() {\n      const { colorList, curColorIndex } = this\n      this.canvasList.forEach((item, index) => {\n        const options = {\n          width: item.width,\n          height: item.height,\n          minWidth: MIN_WIDTH, // 画笔最小宽度(px)\n          maxWidth: MAX_WIDTH, // 画笔最大宽度\n          minSpeed: 1.8, // 画笔达到最小宽度所需最小速度(px/ms)，取值范围1.0-10.0\n          color: colorList[curColorIndex],\n          // 新增的配置\n          bgImage: item.bgImage,\n        };\n\n        this[INSTANTIATE_NAME + index] = new SmoothSignature(document.querySelector(\'.\' + item.className), options);\n      })\n    },\n\n    initMode2() {\n      this.canvasTop = document.getElementById(\'canvas-top\').getContext(\'2d\')\n      this.canvasLeft = document.getElementById(\'canvas-left\').getContext(\'2d\')\n      this.canvasRight = document.getElementById(\'canvas-right\').getContext(\'2d\')\n\n      // 设字体样式\n      const font = "36px xs, cursive"\n      this.canvasTop.font = font\n      this.canvasLeft.font = font\n      this.canvasRight.font = font\n\n      // 增强清晰度\n      const { scale } = this\n      this.canvasTop.scale(scale, scale);\n      this.canvasLeft.scale(scale, scale);\n      this.canvasRight.scale(scale, scale);\n\n      // 设背景图\n      this.imgObj1 = new Image()\n      this.imgObj2 = new Image()\n      this.imgObj1.src = hp\n      this.imgObj2.src = dl\n      this.imgObj1.onload = () => {\n        // 贴背景\n        this.canvasTop.drawImage(this.imgObj1, 0, 0, 200, 60)\n\n        // 字体加载完成后\n        document.fonts.ready.then(() => {\n          this.handleTopFillText()\n        });\n      }\n      this.imgObj2.onload = () => {\n        // 贴背景\n        this.canvasLeft.drawImage(this.imgObj2, 0, 0, 60, 364)\n        this.canvasRight.drawImage(this.imgObj2, 0, 0, 60, 364)\n\n        // 字体加载完成后\n        document.fonts.ready.then(() => {\n          this.handleLRFillText(this.canvasLeft, this.duilian.shang)\n          this.handleLRFillText(this.canvasRight, this.duilian.xia)\n        });\n      }\n    },\n\n    // 模式2-刷新对联\n    refreshDuilian() {\n      this.canvasTop.drawImage(this.imgObj1, 0, 0, 200, 60)\n      this.canvasLeft.drawImage(this.imgObj2, 0, 0, 60, 364)\n      this.canvasRight.drawImage(this.imgObj2, 0, 0, 60, 364)\n      this.handleTopFillText()\n      this.handleLRFillText(this.canvasLeft, this.duilian.shang)\n      this.handleLRFillText(this.canvasRight, this.duilian.xia)\n    },\n\n    // 模式2-贴横批\n    handleTopFillText() {\n      // 贴文本\n      this.canvasTop.fillStyle = this.colorList[this.curColorIndex]\n      if (this.duilian.heng) {\n        this.duilian.heng.split(\'\').forEach((item, index) => {\n          const left = 42 * (index + 1) - 22\n          this.canvasTop.fillText(item, left, 40)\n        })\n      }\n    },\n\n    // 模式2-贴上下联\n    handleLRFillText(ctx, text) {\n      ctx.fillStyle = this.colorList[this.curColorIndex]\n      if (text) {\n        text.split(\'\').forEach((item, index) => {\n          const top = 50 * (index + 1) - 8\n          ctx.fillText(item, 13, top)\n        })\n      }\n    },\n\n    // 切换模式\n    toggleMode() {\n      if (this.mode === 1) {\n        this.mode = 2\n        this.refreshDuilian()\n      } else {\n        this.mode = 1\n      }\n      localStorage.setItem(\'mode\', this.mode);\n    },\n\n    // 打开调色板\n    togglePalette() {\n      this.colorListVisibility = !this.colorListVisibility\n    },\n\n    // 关闭调色板\n    handleTouchstart(e) {\n      // 不是点击选择颜色时\n      if (e.path[1]?.classList?.value !== \'colorList\' && e.target.classList?.value !== \'cur-color\') {\n        this.colorListVisibility = false\n      }\n    },\n\n    // 选择颜色\n    selectColor(index) {\n      this.curColorIndex = index\n      this.colorListVisibility = false\n    },\n\n    // 切换画布\n    changeCanvas(index) {\n      this.curCanvasIndex = index\n    },\n\n    // 清空画布\n    handleClear() {\n      this.curCanvasInstantiate.clear();\n    },\n\n    // 撤销笔画\n    handleUndo() {\n      this.curCanvasInstantiate.undo();\n    },\n\n    // 预览\n    handlePreview() {\n      this.showTopTip();\n      this.isShowTip = true\n      const _this = this\n      ImagePreview({\n        images: this.getImageList(),\n        closeable: true,\n        startPosition: this.curCanvasIndex,\n        onClose() {\n          _this.isShowTip = false\n        },\n      });\n    },\n\n    // 打开海报预览\n    openPosters() {\n      // 创建画布\n      const canvas = document.createElement(\'canvas\');\n      canvas.width = BG_WIDTH\n      canvas.height = BG_HEIGHT\n      const ctx = canvas.getContext(\'2d\');\n      const resultImageList = [];\n\n      // 是否隐藏福字\n      const isHideFu = this[INSTANTIATE_NAME + 3].isEmpty()\n      this.bgList.forEach((item, index) => {\n        // 贴背景图\n        ctx.drawImage(document.getElementById(\'bg-img-\' + index), 0, 0, BG_WIDTH, BG_HEIGHT)\n\n        // 贴对联\n        if (this.mode === 1) {\n          this.canvasList.forEach((item, index) => {\n            if (index === 3 && isHideFu) return;\n            const dlCanvas = document.querySelector(\'.\' + item.className)\n            const { left, top, width, height } = POSITION[index]\n            ctx.drawImage(dlCanvas, left, top, width, height)\n          })\n        } else {\n          [\'canvas-left\', \'canvas-right\', \'canvas-top\'].forEach((item, index) => {\n            const dlCanvas = document.getElementById(item)\n            const { left, top, width, height } = POSITION[index]\n            ctx.drawImage(dlCanvas, left, top, width, height)\n          })\n        }\n\n        // 贴二维码\n        ctx.drawImage(document.getElementById("qrcode"), 40, 1280, 580, 136)\n\n        // 贴文本\n        ctx.font = "18px sans-serif"\n        ctx.fillStyle = "#666666"\n        ctx.fillText(\'©公众号「有趣研究社」\', 550, 1420)\n\n        // 导出图片\n        resultImageList.push(canvas.toDataURL(\'image/jpeg\', 0.8))\n      })\n\n      // 打开图片预览\n      this.isShowTip = true\n      const _this = this\n      ImagePreview({\n        images: resultImageList,\n        closeable: true,\n        onClose() {\n          _this.isShowTip = false\n        },\n      });\n      this.showTopTip();\n    },\n\n    // 弹出顶部提示语\n    showTopTip() {\n      if (!sessionStorage.getItem(\'showTip\')) {\n        sessionStorage.setItem(\'showTip\', \'true\');\n        Notify({\n          message: \'长按图片可保存到本地\',\n          color: \'#c33825\',\n          background: \'#eed3ae\',\n        });\n      }\n    },\n\n    // 获取对联图片列表\n    getImageList(type = \'image/png\') {\n      const imageList = []\n      this.canvasList.forEach((item, index) => {\n        if (index === 3) {\n          // `福`字必须是png格式\n          type = \'image/png\'\n        }\n        imageList.push(this[INSTANTIATE_NAME + index].toDataURL(type, 0.8))\n      })\n      return imageList\n    },\n\n    // 进度改变时\n    changeProgress(progress) {\n      this.progress = progress\n      this.handleChangeSize()\n    },\n\n    // 调整画笔大小\n    handleChangeSize() {\n      const { progress } = this\n      this.curCanvasInstantiate.minWidth = MIN_WIDTH * progress / 100\n      this.curCanvasInstantiate.maxWidth = MAX_WIDTH * progress / 100\n    },\n\n    // 刷新对联\n    handleRefresh(rotate) {\n      this.duilian = duilianList[Math.floor(Math.random() * duilianList.length)]\n\n      if (rotate) {\n        if (this.mode === 2) {\n          this.refreshDuilian()\n        }\n        // 使icon旋转\n        this.isRotate = true\n        setTimeout(() => {\n          this.isRotate = false\n        }, 300)\n      }\n    },\n\n    // 播放背景音乐\n    handlePlay() {\n      const { bgm } = this.$refs\n      if (bgm.paused) {\n        bgm.play()\n        this.isPlay = true\n      } else {\n        bgm.pause()\n        this.isPlay = false\n      }\n    },\n\n    // 完成输入对联\n    handleSubmitInput(values) {\n      this.duilian = values\n      this.showInputBox = false\n      this.refreshDuilian()\n    },\n\n    // 完成挑选对联\n    handlePickDuilian(item) {\n      this.duilian = item\n      this.showPickBox = false\n      this.refreshDuilian()\n    }\n  },\n};\n<\/script>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n\n\n更多有趣的小网页欢迎关注公众号有趣研究社:\n\n> 手写春联\n> FC在线模拟器\n> 爱国头像生成器\n> 到账语音生成器',normalizedContent:'手写春联：https://cl.xugaoyi.com/\n\n\n# 前言\n\n虎年春节快到了，首先祝大家新年快乐，轻松暴富。 最近在网上经常看到生成春联的文章，不过这些小demo要么功能简陋,要么ui特别‘程序员’，满足不了我挑剔的眼光。干脆我自己做一个吧，顺便简单体验一下vite+vue3。（因为页面相对简单，vue组件风格还是使用选项式api，重点还是想把产品快速做出来。）\n\n\n\n\n# 产品构思\n\n包含手写春节和生成春联两大功能：\n\n * 手写春联\n   \n   * 模拟用笔写字的字迹\n   * 选择画笔颜色\n   * 调整画笔大小\n   * 清空画布\n   * 撤回笔画\n   * 切换上、下联、横批、福字\n   * 随机切换对联提示\n   * 预览图片和下载\n   * 贴春联海报和下载\n\n * 生成模式\n   \n   * 选择画笔颜色\n   * 挑选生成的对联\n   * 输入对联\n   * 随机切换对联\n   * 贴春联海报和下载\n\n * 其他\n   \n   * 快速切换模式按钮\n   * 可控制的背景音乐\n   * 微信分享网页\n\n\n# 设计\n\n\n\n\n# 开发\n\n * 技术栈\n   * vite (打包&构建)\n   * vue3 (页面开发)\n   * vant（ui）\n   * sass (css)\n   * smooth-signature.js (带笔锋手写库)\n\n<template>\n  <div class="wrap" :class="\'mode-\' + mode" @touchstart="handletouchstart">\n    \x3c!-- 切换模式按钮 --\x3e\n    <div class="toggle-mode-btn" @click="togglemode">\n      {{ mode === 1 ? \'手写\' : \'生成\' }}\n      <i class="iconfont icon-qiehuan"></i>\n    </div>\n\n    \x3c!-- 工具栏 --\x3e\n    <div\n      class="actions"\n      :style="{ bordertoprightradius: colorlistvisibility ? \'0\' : \'5px\' }"\n    >\n      \x3c!-- 手写模式显示 --\x3e\n      <template v-if="mode === 1">\n        \x3c!-- 调色板 --\x3e\n        <div class="palette btn-block">\n          <div\n            class="cur-color"\n            @click="togglepalette"\n            :style="{ background: colorlist[curcolorindex] }"\n          ></div>\n          <ul class="colorlist" v-show="colorlistvisibility">\n            <li\n              v-for="(item, index) in colorlist"\n              :key="item"\n              :style="{ background: item }"\n              @click="selectcolor(index)"\n            ></li>\n          </ul>\n        </div>\n\n        \x3c!-- 滑块 --\x3e\n        <div class="slider-box btn-block">\n          <van-slider\n            v-model="progress"\n            vertical\n            @change="changeprogress"\n            bar-height="28"\n            active-color="transparent"\n            :min="50"\n            :max="150"\n          >\n            <template #button>\n              <div class="custom-button"></div>\n            </template>\n          </van-slider>\n        </div>\n\n        \x3c!-- 清空 --\x3e\n        <div class="btn" @click="handleclear">\n          <i class="iconfont icon-lajitong"></i>\n        </div>\n\n        \x3c!-- 撤销 --\x3e\n        <div class="btn" @click="handleundo">\n          <i class="iconfont icon-fanhui"></i>\n        </div>\n\n        <div class="line"></div>\n\n        \x3c!-- 切换画布的按钮 --\x3e\n        <div\n          class="btn"\n          :class="{ \'cur-active\': curcanvasindex === index }"\n          v-for="(item, index) in canvaslist"\n          :key="item.name"\n          @click="changecanvas(index)"\n        >\n          {{ item.name }}\n        </div>\n\n        <div class="line"></div>\n\n        <div class="btn prominent" @click="handlepreview">预览</div>\n        <div class="btn prominent" @click="openposters">贴联</div>\n      </template>\n\n      \x3c!-- 生成模式显示 --\x3e\n      <template v-else>\n        \x3c!-- 选颜色 --\x3e\n        <div\n          class="color-list-quick"\n          :class="{ active: curcolorindex === index }"\n          v-for="(item, index) in colorlist"\n          :key="item"\n          :style="{ background: item }"\n          @click="selectcolor(index)"\n        ></div>\n        <div class="line"></div>\n        <div class="btn" @click="showpickbox = true">挑选</div>\n        <div class="btn" @click="showinputbox = true">输入</div>\n\n        \x3c!-- 挑选对联弹窗 --\x3e\n        <van-action-sheet v-model:show="showpickbox" title="请挑选对联">\n          <ul class="duilian-list">\n            <li\n              v-for="(item, index) in duilianlist"\n              :key="index"\n              @click="handlepickduilian(item)"\n            >\n              <span>{{ item.shang }}</span\n              >， <span>{{ item.xia }}</span\n              >。\n              <span>{{ item.heng }}</span>\n            </li>\n          </ul>\n        </van-action-sheet>\n\n        \x3c!-- 输入对联弹窗 --\x3e\n        <van-action-sheet v-model:show="showinputbox" title="请输入对联">\n          <van-form @submit="handlesubmitinput">\n            <van-cell-group inset>\n              <van-field\n                v-model="shanglian"\n                name="shang"\n                label="上联"\n                placeholder="上联"\n                :rules="[\n                  {\n                    required: true,\n                    message: \'请输入7位汉字上联\',\n                    pattern: /^[\\u4e00-\\u9fa5]{7}$/\n                  }\n                ]"\n                clearable\n              />\n              <van-field\n                v-model="xialian"\n                name="xia"\n                label="下联"\n                placeholder="下联"\n                :rules="[\n                  {\n                    required: true,\n                    message: \'请输入7位汉字下联\',\n                    pattern: /^[\\u4e00-\\u9fa5]{7}$/\n                  }\n                ]"\n                clearable\n              />\n              <van-field\n                v-model="hengpi"\n                name="heng"\n                label="横批"\n                placeholder="横批"\n                :rules="[\n                  {\n                    required: true,\n                    message: \'请输入4位汉字横批\',\n                    pattern: /^[\\u4e00-\\u9fa5]{4}$/\n                  }\n                ]"\n                clearable\n              />\n            </van-cell-group>\n            <div style="margin: 16px">\n              <van-button\n                round\n                block\n                type="primary"\n                native-type="submit"\n                color="linear-gradient(to right, #ff6034, #c33825)"\n              >\n                完成\n              </van-button>\n            </div>\n          </van-form>\n        </van-action-sheet>\n      </template>\n    </div>\n\n    \x3c!-- 模式1-春联画布 --\x3e\n    <div\n      v-show="mode === 1"\n      v-for="(item, index) in canvaslist"\n      :key="item.name"\n    >\n      <canvas\n        class="canvas"\n        :class="item.classname"\n        v-show="curcanvasindex === index"\n        :style="{\n          margintop:\n            item.height < clientheight\n              ? `${(clientheight - item.height) / 2}px`\n              : 0,\n          marginleft:\n            item.width < clientwidth ? `${(clientwidth - item.width) / 2}px` : 0\n        }"\n      />\n    </div>\n\n    \x3c!-- 模式2-春联画布 --\x3e\n    <div v-show="mode === 2" class="canvas-mode-2">\n      <div class="row">\n        <canvas id="canvas-top" :width="200 * scale" :height="60 * scale" />\n      </div>\n      <div class="row">\n        <canvas id="canvas-left" :width="60 * scale" :height="364 * scale" />\n        <canvas id="canvas-right" :width="60 * scale" :height="364 * scale" />\n      </div>\n    </div>\n\n    \x3c!-- 贴春联按钮 --\x3e\n    <button class="btn-posters" @click="openposters" />\n\n    \x3c!-- footer-当前对联提示 --\x3e\n    <footer v-if="duilian.shang">\n      <div class="refresh-btn" @click="handlerefresh(true)">\n        <i class="iconfont icon-shuaxin" :class="{ rotate: isrotate }"></i>\n      </div>\n      <dl class="duilian">\n        <dt>对联</dt>\n        <dd>\n          <div>{{ duilian.shang }}</div>\n          <div>{{ duilian.xia }}</div>\n        </dd>\n      </dl>\n      <dl>\n        <dt>横批</dt>\n        <dd>{{ duilian.heng }}</dd>\n      </dl>\n    </footer>\n\n    \x3c!-- 分享按钮 --\x3e\n    <div class="share-btn" v-if="isshowsharebtn" @click="isshowsharetip = true">\n      <i class="iconfont icon-fenxiang"></i>\n    </div>\n    \x3c!-- 微信分享提示语 --\x3e\n    <div\n      class="share-tip"\n      v-if="isshowsharetip"\n      @click="isshowsharetip = false"\n    >\n      点击右上角把这个工具分享给朋友\n      <div class="hand">👆</div>\n    </div>\n\n    \x3c!-- 保存tip --\x3e\n    <p v-if="isshowtip" class="download-tip">*长按图片保存或转发</p>\n\n    \x3c!-- 版权 --\x3e\n    <div class="copyright">公众号「有趣研究社」 ©版权所有</div>\n\n    \x3c!-- 载入图片元素，用于快速贴图使用, 注意设置crossorigin="anonymous"解决跨域 --\x3e\n    <div v-if="isreadimages">\n      <img\n        crossorigin="anonymous"\n        v-for="(item, index) in bglist"\n        :src="item"\n        :key="item"\n        class="hide-img"\n        :id="`bg-img-` + index"\n      />\n      <img\n        crossorigin="anonymous"\n        class="hide-img"\n        id="qrcode"\n        src="https://cdn.jsdelivr.net/gh/xugaoyi/image_store2@master/img/qrcode.zul0pldsuao.png"\n      />\n    </div>\n\n    \x3c!-- 背景音乐 --\x3e\n    <audio\n      src="https://cdn.jsdelivr.net/gh/xugaoyi/image_store2@master/cjxq.mp3"\n      id="bgm"\n      ref="bgm"\n      loop\n    />\n    <div\n      class="play-btn"\n      :class="{ paused: !isplay }"\n      ref="playbtn"\n      @click="handleplay"\n    >\n      <i class="iconfont icon-yinle"></i>\n    </div>\n  </div>\n\n  <div class="body-bg-img"></div>\n</template>\n\n<script>\nimport { imagepreview, notify } from \'vant\'\nimport { iswx, ismobile } from \'@/utils\'\nimport button from \'@/components/button.vue\'\nimport dl from \'@/assets/img/yh/dl.jpeg\'\nimport hp from \'@/assets/img/yh/hp.jpeg\'\nimport fz from \'@/assets/img/yh/fz.png\'\n\n// 对联数据\nimport duilianlist from \'@/mock/duilian\'\n\nconst proportion = 0.37 // 图片缩小比例\nconst instantiate_name = \'signature\' // 实例名称\nconst min_width = 3 // 画笔最小宽\nconst max_width = 12 // 画笔最大宽\n\n// 海报背景图大小\nconst bg_width = 750\nconst bg_height = 1448\n\n// 贴图定位和大小\nconst position = [\n  { left: 57, top: 510, width: 90, height: 546 }, // 上联\n  { left: 600, top: 510, width: 90, height: 546 }, // 下联\n  { left: 225, top: 345, width: 300, height: 90 }, // 横幅\n  { left: 460, top: 450, width: 130, height: 130 }, // 福字\n]\n\nexport default {\n  name: "home",\n  components: {\n    button\n  },\n  data() {\n    return {\n      duilianlist,\n      mode: number(localstorage.getitem(\'mode\')) || 1, // 1 手写，2 生成\n      curcanvasindex: 0, // 显示哪个画布\n      progress: 100, // 画笔大小的刻度\n      clientwidth: document.documentelement.clientwidth,\n      clientheight: document.documentelement.clientheight,\n      canvaslist: [\n        {\n          name: \'上联\',\n          classname: \'canvas-a\',\n          bgimage: dl,\n          width: 600 * proportion,\n          height: 3640 * proportion,\n        },\n        {\n          name: \'下联\',\n          classname: \'canvas-b\',\n          bgimage: dl,\n          width: 600 * proportion,\n          height: 3640 * proportion,\n        },\n        {\n          name: \'横批\',\n          classname: \'canvas-c\',\n          bgimage: hp,\n          width: 2000 * proportion,\n          height: 600 * proportion,\n        },\n        {\n          name: \'福字\',\n          classname: \'canvas-d\',\n          bgimage: fz,\n          width: 366,\n          height: 366,\n        }\n      ],\n      colorlist: [\'#000000\', \'#ffd800\', \'#e8bd48\', \'#ddc08c\',],\n      curcolorindex: 0,\n      colorlistvisibility: false, // 画布颜色选择列表可见性\n      isshowtip: false, // 是否显示底部提示语\n      duilian: {}, // 当前对联文本对象\n      isrotate: false, // 刷新icon旋转\n      bglist: [\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/1.4j8qpdnq80i0.jpeg\',\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/4.4460an8ag5o0.jpeg\',\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/5.3axtl4xpvy00.jpeg\',\n        \'https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/6.2lnbphdqjaq0.jpeg\',\n      ],\n      isreadimages: false, // 延迟加载图片用\n      isshowsharebtn: false, // 是否显示分享按钮\n      isshowsharetip: false, // 是否显示分享提示语\n      isplay: false, // 是否在播放\n\n      // 模式2\n      canvastop: null, // 横批\n      canvasleft: null, // 上联\n      canvasright: null, // 下联\n      imgobj1: null, // 横批图片对象\n      imgobj2: null, // 上下联图片对象\n      scale: math.max(window.devicepixelratio || 1, 2), // 用于增加画布清晰度\n      showpickbox: false, // 挑选对联的弹框\n      showinputbox: false, // 输入对联的弹框\n      shanglian: \'\', // 输入的上联\n      xialian: \'\', // 输入的下联\n      hengpi: \'\', // 输入的横批\n    };\n  },\n  computed: {\n    // 模式1-当前画布实例\n    curcanvasinstantiate() {\n      return this[instantiate_name + this.curcanvasindex]\n    }\n  },\n  created() {\n    // 微信浏览器显示分享按钮\n    this.isshowsharebtn = iswx()\n  },\n  mounted() {\n    if (!ismobile()) {\n      notify({ type: \'warning\', message: \'请用移动端打开获得最佳体验\', duration: 6000, });\n    }\n\n    this.initmode1();\n\n    // 初始化对联提示\n    this.handlerefresh();\n\n    this.initmode2();\n\n    // 按钮添加激活时发光效果class\n    const btnel = document.queryselectorall(\'.btn,.btn-block\');\n    btnel.foreach((item) => {\n      item.addeventlistener(\'touchstart\', () => {\n        item.classlist.add(\'btn-active\')\n      })\n      item.addeventlistener(\'touchend\', () => {\n        settimeout(() => {\n          item.classlist.remove(\'btn-active\')\n        }, 100)\n      })\n    })\n\n    // 延迟加载贴图背景\n    settimeout(() => {\n      this.isreadimages = true\n    }, 1000)\n  },\n\n  watch: {\n    // 切换画笔颜色\n    curcolorindex() {\n      this.curcanvasinstantiate.color = this.colorlist[this.curcolorindex]\n      if (this.mode === 2) {\n        this.refreshduilian()\n      }\n    },\n    // 切换画布时应用当前画笔颜色和大小\n    curcanvasindex() {\n      this.curcanvasinstantiate.color = this.colorlist[this.curcolorindex]\n      this.handlechangesize()\n      window.scrollto(0, 0)\n    }\n  },\n\n  methods: {\n    initmode1() {\n      const { colorlist, curcolorindex } = this\n      this.canvaslist.foreach((item, index) => {\n        const options = {\n          width: item.width,\n          height: item.height,\n          minwidth: min_width, // 画笔最小宽度(px)\n          maxwidth: max_width, // 画笔最大宽度\n          minspeed: 1.8, // 画笔达到最小宽度所需最小速度(px/ms)，取值范围1.0-10.0\n          color: colorlist[curcolorindex],\n          // 新增的配置\n          bgimage: item.bgimage,\n        };\n\n        this[instantiate_name + index] = new smoothsignature(document.queryselector(\'.\' + item.classname), options);\n      })\n    },\n\n    initmode2() {\n      this.canvastop = document.getelementbyid(\'canvas-top\').getcontext(\'2d\')\n      this.canvasleft = document.getelementbyid(\'canvas-left\').getcontext(\'2d\')\n      this.canvasright = document.getelementbyid(\'canvas-right\').getcontext(\'2d\')\n\n      // 设字体样式\n      const font = "36px xs, cursive"\n      this.canvastop.font = font\n      this.canvasleft.font = font\n      this.canvasright.font = font\n\n      // 增强清晰度\n      const { scale } = this\n      this.canvastop.scale(scale, scale);\n      this.canvasleft.scale(scale, scale);\n      this.canvasright.scale(scale, scale);\n\n      // 设背景图\n      this.imgobj1 = new image()\n      this.imgobj2 = new image()\n      this.imgobj1.src = hp\n      this.imgobj2.src = dl\n      this.imgobj1.onload = () => {\n        // 贴背景\n        this.canvastop.drawimage(this.imgobj1, 0, 0, 200, 60)\n\n        // 字体加载完成后\n        document.fonts.ready.then(() => {\n          this.handletopfilltext()\n        });\n      }\n      this.imgobj2.onload = () => {\n        // 贴背景\n        this.canvasleft.drawimage(this.imgobj2, 0, 0, 60, 364)\n        this.canvasright.drawimage(this.imgobj2, 0, 0, 60, 364)\n\n        // 字体加载完成后\n        document.fonts.ready.then(() => {\n          this.handlelrfilltext(this.canvasleft, this.duilian.shang)\n          this.handlelrfilltext(this.canvasright, this.duilian.xia)\n        });\n      }\n    },\n\n    // 模式2-刷新对联\n    refreshduilian() {\n      this.canvastop.drawimage(this.imgobj1, 0, 0, 200, 60)\n      this.canvasleft.drawimage(this.imgobj2, 0, 0, 60, 364)\n      this.canvasright.drawimage(this.imgobj2, 0, 0, 60, 364)\n      this.handletopfilltext()\n      this.handlelrfilltext(this.canvasleft, this.duilian.shang)\n      this.handlelrfilltext(this.canvasright, this.duilian.xia)\n    },\n\n    // 模式2-贴横批\n    handletopfilltext() {\n      // 贴文本\n      this.canvastop.fillstyle = this.colorlist[this.curcolorindex]\n      if (this.duilian.heng) {\n        this.duilian.heng.split(\'\').foreach((item, index) => {\n          const left = 42 * (index + 1) - 22\n          this.canvastop.filltext(item, left, 40)\n        })\n      }\n    },\n\n    // 模式2-贴上下联\n    handlelrfilltext(ctx, text) {\n      ctx.fillstyle = this.colorlist[this.curcolorindex]\n      if (text) {\n        text.split(\'\').foreach((item, index) => {\n          const top = 50 * (index + 1) - 8\n          ctx.filltext(item, 13, top)\n        })\n      }\n    },\n\n    // 切换模式\n    togglemode() {\n      if (this.mode === 1) {\n        this.mode = 2\n        this.refreshduilian()\n      } else {\n        this.mode = 1\n      }\n      localstorage.setitem(\'mode\', this.mode);\n    },\n\n    // 打开调色板\n    togglepalette() {\n      this.colorlistvisibility = !this.colorlistvisibility\n    },\n\n    // 关闭调色板\n    handletouchstart(e) {\n      // 不是点击选择颜色时\n      if (e.path[1]?.classlist?.value !== \'colorlist\' && e.target.classlist?.value !== \'cur-color\') {\n        this.colorlistvisibility = false\n      }\n    },\n\n    // 选择颜色\n    selectcolor(index) {\n      this.curcolorindex = index\n      this.colorlistvisibility = false\n    },\n\n    // 切换画布\n    changecanvas(index) {\n      this.curcanvasindex = index\n    },\n\n    // 清空画布\n    handleclear() {\n      this.curcanvasinstantiate.clear();\n    },\n\n    // 撤销笔画\n    handleundo() {\n      this.curcanvasinstantiate.undo();\n    },\n\n    // 预览\n    handlepreview() {\n      this.showtoptip();\n      this.isshowtip = true\n      const _this = this\n      imagepreview({\n        images: this.getimagelist(),\n        closeable: true,\n        startposition: this.curcanvasindex,\n        onclose() {\n          _this.isshowtip = false\n        },\n      });\n    },\n\n    // 打开海报预览\n    openposters() {\n      // 创建画布\n      const canvas = document.createelement(\'canvas\');\n      canvas.width = bg_width\n      canvas.height = bg_height\n      const ctx = canvas.getcontext(\'2d\');\n      const resultimagelist = [];\n\n      // 是否隐藏福字\n      const ishidefu = this[instantiate_name + 3].isempty()\n      this.bglist.foreach((item, index) => {\n        // 贴背景图\n        ctx.drawimage(document.getelementbyid(\'bg-img-\' + index), 0, 0, bg_width, bg_height)\n\n        // 贴对联\n        if (this.mode === 1) {\n          this.canvaslist.foreach((item, index) => {\n            if (index === 3 && ishidefu) return;\n            const dlcanvas = document.queryselector(\'.\' + item.classname)\n            const { left, top, width, height } = position[index]\n            ctx.drawimage(dlcanvas, left, top, width, height)\n          })\n        } else {\n          [\'canvas-left\', \'canvas-right\', \'canvas-top\'].foreach((item, index) => {\n            const dlcanvas = document.getelementbyid(item)\n            const { left, top, width, height } = position[index]\n            ctx.drawimage(dlcanvas, left, top, width, height)\n          })\n        }\n\n        // 贴二维码\n        ctx.drawimage(document.getelementbyid("qrcode"), 40, 1280, 580, 136)\n\n        // 贴文本\n        ctx.font = "18px sans-serif"\n        ctx.fillstyle = "#666666"\n        ctx.filltext(\'©公众号「有趣研究社」\', 550, 1420)\n\n        // 导出图片\n        resultimagelist.push(canvas.todataurl(\'image/jpeg\', 0.8))\n      })\n\n      // 打开图片预览\n      this.isshowtip = true\n      const _this = this\n      imagepreview({\n        images: resultimagelist,\n        closeable: true,\n        onclose() {\n          _this.isshowtip = false\n        },\n      });\n      this.showtoptip();\n    },\n\n    // 弹出顶部提示语\n    showtoptip() {\n      if (!sessionstorage.getitem(\'showtip\')) {\n        sessionstorage.setitem(\'showtip\', \'true\');\n        notify({\n          message: \'长按图片可保存到本地\',\n          color: \'#c33825\',\n          background: \'#eed3ae\',\n        });\n      }\n    },\n\n    // 获取对联图片列表\n    getimagelist(type = \'image/png\') {\n      const imagelist = []\n      this.canvaslist.foreach((item, index) => {\n        if (index === 3) {\n          // `福`字必须是png格式\n          type = \'image/png\'\n        }\n        imagelist.push(this[instantiate_name + index].todataurl(type, 0.8))\n      })\n      return imagelist\n    },\n\n    // 进度改变时\n    changeprogress(progress) {\n      this.progress = progress\n      this.handlechangesize()\n    },\n\n    // 调整画笔大小\n    handlechangesize() {\n      const { progress } = this\n      this.curcanvasinstantiate.minwidth = min_width * progress / 100\n      this.curcanvasinstantiate.maxwidth = max_width * progress / 100\n    },\n\n    // 刷新对联\n    handlerefresh(rotate) {\n      this.duilian = duilianlist[math.floor(math.random() * duilianlist.length)]\n\n      if (rotate) {\n        if (this.mode === 2) {\n          this.refreshduilian()\n        }\n        // 使icon旋转\n        this.isrotate = true\n        settimeout(() => {\n          this.isrotate = false\n        }, 300)\n      }\n    },\n\n    // 播放背景音乐\n    handleplay() {\n      const { bgm } = this.$refs\n      if (bgm.paused) {\n        bgm.play()\n        this.isplay = true\n      } else {\n        bgm.pause()\n        this.isplay = false\n      }\n    },\n\n    // 完成输入对联\n    handlesubmitinput(values) {\n      this.duilian = values\n      this.showinputbox = false\n      this.refreshduilian()\n    },\n\n    // 完成挑选对联\n    handlepickduilian(item) {\n      this.duilian = item\n      this.showpickbox = false\n      this.refreshduilian()\n    }\n  },\n};\n<\/script>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n\n\n更多有趣的小网页欢迎关注公众号有趣研究社:\n\n> 手写春联\n> fc在线模拟器\n> 爱国头像生成器\n> 到账语音生成器',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"BS架构",frontmatter:{title:"BS架构",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/23186a/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/01.BS%E6%9E%B6%E6%9E%84.html",relativePath:"前端/01.html/01.BS架构.md",key:"v-60cd2f2a",path:"/pages/23186a/",headersStr:null,content:"# BS架构\n\n 1. 资源分类\n    \n    1. 静态资源\n       \n       特点：\n       \n       所有用户访问，得到的结果是一样的（固定的）。\n       \n       如：文本，图片，音频，视频，HTML，CSS，JavaScript\n    \n    2. 动态资源\n       \n       特点：\n       \n       所有用户访问，得到的结果可能不一样。\n       \n       如jsp/servlet，php，asp\n       \n       如果用户请求的是动态资源，服务器会执行动态资源转换为静态资源发送给用户浏览器\n    \n    静态资源：\n    \n    Html：用于搭建基础网页，展示页面的内容\n    \n    CSS:用于美化页面，布局页面\n    \n    JavaScript：控制页面的元素，让页面有一些动态的效果",normalizedContent:"# bs架构\n\n 1. 资源分类\n    \n    1. 静态资源\n       \n       特点：\n       \n       所有用户访问，得到的结果是一样的（固定的）。\n       \n       如：文本，图片，音频，视频，html，css，javascript\n    \n    2. 动态资源\n       \n       特点：\n       \n       所有用户访问，得到的结果可能不一样。\n       \n       如jsp/servlet，php，asp\n       \n       如果用户请求的是动态资源，服务器会执行动态资源转换为静态资源发送给用户浏览器\n    \n    静态资源：\n    \n    html：用于搭建基础网页，展示页面的内容\n    \n    css:用于美化页面，布局页面\n    \n    javascript：控制页面的元素，让页面有一些动态的效果",charsets:{cjk:!0}},{title:"HTML",frontmatter:{title:"HTML",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/3ba523/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/02.HTML.html",relativePath:"前端/01.html/02.HTML.md",key:"v-2c5e7662",path:"/pages/3ba523/",headers:[{level:2,title:"基本标签",slug:"基本标签",normalizedTitle:"基本标签",charIndex:549},{level:2,title:"文本标签",slug:"文本标签",normalizedTitle:"文本标签",charIndex:686},{level:2,title:"图片标签",slug:"图片标签",normalizedTitle:"图片标签",charIndex:1018},{level:2,title:"列表标签",slug:"列表标签",normalizedTitle:"列表标签",charIndex:1346},{level:2,title:"链接标签",slug:"链接标签",normalizedTitle:"链接标签",charIndex:2006},{level:2,title:"锚点 定位",slug:"锚点-定位",normalizedTitle:"锚点 定位",charIndex:2194},{level:2,title:"预格式标签",slug:"预格式标签",normalizedTitle:"预格式标签",charIndex:2390},{level:2,title:"特殊字符",slug:"特殊字符",normalizedTitle:"特殊字符",charIndex:2446},{level:2,title:"块标签",slug:"块标签",normalizedTitle:"块标签",charIndex:2488},{level:2,title:"表格标签",slug:"表格标签",normalizedTitle:"表格标签",charIndex:2554},{level:2,title:"路径",slug:"路径",normalizedTitle:"路径",charIndex:1039}],headersStr:"基本标签 文本标签 图片标签 列表标签 链接标签 锚点 定位 预格式标签 特殊字符 块标签 表格标签 路径",content:'# HTML\n\n 1. 概念：是最精彩的网页开发语言\n    \n    Hyper Text Markup Language 超文本标记语言\n    \n    超文本：超级文本（超链接）\n    \n    标记语言 ：由标签构成的语言<标签名称>如html，xml\n    \n    标记语言不是编程语言\n\n 2. 规范\n    \n    语法：\n    \n    1. html文档后缀名 .html或者.htm\n    \n    2. 标签分为\n       \n       1. 围堵标签：有开始标签和结束标签。如>\n       2. 自闭和标签：开始标签和结束标签在一起。如\n          \n    \n    3. 标签可以嵌套：\n       \n       需要正确嵌套，不能你中有我 ，我中你\n       \n       <a><b></a></b> #错误\n       <a><b></b></a> #正确\n       \n       \n       1\n       2\n       \n    \n    4. 在开始标签中可以定义属性，属性是由键值对构成，值需要用引号（单双都可）引起来\n    \n    5. html标签不区分大小写，但是建议使用小写\n\n\n# 基本标签\n\n基本标签：构成html最基本的标签\n\n * html：html文档的根标签\n\n * head：头标签。用于指定html文档的一些属性。引入外部的资源\n\n * title：标题标签\n\n * body:体标签\n\n * ：html5中定义该文档是html文档\n\n\n# 文本标签\n\n和文本有关的标签\n\n注释:\x3c!-- 注释内容 --\x3e \n<h1> -<h6>标题标签  不同等级的标题\n<p>  段落标签\n<br>：换行标签\n<hr>  展示一条水平线 拥有属性  color颜色 width宽度 size高度 aign对齐方式 （center居中  left左对齐 right右对齐）\n<b> / <strong> 字体加粗\n<i> / <em> 斜体\n<s> / <del> 删除线\n<u> / <ins> 下划线\n<font> h5中已经淘汰 字体标签 拥有属性  color颜色  size字体大小  face字体  \n<center> h5中已淘汰  居中标签\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 图片标签\n\n   <img src="图片路径" align="图片对齐方式如右对齐" alt="图片加载失败显示的文字 widt="宽度像素大小" height="高度像素大小">\n\x3c!--\n   src:图片路径  相对路径:以.开头的路径\n./:开头的目录为当前目录  默认为./如./image/tv01.jpg\n   ../:代表上一级目录的文件\n   align:图片网页放置位置\n   alt:图片加载失败显示文字\n   width:宽度 即像素点大小\n   height:高度\n   border:设置图像的边框粗细\n   title:鼠标悬浮在图片上提示的文字\n--\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 列表标签\n\n\x3c!-- 列表标签  分无序列表和有序列表--\x3e\n\x3c!-- 有序列表  ol:为列表的包头   li:为每个序列的包裹--\x3e\n有序列表\n<ol type="A" start="2">  \x3c!--type属性定义序号类型   start属性为起始序号 --\x3e\n    <li>第一行</li>\n    <li>第二行</li>\n    <li>第三行</li>\n    <li>第四行</li>\n</ol>\n\x3c!-- 无序列表  ul:为列表的包头   li:为每个序列的包裹--\x3e\n无序列表\n<ul type="disc"> \x3c!-- type同时可以定义列表序号的图像  disc实心圆  square方框  circle圆 --\x3e\n    <li>第一行</li>\n    <li>第二行</li>\n    <li>第三行</li>\n <li>第四行</li>\n </ul>\n \n \x3c!-- 自定义列表  dl:列表的包头  dt:名词   dd:围绕着dt的名词来解释 --\x3e\n <dl>\n \t<td>蔬菜</td>  \x3c!-- 不限定为1个  --\x3e\n \t<dd>大白菜</dd>\n \t<dd>通心菜</dd>\n \t<dd>生菜</dd>\n \t<td>肉</td>\n \t<dd>猪肉</dd>\n \t<dd>牛肉</dd>\n </dl>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 链接标签\n\n<a>:定义一个超链接\n属性: href:指定访问资源的url(统一资源定位符)  必须是http:// 开头   如为空链接则填入 # 号即可\ntarget:指定打开资源的方式  1._self:默认值,在当前页面打开 2._blank:在空白页面打开\n\n<base>:自闭标签,放在head中,定义所有的a标签跳转方式\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 锚点 定位\n\n1.使用相对应的id名标注到要跳转的位置  当前页面跳转到指定内容的位置\n<h3 id = "top">test</h3>\n\n2.使用超链接标签 href跳转为#id名\n<a href="#top">瞄跳转</a>\n\n <h3 id="two">\n <a href="#two">:跳转链接 跳转到href指定的id前面必须要有#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 预格式标签\n\n<pre>\n   什  么 ,\n   连空格和回车都识别?\n</pre>\n\npre标签文字什么样子特殊字符都可以显示出来\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 特殊字符\n\n\n\n\n# 块标签\n\n<div>:每一个div沾满一整行。块级标签\n<span>:文本信息在一行展示,行内标签 内联标签\n\n\n1\n2\n\n\n\n# 表格标签\n\n<table>:定义表格  拥有属性  width:宽度  border:边框 cellpadding:定义内容和单元格之间的距离.如果指定为0则单元格的线合为一条   cellspacing:定义单元格之间的距离.如果指定为0则单元格的线合为一   bgcolor:表格背景颜色  align:表格对齐方式(left左对齐,center居中对齐,right右对齐)  valigh:表格内容垂直内容对齐方式(beseline基线对齐,top上对齐,middle居中对齐,bottom下对齐)\n<tr>:定义行  拥有属性 bgcolor:表格背景颜色  align:表格单元格内容对齐方式  \n<td>:定义单元格  拥有属性 colspan:合并列   rowspan:合并行\n<th>:定义表头单元格\n<caption> 定义表格标题\n<thead> 表示表格的头部分\n<tbody> 表示表格体部分\n<tfoot> 表示表格尾部\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 路径\n\n 1. 相对路径\n    \n    同级路径\n    \n    下级路径 /\n    \n    上级路径 ../\n\n 2. 绝对路径 :完整路径',normalizedContent:'# html\n\n 1. 概念：是最精彩的网页开发语言\n    \n    hyper text markup language 超文本标记语言\n    \n    超文本：超级文本（超链接）\n    \n    标记语言 ：由标签构成的语言<标签名称>如html，xml\n    \n    标记语言不是编程语言\n\n 2. 规范\n    \n    语法：\n    \n    1. html文档后缀名 .html或者.htm\n    \n    2. 标签分为\n       \n       1. 围堵标签：有开始标签和结束标签。如>\n       2. 自闭和标签：开始标签和结束标签在一起。如\n          \n    \n    3. 标签可以嵌套：\n       \n       需要正确嵌套，不能你中有我 ，我中你\n       \n       <a><b></a></b> #错误\n       <a><b></b></a> #正确\n       \n       \n       1\n       2\n       \n    \n    4. 在开始标签中可以定义属性，属性是由键值对构成，值需要用引号（单双都可）引起来\n    \n    5. html标签不区分大小写，但是建议使用小写\n\n\n# 基本标签\n\n基本标签：构成html最基本的标签\n\n * html：html文档的根标签\n\n * head：头标签。用于指定html文档的一些属性。引入外部的资源\n\n * title：标题标签\n\n * body:体标签\n\n * ：html5中定义该文档是html文档\n\n\n# 文本标签\n\n和文本有关的标签\n\n注释:\x3c!-- 注释内容 --\x3e \n<h1> -<h6>标题标签  不同等级的标题\n<p>  段落标签\n<br>：换行标签\n<hr>  展示一条水平线 拥有属性  color颜色 width宽度 size高度 aign对齐方式 （center居中  left左对齐 right右对齐）\n<b> / <strong> 字体加粗\n<i> / <em> 斜体\n<s> / <del> 删除线\n<u> / <ins> 下划线\n<font> h5中已经淘汰 字体标签 拥有属性  color颜色  size字体大小  face字体  \n<center> h5中已淘汰  居中标签\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 图片标签\n\n   <img src="图片路径" align="图片对齐方式如右对齐" alt="图片加载失败显示的文字 widt="宽度像素大小" height="高度像素大小">\n\x3c!--\n   src:图片路径  相对路径:以.开头的路径\n./:开头的目录为当前目录  默认为./如./image/tv01.jpg\n   ../:代表上一级目录的文件\n   align:图片网页放置位置\n   alt:图片加载失败显示文字\n   width:宽度 即像素点大小\n   height:高度\n   border:设置图像的边框粗细\n   title:鼠标悬浮在图片上提示的文字\n--\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 列表标签\n\n\x3c!-- 列表标签  分无序列表和有序列表--\x3e\n\x3c!-- 有序列表  ol:为列表的包头   li:为每个序列的包裹--\x3e\n有序列表\n<ol type="a" start="2">  \x3c!--type属性定义序号类型   start属性为起始序号 --\x3e\n    <li>第一行</li>\n    <li>第二行</li>\n    <li>第三行</li>\n    <li>第四行</li>\n</ol>\n\x3c!-- 无序列表  ul:为列表的包头   li:为每个序列的包裹--\x3e\n无序列表\n<ul type="disc"> \x3c!-- type同时可以定义列表序号的图像  disc实心圆  square方框  circle圆 --\x3e\n    <li>第一行</li>\n    <li>第二行</li>\n    <li>第三行</li>\n <li>第四行</li>\n </ul>\n \n \x3c!-- 自定义列表  dl:列表的包头  dt:名词   dd:围绕着dt的名词来解释 --\x3e\n <dl>\n \t<td>蔬菜</td>  \x3c!-- 不限定为1个  --\x3e\n \t<dd>大白菜</dd>\n \t<dd>通心菜</dd>\n \t<dd>生菜</dd>\n \t<td>肉</td>\n \t<dd>猪肉</dd>\n \t<dd>牛肉</dd>\n </dl>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 链接标签\n\n<a>:定义一个超链接\n属性: href:指定访问资源的url(统一资源定位符)  必须是http:// 开头   如为空链接则填入 # 号即可\ntarget:指定打开资源的方式  1._self:默认值,在当前页面打开 2._blank:在空白页面打开\n\n<base>:自闭标签,放在head中,定义所有的a标签跳转方式\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 锚点 定位\n\n1.使用相对应的id名标注到要跳转的位置  当前页面跳转到指定内容的位置\n<h3 id = "top">test</h3>\n\n2.使用超链接标签 href跳转为#id名\n<a href="#top">瞄跳转</a>\n\n <h3 id="two">\n <a href="#two">:跳转链接 跳转到href指定的id前面必须要有#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 预格式标签\n\n<pre>\n   什  么 ,\n   连空格和回车都识别?\n</pre>\n\npre标签文字什么样子特殊字符都可以显示出来\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 特殊字符\n\n\n\n\n# 块标签\n\n<div>:每一个div沾满一整行。块级标签\n<span>:文本信息在一行展示,行内标签 内联标签\n\n\n1\n2\n\n\n\n# 表格标签\n\n<table>:定义表格  拥有属性  width:宽度  border:边框 cellpadding:定义内容和单元格之间的距离.如果指定为0则单元格的线合为一条   cellspacing:定义单元格之间的距离.如果指定为0则单元格的线合为一   bgcolor:表格背景颜色  align:表格对齐方式(left左对齐,center居中对齐,right右对齐)  valigh:表格内容垂直内容对齐方式(beseline基线对齐,top上对齐,middle居中对齐,bottom下对齐)\n<tr>:定义行  拥有属性 bgcolor:表格背景颜色  align:表格单元格内容对齐方式  \n<td>:定义单元格  拥有属性 colspan:合并列   rowspan:合并行\n<th>:定义表头单元格\n<caption> 定义表格标题\n<thead> 表示表格的头部分\n<tbody> 表示表格体部分\n<tfoot> 表示表格尾部\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 路径\n\n 1. 相对路径\n    \n    同级路径\n    \n    下级路径 /\n    \n    上级路径 ../\n\n 2. 绝对路径 :完整路径',charsets:{cjk:!0}},{title:"HTML5",frontmatter:{title:"HTML5",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/8600fc/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/03.HTML5.html",relativePath:"前端/01.html/03.HTML5.md",key:"v-7c2eb23b",path:"/pages/8600fc/",headers:[{level:2,title:"语义化标签",slug:"语义化标签",normalizedTitle:"语义化标签",charIndex:12},{level:2,title:"多媒体标签",slug:"多媒体标签",normalizedTitle:"多媒体标签",charIndex:189},{level:3,title:"音频标签",slug:"音频标签",normalizedTitle:"音频标签",charIndex:199},{level:3,title:"视频标签",slug:"视频标签",normalizedTitle:"视频标签",charIndex:604},{level:2,title:"表单属性",slug:"表单属性",normalizedTitle:"表单属性",charIndex:938},{level:2,title:"表单",slug:"表单",normalizedTitle:"表单",charIndex:938}],headersStr:"语义化标签 多媒体标签 音频标签 视频标签 表单属性 表单",content:'# HTML5\n\n\n# 语义化标签\n\nhtml5中为了提高可读性,提供了一些标签,这些标签主要针对于搜索引擎的,移动端经常使用这些标签,在IE9中需要把这些标签转为块元素.\n\n<header> 头部标签\n<nav>导航标签\n<article>内容标签\n<section>块级标签\n<aside>侧边栏标签\n<footer> 尾部标签\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 多媒体标签\n\n\n# 音频标签\n\n使用<audio>关键字,拥有属性\n\n * src:路径 如:src:"../audio/xx.MP3"\n * controls:如果出现该属性则向用户显示控件如播放按钮 如:controls="controls"\n * autoplay:音频加载完后自动播放 如:autoplay ="autoplay"\n * loop:循环播放 如:loop="loop"\n\n不同的浏览器支持不同的音频格式,比如有的浏览器支持MP3有的只支持ogg,我们的解决方法是为不同浏览器提供不同的音频格式\n\n <audio controls="controls">\n        <source src="xxx.mp3" type="audio/mpeg">\n        <source src="xx.ogg" type="audio/ogg" > \n    </audio>\n\n\n1\n2\n3\n4\n\n\n\n# 视频标签\n\n使用<video>,拥有属性\n\n * src:路径 如:src:"../audio/xx.MP4"\n * controls:如果出现该属性则向用户显示控件如播放按钮 如:controls="controls"\n * autoplay:视频加载完后自动播放 如:autoplay ="autoplay" 谷歌中默认禁用自动播放,只需静音播放即可\n * loop:循环播放 如:loop="loop"\n * poster:加载等待的图片 如:poster:"图片路径"\n * muted:静音播放 如:muted="muted"\n * preload:是否加载完再播放 如:preload="preload"\n\n同样有格式冲突,与音频处理方法一致\n\n\n\n\n# 表单属性\n\n * type="email" :限定用户使用email类型\n * url; url类型\n * date 日期类型\n * time 时间类型\n * month 月类型\n * week 周类型\n * number; 必须为数字\n * tel 手机号码\n * search 搜索框 可以快速清除输入框内容\n * color 颜色选择\n\n\n# 表单\n\n * required:其内容不能为空 如:required="required"\n * placeholder:提示文本,光标在表单或存在值则不显示 如:placeholder="请输入用户名"\n * autofocus:自动聚焦,页面加载完光标自动聚焦在此表单 如:autofocus="autofocus"\n * autocompleate:浏览器根据之前键入的值,出现提示, 默认是打开,有on和off选项,同时需要在表单内设置name属性 如:autocompleate:off;\n * multiple:可以多选文件提交 如:multiple="multiple"',normalizedContent:'# html5\n\n\n# 语义化标签\n\nhtml5中为了提高可读性,提供了一些标签,这些标签主要针对于搜索引擎的,移动端经常使用这些标签,在ie9中需要把这些标签转为块元素.\n\n<header> 头部标签\n<nav>导航标签\n<article>内容标签\n<section>块级标签\n<aside>侧边栏标签\n<footer> 尾部标签\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 多媒体标签\n\n\n# 音频标签\n\n使用<audio>关键字,拥有属性\n\n * src:路径 如:src:"../audio/xx.mp3"\n * controls:如果出现该属性则向用户显示控件如播放按钮 如:controls="controls"\n * autoplay:音频加载完后自动播放 如:autoplay ="autoplay"\n * loop:循环播放 如:loop="loop"\n\n不同的浏览器支持不同的音频格式,比如有的浏览器支持mp3有的只支持ogg,我们的解决方法是为不同浏览器提供不同的音频格式\n\n <audio controls="controls">\n        <source src="xxx.mp3" type="audio/mpeg">\n        <source src="xx.ogg" type="audio/ogg" > \n    </audio>\n\n\n1\n2\n3\n4\n\n\n\n# 视频标签\n\n使用<video>,拥有属性\n\n * src:路径 如:src:"../audio/xx.mp4"\n * controls:如果出现该属性则向用户显示控件如播放按钮 如:controls="controls"\n * autoplay:视频加载完后自动播放 如:autoplay ="autoplay" 谷歌中默认禁用自动播放,只需静音播放即可\n * loop:循环播放 如:loop="loop"\n * poster:加载等待的图片 如:poster:"图片路径"\n * muted:静音播放 如:muted="muted"\n * preload:是否加载完再播放 如:preload="preload"\n\n同样有格式冲突,与音频处理方法一致\n\n\n\n\n# 表单属性\n\n * type="email" :限定用户使用email类型\n * url; url类型\n * date 日期类型\n * time 时间类型\n * month 月类型\n * week 周类型\n * number; 必须为数字\n * tel 手机号码\n * search 搜索框 可以快速清除输入框内容\n * color 颜色选择\n\n\n# 表单\n\n * required:其内容不能为空 如:required="required"\n * placeholder:提示文本,光标在表单或存在值则不显示 如:placeholder="请输入用户名"\n * autofocus:自动聚焦,页面加载完光标自动聚焦在此表单 如:autofocus="autofocus"\n * autocompleate:浏览器根据之前键入的值,出现提示, 默认是打开,有on和off选项,同时需要在表单内设置name属性 如:autocompleate:off;\n * multiple:可以多选文件提交 如:multiple="multiple"',charsets:{cjk:!0}},{title:"表单标签",frontmatter:{title:"表单标签",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/c2f00e/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/04.%E8%A1%A8%E5%8D%95%E6%A0%87%E7%AD%BE.html",relativePath:"前端/01.html/04.表单标签.md",key:"v-5cb205ae",path:"/pages/c2f00e/",headers:[{level:2,title:"form表单",slug:"form表单",normalizedTitle:"form表单",charIndex:40},{level:2,title:"input:可以通过type属性值,改变元素的样式",slug:"input-可以通过type属性值-改变元素的样式",normalizedTitle:"input:可以通过type属性值,改变元素的样式",charIndex:643},{level:2,title:"label:指定输入项的文字描述信息",slug:"label-指定输入项的文字描述信息",normalizedTitle:"label:指定输入项的文字描述信息",charIndex:1357},{level:2,title:"select:下拉列表",slug:"select-下拉列表",normalizedTitle:"select:下拉列表",charIndex:1465},{level:2,title:"textarea:文本域",slug:"textarea-文本域",normalizedTitle:"textarea:文本域",charIndex:1776}],headersStr:"form表单 input:可以通过type属性值,改变元素的样式 label:指定输入项的文字描述信息 select:下拉列表 textarea:文本域",content:'# 表单标签\n\n表单:用于采集用户输入的数据的.用于和服务器进行交互.\n\n\n# form表单\n\n<form action="#" method="get" name="f">\n    \x3c!--form 用于定义表单的,可以定义一个范围,范围代表采集用户的范围\n        属性:\n            action:指定提交数据的url\n            method:指定提交方式\n                分类:一共7种,有2种比较常用\n                     get:\n                        1.请求参数会在地址栏中显示\n                        2.请求参数的长的是有限制的\n                        3.不太安全\n                    post:\n                        1.请求参数不会在地址中显示,会封装在请求体中(http协议)\n                        2.请求参数的大小没有限制.\n                        3.较为安全\n                 表单项中的数据想要被提交,必须指定其name属性\n\t\t\tname:表单名字\n\n    --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n表单项标签:\n\n\n# input:可以通过type属性值,改变元素的样式\n\n * type属性:\n\n * text:文本输入框,默认值\n   \n   * placeholder:指定输入框的提示信息,当输入框的内容发生变化,会自动清空提示信息\n\n * password:密码输入框\n\n * radio:单选框\n   \n   * 属性:\n     * name:要想实现单选多个单选框name属性必须相同\n     * value:一般回给每个单选框提供value值,提交时返回value值\n     * chec ked:设置默认是否勾选\n\n * checkbox:复选框\n   \n   * 属性\n     * name:要想实现单选多个单选框name属性必须相同\n     * value:一般回给每个单选框提供value值,提交时返回value值\n     * checked:设置默认是否勾选\n\n * file:文件选择框 上传文件用\n\n * hidden:隐藏域,用于提交一些信息\n\n * 按钮:\n   \n   * submit:提交按钮,可以提交表单\n   * button: 普通按钮,配合其他JavaScript使用\n   * image:让图片成为按钮 src为图片路径\n   * reset:重置表单中的文本\n\n * color:取色器,设置name属性后可以提交颜色表单\n\n * date:日期\n\n * datetime-local:属性为日期 24小时进制时间 设置name属性后提交表单返回日期和格式化后的时间\n\n * email: email属性为邮箱 name属性提交表单返回值 会做判断用户的输入是否包含@符号\n\n\n# label:指定输入项的文字描述信息\n\n * label的for属性一般回合input的id属性＝,如果对应了则点击label区域会自动让input输入框获取焦点\n * 或者可以在label标签中定义输入框\n\n\n# select:下拉列表\n\n省份:<select name="province">  \x3c!-- <select>下拉列表  name属性提交表单--\x3e\n    <option value="1" >广东</option>  \x3c!-- 列表元素用<option>包裹 设置value后提交value值 如果不设置则返回包裹的文字--\x3e\n    <option value="2" selected>上海</option>  \x3c!-- selected属性  将改元素设置为默认选项  如不指定则默认为下拉列表中第一个元素--\x3e\n    <option value="3">北京</option>\n\n\n1\n2\n3\n4\n\n\n\n# textarea:文本域\n\n * cols:指定列数,每一行有多少个字符\n * rows:默认多少',normalizedContent:'# 表单标签\n\n表单:用于采集用户输入的数据的.用于和服务器进行交互.\n\n\n# form表单\n\n<form action="#" method="get" name="f">\n    \x3c!--form 用于定义表单的,可以定义一个范围,范围代表采集用户的范围\n        属性:\n            action:指定提交数据的url\n            method:指定提交方式\n                分类:一共7种,有2种比较常用\n                     get:\n                        1.请求参数会在地址栏中显示\n                        2.请求参数的长的是有限制的\n                        3.不太安全\n                    post:\n                        1.请求参数不会在地址中显示,会封装在请求体中(http协议)\n                        2.请求参数的大小没有限制.\n                        3.较为安全\n                 表单项中的数据想要被提交,必须指定其name属性\n\t\t\tname:表单名字\n\n    --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n表单项标签:\n\n\n# input:可以通过type属性值,改变元素的样式\n\n * type属性:\n\n * text:文本输入框,默认值\n   \n   * placeholder:指定输入框的提示信息,当输入框的内容发生变化,会自动清空提示信息\n\n * password:密码输入框\n\n * radio:单选框\n   \n   * 属性:\n     * name:要想实现单选多个单选框name属性必须相同\n     * value:一般回给每个单选框提供value值,提交时返回value值\n     * chec ked:设置默认是否勾选\n\n * checkbox:复选框\n   \n   * 属性\n     * name:要想实现单选多个单选框name属性必须相同\n     * value:一般回给每个单选框提供value值,提交时返回value值\n     * checked:设置默认是否勾选\n\n * file:文件选择框 上传文件用\n\n * hidden:隐藏域,用于提交一些信息\n\n * 按钮:\n   \n   * submit:提交按钮,可以提交表单\n   * button: 普通按钮,配合其他javascript使用\n   * image:让图片成为按钮 src为图片路径\n   * reset:重置表单中的文本\n\n * color:取色器,设置name属性后可以提交颜色表单\n\n * date:日期\n\n * datetime-local:属性为日期 24小时进制时间 设置name属性后提交表单返回日期和格式化后的时间\n\n * email: email属性为邮箱 name属性提交表单返回值 会做判断用户的输入是否包含@符号\n\n\n# label:指定输入项的文字描述信息\n\n * label的for属性一般回合input的id属性＝,如果对应了则点击label区域会自动让input输入框获取焦点\n * 或者可以在label标签中定义输入框\n\n\n# select:下拉列表\n\n省份:<select name="province">  \x3c!-- <select>下拉列表  name属性提交表单--\x3e\n    <option value="1" >广东</option>  \x3c!-- 列表元素用<option>包裹 设置value后提交value值 如果不设置则返回包裹的文字--\x3e\n    <option value="2" selected>上海</option>  \x3c!-- selected属性  将改元素设置为默认选项  如不指定则默认为下拉列表中第一个元素--\x3e\n    <option value="3">北京</option>\n\n\n1\n2\n3\n4\n\n\n\n# textarea:文本域\n\n * cols:指定列数,每一行有多少个字符\n * rows:默认多少',charsets:{cjk:!0}},{title:"CSS",frontmatter:{title:"CSS",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/b3f43c/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/05.CSS.html",relativePath:"前端/01.html/05.CSS.md",key:"v-3aba634a",path:"/pages/b3f43c/",headers:[{level:2,title:"概念:Cascading Style Sheets 层叠样式表",slug:"概念-cascading-style-sheets-层叠样式表",normalizedTitle:"概念:cascading style sheets 层叠样式表",charIndex:25},{level:2,title:"css语法",slug:"css语法",normalizedTitle:"css语法",charIndex:690},{level:2,title:"选择器:筛选具有相似特征的元素",slug:"选择器-筛选具有相似特征的元素",normalizedTitle:"选择器:筛选具有相似特征的元素",charIndex:797},{level:3,title:"多类名",slug:"多类名",normalizedTitle:"多类名",charIndex:1522},{level:2,title:"扩展选择器",slug:"扩展选择器",normalizedTitle:"扩展选择器",charIndex:1647},{level:2,title:"标签显示模式(display)",slug:"标签显示模式-display",normalizedTitle:"标签显示模式(display)",charIndex:2145},{level:3,title:"什么是标签显示模式",slug:"什么是标签显示模式",normalizedTitle:"什么是标签显示模式",charIndex:2165},{level:3,title:"块级元素",slug:"块级元素",normalizedTitle:"块级元素",charIndex:2225},{level:3,title:"行内元素",slug:"行内元素",normalizedTitle:"行内元素",charIndex:2216},{level:3,title:"行内块元素",slug:"行内块元素",normalizedTitle:"行内块元素",charIndex:2534},{level:3,title:"标签显示模式转换",slug:"标签显示模式转换",normalizedTitle:"标签显示模式转换",charIndex:2662},{level:2,title:"（font）字体",slug:"font-字体",normalizedTitle:"（font）字体",charIndex:2758},{level:2,title:"CCS外观属性 文本(text)",slug:"ccs外观属性-文本-text",normalizedTitle:"ccs外观属性 文本(text)",charIndex:3188},{level:2,title:"行高(line-height)",slug:"行高-line-height",normalizedTitle:"行高(line-height)",charIndex:3508},{level:2,title:"背景(background)",slug:"背景-background",normalizedTitle:"背景(background)",charIndex:3687},{level:2,title:"CSS 三大特性",slug:"css-三大特性",normalizedTitle:"css 三大特性",charIndex:4293},{level:3,title:"CSS 层叠性",slug:"css-层叠性",normalizedTitle:"css 层叠性",charIndex:4306},{level:3,title:"CSS 继承性",slug:"css-继承性",normalizedTitle:"css 继承性",charIndex:4352},{level:3,title:"CSS 优先级",slug:"css-优先级",normalizedTitle:"css 优先级",charIndex:4473},{level:3,title:"权重叠加",slug:"权重叠加",normalizedTitle:"权重叠加",charIndex:4538},{level:2,title:"盒子模型",slug:"盒子模型",normalizedTitle:"盒子模型",charIndex:4604},{level:3,title:"边框",slug:"边框",normalizedTitle:"边框",charIndex:4613},{level:3,title:"尺寸",slug:"尺寸",normalizedTitle:"尺寸",charIndex:5167},{level:3,title:"内边距",slug:"内边距",normalizedTitle:"内边距",charIndex:2648},{level:3,title:"外边距",slug:"外边距",normalizedTitle:"外边距",charIndex:2644},{level:3,title:"让块级盒子居中水平对齐",slug:"让块级盒子居中水平对齐",normalizedTitle:"让块级盒子居中水平对齐",charIndex:5856},{level:3,title:"让文字居中和盒子居中",slug:"让文字居中和盒子居中",normalizedTitle:"让文字居中和盒子居中",charIndex:5917},{level:3,title:"插入图片和背景图片的区别",slug:"插入图片和背景图片的区别",normalizedTitle:"插入图片和背景图片的区别",charIndex:6005},{level:3,title:"清除元素的默认的内外边距",slug:"清除元素的默认的内外边距",normalizedTitle:"清除元素的默认的内外边距",charIndex:6125},{level:3,title:"去掉列表默认的样式",slug:"去掉列表默认的样式",normalizedTitle:"去掉列表默认的样式",charIndex:6206},{level:3,title:"外边距合并",slug:"外边距合并",normalizedTitle:"外边距合并",charIndex:6238},{level:3,title:"盒子模型布局稳定性",slug:"盒子模型布局稳定性",normalizedTitle:"盒子模型布局稳定性",charIndex:6586},{level:3,title:"盒子阴影",slug:"盒子阴影",normalizedTitle:"盒子阴影",charIndex:6676},{level:3,title:"子盒子可以比父盒子大",slug:"子盒子可以比父盒子大",normalizedTitle:"子盒子可以比父盒子大",charIndex:6780},{level:2,title:"浮动(float)",slug:"浮动-float",normalizedTitle:"浮动(float)",charIndex:6834},{level:3,title:"浮动元素与父盒子关系",slug:"浮动元素与父盒子关系",normalizedTitle:"浮动元素与父盒子关系",charIndex:7067},{level:3,title:"清除浮动",slug:"清除浮动",normalizedTitle:"清除浮动",charIndex:7123},{level:2,title:"CSS属性书写顺序",slug:"css属性书写顺序",normalizedTitle:"css属性书写顺序",charIndex:7774},{level:2,title:"定位(position)",slug:"定位-position",normalizedTitle:"定位(position)",charIndex:8092},{level:3,title:"边偏移",slug:"边偏移",normalizedTitle:"边偏移",charIndex:8147},{level:3,title:"定位模式",slug:"定位模式",normalizedTitle:"定位模式",charIndex:8193},{level:2,title:"布局总结",slug:"布局总结",normalizedTitle:"布局总结",charIndex:9121},{level:2,title:"display 显示",slug:"display-显示",normalizedTitle:"display 显示",charIndex:9278},{level:2,title:"visibility 可见",slug:"visibility-可见",normalizedTitle:"visibility 可见",charIndex:9371},{level:2,title:"overflow 溢出",slug:"overflow-溢出",normalizedTitle:"overflow 溢出",charIndex:9457},{level:2,title:"CSS用户界面样式",slug:"css用户界面样式",normalizedTitle:"css用户界面样式",charIndex:9628},{level:3,title:"鼠标样式 cursor",slug:"鼠标样式-cursor",normalizedTitle:"鼠标样式 cursor",charIndex:9642},{level:3,title:"轮廓线 outline",slug:"轮廓线-outline",normalizedTitle:"轮廓线 outline",charIndex:9750},{level:3,title:"防止拖拽文本域 resize",slug:"防止拖拽文本域-resize",normalizedTitle:"防止拖拽文本域 resize",charIndex:9828},{level:2,title:"垂直对齐 vertical-align",slug:"垂直对齐-vertical-align",normalizedTitle:"垂直对齐 vertical-align",charIndex:9887},{level:2,title:"去除图片底部空白缝隙",slug:"去除图片底部空白缝隙",normalizedTitle:"去除图片底部空白缝隙",charIndex:10054},{level:2,title:"溢出的文字省略号显示",slug:"溢出的文字省略号显示",normalizedTitle:"溢出的文字省略号显示",charIndex:10206},{level:3,title:"white-space 强制文字一行显示",slug:"white-space-强制文字一行显示",normalizedTitle:"white-space 强制文字一行显示",charIndex:10324},{level:3,title:"text-overflow  文字用省略号代表超出的部分",slug:"text-overflow-文字用省略号代表超出的部分",normalizedTitle:"text-overflow  文字用省略号代表超出的部分",charIndex:null},{level:2,title:"CSS精灵技术 (sprite)",slug:"css精灵技术-sprite",normalizedTitle:"css精灵技术 (sprite)",charIndex:10506},{level:2,title:"滑动门",slug:"滑动门",normalizedTitle:"滑动门",charIndex:10682},{level:2,title:"margin负值",slug:"margin负值",normalizedTitle:"margin负值",charIndex:10768},{level:2,title:"CSS三角形",slug:"css三角形",normalizedTitle:"css三角形",charIndex:10839}],headersStr:"概念:Cascading Style Sheets 层叠样式表 css语法 选择器:筛选具有相似特征的元素 多类名 扩展选择器 标签显示模式(display) 什么是标签显示模式 块级元素 行内元素 行内块元素 标签显示模式转换 （font）字体 CCS外观属性 文本(text) 行高(line-height) 背景(background) CSS 三大特性 CSS 层叠性 CSS 继承性 CSS 优先级 权重叠加 盒子模型 边框 尺寸 内边距 外边距 让块级盒子居中水平对齐 让文字居中和盒子居中 插入图片和背景图片的区别 清除元素的默认的内外边距 去掉列表默认的样式 外边距合并 盒子模型布局稳定性 盒子阴影 子盒子可以比父盒子大 浮动(float) 浮动元素与父盒子关系 清除浮动 CSS属性书写顺序 定位(position) 边偏移 定位模式 布局总结 display 显示 visibility 可见 overflow 溢出 CSS用户界面样式 鼠标样式 cursor 轮廓线 outline 防止拖拽文本域 resize 垂直对齐 vertical-align 去除图片底部空白缝隙 溢出的文字省略号显示 white-space 强制文字一行显示 text-overflow  文字用省略号代表超出的部分 CSS精灵技术 (sprite) 滑动门 margin负值 CSS三角形",content:'# CSS\n\nCSS:页面美化和布局控制\n\n\n# 概念:Cascading Style Sheets 层叠样式表\n\n * 层叠:多个样式可以作用在同一个html的元素上,同时生效\n\n 2. 好处\n\n * 功能强大\n * 将内容展示和样式控制分离 1. 降低耦合度.解耦 2. 让分工协作更容易 3. 提高开发效率\n\n 3. CSS的使用:CSS与html结合方式\n\n * 内联样式:在标签内使用style生效指定css代码\n\n\x3c!-- 方式一  内联方式  在标签内使用style属性指定css代码 --\x3e\n<div style="color: cornflowerblue;">hello world</div>\n\n\n1\n2\n\n * 内部样式:在head标签内,定义style标签,style标签的标签体内容就是css代码\n\n \x3c!-- 方式二 内部样式\n            * 在head标签内,定义style标签,style标签的标签体内容就是css代码--\x3e\n<style>\n        div{\n            color: blueviolet;\n        }\n\n    </style>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * 外部样式: 1.在外部定义css资源文件 2.在head标签内,定义link标签,引用外部的css资源\n\n<link rel="stylesheet" href="css/外部.css">\n \x3c!-- 通过<link>标签 引用外部css标签  rel为  href为css路径  --\x3e\n\n\n1\n2\n\n\n\n# css语法\n\n * 格式:\n   \n   * 选择器{\n     \n     属性名1:属性值1;\n     \n     属性名2:属性值2;\n     \n     …\n     \n     }\n   \n   * 选择器:筛选具有相似特征的元素 每一对属性需要使用分号隔开,最后一对可以不加;\n\n\n# 选择器:筛选具有相似特征的元素\n\n * 分类:\n   \n   * 基础选择器\n     \n     * id选择器:选择具体id属性值的元素,建议在一个html页面中id值唯一 语法:#id属性值{}\n       \n       #div1{  \x3c!--id选择器 根据id选择html标签 --\x3e\n                   color: blue;\n               }\n       \n       \n       1\n       2\n       3\n       \n     \n     * 元素选择器:选择具有相同标签名称的元素 语法:标签名称{} 注意:id选择器优先级高于元素选择器\n     \n     div{  \x3c!-- 元素选择器  根据标签名选择html标签 --\x3e\n                 color: rebeccapurple;\n             }\n     \n     \n     \n     1\n     2\n     3\n     4\n     \n     * 类选择器:选择具有相同class的属性值的元素 语法:.class属性值{} 注意:类选择器优先级高于元素选择器 id>类>元素\n     \n     .cls1{ \x3c!-- 类选择器  根据类选择html标签 --\x3e\n                 color: cornflowerblue;\n             }\n     \n     \n     1\n     2\n     3\n     \n\n\n# 多类名\n\n一个标签只有拥有一个class属性\n\n但class属性能写多个名 用空格分开 每个类名都能单独调用css\n\n<span class="class1 class2"> </span>\n\n\n1\n\n\nid在页面中不能重复 class可以\n\n\n# 扩展选择器\n\n * 选择所有元素: 语法:*{}\n\n * 并集选择器: 语法:选择器1,选择器2{} 如: .box1,.box2 {}\n\n * 交集选择器: 只选择该标签并且是此选择 器的 语法:标签名选择器{} 如:div.box1{}\n\n * 后代选择器: 又称包含选择器 筛选选择器1元素下的选择器元素\n\n * 语法: 父级 子级{} 如: .box box2{}\n\n * 父选择器: 筛选选择器2的父元素选择器1 只选下一级的元素 有其他标签包含都不选择 语法:选择器1 > 选择器2{} 如: .box1 > p{}\n\n * 属性选择器:选择元素名称,属性名=属性值的元素 语法:元素名称[属性名=“属性值”]{}\n\n * 伪类选择器:选择一些元素具有的状态 语法:元素:状态{} 注意状态顺序 否则会出不必要的错误 lvha\n   \n   * 状态:\n     \n     1. link:初始化的状态\n     \n     2. visited:被访问过的状态\n     \n     3. hover:鼠标悬浮状态\n     \n     4. active:在在访问状态\n\n\n# 标签显示模式(display)\n\n\n# 什么是标签显示模式\n\n标签一什么方式进行显示如:div自己占一行称为块元素,span一行可以放多个称为行内元素\n\n\n# 块级元素\n\n常见的块元素有<h1>~<h6>、<p>、<div>、<ul>、<ol>、<li>等\n\np和h、dt里面尽量不要放块元素\n\n特点：\n\n1. 独占一行\n2. 宽度默认是父级的100%\n3. 里面可以块元素和行内元素\n\n\n\n# 行内元素\n\n常见的行内元素有<a>、<strong>、<b>、<em>、<i>、<del>、<s>、<ins>、<i>、<span>等\n\n特点：\n\n 1. 高、宽直接设置是没有任何效果的\n 2. 默认的宽度就是它本身内容的宽度\n 3. 行内元素只能容纳文本或其他行内元素\n 4. 链接标签不能再包含链接标签\n 5. 特殊情况a里面可以放块元素，但是给a转换一下块级模式更好\n\n\n# 行内块元素\n\n在行内元素中有几个特殊的标签<img>、<input>、<td>\n\n特点：\n\n1. 和相邻的行内元素在一行上，但是之间会有空白缝隙。一行内可以显示多个\n2. 默认宽度就是它本身内容的宽度\n3. 高度、行高、外边距和内边距都可以控制\n\n\n\n# 标签显示模式转换\n\n * 块转内：dispaly：inline\n * 行内转块：display：block\n * 块、行内元素转元素为行内块：display：inline-block\n\n\n# （font）字体\n\n * font-size:字体大小 如: font-size:10px;\n * font-family:设置字体 如:font-family:"微软雅黑";\n * 可以同时指定多个字体用逗号分隔,当浏览器不支持前面的字体时自动选择后面的字体 如:font-famil: Arial,"宋体","微软雅黑"; 如字体名有特殊字符或中文要加引号,可以用Unicode写中文字体名\n * \n * font-weight:字体粗细 如:font-weight:normal; normal 默认值(不加粗) bold(加粗) 100-900 400为默认 700为加粗\n * font-style:设置斜体 如:font-style:italic; normal默认值 italic斜体\n * 可以混成综合使用 如: font: italic 700 20px "宋体"; 按style-weight-size-family顺序.\n * \n\n\n# CCS外观属性 文本(text)\n\n * color:文本颜色 如:color:"#FFF" 支持英文颜色 十六进制 rgb rgba\n\n * text-align:文本对其方式 如:text-align:center; left左对齐 right 右对齐 center 居中\n\n * line-height:行间距 如:line-height:28px;\n\n * text-indext:首行缩进 如:text-indext:2em; 可以写px但建议写em,em是倍数关系,1em为1个字\n\n * text-decoration:文本的装饰 一般用来取消超链接的下划线 如:text-decoration:none;\n\n * \n\n\n# 行高(line-height)\n\n中文的行高为 第一行文字的底线到第二行文字的底线的高度\n\n英文的行高为 第一行英文的基线到第二行文字的基线高度\n\n行高 = 上距离 + 内容高度 + 下距离,文字的行高等于盒子的高度则会垂直居中(单行文本)\n\n 1. 行高等于高度 文字会垂直居中\n 2. 行高大于高度 文字会偏下\n 3. 行高小于高度 文字会偏上\n\n\n# 背景(background)\n\n * background-color: 背景颜色 如：background-color:red\n * background-image:背景图片 如:background-image:url(图片路径);路径不需要引号\n * background-repeat:背景平铺 如：background-repeat:no-rpeat;\n   * repeat 默认的在x轴和y轴上平铺\n   * no-repeat 不平铺\n   * repeat-x 在x轴平铺\n   * repeat-y 在y轴平铺\n * background-position:背景位置 如：backg-position：10px center；\n   * 可以填百分百 或xy坐标\n   * 两个位置也可以写绝对位置 top、center、bottom、left、right\n * background-attachment：背景附着 如:background-attachment：scroll;\n   * scroll 背景图随着对象内容滚动\n   * fixed 背景图像固定\n * 背景简写: background:背景颜色 背景图片地址 背景平铺 背景滚动 背景位置;(没有位置顺序,可省略不要的参数)\n * 背景半透明:background-color :rgba(0,0,0,.3);\n\n\n# CSS 三大特性\n\n\n# CSS 层叠性\n\n后面的样式覆盖掉前面的样式 不会发生冲突 样式不冲突不会进行覆盖\n\n\n# CSS 继承性\n\n子标签会继承父标签的某些样式,如文本颜色和字号\n\n想要设置一个可继承的属性,只需将它应用于父元素即可.\n\n子元素可以继承父元素的样式有(text-，font-，line-这些元素开头的可以继承，以及color属性)\n\n\n# CSS 优先级\n\n\n\n继承＜元素＜class＜id＜标签内的style＜!important(将此CSS属性设为最重要)\n\n\n# 权重叠加\n\n如:.box a{} 此时的权重为 0,0,1,0 + 0,0,0,1 =0,0,1,1\n\n如超过10不会进制\n\n\n# 盒子模型\n\n\n# 边框\n\n * border-style: 边框的线风格 如:border-style:solid\n   * solid 实线\n   * node 没有边框 默认值\n   * dashed 虚线\n   * dotted 点线\n * border-color:边框颜色 如:border-color:red\n   * border-width:边框宽度 border-width:10px\n   * 边框简写: border: 1px solid red; (没有顺序)\n   * 指定单方向的边框 如:border-top-width:10px\n   * border-radius:边框圆角过渡 如:border-radius:10px; 或 border-radius:50%; 矩形是高度的一半.\n     * border-top-left\n     * border-top-right\n     * borde-bottom-left\n     * border-bottom-right\n     * 简写 border:左上角 右上角 左下角 右下角\n   * border-collapse:表格 单元格 th 合并相邻的边框 如:border-collapse:collapse;\n\n\n# 尺寸\n\n * width:宽度\n * height:高度\n\n\n# 内边距\n\npadding用来设置内边距,指边框与内容之间的距离\n\n 1. 内容与边框有了距离\n 2. 盒子会变大\n\n * padding-left:左内边距\n * padding-right:右内边距\n * padding-top:上内边距\n * padding-bottom:下内边距\n * padding:简写\n   * padding: 1px 2px 3px 0px(上 右 下 左 顺时针)\n   * padding: 20px 上下左右都为20px内边距\n   * padding:10px 20px 上下为10px 左右为20px (如无指定则自动为对称的内边距)\n   * padding:10px 20px 30px 上为10 左右为20 下为30 的内边距\n\n盒子的实际大小 = 内容的宽度 高度 + 内边距 + 边框 = 盒子的大小 - 内边距 -边框 =css中设置盒子的大小\n\n也可以设置盒子属性 box-sizing: border-box;使得盒子实际大小等于边框+内边距的大小\n\n * padding不影响盒子大小情况 特殊情况\n   * 如果没有给一个盒子指定宽度,此时再给这个盒子padding则不会撑开盒子\n\n\n# 外边距\n\nmargin属性用于设置外边距.margin就是控制盒子和盒子之间的距离\n\n * margin-left:左外边距\n * margin-right:右外边距\n * margin-top:上外边距\n * margin-bottom:下外边距\n\n\n# 让块级盒子居中水平对齐\n\n 1. 必须要设置宽度\n 2. 设置margin左右 为auto 上下auto没有效果\n\n\n# 让文字居中和盒子居中\n\n 1. 让盒子内文字居中是 text-align:center,并且还可以让行内元素和行内块居中对齐\n 2. 块级盒子居中 左右外边距auto\n\n\n# 插入图片和背景图片的区别\n\n 1. 插入图片 我们用的最多 比如产品展示 移动位置只能靠盒模型 padding margin\n 2. 背景图片我们一般用于小图片背景 或者超大背景图片 只能通过background-position\n\n\n# 清除元素的默认的内外边距\n\n*{\n\npadding:0px;\n\nmargin:0px;\n\n}\n\n行内元素尽量只设置左边内外边距,因为上下内外边距不起作用\n\n\n# 去掉列表默认的样式\n\nlist-style:none;\n\n\n# 外边距合并\n\n上下:当上下相邻的两个块元素相遇时,如果上面的元素有margin-bottom,下面的有margin-top时,他们之间的垂直间距并不是bottom+top的值,而是取它们两值之间最大的值.如涉及到上下两个外边距合并尽量只给一个元素外边距\n\n嵌套塌陷:当一个父盒子中的子盒子,想于父盒子之间有距离,但又不想把父盒子撑开,如我们在子盒子中设置外边距top,则会出现父盒子向下移动了,但子盒子还是与父盒子没有间距.\n\n解决方案:\n\n\t1. 在父盒子中指定一个top边框  如:border-top:1px solid transport;\n\t2. 给父盒子指定一个内边距top  如:padding-top : 1px;\n\t3. 给父盒子添加overflow:hidden;\n\n\n\n# 盒子模型布局稳定性\n\n我们优先使用 width > padding > margin\n\n因为width最稳定 padding会影响盒子大小 margin会出现合并塌陷问题\n\n\n# 盒子阴影\n\n * box-shadow:水平阴影大小 垂直阴影大小 模糊大小 阴影大小 阴影颜色 内/外阴影(默认为外);\n * 如:box-shadow:2px 2px 2px 2px #000;\n\n\n# 子盒子可以比父盒子大\n\n如子盒子没有指定大小则会继承父盒子大小\n\n但是子盒子可以指定比父盒子大的值\n\n\n# 浮动(float)\n\nfloat:使元素浮动 如:float:left;\n\n * none\n\n * left 向左浮动\n * right 向右浮动\n\n浮动元素会漂浮在普通流上面,不占位置,脱标\n\n特性:float属性会改变元素的display属性 与行内块相似但是没有之间没有缝隙 如果父盒子装不下浮动元素 则会自动换行\n\n一般我们会给浮动的元素套一个标准流的父盒子,减少对其他的标准流的影响\n\n浮动元素只会影响后面的标准流元素 并不会影响前面的标准流元素\n\n\n# 浮动元素与父盒子关系\n\n如果父盒子有边框 浮动元素不会压着边框\n\n如果父盒子有内边距 也不会压着内边距\n\n\n# 清除浮动\n\n清除浮动主要是为了解决父元素因为子元素引起内部高度为0的问题.清除浮动后父元素就会根据浮动的子元素自动检测高度.父元素有了高度就不会影响下面的标准流.\n\nclear:both; 清除两侧的浮动影响 也可单独清除 left或right浮动\n\n# 额外标签法\n\n在浮动元素后额外添加一个空的标签用来清除浮动\n\n缺点:结构化被打乱\n\n# 父级添加overflow属性\n\n给父元素添加 overflow: hidden | auto | scroll都可以实现清除浮动\n\n缺点:超出父盒子的内容被切割\n\n# 使用伪元素清除浮动\n\n父级元素添加伪元素\n\n.box:after {\n\tcontent:"";\n\tdisplay:block;\n\theight:0\n\tclear:both;\n\tvisibility:hidden;\n}\n.box{\n    *zoom:1;  /*IE6、7专有的 */\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 双伪元素清除浮动\n\n.box:before,\n.box:after {\n\tcontent:"";\n\tdisplay:table;\n}\n.box:aftr {\n    clear:both;\n}\n.box {\n    *zoom:1;  /*IE6、7专有的 */\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 什么时候用清除浮动\n\n 1. 父元素没 有高度\n 2. 子元素浮动\n 3. 影响下面布局,我们应该清除浮动.\n\n\n# CSS属性书写顺序\n\n 1. 布局定位属性 display/position/float/clear/visibility/overflow(display建议第一个写)\n 2. 自身属性 width/height/margin/padding/border/background\n 3. 文本属性 color/font/text-decoration/text-align/vertical-align/white-space/break-word\n 4. 其他属性 content/cursor/border-radius/box-shadow/text-shadow/background-linear-gradient\n\n\n# 定位(position)\n\n将盒子定在某一个位置,可以自由的漂浮在其他盒子(包括标准流和浮动)的上面\n\n\n# 边偏移\n\n通过top、bottom、left、right属性定义元素的边偏移：\n\n\n\n\n# 定位模式\n\nposition：定位模式 如：position：absolute；\n\n * static 静态定位\n * relative 相对定位\n * absolute 绝对定位\n * fixed 固定定位\n * \n\n# 静态定位(static )\n\n静态定位就是默认定位方式,为无定位,没有边偏移\n\n静态定位几乎不用\n\n# 相对定位(relative)\n\n相对定位是元素对于 原本在标准流的位置来定位\n\n原来在标准流的位置 加上 偏移值\n\n特点:在原来标准流的位置上继续占有,虽然位置已发生改变,但后面的标准流还是无法占用它原来的位置\n\n# 绝对定位(absolute)\n\n绝对定位是元素以带定位的父级元素来移动位置的\n\n如果父级元素没有定位,则以浏览器为准定位\n\n如果父级有定位,则以父级的位置 加上偏移值来定位\n\n不一定需要父级,只要是嵌套包含此子元素则以此元素为基准偏移定位(就近原则)\n\n特点:绝对定位后的元素,在原来标准流中不占用位置,后面的元素可以占用它原来的位置\n\n子绝父相:子级是绝对定位,父级要用相对定位\n\n# 固定定位(fixed)\n\n固定定位是绝对定位的一种特殊形式.\n\n 1. 不会占用原来的位置\n 2. 不随着滚动条滚动\n 3. 跟父级没有关系\n\n# 绝对定位的盒子居中\n\n绝对定位的盒子是不能使用margin:auto 设置居中对齐的\n\n 1. left:50% (定位到父盒子的右边的一半,其他方向都可以用此方法)\n 2. margin-left: 负的自身宽度的一半; (回退自身宽度的一半)\n\n# 堆叠顺序(z-index)\n\n在定位中,可能会出现盒子层叠,后面的盒子会压掉前面的盒子,后来者居上\n\n我们可以使用z-index 层叠等级属性来调整盒子的堆叠顺序,默认值为0\n\n# 定位改变display 属性\n\n一个行内盒子,如果加了浮动、固定定位和绝对定位，不用转换，就可以给这盒子直接设置宽度和高度\n\n# 绝对定位和浮动不会触发外边距合并\n\n之前嵌套盒子给子盒子margin-top会出现塌陷问题,我们通过添加父盒子边框\n\n现在我们可以添加添加浮动 或者 绝对定位来解决这个问题,给予父元素或子元素都可以\n\n\n# 布局总结\n\n 1. 标准流\n    * 可以让盒子上下排列 或者 左右排列\n 2. 浮动\n    * 可以让多个块级元素 一行显示 或者 左右对齐盒子 浮动的盒子就是按照顺序左右排列\n 3. 定位\n    * 定位最大的特点是有层叠的概念,就是可以让多个盒子 前后 叠压来显示. 但是美国盒子需要测量数值\n\n\n# display 显示\n\n隐藏元素\n\ndisplay: none;\n\n隐藏元素,不保留位置\n\n显示元素\n\ndisplay:black;\n\n配合a标签的hover 可以控制隐藏显示\n\n\n# visibility 可见\n\n隐藏元素\n\nvisibility :hidden;\n\n隐藏元素,保留原本的位置\n\n显示元素\n\nvisibility:visible;\n\n\n# overflow 溢出\n\n检索或者设置当前对象的内容超出其指定的高度及宽度管理如何显示\n\noverflow:\n\n * visible 不剪切内容也不添加滚动条 (默认)\n * hidden 不显示超出的内容,超出部分隐藏掉 (溢出隐藏)\n * scrool 不管超出多少都显示滚动条\n * auto 超出自动显示滚动条,不超出不显示\n\n\n# CSS用户界面样式\n\n\n# 鼠标样式 cursor\n\n设置该元素当光标指向时 鼠标样式的形状\n\n * default 默认光标\n * pointer 小手\n * move 移动\n * text 文本\n * not-allowed 禁止\n\n\n# 轮廓线 outline\n\n输入框input被用户选中时高亮提示的轮廓,一般我们都是取消轮廓\n\noutline:0; 或 outline:none;\n\n\n# 防止拖拽文本域 resize\n\n文本域默认的大小用户是可以拖拽,我们一般取消拖拽\n\nresize:none;\n\n\n# 垂直对齐 vertical-align\n\nvertical-align 它只针对行内元素 或 行内块元素 起作用 对 块元素不起任何作用\n\n * baseline 基线对齐 默认图片会与文字基线对齐\n * middle 垂直居中 图片与文字中线对齐\n * top 顶部对齐 图片与文字的顶线对齐\n * bottom 底部对齐\n\n\n# 去除图片底部空白缝隙\n\n因为图片默认是与文字的基线对齐,而有的字是超出的基线系统会自动空出这部分来,我们只需要把图片与文字基线以外的对齐即可以解决\n\nvertical-align:bottom/top/middle;\n\n第二种的方法,把图片的行内块元素转成块元素\n\ndisplay:block;\n\n\n# 溢出的文字省略号显示\n\n 1. 强制文字在一行内显示 white-space:nowrap;\n 2. 超出部分隐藏 overflow:hidden;\n 3. 超出部分用省略号代替 text-overflow:ellipsis;\n\n\n# white-space 强制文字一行显示\n\nwhite-space:\n\n * normal 当文字超出时,自动换行 (默认)\n * nowrap 强制一行显示文字 除了br\n\n\n# text-overflow 文字用省略号代表超出的部分\n\ntext-overflow:\n\n * clip 不显示省略号(默认)\n * ellipsis 超出部分以省略号代替\n\n\n# CSS精灵技术 (sprite)\n\nCSS 精灵其实是将网页的一些背景图像整合到一张大图中(精灵图),浏览器请求服务端只需要请求一次就可以返回多个背景,不同位置的小图,根据精灵图的位置来设置\n\nbackground-position:x坐标 y坐标; (一般背景定位都是负值 )\n\n先用一个父盒子把整张精灵图加载,后面再用子盒子定位到需要的图\n\n\n# 滑动门\n\n 1. a标签设置背景图左侧 padding撑开合适的左侧宽度使文字与左侧有内边距\n 2. span标签设置背景图右侧 padding撑开右侧背景内边距\n\n\n# margin负值\n\n2个浮动的盒子边框如何合并,我们使用负值的margin值就可以解决此问题,如:margin-left:-1px;\n\n\n# CSS三角形\n\n当一个盒子高度和宽度为0时,我们给予指定的大小边框,它会以三角形的形式呈现出来\n\n注意盒子的4个边框都要指定大小,我们只给予需要的边框保留,其他我们设置为transport 透明色就好',normalizedContent:'# css\n\ncss:页面美化和布局控制\n\n\n# 概念:cascading style sheets 层叠样式表\n\n * 层叠:多个样式可以作用在同一个html的元素上,同时生效\n\n 2. 好处\n\n * 功能强大\n * 将内容展示和样式控制分离 1. 降低耦合度.解耦 2. 让分工协作更容易 3. 提高开发效率\n\n 3. css的使用:css与html结合方式\n\n * 内联样式:在标签内使用style生效指定css代码\n\n\x3c!-- 方式一  内联方式  在标签内使用style属性指定css代码 --\x3e\n<div style="color: cornflowerblue;">hello world</div>\n\n\n1\n2\n\n * 内部样式:在head标签内,定义style标签,style标签的标签体内容就是css代码\n\n \x3c!-- 方式二 内部样式\n            * 在head标签内,定义style标签,style标签的标签体内容就是css代码--\x3e\n<style>\n        div{\n            color: blueviolet;\n        }\n\n    </style>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * 外部样式: 1.在外部定义css资源文件 2.在head标签内,定义link标签,引用外部的css资源\n\n<link rel="stylesheet" href="css/外部.css">\n \x3c!-- 通过<link>标签 引用外部css标签  rel为  href为css路径  --\x3e\n\n\n1\n2\n\n\n\n# css语法\n\n * 格式:\n   \n   * 选择器{\n     \n     属性名1:属性值1;\n     \n     属性名2:属性值2;\n     \n     …\n     \n     }\n   \n   * 选择器:筛选具有相似特征的元素 每一对属性需要使用分号隔开,最后一对可以不加;\n\n\n# 选择器:筛选具有相似特征的元素\n\n * 分类:\n   \n   * 基础选择器\n     \n     * id选择器:选择具体id属性值的元素,建议在一个html页面中id值唯一 语法:#id属性值{}\n       \n       #div1{  \x3c!--id选择器 根据id选择html标签 --\x3e\n                   color: blue;\n               }\n       \n       \n       1\n       2\n       3\n       \n     \n     * 元素选择器:选择具有相同标签名称的元素 语法:标签名称{} 注意:id选择器优先级高于元素选择器\n     \n     div{  \x3c!-- 元素选择器  根据标签名选择html标签 --\x3e\n                 color: rebeccapurple;\n             }\n     \n     \n     \n     1\n     2\n     3\n     4\n     \n     * 类选择器:选择具有相同class的属性值的元素 语法:.class属性值{} 注意:类选择器优先级高于元素选择器 id>类>元素\n     \n     .cls1{ \x3c!-- 类选择器  根据类选择html标签 --\x3e\n                 color: cornflowerblue;\n             }\n     \n     \n     1\n     2\n     3\n     \n\n\n# 多类名\n\n一个标签只有拥有一个class属性\n\n但class属性能写多个名 用空格分开 每个类名都能单独调用css\n\n<span class="class1 class2"> </span>\n\n\n1\n\n\nid在页面中不能重复 class可以\n\n\n# 扩展选择器\n\n * 选择所有元素: 语法:*{}\n\n * 并集选择器: 语法:选择器1,选择器2{} 如: .box1,.box2 {}\n\n * 交集选择器: 只选择该标签并且是此选择 器的 语法:标签名选择器{} 如:div.box1{}\n\n * 后代选择器: 又称包含选择器 筛选选择器1元素下的选择器元素\n\n * 语法: 父级 子级{} 如: .box box2{}\n\n * 父选择器: 筛选选择器2的父元素选择器1 只选下一级的元素 有其他标签包含都不选择 语法:选择器1 > 选择器2{} 如: .box1 > p{}\n\n * 属性选择器:选择元素名称,属性名=属性值的元素 语法:元素名称[属性名=“属性值”]{}\n\n * 伪类选择器:选择一些元素具有的状态 语法:元素:状态{} 注意状态顺序 否则会出不必要的错误 lvha\n   \n   * 状态:\n     \n     1. link:初始化的状态\n     \n     2. visited:被访问过的状态\n     \n     3. hover:鼠标悬浮状态\n     \n     4. active:在在访问状态\n\n\n# 标签显示模式(display)\n\n\n# 什么是标签显示模式\n\n标签一什么方式进行显示如:div自己占一行称为块元素,span一行可以放多个称为行内元素\n\n\n# 块级元素\n\n常见的块元素有<h1>~<h6>、<p>、<div>、<ul>、<ol>、<li>等\n\np和h、dt里面尽量不要放块元素\n\n特点：\n\n1. 独占一行\n2. 宽度默认是父级的100%\n3. 里面可以块元素和行内元素\n\n\n\n# 行内元素\n\n常见的行内元素有<a>、<strong>、<b>、<em>、<i>、<del>、<s>、<ins>、<i>、<span>等\n\n特点：\n\n 1. 高、宽直接设置是没有任何效果的\n 2. 默认的宽度就是它本身内容的宽度\n 3. 行内元素只能容纳文本或其他行内元素\n 4. 链接标签不能再包含链接标签\n 5. 特殊情况a里面可以放块元素，但是给a转换一下块级模式更好\n\n\n# 行内块元素\n\n在行内元素中有几个特殊的标签<img>、<input>、<td>\n\n特点：\n\n1. 和相邻的行内元素在一行上，但是之间会有空白缝隙。一行内可以显示多个\n2. 默认宽度就是它本身内容的宽度\n3. 高度、行高、外边距和内边距都可以控制\n\n\n\n# 标签显示模式转换\n\n * 块转内：dispaly：inline\n * 行内转块：display：block\n * 块、行内元素转元素为行内块：display：inline-block\n\n\n# （font）字体\n\n * font-size:字体大小 如: font-size:10px;\n * font-family:设置字体 如:font-family:"微软雅黑";\n * 可以同时指定多个字体用逗号分隔,当浏览器不支持前面的字体时自动选择后面的字体 如:font-famil: arial,"宋体","微软雅黑"; 如字体名有特殊字符或中文要加引号,可以用unicode写中文字体名\n * \n * font-weight:字体粗细 如:font-weight:normal; normal 默认值(不加粗) bold(加粗) 100-900 400为默认 700为加粗\n * font-style:设置斜体 如:font-style:italic; normal默认值 italic斜体\n * 可以混成综合使用 如: font: italic 700 20px "宋体"; 按style-weight-size-family顺序.\n * \n\n\n# ccs外观属性 文本(text)\n\n * color:文本颜色 如:color:"#fff" 支持英文颜色 十六进制 rgb rgba\n\n * text-align:文本对其方式 如:text-align:center; left左对齐 right 右对齐 center 居中\n\n * line-height:行间距 如:line-height:28px;\n\n * text-indext:首行缩进 如:text-indext:2em; 可以写px但建议写em,em是倍数关系,1em为1个字\n\n * text-decoration:文本的装饰 一般用来取消超链接的下划线 如:text-decoration:none;\n\n * \n\n\n# 行高(line-height)\n\n中文的行高为 第一行文字的底线到第二行文字的底线的高度\n\n英文的行高为 第一行英文的基线到第二行文字的基线高度\n\n行高 = 上距离 + 内容高度 + 下距离,文字的行高等于盒子的高度则会垂直居中(单行文本)\n\n 1. 行高等于高度 文字会垂直居中\n 2. 行高大于高度 文字会偏下\n 3. 行高小于高度 文字会偏上\n\n\n# 背景(background)\n\n * background-color: 背景颜色 如：background-color:red\n * background-image:背景图片 如:background-image:url(图片路径);路径不需要引号\n * background-repeat:背景平铺 如：background-repeat:no-rpeat;\n   * repeat 默认的在x轴和y轴上平铺\n   * no-repeat 不平铺\n   * repeat-x 在x轴平铺\n   * repeat-y 在y轴平铺\n * background-position:背景位置 如：backg-position：10px center；\n   * 可以填百分百 或xy坐标\n   * 两个位置也可以写绝对位置 top、center、bottom、left、right\n * background-attachment：背景附着 如:background-attachment：scroll;\n   * scroll 背景图随着对象内容滚动\n   * fixed 背景图像固定\n * 背景简写: background:背景颜色 背景图片地址 背景平铺 背景滚动 背景位置;(没有位置顺序,可省略不要的参数)\n * 背景半透明:background-color :rgba(0,0,0,.3);\n\n\n# css 三大特性\n\n\n# css 层叠性\n\n后面的样式覆盖掉前面的样式 不会发生冲突 样式不冲突不会进行覆盖\n\n\n# css 继承性\n\n子标签会继承父标签的某些样式,如文本颜色和字号\n\n想要设置一个可继承的属性,只需将它应用于父元素即可.\n\n子元素可以继承父元素的样式有(text-，font-，line-这些元素开头的可以继承，以及color属性)\n\n\n# css 优先级\n\n\n\n继承＜元素＜class＜id＜标签内的style＜!important(将此css属性设为最重要)\n\n\n# 权重叠加\n\n如:.box a{} 此时的权重为 0,0,1,0 + 0,0,0,1 =0,0,1,1\n\n如超过10不会进制\n\n\n# 盒子模型\n\n\n# 边框\n\n * border-style: 边框的线风格 如:border-style:solid\n   * solid 实线\n   * node 没有边框 默认值\n   * dashed 虚线\n   * dotted 点线\n * border-color:边框颜色 如:border-color:red\n   * border-width:边框宽度 border-width:10px\n   * 边框简写: border: 1px solid red; (没有顺序)\n   * 指定单方向的边框 如:border-top-width:10px\n   * border-radius:边框圆角过渡 如:border-radius:10px; 或 border-radius:50%; 矩形是高度的一半.\n     * border-top-left\n     * border-top-right\n     * borde-bottom-left\n     * border-bottom-right\n     * 简写 border:左上角 右上角 左下角 右下角\n   * border-collapse:表格 单元格 th 合并相邻的边框 如:border-collapse:collapse;\n\n\n# 尺寸\n\n * width:宽度\n * height:高度\n\n\n# 内边距\n\npadding用来设置内边距,指边框与内容之间的距离\n\n 1. 内容与边框有了距离\n 2. 盒子会变大\n\n * padding-left:左内边距\n * padding-right:右内边距\n * padding-top:上内边距\n * padding-bottom:下内边距\n * padding:简写\n   * padding: 1px 2px 3px 0px(上 右 下 左 顺时针)\n   * padding: 20px 上下左右都为20px内边距\n   * padding:10px 20px 上下为10px 左右为20px (如无指定则自动为对称的内边距)\n   * padding:10px 20px 30px 上为10 左右为20 下为30 的内边距\n\n盒子的实际大小 = 内容的宽度 高度 + 内边距 + 边框 = 盒子的大小 - 内边距 -边框 =css中设置盒子的大小\n\n也可以设置盒子属性 box-sizing: border-box;使得盒子实际大小等于边框+内边距的大小\n\n * padding不影响盒子大小情况 特殊情况\n   * 如果没有给一个盒子指定宽度,此时再给这个盒子padding则不会撑开盒子\n\n\n# 外边距\n\nmargin属性用于设置外边距.margin就是控制盒子和盒子之间的距离\n\n * margin-left:左外边距\n * margin-right:右外边距\n * margin-top:上外边距\n * margin-bottom:下外边距\n\n\n# 让块级盒子居中水平对齐\n\n 1. 必须要设置宽度\n 2. 设置margin左右 为auto 上下auto没有效果\n\n\n# 让文字居中和盒子居中\n\n 1. 让盒子内文字居中是 text-align:center,并且还可以让行内元素和行内块居中对齐\n 2. 块级盒子居中 左右外边距auto\n\n\n# 插入图片和背景图片的区别\n\n 1. 插入图片 我们用的最多 比如产品展示 移动位置只能靠盒模型 padding margin\n 2. 背景图片我们一般用于小图片背景 或者超大背景图片 只能通过background-position\n\n\n# 清除元素的默认的内外边距\n\n*{\n\npadding:0px;\n\nmargin:0px;\n\n}\n\n行内元素尽量只设置左边内外边距,因为上下内外边距不起作用\n\n\n# 去掉列表默认的样式\n\nlist-style:none;\n\n\n# 外边距合并\n\n上下:当上下相邻的两个块元素相遇时,如果上面的元素有margin-bottom,下面的有margin-top时,他们之间的垂直间距并不是bottom+top的值,而是取它们两值之间最大的值.如涉及到上下两个外边距合并尽量只给一个元素外边距\n\n嵌套塌陷:当一个父盒子中的子盒子,想于父盒子之间有距离,但又不想把父盒子撑开,如我们在子盒子中设置外边距top,则会出现父盒子向下移动了,但子盒子还是与父盒子没有间距.\n\n解决方案:\n\n\t1. 在父盒子中指定一个top边框  如:border-top:1px solid transport;\n\t2. 给父盒子指定一个内边距top  如:padding-top : 1px;\n\t3. 给父盒子添加overflow:hidden;\n\n\n\n# 盒子模型布局稳定性\n\n我们优先使用 width > padding > margin\n\n因为width最稳定 padding会影响盒子大小 margin会出现合并塌陷问题\n\n\n# 盒子阴影\n\n * box-shadow:水平阴影大小 垂直阴影大小 模糊大小 阴影大小 阴影颜色 内/外阴影(默认为外);\n * 如:box-shadow:2px 2px 2px 2px #000;\n\n\n# 子盒子可以比父盒子大\n\n如子盒子没有指定大小则会继承父盒子大小\n\n但是子盒子可以指定比父盒子大的值\n\n\n# 浮动(float)\n\nfloat:使元素浮动 如:float:left;\n\n * none\n\n * left 向左浮动\n * right 向右浮动\n\n浮动元素会漂浮在普通流上面,不占位置,脱标\n\n特性:float属性会改变元素的display属性 与行内块相似但是没有之间没有缝隙 如果父盒子装不下浮动元素 则会自动换行\n\n一般我们会给浮动的元素套一个标准流的父盒子,减少对其他的标准流的影响\n\n浮动元素只会影响后面的标准流元素 并不会影响前面的标准流元素\n\n\n# 浮动元素与父盒子关系\n\n如果父盒子有边框 浮动元素不会压着边框\n\n如果父盒子有内边距 也不会压着内边距\n\n\n# 清除浮动\n\n清除浮动主要是为了解决父元素因为子元素引起内部高度为0的问题.清除浮动后父元素就会根据浮动的子元素自动检测高度.父元素有了高度就不会影响下面的标准流.\n\nclear:both; 清除两侧的浮动影响 也可单独清除 left或right浮动\n\n# 额外标签法\n\n在浮动元素后额外添加一个空的标签用来清除浮动\n\n缺点:结构化被打乱\n\n# 父级添加overflow属性\n\n给父元素添加 overflow: hidden | auto | scroll都可以实现清除浮动\n\n缺点:超出父盒子的内容被切割\n\n# 使用伪元素清除浮动\n\n父级元素添加伪元素\n\n.box:after {\n\tcontent:"";\n\tdisplay:block;\n\theight:0\n\tclear:both;\n\tvisibility:hidden;\n}\n.box{\n    *zoom:1;  /*ie6、7专有的 */\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 双伪元素清除浮动\n\n.box:before,\n.box:after {\n\tcontent:"";\n\tdisplay:table;\n}\n.box:aftr {\n    clear:both;\n}\n.box {\n    *zoom:1;  /*ie6、7专有的 */\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 什么时候用清除浮动\n\n 1. 父元素没 有高度\n 2. 子元素浮动\n 3. 影响下面布局,我们应该清除浮动.\n\n\n# css属性书写顺序\n\n 1. 布局定位属性 display/position/float/clear/visibility/overflow(display建议第一个写)\n 2. 自身属性 width/height/margin/padding/border/background\n 3. 文本属性 color/font/text-decoration/text-align/vertical-align/white-space/break-word\n 4. 其他属性 content/cursor/border-radius/box-shadow/text-shadow/background-linear-gradient\n\n\n# 定位(position)\n\n将盒子定在某一个位置,可以自由的漂浮在其他盒子(包括标准流和浮动)的上面\n\n\n# 边偏移\n\n通过top、bottom、left、right属性定义元素的边偏移：\n\n\n\n\n# 定位模式\n\nposition：定位模式 如：position：absolute；\n\n * static 静态定位\n * relative 相对定位\n * absolute 绝对定位\n * fixed 固定定位\n * \n\n# 静态定位(static )\n\n静态定位就是默认定位方式,为无定位,没有边偏移\n\n静态定位几乎不用\n\n# 相对定位(relative)\n\n相对定位是元素对于 原本在标准流的位置来定位\n\n原来在标准流的位置 加上 偏移值\n\n特点:在原来标准流的位置上继续占有,虽然位置已发生改变,但后面的标准流还是无法占用它原来的位置\n\n# 绝对定位(absolute)\n\n绝对定位是元素以带定位的父级元素来移动位置的\n\n如果父级元素没有定位,则以浏览器为准定位\n\n如果父级有定位,则以父级的位置 加上偏移值来定位\n\n不一定需要父级,只要是嵌套包含此子元素则以此元素为基准偏移定位(就近原则)\n\n特点:绝对定位后的元素,在原来标准流中不占用位置,后面的元素可以占用它原来的位置\n\n子绝父相:子级是绝对定位,父级要用相对定位\n\n# 固定定位(fixed)\n\n固定定位是绝对定位的一种特殊形式.\n\n 1. 不会占用原来的位置\n 2. 不随着滚动条滚动\n 3. 跟父级没有关系\n\n# 绝对定位的盒子居中\n\n绝对定位的盒子是不能使用margin:auto 设置居中对齐的\n\n 1. left:50% (定位到父盒子的右边的一半,其他方向都可以用此方法)\n 2. margin-left: 负的自身宽度的一半; (回退自身宽度的一半)\n\n# 堆叠顺序(z-index)\n\n在定位中,可能会出现盒子层叠,后面的盒子会压掉前面的盒子,后来者居上\n\n我们可以使用z-index 层叠等级属性来调整盒子的堆叠顺序,默认值为0\n\n# 定位改变display 属性\n\n一个行内盒子,如果加了浮动、固定定位和绝对定位，不用转换，就可以给这盒子直接设置宽度和高度\n\n# 绝对定位和浮动不会触发外边距合并\n\n之前嵌套盒子给子盒子margin-top会出现塌陷问题,我们通过添加父盒子边框\n\n现在我们可以添加添加浮动 或者 绝对定位来解决这个问题,给予父元素或子元素都可以\n\n\n# 布局总结\n\n 1. 标准流\n    * 可以让盒子上下排列 或者 左右排列\n 2. 浮动\n    * 可以让多个块级元素 一行显示 或者 左右对齐盒子 浮动的盒子就是按照顺序左右排列\n 3. 定位\n    * 定位最大的特点是有层叠的概念,就是可以让多个盒子 前后 叠压来显示. 但是美国盒子需要测量数值\n\n\n# display 显示\n\n隐藏元素\n\ndisplay: none;\n\n隐藏元素,不保留位置\n\n显示元素\n\ndisplay:black;\n\n配合a标签的hover 可以控制隐藏显示\n\n\n# visibility 可见\n\n隐藏元素\n\nvisibility :hidden;\n\n隐藏元素,保留原本的位置\n\n显示元素\n\nvisibility:visible;\n\n\n# overflow 溢出\n\n检索或者设置当前对象的内容超出其指定的高度及宽度管理如何显示\n\noverflow:\n\n * visible 不剪切内容也不添加滚动条 (默认)\n * hidden 不显示超出的内容,超出部分隐藏掉 (溢出隐藏)\n * scrool 不管超出多少都显示滚动条\n * auto 超出自动显示滚动条,不超出不显示\n\n\n# css用户界面样式\n\n\n# 鼠标样式 cursor\n\n设置该元素当光标指向时 鼠标样式的形状\n\n * default 默认光标\n * pointer 小手\n * move 移动\n * text 文本\n * not-allowed 禁止\n\n\n# 轮廓线 outline\n\n输入框input被用户选中时高亮提示的轮廓,一般我们都是取消轮廓\n\noutline:0; 或 outline:none;\n\n\n# 防止拖拽文本域 resize\n\n文本域默认的大小用户是可以拖拽,我们一般取消拖拽\n\nresize:none;\n\n\n# 垂直对齐 vertical-align\n\nvertical-align 它只针对行内元素 或 行内块元素 起作用 对 块元素不起任何作用\n\n * baseline 基线对齐 默认图片会与文字基线对齐\n * middle 垂直居中 图片与文字中线对齐\n * top 顶部对齐 图片与文字的顶线对齐\n * bottom 底部对齐\n\n\n# 去除图片底部空白缝隙\n\n因为图片默认是与文字的基线对齐,而有的字是超出的基线系统会自动空出这部分来,我们只需要把图片与文字基线以外的对齐即可以解决\n\nvertical-align:bottom/top/middle;\n\n第二种的方法,把图片的行内块元素转成块元素\n\ndisplay:block;\n\n\n# 溢出的文字省略号显示\n\n 1. 强制文字在一行内显示 white-space:nowrap;\n 2. 超出部分隐藏 overflow:hidden;\n 3. 超出部分用省略号代替 text-overflow:ellipsis;\n\n\n# white-space 强制文字一行显示\n\nwhite-space:\n\n * normal 当文字超出时,自动换行 (默认)\n * nowrap 强制一行显示文字 除了br\n\n\n# text-overflow 文字用省略号代表超出的部分\n\ntext-overflow:\n\n * clip 不显示省略号(默认)\n * ellipsis 超出部分以省略号代替\n\n\n# css精灵技术 (sprite)\n\ncss 精灵其实是将网页的一些背景图像整合到一张大图中(精灵图),浏览器请求服务端只需要请求一次就可以返回多个背景,不同位置的小图,根据精灵图的位置来设置\n\nbackground-position:x坐标 y坐标; (一般背景定位都是负值 )\n\n先用一个父盒子把整张精灵图加载,后面再用子盒子定位到需要的图\n\n\n# 滑动门\n\n 1. a标签设置背景图左侧 padding撑开合适的左侧宽度使文字与左侧有内边距\n 2. span标签设置背景图右侧 padding撑开右侧背景内边距\n\n\n# margin负值\n\n2个浮动的盒子边框如何合并,我们使用负值的margin值就可以解决此问题,如:margin-left:-1px;\n\n\n# css三角形\n\n当一个盒子高度和宽度为0时,我们给予指定的大小边框,它会以三角形的形式呈现出来\n\n注意盒子的4个边框都要指定大小,我们只给予需要的边框保留,其他我们设置为transport 透明色就好',charsets:{cjk:!0}},{title:"html+css网页",frontmatter:{title:"html+css网页",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/5bc756/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/06.html+css%E7%BD%91%E9%A1%B5.html",relativePath:"前端/01.html/06.html+css网页.md",key:"v-3ce17358",path:"/pages/5bc756/",headers:[{level:2,title:"favicon 图标",slug:"favicon-图标",normalizedTitle:"favicon 图标",charIndex:17},{level:2,title:"搜索引擎优化",slug:"搜索引擎优化",normalizedTitle:"搜索引擎优化",charIndex:128},{level:2,title:"字体图标",slug:"字体图标",normalizedTitle:"字体图标",charIndex:495},{level:3,title:"声明字体",slug:"声明字体",normalizedTitle:"声明字体",charIndex:612},{level:2,title:"logo优化",slug:"logo优化",normalizedTitle:"logo优化",charIndex:826},{level:2,title:"css过渡动画",slug:"css过渡动画",normalizedTitle:"css过渡动画",charIndex:1037},{level:2,title:"表单获得焦点",slug:"表单获得焦点",normalizedTitle:"表单获得焦点",charIndex:1726}],headersStr:"favicon 图标 搜索引擎优化 字体图标 声明字体 logo优化 css过渡动画 表单获得焦点",content:"# html+css网页\n\n\n# favicon 图标\n\n使用link 中的 favicon 来引用 ,需要在网页文件同文件夹下的一个favicon.ico文件图标\n\nhttps://www.bitbug.net/index.php 在线转换地址\n\n\n# 搜索引擎优化\n\n# title 标题\n\n首页标题:网站名(产品名) - 网站的介绍\n\n建议不超28个汉字\n\n# description 网站说明\n\n使用meta标签 name为description content 介绍内容 建议不超120个汉字\n\n如:\n\n<meta name=\"description\" content=\"介绍内容\">\n\n\n1\n\n\n# keywords 关键字\n\n使用meat标签 name为keywords content为关键字 关键字应该限制在6-8个\n\n如:\n\n <meta\n      name=\"keywords\"\n      content=\"网上购物,网上商城,手机,笔记本,电脑,MP3,CD,VCD,DV,相机,数码,配件,手表,存储卡,品优购\"\n    /> \n\n\n1\n2\n3\n4\n\n\n\n# 字体图标\n\n利用自定义字体库,生成个性化字体图标,减少图片请求\n\nhttp://icomoon.io 无序创建项目 添加即可下载字体\n\nhttps://www.iconfont.cn/ 阿里的矢量字库 得创建项目才能下载字体\n\n\n# 声明字体\n\n在css中声明字体才能使用自定义字体\n\n@font-face {\n\tfont-family:'自定义名称';\n    src:url('字体路径'),url('路径2');\n    font-weight:normal;\n    font-style:normal;\n}\n/* 引用字体 */\ndiv {\n    font-family:'自定义名称';\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# logo优化\n\nlogo里面放一个h1标签 提高搜索引擎权重\n\nh1里面放一个a标签 ,可以返回到首页,再给a标签一个大小 和 背景图片\n\na标签 里面要放文字 (网站名称,但文字不要显示出来)\n\n * 方法1:用text-iudent 移动盒子外面 再用overflow:hidden 移除溢出部分\n * 方法2:用font-size:0;\n\n最后 给链接一个 title属性 这样鼠标放上去就可以看到提示文字\n\n\n# css过渡动画\n\ncss过渡动画用transition关键字,经常与:hover配合使用\n\n * transition-property:过渡css属性的名称\n * transition-duration:过渡效果花费的时间,默认为0\n   * 注意要写时间单位 可以填写 s 或者ms\n * transition-timing-function:过渡动画的时间曲线,默认为ease\n   * \n * transition-delay:过渡效果何时开始,默认为0\n * transition: 要过渡的属性 花费时间 运动曲线 何时开始;\n   * 可以拥有多个过渡动画,用逗号隔开不能使用分号,分号代表transition结束.\n   * all属性,我们transition中过渡的属性可以用all关键字,代表着所有的属性都拥有过渡\n   * 过渡动画写在元素本身上\n\n如:\n\ndiv {\n\twidth:400px;\n    height:150px;\n    backround-clor;pink;\n    transition: width 1s ease 0s,height 1s linear 0s;\n    /*当鼠标放在div上时宽度变成800px,transition过渡动画属性为width宽度,时间为1s\n    \t单元素可以拥有多个过渡动画,用逗号隔开.\n    */\n}\n\ndiv:hover {\n    width:800px;\n    height:700px\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 表单获得焦点\n\n当此元素获取光标焦点的情况\n\n伪类选择器 中的 focus , 失去焦点复原\n\n如:\n\ninput:focus {\n    background-color:pink;\n}\n\n\n1\n2\n3\n",normalizedContent:"# html+css网页\n\n\n# favicon 图标\n\n使用link 中的 favicon 来引用 ,需要在网页文件同文件夹下的一个favicon.ico文件图标\n\nhttps://www.bitbug.net/index.php 在线转换地址\n\n\n# 搜索引擎优化\n\n# title 标题\n\n首页标题:网站名(产品名) - 网站的介绍\n\n建议不超28个汉字\n\n# description 网站说明\n\n使用meta标签 name为description content 介绍内容 建议不超120个汉字\n\n如:\n\n<meta name=\"description\" content=\"介绍内容\">\n\n\n1\n\n\n# keywords 关键字\n\n使用meat标签 name为keywords content为关键字 关键字应该限制在6-8个\n\n如:\n\n <meta\n      name=\"keywords\"\n      content=\"网上购物,网上商城,手机,笔记本,电脑,mp3,cd,vcd,dv,相机,数码,配件,手表,存储卡,品优购\"\n    /> \n\n\n1\n2\n3\n4\n\n\n\n# 字体图标\n\n利用自定义字体库,生成个性化字体图标,减少图片请求\n\nhttp://icomoon.io 无序创建项目 添加即可下载字体\n\nhttps://www.iconfont.cn/ 阿里的矢量字库 得创建项目才能下载字体\n\n\n# 声明字体\n\n在css中声明字体才能使用自定义字体\n\n@font-face {\n\tfont-family:'自定义名称';\n    src:url('字体路径'),url('路径2');\n    font-weight:normal;\n    font-style:normal;\n}\n/* 引用字体 */\ndiv {\n    font-family:'自定义名称';\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# logo优化\n\nlogo里面放一个h1标签 提高搜索引擎权重\n\nh1里面放一个a标签 ,可以返回到首页,再给a标签一个大小 和 背景图片\n\na标签 里面要放文字 (网站名称,但文字不要显示出来)\n\n * 方法1:用text-iudent 移动盒子外面 再用overflow:hidden 移除溢出部分\n * 方法2:用font-size:0;\n\n最后 给链接一个 title属性 这样鼠标放上去就可以看到提示文字\n\n\n# css过渡动画\n\ncss过渡动画用transition关键字,经常与:hover配合使用\n\n * transition-property:过渡css属性的名称\n * transition-duration:过渡效果花费的时间,默认为0\n   * 注意要写时间单位 可以填写 s 或者ms\n * transition-timing-function:过渡动画的时间曲线,默认为ease\n   * \n * transition-delay:过渡效果何时开始,默认为0\n * transition: 要过渡的属性 花费时间 运动曲线 何时开始;\n   * 可以拥有多个过渡动画,用逗号隔开不能使用分号,分号代表transition结束.\n   * all属性,我们transition中过渡的属性可以用all关键字,代表着所有的属性都拥有过渡\n   * 过渡动画写在元素本身上\n\n如:\n\ndiv {\n\twidth:400px;\n    height:150px;\n    backround-clor;pink;\n    transition: width 1s ease 0s,height 1s linear 0s;\n    /*当鼠标放在div上时宽度变成800px,transition过渡动画属性为width宽度,时间为1s\n    \t单元素可以拥有多个过渡动画,用逗号隔开.\n    */\n}\n\ndiv:hover {\n    width:800px;\n    height:700px\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 表单获得焦点\n\n当此元素获取光标焦点的情况\n\n伪类选择器 中的 focus , 失去焦点复原\n\n如:\n\ninput:focus {\n    background-color:pink;\n}\n\n\n1\n2\n3\n",charsets:{cjk:!0}},{title:"CSS3",frontmatter:{title:"CSS3",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/54a5fa/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/07.CSS3.html",relativePath:"前端/01.html/07.CSS3.md",key:"v-2751208f",path:"/pages/54a5fa/",headers:[{level:2,title:"属性选择器",slug:"属性选择器",normalizedTitle:"属性选择器",charIndex:11},{level:2,title:"结构伪类选择器",slug:"结构伪类选择器",normalizedTitle:"结构伪类选择器",charIndex:571},{level:2,title:"伪类选择器",slug:"伪类选择器",normalizedTitle:"伪类选择器",charIndex:71},{level:2,title:"2D转换",slug:"_2d转换",normalizedTitle:"2d转换",charIndex:1180},{level:3,title:"2D移动",slug:"_2d移动",normalizedTitle:"2d移动",charIndex:1189},{level:3,title:"2D旋转",slug:"_2d旋转",normalizedTitle:"2d旋转",charIndex:1459},{level:3,title:"2D缩放",slug:"_2d缩放",normalizedTitle:"2d缩放",charIndex:1617},{level:3,title:"缩写",slug:"缩写",normalizedTitle:"缩写",charIndex:1821},{level:2,title:"动画",slug:"动画",normalizedTitle:"动画",charIndex:1884},{level:3,title:"动画常用属性",slug:"动画常用属性",normalizedTitle:"动画常用属性",charIndex:2288},{level:2,title:"3D转换",slug:"_3d转换",normalizedTitle:"3d转换",charIndex:2938},{level:3,title:"3D移动",slug:"_3d移动",normalizedTitle:"3d移动",charIndex:2947},{level:3,title:"透视 perspective",slug:"透视-perspective",normalizedTitle:"透视 perspective",charIndex:3069},{level:3,title:"3D旋转 rotate3d",slug:"_3d旋转-rotate3d",normalizedTitle:"3d旋转 rotate3d",charIndex:3142},{level:3,title:"3D呈现 transform-style",slug:"_3d呈现-transform-style",normalizedTitle:"3d呈现 transform-style",charIndex:3343},{level:2,title:"浏览器私有前缀",slug:"浏览器私有前缀",normalizedTitle:"浏览器私有前缀",charIndex:3500},{level:2,title:"背景线性渐变",slug:"背景线性渐变",normalizedTitle:"背景线性渐变",charIndex:3616}],headersStr:"属性选择器 结构伪类选择器 伪类选择器 2D转换 2D移动 2D旋转 2D缩放 缩写 动画 动画常用属性 3D转换 3D移动 透视 perspective 3D旋转 rotate3d 3D呈现 transform-style 浏览器私有前缀 背景线性渐变",content:'# CSS3\n\n\n# 属性选择器\n\n元素[属性]{} 选择指定元素中拥有该属性的元素 如:button[disable]{};\n\n属性选择器和伪类选择器与类选择器权重一致\n\n<style>\n      button {\n        cursor: pointer;\n      }\n\n      button[disabled] {\n        cursor: default;\n      }\n    </style>\n\n<button>提交</button>\n    <button>提交</button>\n    <button disabled="disabled">提交</button>\n    <button disabled="disabled">提交</button>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * E[att]; 选择具有att属性的E元素\n * E[att="val"]; 选择具有att属性并且值等于val的E元素\n * E[att^="val"] 选择具有att属性并且值以val开头的E元素\n * E[att$="val"] 选择具有att属性并且值以val结尾的E元素\n * E[att*="val"] 选择具有att属性并且值包含val的E元素\n\n\n# 结构伪类选择器\n\n选择该元素中 指定元素的第几个 如:FU E:nth-child(1){}\n\n * E:first-child 匹配父元素中第一个子元素E 可以省略E元素表示all元素 如:span:first-child\n * E:last-child 匹配最后一个E元素\n * E:nth-child(n) 匹配第n个E元素\n   * n可以是数字\n   * n可以是关键字 如even偶数 odd奇数\n   * n可以公式,从0开始计算 如:2n偶数 从0 2 4 6, n+5 从 5 6 7 8.-n+5 从 4 3 2 1\n   * 第0个元素和超出元素的个数会被忽略,元素从1开始\n   * 如匹配指定元素且父类元素中第n孩子并不是元素则不会匹配选择到\n * E:first-of-type 选择指定类型的第一个 如:span:first-of-type\n * E:last-of-type 选择指定类型的最后一个\n * E:nth-of-type(n) 与子类选择一致\n * &:not(:last-child) 排除最后一个子类\n\n\n# 伪类选择器\n\n * ::before 在元素内部的前面插入内容\n * ::after 在元素内部的后面插入内容\n * 都必须拥有content属性\n * 会在内容的前或后,创建一个看不见的元素,但是属于行内元素\n * 伪元素和标签选择器一样,权重为1\n\n\n# 2D转换\n\n\n# 2D移动\n\n使用transform:关键字,不会影响到其他元素的位置, 对行内标签没有效果,如果使用百分百单位是相对于自身元素的尺寸(50%是自身尺寸的一半).\n\n * translate(x,y); 在元素原来的位置移动相对应的x y坐标 如:transform:translate(x,y);\n * translateX(n);\n * translateY(n);\n * 让盒子重置居中方法:首先子绝父相,left:50%,然回自身大小的一半,也就是translate中的transform:translate(50%,0);\n\n\n# 2D旋转\n\ntransform:rotate(度数);\n\n度数单位为deg 如:rotate(45deg)\n\n角度为正时顺时针,逆时为逆时针\n\ntransform-origin:x y ; 默认的旋转点为元素的中心点(50%,50%) ,支持方位名词top、bottom、left、right、center\n\n\n# 2D缩放\n\ntransform:scale(x,y);\n\nx,y指的是原来宽度/高度倍数,默认为1倍;\n\n如果宽度和高度变化的倍数一样可以简写为x 如:transform:scale(2);\n\n缩放不会影响其他盒子\n\n同样拥有中心点\n\ntransform-origin:x y ; 默认的缩放点为元素的中心点(50%,50%) ,支持方位名词top、bottom、left、right、center\n\n\n# 缩写\n\ntransform:translate() rotate() scale()\n\n强调顺序关系,位移总是在最前\n\n\n# 动画\n\n使用keyframes定义动画\n\n@keyframes 动画自定义名称 {\n    /*开始状态*/\n\t0% {\n\t\twidth:100px;\n\t}\n    /*结束状态*/\n\t100% {\n\t\twidth:200px;\n\t}\n}\n\n/*调用动画*/\ndiv {\n    width:200px;\n    height:200px;\n    \n    animation-name:动画名称;\n  \n    animation-duration:持续时间;\n        \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 动画序列\n\n动画有多个过程从0%-100%,也可以使用form,to代替0和100\n\n我们可以定义不同百分百的过程的动画一个动画中有多个过渡动画\n\n如:0%一个 25%一个 75% 100%\n\n\n# 动画常用属性\n\n * @keyframes: 规定动画\n * animation-name: 动画名称\n * animation-duration: 持续时间\n * animation-timing-function:动画的速度曲线 默认是ease\n   * linear 匀速\n   * ease 默认\n   * ease-in 动画由低速开始\n   * ease-out 低速结束\n   * ease-in-out 以低俗开始和结束\n   * steps() 指定了时间函数中的间隔数量(步长),切蛋糕把整个动画切分为若干份\n * animation-delay: 规定动画何时开始 默认是0 单位为s / ms\n * animation-iteration-count:动画播放次数 默认为1,infinite为无限循环\n * animation-direction: 规定动画是否在下一周期逆向播放 默认为normal, alternate是逆播放\n * animation-play-state: 规定动画播放或者暂停 默认为 running 暂停为pause\n * animation-fill-mode: 规定动画结束后状态 保持为forwards 回到起始位置为backwards\n * animation: 动画的简写\n   * animation: 动画名称 持续时间 运动曲线 何时开始 播放次数 是否反向 动画起始或结束状态\n   * 可以使用多个动画,用逗号隔开.\n\n\n# 3D转换\n\n\n# 3D移动\n\n * transform:translateX(x); 往x轴移动\n * translateY 往y移动\n * translateZ 往Z移动\n * transform:translate3d(x,y,z) 指定移动xyz\n\n\n# 透视 perspective\n\n写在被3D化元素的父盒子里\n\nperspective:500px;\n\n必须要有透视才能看到z轴变化的效果\n\n\n# 3D旋转 rotate3d\n\n * transform:rotateX(45deg); 沿x轴正向旋转45度\n * transform:rotateY(45deg); 沿y轴正向旋转45度\n * transform:rotateZ(45deg); 沿z轴正向旋转45度\n * transform:rotate3d(x,y,z,45deg); 沿自定义轴 旋转45度 1为标记此轴旋转 默认为0\n\n\n# 3D呈现 transform-style\n\n控制子元素是否开启三维立体环境,如果不开启 父盒子旋转之后 子盒子不会保留3d效果\n\ntransform:flat 默认子元素不开启3d立体空间\n\ntransform-style:preserve-3d; 子元素开启立体空间\n\n写在父级元素上, 但影响是子元素\n\n\n# 浏览器私有前缀\n\n-moz-: firefox 私有前缀\n\n-ms-: ie浏览器\n\n-webkit-: Safari 和chrome\n\n-o- : Opera\n\n如: -ms-border:1px solid #ccc;\n\n\n# 背景线性渐变\n\nbackground:linear-gradient(起始方向,颜色1,颜色2,....);\n\n * 起始方向为方位名词或度数 默认值为top\n * 可以指定左上角 用空格隔开 如: top left,颜色1,颜色2\n * 要加上浏览器的私有前缀 如:background:-webkit-linear-gradient(left,red,blue);',normalizedContent:'# css3\n\n\n# 属性选择器\n\n元素[属性]{} 选择指定元素中拥有该属性的元素 如:button[disable]{};\n\n属性选择器和伪类选择器与类选择器权重一致\n\n<style>\n      button {\n        cursor: pointer;\n      }\n\n      button[disabled] {\n        cursor: default;\n      }\n    </style>\n\n<button>提交</button>\n    <button>提交</button>\n    <button disabled="disabled">提交</button>\n    <button disabled="disabled">提交</button>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * e[att]; 选择具有att属性的e元素\n * e[att="val"]; 选择具有att属性并且值等于val的e元素\n * e[att^="val"] 选择具有att属性并且值以val开头的e元素\n * e[att$="val"] 选择具有att属性并且值以val结尾的e元素\n * e[att*="val"] 选择具有att属性并且值包含val的e元素\n\n\n# 结构伪类选择器\n\n选择该元素中 指定元素的第几个 如:fu e:nth-child(1){}\n\n * e:first-child 匹配父元素中第一个子元素e 可以省略e元素表示all元素 如:span:first-child\n * e:last-child 匹配最后一个e元素\n * e:nth-child(n) 匹配第n个e元素\n   * n可以是数字\n   * n可以是关键字 如even偶数 odd奇数\n   * n可以公式,从0开始计算 如:2n偶数 从0 2 4 6, n+5 从 5 6 7 8.-n+5 从 4 3 2 1\n   * 第0个元素和超出元素的个数会被忽略,元素从1开始\n   * 如匹配指定元素且父类元素中第n孩子并不是元素则不会匹配选择到\n * e:first-of-type 选择指定类型的第一个 如:span:first-of-type\n * e:last-of-type 选择指定类型的最后一个\n * e:nth-of-type(n) 与子类选择一致\n * &:not(:last-child) 排除最后一个子类\n\n\n# 伪类选择器\n\n * ::before 在元素内部的前面插入内容\n * ::after 在元素内部的后面插入内容\n * 都必须拥有content属性\n * 会在内容的前或后,创建一个看不见的元素,但是属于行内元素\n * 伪元素和标签选择器一样,权重为1\n\n\n# 2d转换\n\n\n# 2d移动\n\n使用transform:关键字,不会影响到其他元素的位置, 对行内标签没有效果,如果使用百分百单位是相对于自身元素的尺寸(50%是自身尺寸的一半).\n\n * translate(x,y); 在元素原来的位置移动相对应的x y坐标 如:transform:translate(x,y);\n * translatex(n);\n * translatey(n);\n * 让盒子重置居中方法:首先子绝父相,left:50%,然回自身大小的一半,也就是translate中的transform:translate(50%,0);\n\n\n# 2d旋转\n\ntransform:rotate(度数);\n\n度数单位为deg 如:rotate(45deg)\n\n角度为正时顺时针,逆时为逆时针\n\ntransform-origin:x y ; 默认的旋转点为元素的中心点(50%,50%) ,支持方位名词top、bottom、left、right、center\n\n\n# 2d缩放\n\ntransform:scale(x,y);\n\nx,y指的是原来宽度/高度倍数,默认为1倍;\n\n如果宽度和高度变化的倍数一样可以简写为x 如:transform:scale(2);\n\n缩放不会影响其他盒子\n\n同样拥有中心点\n\ntransform-origin:x y ; 默认的缩放点为元素的中心点(50%,50%) ,支持方位名词top、bottom、left、right、center\n\n\n# 缩写\n\ntransform:translate() rotate() scale()\n\n强调顺序关系,位移总是在最前\n\n\n# 动画\n\n使用keyframes定义动画\n\n@keyframes 动画自定义名称 {\n    /*开始状态*/\n\t0% {\n\t\twidth:100px;\n\t}\n    /*结束状态*/\n\t100% {\n\t\twidth:200px;\n\t}\n}\n\n/*调用动画*/\ndiv {\n    width:200px;\n    height:200px;\n    \n    animation-name:动画名称;\n  \n    animation-duration:持续时间;\n        \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 动画序列\n\n动画有多个过程从0%-100%,也可以使用form,to代替0和100\n\n我们可以定义不同百分百的过程的动画一个动画中有多个过渡动画\n\n如:0%一个 25%一个 75% 100%\n\n\n# 动画常用属性\n\n * @keyframes: 规定动画\n * animation-name: 动画名称\n * animation-duration: 持续时间\n * animation-timing-function:动画的速度曲线 默认是ease\n   * linear 匀速\n   * ease 默认\n   * ease-in 动画由低速开始\n   * ease-out 低速结束\n   * ease-in-out 以低俗开始和结束\n   * steps() 指定了时间函数中的间隔数量(步长),切蛋糕把整个动画切分为若干份\n * animation-delay: 规定动画何时开始 默认是0 单位为s / ms\n * animation-iteration-count:动画播放次数 默认为1,infinite为无限循环\n * animation-direction: 规定动画是否在下一周期逆向播放 默认为normal, alternate是逆播放\n * animation-play-state: 规定动画播放或者暂停 默认为 running 暂停为pause\n * animation-fill-mode: 规定动画结束后状态 保持为forwards 回到起始位置为backwards\n * animation: 动画的简写\n   * animation: 动画名称 持续时间 运动曲线 何时开始 播放次数 是否反向 动画起始或结束状态\n   * 可以使用多个动画,用逗号隔开.\n\n\n# 3d转换\n\n\n# 3d移动\n\n * transform:translatex(x); 往x轴移动\n * translatey 往y移动\n * translatez 往z移动\n * transform:translate3d(x,y,z) 指定移动xyz\n\n\n# 透视 perspective\n\n写在被3d化元素的父盒子里\n\nperspective:500px;\n\n必须要有透视才能看到z轴变化的效果\n\n\n# 3d旋转 rotate3d\n\n * transform:rotatex(45deg); 沿x轴正向旋转45度\n * transform:rotatey(45deg); 沿y轴正向旋转45度\n * transform:rotatez(45deg); 沿z轴正向旋转45度\n * transform:rotate3d(x,y,z,45deg); 沿自定义轴 旋转45度 1为标记此轴旋转 默认为0\n\n\n# 3d呈现 transform-style\n\n控制子元素是否开启三维立体环境,如果不开启 父盒子旋转之后 子盒子不会保留3d效果\n\ntransform:flat 默认子元素不开启3d立体空间\n\ntransform-style:preserve-3d; 子元素开启立体空间\n\n写在父级元素上, 但影响是子元素\n\n\n# 浏览器私有前缀\n\n-moz-: firefox 私有前缀\n\n-ms-: ie浏览器\n\n-webkit-: safari 和chrome\n\n-o- : opera\n\n如: -ms-border:1px solid #ccc;\n\n\n# 背景线性渐变\n\nbackground:linear-gradient(起始方向,颜色1,颜色2,....);\n\n * 起始方向为方位名词或度数 默认值为top\n * 可以指定左上角 用空格隔开 如: top left,颜色1,颜色2\n * 要加上浏览器的私有前缀 如:background:-webkit-linear-gradient(left,red,blue);',charsets:{cjk:!0}},{title:"移动布局",frontmatter:{title:"移动布局",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/dc74bd/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/08.%E7%A7%BB%E5%8A%A8%E5%B8%83%E5%B1%80.html",relativePath:"前端/01.html/08.移动布局.md",key:"v-0f768c01",path:"/pages/dc74bd/",headers:[{level:2,title:"视口 viewport",slug:"视口-viewport",normalizedTitle:"视口 viewport",charIndex:11},{level:3,title:"布局视口 layout viewport",slug:"布局视口-layout-viewport",normalizedTitle:"布局视口 layout viewport",charIndex:27},{level:3,title:"视觉视口 visual viewport",slug:"视觉视口-visual-viewport",normalizedTitle:"视觉视口 visual viewport",charIndex:88},{level:3,title:"理想视口 ideal viewport",slug:"理想视口-ideal-viewport",normalizedTitle:"理想视口 ideal viewport",charIndex:140},{level:2,title:"meta视口标签",slug:"meta视口标签",normalizedTitle:"meta视口标签",charIndex:163},{level:2,title:"二倍图",slug:"二倍图",normalizedTitle:"二倍图",charIndex:536},{level:2,title:"背景缩放 background-size",slug:"背景缩放-background-size",normalizedTitle:"背景缩放 background-size",charIndex:601},{level:2,title:"移动端 CSS初始化",slug:"移动端-css初始化",normalizedTitle:"移动端 css初始化",charIndex:811},{level:2,title:"CSS3 盒子模型 box-sizing",slug:"css3-盒子模型-box-sizing",normalizedTitle:"css3 盒子模型 box-sizing",charIndex:867},{level:2,title:"移动端特殊样式",slug:"移动端特殊样式",normalizedTitle:"移动端特殊样式",charIndex:997},{level:2,title:"移动端技术选型",slug:"移动端技术选型",normalizedTitle:"移动端技术选型",charIndex:1234},{level:2,title:"flex布局",slug:"flex布局",normalizedTitle:"flex布局",charIndex:1430},{level:3,title:"父项属性",slug:"父项属性",normalizedTitle:"父项属性",charIndex:1522},{level:3,title:"子项属性",slug:"子项属性",normalizedTitle:"子项属性",charIndex:2884},{level:2,title:"rem布局",slug:"rem布局",normalizedTitle:"rem布局",charIndex:3200},{level:3,title:"媒体查询",slug:"媒体查询",normalizedTitle:"媒体查询",charIndex:1371},{level:3,title:"rem和结合媒体查询",slug:"rem和结合媒体查询",normalizedTitle:"rem和结合媒体查询",charIndex:3867},{level:3,title:"px转rem插件",slug:"px转rem插件",normalizedTitle:"px转rem插件",charIndex:4206},{level:3,title:"引入资源",slug:"引入资源",normalizedTitle:"引入资源",charIndex:4253},{level:2,title:"Less",slug:"less",normalizedTitle:"less",charIndex:4493},{level:3,title:"Less 安装",slug:"less-安装",normalizedTitle:"less 安装",charIndex:4562},{level:3,title:"Less 变量",slug:"less-变量",normalizedTitle:"less 变量",charIndex:4803},{level:3,title:"Less编译",slug:"less编译",normalizedTitle:"less编译",charIndex:4980},{level:3,title:"Less嵌套",slug:"less嵌套",normalizedTitle:"less嵌套",charIndex:5070},{level:3,title:"Less运算",slug:"less运算",normalizedTitle:"less运算",charIndex:5414},{level:3,title:"Less导入其他less",slug:"less导入其他less",normalizedTitle:"less导入其他less",charIndex:5611},{level:2,title:"Bootstrap",slug:"bootstrap",normalizedTitle:"bootstrap",charIndex:5666}],headersStr:"视口 viewport 布局视口 layout viewport 视觉视口 visual viewport 理想视口 ideal viewport meta视口标签 二倍图 背景缩放 background-size 移动端 CSS初始化 CSS3 盒子模型 box-sizing 移动端特殊样式 移动端技术选型 flex布局 父项属性 子项属性 rem布局 媒体查询 rem和结合媒体查询 px转rem插件 引入资源 Less Less 安装 Less 变量 Less编译 Less嵌套 Less运算 Less导入其他less Bootstrap",content:'# 移动布局\n\n\n# 视口 viewport\n\n\n# 布局视口 layout viewport\n\n布局视口分辨率设置为980px,但一般元素很小,可以通过手动缩放网页\n\n\n# 视觉视口 visual viewport\n\n我们可以通过缩放来操作视觉视口,但不会影响布局视口\n\n\n# 理想视口 ideal viewport\n\n通过meta视口标签,告知网页,设备有多宽网页有多宽\n\n\n# meta视口标签\n\n <meta\n      name="viewport"\n      content="width=device-width, initial-scale=1.0,user-scalable=no,maxium-scale=1.0,minimum-scale=1.0"\n    />\n\n\n1\n2\n3\n4\n\n * width: 宽度设置是viewport宽度,特殊值为device-width 宽度为设备宽度\n * initial-scale: 初始缩放比 大于0的值\n * maximum-scale:最大缩放比 默认为1\n * minimum-scale:最小缩放比 默认为1\n * user-scaleable:用户是否可以缩放,yes或no (1或0)\n\n\n# 二倍图\n\npc端1px为1个物理像素\n\n而手机一般都为1px为2个物理像素\n\n一般移动端都给一个2X再给一个1倍的盒子装\n\n\n# 背景缩放 background-size\n\nbackground-size: 背景图片的宽度 高度;\n\n * 如只写一个参数会等比例缩放\n * 里面可以写百分百,相当于父盒子的尺寸\n * cover:把图扩展到足够大直到填满盒子,等比缩放,会丢失部分图像\n * contain:拉伸到高或宽直到填满高或宽就停止拉伸,可能会有部分空白图像无法填充\n * 移动端的2X图一般用一个1X的盒子装,缩小到1X放置到背景\n\n\n# 移动端 CSS初始化\n\nhttp://necolas.github.io/normalize.css/\n\n\n# CSS3 盒子模型 box-sizing\n\nbox-sizing:border-box; 把传统盒子转为CSS3盒子\n\n传统盒子 = width +border +padding 的值\n\nCSS3盒子 = padding和border不会影响盒子大小\n\n\n# 移动端特殊样式\n\n如:移动端转为css3模型\n\nbox-sizing:border-box;\n\n-webkit-box-sizing:border-box;\n\na标准在移动端点击会有高亮提示,我需要清除\n\n-webkit-tap-highlight-color:transparent;\n\n清除ios的按钮样式\n\n-webkit-appearance:none;\n\n禁止长按弹出菜单\n\nimg,a {\n\n-webkit-touch-callout:none;\n\n}\n\n\n# 移动端技术选型\n\n * 单独制作移动端页面\n   * 流式布局(百分百布局)\n     * max-width:最大宽度 或 max-height\n     * min-width:最小宽度 或min-height\n   * flex 弹性布局\n   * less+rem+媒体查询布局\n   * 混合布局\n * 响应式页面兼容移动端\n   * 媒体查询\n   * botstarp\n\n\n# flex布局\n\n当父盒子设为flex布局后,子元素的float、clear和vertical-align属性失效\n\n通过给父盒子添加flex属性 来控制子盒子的位置和排列方式\n\n\n# 父项属性\n\n# flex-direction:设置主轴的方向\n\n\n\n主轴与侧轴 x轴和y轴\n\n * row:(默认值) 从左到右 默认主轴为x 侧轴为y 子元素是跟着主轴来排列的\n   \n   * display: flex;\n     flex-direction: row;\n     \n     \n     1\n     2\n     \n\n * row-reverse:从右到左\n\n * column:从上到下 主轴为y 侧轴为x\n   \n   * display: flex;\n     flex-direction: column; /* 将主轴设置为y轴 元素从上到下排列 */\n     \n     \n     1\n     2\n     \n\n * column-revers:从下到上\n\n# justify-content 设置主轴上的子元素排列方式\n\n使用这个属性前一定要确认好主轴是哪个\n\n * flex-start:(默认值) 从头部开始排列 如果主轴是x轴,则从左到右\n\n * flex-end: 从尾部开始排列\n\n * center:在主轴居中对齐 (如果主轴x轴则是水平居中)\n\n * space-around:平分剩余空间\n   \n   * display: flex;\n     justify-content: space-around; /* 将父盒子平均切分给n个子盒子的空间 */\n     \n     \n     1\n     2\n     \n   \n   * \n\n * space-between: 先两边贴边 再平分剩余空间\n   \n   * \n\n# flex-wrap:设置子元素是否换行\n\n默认情况下,项目都是在轴线上.由flex-wrap定义,默认不换行,如果装不开,会缩小 子元素宽度,加入到轴线上排列\n\n * nowarp:(默认值) 不换行\n\n * warp:换行,当父盒子装不下后,自动换行\n   \n   * display: flex;\n     flex-wrap: wawrp; /* 子元素需要先设置大小才会换行 不然会自动分配大小 */\n     \n     \n     1\n     2\n     \n\n# align-items:设置侧轴上的子元素排列方式 (单行)\n\n * flex-start: 默认值 从上到下\n * flex-end:从下到上\n * center:挤在一起居中(垂直居中)\n * stretch:拉伸 (子盒子不要给高度,不然没效果)\n\n# align-content:设置侧轴上的子元素的排列方式(多行)\n\n要配合flex-wrap:warp 使用,不然flex项目无法多行排列\n\n * flex-start: 默认值 在侧轴的头到尾排列\n * flex-end:尾到头排列\n * center:在侧轴中间排列\n * space-around:子项在侧轴平分剩余空间\n * space-between:子项在侧轴先在头和尾,再平分剩余空间\n * stretch:设置子项元高度平分父元素高度\n\n# flex-flow 简写属性\n\n可以将设置主轴与是否换行进行简写\n\n如:flex-flow: column warp;\n\n\n# 子项属性\n\n# flex 属性\n\n定义子项分配剩余空间,占多少份\n\nflex: n; 默认为0 如:flex:2;\n\n也可以写百分百 为父盒子大小的多少\n\n如:flex:20%;\n\n# align-self 控制子项自己在侧轴上的排列方式\n\n让子项单独自己一个排列方式\n\n继承侧轴上的排序方式属性\n\n * flex-start: 默认值 从上到下\n * flex-end:从下到上\n * center:挤在一起居中(垂直居中)\n * stretch:拉伸 (子盒子不要给高度,不然没效果)\n\n# order 属性定义项目的排列顺序\n\n数值越小,排名越靠前,默认值为0\n\norder:n; 可以为负数 如:order:-1;\n\n\n# rem布局\n\nflex缺陷:文字和高度无法自适应 因为是按父盒子切份来分配的 而rem可以更好自适应文字和高度\n\nem相对应父元素 的字体大小 来等比缩放 font-size =12 px; 10em =120px 1em =12px;\n\nrem 相对应根元素(html) 设置font-size=12px ; 10rem=120px 1rem=12px\n\nrem的优点就是通过修改html里面的文字大小来\n\nrem 配合媒体查询 修改html中font-size 大小\n\n\n# 媒体查询\n\n@media 媒体类型 关键字 (媒体特征) {\n\nCSS-Code;\n\n}\n\n * 媒体类型\n   * all:用于所以设备\n   * print:用户打印和打印预览\n   * screen:用于电脑屏幕,平板电脑,智能手机等\n * 关键字\n   * and:可将多个媒体特性连接到一起\n   * not:排除某个媒体类型\n   * onle:指定某个媒体类型\n * 媒体特性\n   * width:页面可见宽度\n   * min-width:页面最小可见区域宽度\n   * max-width:最大可见区域宽度\n\n@media screen and print (max-width:800px){\n    body {\n        /*当max-width小于等于800时 背景颜色为粉色*/\n        background-color:pink;\n        \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# rem和结合媒体查询\n\nrem单位是跟着html变化的 媒体查询+rem 就可以实现不同设备宽度 实现自适应\n\n@media screen and (min-width:320px){\n    html {\n        font-size: 50px;\n    }\n}\n@media screen and (min-width:640px){\n    html {\n        font-size: 100px;\n    }\n}\n.div{\n    height: 1rem;\n    font-size: .5rem;\n    background-color: green;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# px转rem插件\n\nvscode下载px to rem & rpx (cssrem)\n\n\n# 引入资源\n\n当我们样式比较多 我们可以针对不同的设备使用不同的css文件\n\n根据link中判断设备的尺寸 引用不同的css文件\n\n<link rel="stylesheet" href="style320.css" media="screen and (min-width: 320px)">\n<link rel="stylesheet" href="style640.css" media="screen and (min-width: 640px)">\n\n\n1\n2\n\n\n\n# Less\n\nLess 是 一门 css 扩展语言 是CSS的预处理器\n\n常见的CSS预处理器: Sass Less Stylus\n\n\n# Less 安装\n\nnode.js环境下\n\n先安装node.js\n\nnpm install -g less\nlessc -v\n\n\n1\n2\n\n\n浏览器环境使用\n\n<link rel="stylesheet/less" type="text/css" href="styles.less" />\n<script src="//cdnjs.cloudflare.com/ajax/libs/less.js/3.11.1/less.min.js" ><\/script>\n\n\n1\n2\n\n\n\n# Less 变量\n\n必须@为前缀 不能包含特殊字符 不能以数字开头 大小写敏感\n\n/* @变量名:值 */\n@color: pink;\n@font14: 14px;\nbody {\n    background-color: @color; /* 使用变量 */\n    font-size: @font14;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Less编译\n\n我们需要把我们的less文件编译成css文件 才能在页面中使用\n\n我们可以在vscoe中下载 Easy LESS插件 保存less文件后自动生成css文件\n\n\n# Less嵌套\n\nLess中可以进行嵌套选择器 可以减少我们重复书写后代类选择器父类的名称\n\n后代选择器\n\n.div{\n    width: 200px\n        a{\n            color: red;\n            }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n伪类 交集选择器 伪元素选择器 我们需要在前面加上&拼接\n\n伪类选择器\n\na{\n    color: red;\n    &:hover{\n        color: blue;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n伪元素\n\na{\n    color: red;\n    &::before{\n        content: "";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Less运算\n\n任何数字 颜色 或 变量都可以参与四则运算\n\n@font14: 14px;\nbody {\n    background-color: #666 - #222;\n    font-size:(@font14 + 5) / 2;\n    width: 1.6rem + 5px; /* 如果两个数单位不相同 结果以第一数的单位为准  */\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Less导入其他less\n\n导入common.less\n\n@import \'common\'\n\n\n1\n\n\n\n# Bootstrap',normalizedContent:'# 移动布局\n\n\n# 视口 viewport\n\n\n# 布局视口 layout viewport\n\n布局视口分辨率设置为980px,但一般元素很小,可以通过手动缩放网页\n\n\n# 视觉视口 visual viewport\n\n我们可以通过缩放来操作视觉视口,但不会影响布局视口\n\n\n# 理想视口 ideal viewport\n\n通过meta视口标签,告知网页,设备有多宽网页有多宽\n\n\n# meta视口标签\n\n <meta\n      name="viewport"\n      content="width=device-width, initial-scale=1.0,user-scalable=no,maxium-scale=1.0,minimum-scale=1.0"\n    />\n\n\n1\n2\n3\n4\n\n * width: 宽度设置是viewport宽度,特殊值为device-width 宽度为设备宽度\n * initial-scale: 初始缩放比 大于0的值\n * maximum-scale:最大缩放比 默认为1\n * minimum-scale:最小缩放比 默认为1\n * user-scaleable:用户是否可以缩放,yes或no (1或0)\n\n\n# 二倍图\n\npc端1px为1个物理像素\n\n而手机一般都为1px为2个物理像素\n\n一般移动端都给一个2x再给一个1倍的盒子装\n\n\n# 背景缩放 background-size\n\nbackground-size: 背景图片的宽度 高度;\n\n * 如只写一个参数会等比例缩放\n * 里面可以写百分百,相当于父盒子的尺寸\n * cover:把图扩展到足够大直到填满盒子,等比缩放,会丢失部分图像\n * contain:拉伸到高或宽直到填满高或宽就停止拉伸,可能会有部分空白图像无法填充\n * 移动端的2x图一般用一个1x的盒子装,缩小到1x放置到背景\n\n\n# 移动端 css初始化\n\nhttp://necolas.github.io/normalize.css/\n\n\n# css3 盒子模型 box-sizing\n\nbox-sizing:border-box; 把传统盒子转为css3盒子\n\n传统盒子 = width +border +padding 的值\n\ncss3盒子 = padding和border不会影响盒子大小\n\n\n# 移动端特殊样式\n\n如:移动端转为css3模型\n\nbox-sizing:border-box;\n\n-webkit-box-sizing:border-box;\n\na标准在移动端点击会有高亮提示,我需要清除\n\n-webkit-tap-highlight-color:transparent;\n\n清除ios的按钮样式\n\n-webkit-appearance:none;\n\n禁止长按弹出菜单\n\nimg,a {\n\n-webkit-touch-callout:none;\n\n}\n\n\n# 移动端技术选型\n\n * 单独制作移动端页面\n   * 流式布局(百分百布局)\n     * max-width:最大宽度 或 max-height\n     * min-width:最小宽度 或min-height\n   * flex 弹性布局\n   * less+rem+媒体查询布局\n   * 混合布局\n * 响应式页面兼容移动端\n   * 媒体查询\n   * botstarp\n\n\n# flex布局\n\n当父盒子设为flex布局后,子元素的float、clear和vertical-align属性失效\n\n通过给父盒子添加flex属性 来控制子盒子的位置和排列方式\n\n\n# 父项属性\n\n# flex-direction:设置主轴的方向\n\n\n\n主轴与侧轴 x轴和y轴\n\n * row:(默认值) 从左到右 默认主轴为x 侧轴为y 子元素是跟着主轴来排列的\n   \n   * display: flex;\n     flex-direction: row;\n     \n     \n     1\n     2\n     \n\n * row-reverse:从右到左\n\n * column:从上到下 主轴为y 侧轴为x\n   \n   * display: flex;\n     flex-direction: column; /* 将主轴设置为y轴 元素从上到下排列 */\n     \n     \n     1\n     2\n     \n\n * column-revers:从下到上\n\n# justify-content 设置主轴上的子元素排列方式\n\n使用这个属性前一定要确认好主轴是哪个\n\n * flex-start:(默认值) 从头部开始排列 如果主轴是x轴,则从左到右\n\n * flex-end: 从尾部开始排列\n\n * center:在主轴居中对齐 (如果主轴x轴则是水平居中)\n\n * space-around:平分剩余空间\n   \n   * display: flex;\n     justify-content: space-around; /* 将父盒子平均切分给n个子盒子的空间 */\n     \n     \n     1\n     2\n     \n   \n   * \n\n * space-between: 先两边贴边 再平分剩余空间\n   \n   * \n\n# flex-wrap:设置子元素是否换行\n\n默认情况下,项目都是在轴线上.由flex-wrap定义,默认不换行,如果装不开,会缩小 子元素宽度,加入到轴线上排列\n\n * nowarp:(默认值) 不换行\n\n * warp:换行,当父盒子装不下后,自动换行\n   \n   * display: flex;\n     flex-wrap: wawrp; /* 子元素需要先设置大小才会换行 不然会自动分配大小 */\n     \n     \n     1\n     2\n     \n\n# align-items:设置侧轴上的子元素排列方式 (单行)\n\n * flex-start: 默认值 从上到下\n * flex-end:从下到上\n * center:挤在一起居中(垂直居中)\n * stretch:拉伸 (子盒子不要给高度,不然没效果)\n\n# align-content:设置侧轴上的子元素的排列方式(多行)\n\n要配合flex-wrap:warp 使用,不然flex项目无法多行排列\n\n * flex-start: 默认值 在侧轴的头到尾排列\n * flex-end:尾到头排列\n * center:在侧轴中间排列\n * space-around:子项在侧轴平分剩余空间\n * space-between:子项在侧轴先在头和尾,再平分剩余空间\n * stretch:设置子项元高度平分父元素高度\n\n# flex-flow 简写属性\n\n可以将设置主轴与是否换行进行简写\n\n如:flex-flow: column warp;\n\n\n# 子项属性\n\n# flex 属性\n\n定义子项分配剩余空间,占多少份\n\nflex: n; 默认为0 如:flex:2;\n\n也可以写百分百 为父盒子大小的多少\n\n如:flex:20%;\n\n# align-self 控制子项自己在侧轴上的排列方式\n\n让子项单独自己一个排列方式\n\n继承侧轴上的排序方式属性\n\n * flex-start: 默认值 从上到下\n * flex-end:从下到上\n * center:挤在一起居中(垂直居中)\n * stretch:拉伸 (子盒子不要给高度,不然没效果)\n\n# order 属性定义项目的排列顺序\n\n数值越小,排名越靠前,默认值为0\n\norder:n; 可以为负数 如:order:-1;\n\n\n# rem布局\n\nflex缺陷:文字和高度无法自适应 因为是按父盒子切份来分配的 而rem可以更好自适应文字和高度\n\nem相对应父元素 的字体大小 来等比缩放 font-size =12 px; 10em =120px 1em =12px;\n\nrem 相对应根元素(html) 设置font-size=12px ; 10rem=120px 1rem=12px\n\nrem的优点就是通过修改html里面的文字大小来\n\nrem 配合媒体查询 修改html中font-size 大小\n\n\n# 媒体查询\n\n@media 媒体类型 关键字 (媒体特征) {\n\ncss-code;\n\n}\n\n * 媒体类型\n   * all:用于所以设备\n   * print:用户打印和打印预览\n   * screen:用于电脑屏幕,平板电脑,智能手机等\n * 关键字\n   * and:可将多个媒体特性连接到一起\n   * not:排除某个媒体类型\n   * onle:指定某个媒体类型\n * 媒体特性\n   * width:页面可见宽度\n   * min-width:页面最小可见区域宽度\n   * max-width:最大可见区域宽度\n\n@media screen and print (max-width:800px){\n    body {\n        /*当max-width小于等于800时 背景颜色为粉色*/\n        background-color:pink;\n        \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# rem和结合媒体查询\n\nrem单位是跟着html变化的 媒体查询+rem 就可以实现不同设备宽度 实现自适应\n\n@media screen and (min-width:320px){\n    html {\n        font-size: 50px;\n    }\n}\n@media screen and (min-width:640px){\n    html {\n        font-size: 100px;\n    }\n}\n.div{\n    height: 1rem;\n    font-size: .5rem;\n    background-color: green;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# px转rem插件\n\nvscode下载px to rem & rpx (cssrem)\n\n\n# 引入资源\n\n当我们样式比较多 我们可以针对不同的设备使用不同的css文件\n\n根据link中判断设备的尺寸 引用不同的css文件\n\n<link rel="stylesheet" href="style320.css" media="screen and (min-width: 320px)">\n<link rel="stylesheet" href="style640.css" media="screen and (min-width: 640px)">\n\n\n1\n2\n\n\n\n# less\n\nless 是 一门 css 扩展语言 是css的预处理器\n\n常见的css预处理器: sass less stylus\n\n\n# less 安装\n\nnode.js环境下\n\n先安装node.js\n\nnpm install -g less\nlessc -v\n\n\n1\n2\n\n\n浏览器环境使用\n\n<link rel="stylesheet/less" type="text/css" href="styles.less" />\n<script src="//cdnjs.cloudflare.com/ajax/libs/less.js/3.11.1/less.min.js" ><\/script>\n\n\n1\n2\n\n\n\n# less 变量\n\n必须@为前缀 不能包含特殊字符 不能以数字开头 大小写敏感\n\n/* @变量名:值 */\n@color: pink;\n@font14: 14px;\nbody {\n    background-color: @color; /* 使用变量 */\n    font-size: @font14;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# less编译\n\n我们需要把我们的less文件编译成css文件 才能在页面中使用\n\n我们可以在vscoe中下载 easy less插件 保存less文件后自动生成css文件\n\n\n# less嵌套\n\nless中可以进行嵌套选择器 可以减少我们重复书写后代类选择器父类的名称\n\n后代选择器\n\n.div{\n    width: 200px\n        a{\n            color: red;\n            }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n伪类 交集选择器 伪元素选择器 我们需要在前面加上&拼接\n\n伪类选择器\n\na{\n    color: red;\n    &:hover{\n        color: blue;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n伪元素\n\na{\n    color: red;\n    &::before{\n        content: "";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# less运算\n\n任何数字 颜色 或 变量都可以参与四则运算\n\n@font14: 14px;\nbody {\n    background-color: #666 - #222;\n    font-size:(@font14 + 5) / 2;\n    width: 1.6rem + 5px; /* 如果两个数单位不相同 结果以第一数的单位为准  */\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# less导入其他less\n\n导入common.less\n\n@import \'common\'\n\n\n1\n\n\n\n# bootstrap',charsets:{cjk:!0}},{title:"vue 补充",frontmatter:{title:"vue 补充",date:"2022-03-18T00:55:26.000Z",permalink:"/pages/6af871/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/00.vue%20%E8%A1%A5%E5%85%85.html",relativePath:"前端/02.Vue2/00.vue 补充.md",key:"v-a103715e",path:"/pages/6af871/",headers:[{level:2,title:"获取input中的值",slug:"获取input中的值",normalizedTitle:"获取input中的值",charIndex:13},{level:2,title:"V-once",slug:"v-once",normalizedTitle:"v-once",charIndex:180},{level:2,title:"计算属性 computed",slug:"计算属性-computed",normalizedTitle:"计算属性 computed",charIndex:257},{level:2,title:"监控 watch",slug:"监控-watch",normalizedTitle:"监控 watch",charIndex:419},{level:2,title:":class",slug:"class",normalizedTitle:":class",charIndex:600},{level:2,title:":Style",slug:"style",normalizedTitle:":style",charIndex:644},{level:2,title:"MVVM模型",slug:"mvvm模型",normalizedTitle:"mvvm模型",charIndex:707},{level:2,title:"虚拟DOM DIFF算法",slug:"虚拟dom-diff算法",normalizedTitle:"虚拟dom diff算法",charIndex:871},{level:2,title:"template",slug:"template",normalizedTitle:"template",charIndex:1033},{level:2,title:"实例属性",slug:"实例属性",normalizedTitle:"实例属性",charIndex:1159},{level:2,title:"$ref",slug:"ref",normalizedTitle:"$ref",charIndex:1215},{level:2,title:"$mount",slug:"mount",normalizedTitle:"$mount",charIndex:1301},{level:2,title:"component 组件全局注册",slug:"component-组件全局注册",normalizedTitle:"component 组件全局注册",charIndex:1345},{level:2,title:"components 组件本地(局部)注册",slug:"components-组件本地-局部-注册",normalizedTitle:"components 组件本地(局部)注册",charIndex:1547},{level:2,title:"生命周期",slug:"生命周期",normalizedTitle:"生命周期",charIndex:1651},{level:2,title:"props",slug:"props",normalizedTitle:"props",charIndex:1407}],headersStr:"获取input中的值 V-once 计算属性 computed 监控 watch :class :Style MVVM模型 虚拟DOM DIFF算法 template 实例属性 $ref $mount component 组件全局注册 components 组件本地(局部)注册 生命周期 props",content:'# vue 补充\n\n\n# 获取input中的值\n\n<input @input="change"/>\n    \nmethods:{\n    // 内置参数 默认传回当前事件对象的值\n    change(event){\n        console.log(event.target.value)\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# V-once\n\n只改变一次这个值,data之后发生改变也不更新dom中的差值表达式的值\n\n<p v-once>{{data}}</p>\n\n\n1\n\n\n\n# 计算属性 computed\n\n计算属性是一个能将计算结果缓存起来的属性,不用每次调用而去重新生成。一些常用的函数，可以缓存起来，在调用时直接使用缓存中的过程（结果），以此来提高效率。\n\ncomputed 虽然写是一个方法（函数），但调用时，调用的computed中的属性。计算属性会观测内部数据变化而变化而重新求值\n\n\n# 监控 watch\n\n监控vue中某个属性的值的变化，发生变化时自动调用方法\n\ndata:{\n    title:"hahaha",\n    data:"cscs"\n},\nwatch:{\n    title:function(){},\n    data:function(newValue,oldValue){}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# :class\n\n:class 可以绑定多个值 使用数组 ["fl","fr"]\n\n\n# :Style\n\n:style="{backgroundCloro:data}"\n\n必须为驼峰命名法,必须用表达式包裹\n\n\n# MVVM模型\n\nModel View ViewModel 是 MVC 模式的改进版\n\n前端中 JS对象为Model 页面为 View 两者做到了最大限度的分离\n\n将 两者关联起来就是ViewModel 它是桥梁\n\nViewModel 负责把model的数据同步到view显示 还负责把view的数据同步回model\n\n\n\n\n# 虚拟DOM DIFF算法\n\n在普通js中我们操作dom元素,在通过html转为DOM数,并且通过document.getElementByid来找到该元素,去修改DOM树的节点.\n\n而在Vue中是直接修改页面中DOM元素,此时该元素虚拟的DOM,通过DIFF算法来哦计算出虚拟的DOM与原来的DOM的区别来进行修改\n\n\n# template\n\n模板标签\n\n在vue中经常用模板标签来包裹\n\ntemplate标签天生不可见,使用v-show:为设置样式 display:none 没有意义,只能使用v-if\n\ntemplate必须要一个根元素<div>,否则不能实例化\n\n\n# 实例属性\n\nvue全局暴露属性,我可以通过$加属性名,调用vue中的数据\n\n如:this.$data this.$ref\n\n\n# $ref\n\n相当于标签中id,不可重复, 只要该标签绑定了ref属性则可以通过this.$refs.(ref名称),就可以快速修改某个元素的内容\n\n\n# $mount\n\n相当于挂载(加载) el,实现了页面元素和vue对象的动态绑定\n\n\n# component 组件全局注册\n\n在当前实例中,注册一个组件,可以在里面定义 template data methods props 等等\n\n本地注册只绑定(挂载)vue的html才能使用组件\n\n通过Vue.component("组件名",{\n\nprops:[]\n\ndata(){},\n\nmethods(){},\n\ntemplate :"使用<组件>标签时解析这里的标签 预先设置好"\n\n})\n\n\n# components 组件本地(局部)注册\n\n通过vue实例中components 注册 组件名\n\n只能在这个vue实例中使用注册过的组件\n\n如:\n\ncomponents: { ImgList },\n\n\n# 生命周期\n\n\n\n一个vue对象会经历初始化、创建、绑定、更新、销毁等阶段，不同的阶段都会触发不同的生命周期hook\n\n 1. beforeCreate 创建实例之前的钩子\n 2. created 实例创建完成后执行的钩子\n 3. beforeMount 将编译完的html挂载到Vue的虚拟DOM时触发的钩子\n 4. mounted 编译好的Html挂载到页面完成后执行的钩子\n 5. beforeUpdate 绑定的虚拟DOM更新之前的钩子\n 6. updateed 更新之后的钩子\n 7. beforeUmmount 实例销毁之前的钩子\n 8. ummounted 实例销毁后的钩子\n\nimport { createApp } from \'vue\'\nconst app = createApp({})\n\n// 注册\napp.directive(\'my-directive\', {\n  // 指令是具有一组生命周期的钩子：\n  // 在绑定元素的 attribute 或事件监听器被应用之前调用\n  created() {},\n  // 在绑定元素的父组件挂载之前调用\n  beforeMount() {},\n  // 绑定元素的父组件被挂载时调用\n  mounted() {},\n  // 在包含组件的 VNode 更新之前调用\n  beforeUpdate() {},\n  // 在包含组件的 VNode 及其子组件的 VNode 更新之后调用\n  updated() {},\n  // 在绑定元素的父组件卸载之前调用\n  beforeUnmount() {},\n  // 卸载绑定元素的父组件时调用\n  unmounted() {}\n})\n\n// 注册 (功能指令)\napp.directive(\'my-directive\', () => {\n  // 这将被作为 `mounted` 和 `updated` 调用\n})\n\n// getter, 如果已注册，则返回指令定义\nconst myDirective = app.directive(\'my-directive\')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# props\n\n父传子\n\n子组件声明\n\nprops: {\n    传的参数名: {\n      type: 参数的类型,\n      require: true  //不能空\n    }\n  },\n\n\n1\n2\n3\n4\n5\n6\n\n\n父组件 注册 子组件 在标签中传参\n\n注册组件\n\n components: {\n    ArtcleList\n  }\n\n\n1\n2\n3\n\n\n传参\n\n<artcle-list :channel="channel"\n      />\n\n\n1\n2\n',normalizedContent:'# vue 补充\n\n\n# 获取input中的值\n\n<input @input="change"/>\n    \nmethods:{\n    // 内置参数 默认传回当前事件对象的值\n    change(event){\n        console.log(event.target.value)\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# v-once\n\n只改变一次这个值,data之后发生改变也不更新dom中的差值表达式的值\n\n<p v-once>{{data}}</p>\n\n\n1\n\n\n\n# 计算属性 computed\n\n计算属性是一个能将计算结果缓存起来的属性,不用每次调用而去重新生成。一些常用的函数，可以缓存起来，在调用时直接使用缓存中的过程（结果），以此来提高效率。\n\ncomputed 虽然写是一个方法（函数），但调用时，调用的computed中的属性。计算属性会观测内部数据变化而变化而重新求值\n\n\n# 监控 watch\n\n监控vue中某个属性的值的变化，发生变化时自动调用方法\n\ndata:{\n    title:"hahaha",\n    data:"cscs"\n},\nwatch:{\n    title:function(){},\n    data:function(newvalue,oldvalue){}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# :class\n\n:class 可以绑定多个值 使用数组 ["fl","fr"]\n\n\n# :style\n\n:style="{backgroundcloro:data}"\n\n必须为驼峰命名法,必须用表达式包裹\n\n\n# mvvm模型\n\nmodel view viewmodel 是 mvc 模式的改进版\n\n前端中 js对象为model 页面为 view 两者做到了最大限度的分离\n\n将 两者关联起来就是viewmodel 它是桥梁\n\nviewmodel 负责把model的数据同步到view显示 还负责把view的数据同步回model\n\n\n\n\n# 虚拟dom diff算法\n\n在普通js中我们操作dom元素,在通过html转为dom数,并且通过document.getelementbyid来找到该元素,去修改dom树的节点.\n\n而在vue中是直接修改页面中dom元素,此时该元素虚拟的dom,通过diff算法来哦计算出虚拟的dom与原来的dom的区别来进行修改\n\n\n# template\n\n模板标签\n\n在vue中经常用模板标签来包裹\n\ntemplate标签天生不可见,使用v-show:为设置样式 display:none 没有意义,只能使用v-if\n\ntemplate必须要一个根元素<div>,否则不能实例化\n\n\n# 实例属性\n\nvue全局暴露属性,我可以通过$加属性名,调用vue中的数据\n\n如:this.$data this.$ref\n\n\n# $ref\n\n相当于标签中id,不可重复, 只要该标签绑定了ref属性则可以通过this.$refs.(ref名称),就可以快速修改某个元素的内容\n\n\n# $mount\n\n相当于挂载(加载) el,实现了页面元素和vue对象的动态绑定\n\n\n# component 组件全局注册\n\n在当前实例中,注册一个组件,可以在里面定义 template data methods props 等等\n\n本地注册只绑定(挂载)vue的html才能使用组件\n\n通过vue.component("组件名",{\n\nprops:[]\n\ndata(){},\n\nmethods(){},\n\ntemplate :"使用<组件>标签时解析这里的标签 预先设置好"\n\n})\n\n\n# components 组件本地(局部)注册\n\n通过vue实例中components 注册 组件名\n\n只能在这个vue实例中使用注册过的组件\n\n如:\n\ncomponents: { imglist },\n\n\n# 生命周期\n\n\n\n一个vue对象会经历初始化、创建、绑定、更新、销毁等阶段，不同的阶段都会触发不同的生命周期hook\n\n 1. beforecreate 创建实例之前的钩子\n 2. created 实例创建完成后执行的钩子\n 3. beforemount 将编译完的html挂载到vue的虚拟dom时触发的钩子\n 4. mounted 编译好的html挂载到页面完成后执行的钩子\n 5. beforeupdate 绑定的虚拟dom更新之前的钩子\n 6. updateed 更新之后的钩子\n 7. beforeummount 实例销毁之前的钩子\n 8. ummounted 实例销毁后的钩子\n\nimport { createapp } from \'vue\'\nconst app = createapp({})\n\n// 注册\napp.directive(\'my-directive\', {\n  // 指令是具有一组生命周期的钩子：\n  // 在绑定元素的 attribute 或事件监听器被应用之前调用\n  created() {},\n  // 在绑定元素的父组件挂载之前调用\n  beforemount() {},\n  // 绑定元素的父组件被挂载时调用\n  mounted() {},\n  // 在包含组件的 vnode 更新之前调用\n  beforeupdate() {},\n  // 在包含组件的 vnode 及其子组件的 vnode 更新之后调用\n  updated() {},\n  // 在绑定元素的父组件卸载之前调用\n  beforeunmount() {},\n  // 卸载绑定元素的父组件时调用\n  unmounted() {}\n})\n\n// 注册 (功能指令)\napp.directive(\'my-directive\', () => {\n  // 这将被作为 `mounted` 和 `updated` 调用\n})\n\n// getter, 如果已注册，则返回指令定义\nconst mydirective = app.directive(\'my-directive\')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# props\n\n父传子\n\n子组件声明\n\nprops: {\n    传的参数名: {\n      type: 参数的类型,\n      require: true  //不能空\n    }\n  },\n\n\n1\n2\n3\n4\n5\n6\n\n\n父组件 注册 子组件 在标签中传参\n\n注册组件\n\n components: {\n    artclelist\n  }\n\n\n1\n2\n3\n\n\n传参\n\n<artcle-list :channel="channel"\n      />\n\n\n1\n2\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"JavaScript",frontmatter:{title:"JavaScript",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/e5e5b8/",categories:["前端","Html"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/01.html/09.JavaScript.html",relativePath:"前端/01.html/09.JavaScript.md",key:"v-2e5129d9",path:"/pages/e5e5b8/",headers:[{level:2,title:"引入方式",slug:"引入方式",normalizedTitle:"引入方式",charIndex:97},{level:2,title:"注释",slug:"注释",normalizedTitle:"注释",charIndex:165},{level:2,title:"输入输出语句",slug:"输入输出语句",normalizedTitle:"输入输出语句",charIndex:204},{level:2,title:"变量和常量",slug:"变量和常量",normalizedTitle:"变量和常量",charIndex:365},{level:2,title:"原始数据类型",slug:"原始数据类型",normalizedTitle:"原始数据类型",charIndex:473},{level:3,title:"typeof 方法",slug:"typeof-方法",normalizedTitle:"typeof 方法",charIndex:607},{level:2,title:"算数运算符",slug:"算数运算符",normalizedTitle:"算数运算符",charIndex:643},{level:2,title:"流程控制和循环语句",slug:"流程控制和循环语句",normalizedTitle:"流程控制和循环语句",charIndex:723},{level:2,title:"数组",slug:"数组",normalizedTitle:"数组",charIndex:788},{level:2,title:"函数",slug:"函数",normalizedTitle:"函数",charIndex:1034},{level:2,title:"DOM",slug:"dom",normalizedTitle:"dom",charIndex:41},{level:3,title:"Element  元素操作",slug:"element-元素操作",normalizedTitle:"element  元素操作",charIndex:null},{level:2,title:"Attribute 属性的操作",slug:"attribute-属性的操作",normalizedTitle:"attribute 属性的操作",charIndex:1710},{level:2,title:"Text 文本操作",slug:"text-文本操作",normalizedTitle:"text 文本操作",charIndex:1941},{level:2,title:"事件",slug:"事件",normalizedTitle:"事件",charIndex:2025},{level:3,title:"绑定事件",slug:"绑定事件",normalizedTitle:"绑定事件",charIndex:2416},{level:2,title:"面向对象",slug:"面向对象",normalizedTitle:"面向对象",charIndex:2775},{level:3,title:"定义和使用",slug:"定义和使用",normalizedTitle:"定义和使用",charIndex:2784},{level:3,title:"继承",slug:"继承",normalizedTitle:"继承",charIndex:3227},{level:3,title:"内置对象",slug:"内置对象",normalizedTitle:"内置对象",charIndex:3532},{level:2,title:"BOM",slug:"bom",normalizedTitle:"bom",charIndex:37},{level:2,title:"jQuery",slug:"jquery",normalizedTitle:"jquery",charIndex:5528},{level:3,title:"jQuery对象",slug:"jquery对象",normalizedTitle:"jquery对象",charIndex:5539},{level:3,title:"事件",slug:"事件-2",normalizedTitle:"事件",charIndex:2025},{level:3,title:"遍历",slug:"遍历",normalizedTitle:"遍历",charIndex:5923},{level:3,title:"选择器",slug:"选择器",normalizedTitle:"选择器",charIndex:6102},{level:3,title:"DOM",slug:"dom-2",normalizedTitle:"dom",charIndex:41},{level:3,title:"BOM",slug:"bom-2",normalizedTitle:"bom",charIndex:37},{level:2,title:"AJAX",slug:"ajax",normalizedTitle:"ajax",charIndex:7587},{level:3,title:"JS实现AJAX",slug:"js实现ajax",normalizedTitle:"js实现ajax",charIndex:7675},{level:3,title:"jQuery 实现AJAX",slug:"jquery-实现ajax",normalizedTitle:"jquery 实现ajax",charIndex:8461},{level:3,title:"POST 方法",slug:"post-方法",normalizedTitle:"post 方法",charIndex:8925},{level:3,title:"通用方式",slug:"通用方式",normalizedTitle:"通用方式",charIndex:9110}],headersStr:"引入方式 注释 输入输出语句 变量和常量 原始数据类型 typeof 方法 算数运算符 流程控制和循环语句 数组 函数 DOM Element  元素操作 Attribute 属性的操作 Text 文本操作 事件 绑定事件 面向对象 定义和使用 继承 内置对象 BOM jQuery jQuery对象 事件 遍历 选择器 DOM BOM AJAX JS实现AJAX jQuery 实现AJAX POST 方法 通用方式",content:'# JavaScript\n\nJavaScript 是ECMAScript+BOM+DOM\n\nECMAScript:客户端脚本语言的标准 脚本语言:不需要编译,就可以被浏览器解析解析执行\n\n\n# 引入方式\n\n * 在html内部 由<script>标签引入\n * 外部引入 由<script src="js路径">标签引入\n\n\n# 注释\n\n * 单行注释 // 内容\n * 多行注释 /* 内容 */\n\n\n# 输入输出语句\n\n * 输入框 prompt("提示内容")\n * 弹出警告框 alert("提示内容")\n * 控制台输出 console.log("显示内容")\n * 页面内容输出 document.write("显示内容")\n * 弹出确认框 确认返ture 取消返false confirm("提示内容")\n\n\n# 变量和常量\n\nJavaScript属于弱类型语言,定义变量时不区分具体的数据类型\n\n * 定义局部变量 let 变量名 = 值\n * 定义全局变量 变量名 = 值\n * 定义常量 const 常量名 = 值\n\n\n# 原始数据类型\n\n * boolean 布尔类型\n * null 声明null值的特殊关键字\n * undefined 代表变量未定义\n * numbner 整数或者浮点数\n * string 字符串\n * bigint 大整数 如: let num =10n\n\n\n# typeof 方法\n\ntpyeof() 用于判断变量的数据类型\n\n\n# 算数运算符\n\n\n\n\n\n与JAVA没有太大差别 多个全等于 ===\n\n字符串 做 + 运算时 都是拼接\n\n而与数字做 - * % 等运算时会自动转换类型\n\n\n# 流程控制和循环语句\n\n * if语句\n * switch语句\n * for循环\n * while循环 与java都差不多\n\n\n# 数组\n\n与java数组基本一致 , 但数组类型和长度都没有限制\n\n * let 数组名 =[元素1 , 元素2, ...]\n * 索引 从0开始 最大到数组长度-1 没有索引越界 而且长度没有限制 可以直接越界赋值\n * 数组长度 数组名.length\n * 数组高级运算符 ...\n   * 数组复制 let arr2=[...arr]\n   * 合并数组 let arr4 =[...arr2,...arr3]\n   * 字符串转数组 let arr5=[..."hello"]\n\n\n# 函数\n\n类似于方法\n\n * function 方法名(参数) { return 返回值}\n * 可变参数 function 方法名(...参数名) { return 返回值}\n * 匿名函数 function(参数列表) {方法体 }\n\n\n# DOM\n\nDOM(document object model) 文档对象模型\n\n将html 文档的各个组成方法 , 封装成对象 . 借助这些东西,可以对html文档进行增删改查的动态操作\n\n\n\n * Document 文档对象\n * Element 元素对象\n * Attribute 属性对象\n * Text 文本对象\n\n\n# Element 元素操作\n\n# 获取元素\n\n * getELmentByid(id属性值) 根据id获取一个元素\n * getELmentByTagName(标签名称) 标签标签名称获取多个元素\n * getELmentByName(name属性值) 根据name属性获取多个元素\n * getELmentByClassName(class属性值) 根据class属性获取多个元素\n * 子元素对象.parentElemnet属性 获取当前元素的父元素\n\n# 操作元素\n\n * createElemnt(标签名) 创建一个标签 创建后需要配合添加使用\n * appendChild(子元素对象) 将指定子元素添加到父元素中\n * removeChild(子元素对象) 用父元素删除指定子元素\n * replaceChild(新元素对象,旧元素对象) 用新元素替换旧元素\n\n\n# Attribute 属性的操作\n\n * setAttribute(属性名,属性值) 设置属性\n * getAttribute(属性名) 根据属性名获取属性值\n * removeAttribute(属性名) 根据筛选名移除指定的属性\n * style属性 为元素添加样式 如: div.style.color = "red"\n * className属性 添加指定样式 (其实是为元素添加class属性) 如: div.className = "div"\n\n\n# Text 文本操作\n\n * innerText 只添加文本内容,不解析标签\n * innerHTML 添加文本内容,并解析标签 如<a> 解析为标签而不是文本\n\n\n# 事件\n\n事件指当某些组件执行了某些操作后,会触发某些代码的执行\n\n * onload 某个页面或者图像被完成加载\n * onsubmit 当表单提交时触发\n * onclick 鼠标点击事件\n * ondblclick 鼠标双击双击\n * onblur 元素失去焦点\n * onfocus 元素获得焦点\n * onchange 用户改变域的内容\n\n----------------------------------------\n\n * onkeydown 某个键盘的键被按下\n * onkeypress 某个键盘的键被按下或者按住\n * onkeyup 某个键盘的键被松开\n * onmouserdown 某个鼠标按键被按下\n * onmouseup 某个鼠标按键被松开\n * onmouseover 鼠标被移到某元素之上\n * onmouserout 鼠标从某元素移开\n\n\n# 绑定事件\n\n * 通过标签中事件属性绑定\n   \n   * <button id="up" onclick="up()"></button>\n     \n     \n     1\n     \n\n * 通过DOM元素属性绑定\n   \n   * document.getElementById("down").onclick = function(){\n             let img =  document.getElementById("img")\n             img.setAttribute("src","../../品优购/img/arrow-prev.png")\n         }\n     \n     \n     1\n     2\n     3\n     4\n     \n\n\n# 面向对象\n\n\n# 定义和使用\n\nclass 类名{\n    //构造方法\n    constructor(变量列表){\n        变量赋值\n    }\n\n    方法名(参数列表){\n        方法体\n        return 返回值\n    }\n}\n\n//使用\nlet 对象名 = new 类名(实际变量值)\n对象名.方法名()\n\n\nclass Person {\n  constructor(name, age) {\n    this.name = name\n    this.age = age\n  }\n\n  show(){\n    document.write(this.name + " " + this.age)\n  }\n}\n\nlet P = new Person("ZZ",19)\nP.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 继承\n\nextends关键字 顶级父类 object\n\nclass worker extends Person {\n  constructor(name, age, salary) {\n    super(name, age)\n    this.salary = salary\n  }\n\n  show() {\n    document.write(this.name + \' \' + this.age + \' \' + this.salary)\n  }\n}\n\nlet w =new worker("qq",22,33)\nw.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 内置对象\n\n * Number\n   * parseFloat() 将传入的字符串浮点数转为浮点数\n   * parseInt() 将传入的字符串整数转为整数 从左到右转化 一直转换到有非数字字符停止 如200a13bc 则转为200 后面的忽略\n * Math\n   * ceil(x) 向上取整\n   * floor(x) 向下取整\n   * round(x) 四舍五入最接近的整数\n   * random() 随机数返回 [0.0 , 1.0) 之间的数\n   * pow(x,y) 幂运算x的y次方\n * Date\n   * Date() 根据当前时间创建对象\n   * Date(value) 根据指定毫秒值创建对象\n   * Date(year,month,[day,hours,minutes,seconds,milliseconds])\n     * getFullYear() 获取年\n     * getMonth() 获取月 从0-11\n     * getDate() 获取天\n     * getHours() 获取小时\n     * getMinutes() 获取秒\n     * getTime() 返回时间戳 1970年1月1日到now的毫秒数\n     * toLocaleString() 返回本地日期格式的字符串\n * String\n   * String(value) 根据指定字符串创建对象\n   * let s = "字符串" 赋值\n     * length属性 长度\n     * charAt(index) 获取指定索引的字符\n     * indexOf(value) 查找指定字符的索引 没有为-1\n     * substring(start,end) 截取字符串 (含头不含尾)\n     * split(value) 切割字符串 返回数组\n     * replace(old,new) 字符串替换\n * RegExp 正则表达式\n   * RegExp(规则) 根据规则创建对象\n   * let reg = /^规则$/ 赋值\n     * test(匹配的字符串) 根据指定规则验证字符串是否符合 返回布尔值\n   * \n * Array\n   * push(元素) 添加元素到末尾\n   * pop() 删除末尾的元素\n   * shift() 删除数组最前面的元素\n   * includes(元素) 判断数组是否包含此元素\n   * reverse() 反转数组\n   * sort() 排序数组\n * Set 元素唯一 存取顺序一致\n   * Set() 创建set对象\n     * add(元素) 添加元素\n     * size顺序 获取长度\n     * keys() 获取迭代器对象\n       * next().value 获取迭代器元素\n     * delete(元素) 删除指定元素\n * Map 存取顺序一致\n   * Map() 创建Map对象\n     * set(key,value) 添加元素\n     * size属性 获取长度\n     * get(key) 根据key获取value\n     * entries() 获取迭代器对象\n       * next().value 获取迭代器元素 键值对\n     * delete(key) 根据key删除键值对\n * JSON(JavaScript Object Notation)\n   * stringify(对象) 将指定对象转为json字符串\n   * parse(字符串) 将JSON字符串 转为js对象\n * decodeURIComponent(string s) 将url字符转为UTF-8\n\n\n# BOM\n\nBOM(Browser Object Model) 浏览器对象模型 将浏览器的各个组成部分封装成不同的对象\n\n * Windows 窗口对象\n   \n   * 定时器\n     * setTimeout(功能,毫秒值) 设置一次性定时器 返回一个唯一标识\n     * clearTimeout(标识) 取消一次性定时器\n     * setInterval(功能,毫秒值) 设置循环定时器 返回一个唯一标识\n     * clearInterval(标识) 取消循环定时器\n   * 加载事件\n     * window.onload 在页面加载完毕后触发此事件\n\n * Location 地址栏对象\n   \n   * href属性 设置新的URL 使浏览器读取并显示新的URL\n\n * History 历史记录对象\n\n * Navigator 浏览器对象\n\n * Screen 显示器屏幕对象\n\n\n# jQuery\n\n\n# jQuery对象\n\n * $(JS的 DOM 对象) JS对象和jQuery对象转换\n * jQuery 对象[索引] jQuery对象转为js对象\n * jQuery 对象.get(索引) jQuery对象转为js对象\n\n\n# 事件\n\n * 去掉js中.on的语句 如: onclick ===> click jQuery对象.click(功能)\n\n * 绑定事件 jQuery对象.on(事件名称,执行的功能)\n\n * 解绑事件 jQuery对象.off(事件名称) 如果不指定事件名称,则会将此对象所有事件都解绑\n\n# 事件切换\n\n需要给同一个对象绑定多个事件,并且多个事件有先后顺序关系\n\n * 单独定义\n   $(元素).事件方法名1(功能) $(元素).事件方法名2(功能)\n * 链式定义 $(元素).事件方法名1(功能).事件方法名2(功能)\n\n\n# 遍历\n\n * for循环\n\n * 对象.each(function(index,ele)) 方法 index为索引 ele为元素\n\n * $.each(容器对象,function(index,ele){}) 方法 先 $(元素) 获取容器对象后使用\n\n * for(ele of 容器对象){} 方法 增强for 先 $(元素) 获取容器对象后使用\n\n\n# 选择器\n\n * 基本选择器\n   \n   * $("元素名称") 根据元素获取元素对象数组\n   * $("#id的属性值") 根据id获取元素对象\n   * $(".class的属性值") 根据class获取元素对象数组\n\n * 层级选择器\n   \n   * $("A B") 后代选择器 A元素下面所有的B元素 (包含B的子元素)\n   * $("A > B") 子代选择器 A下的所有B (不包含B的子级)\n   * $("A + B") 兄弟选择器 A下相邻最近的B\n   * $("A ~ B") 兄弟选择器 A相邻的所有B\n\n * 属性选择器\n   \n   * $("A[属性名]") 属性选择器 根据指定属性名获取元素对象数组\n   * $("A[属性名]=属性值") 根据指定属性名和值获取元素数组\n\n * 过滤器选择器\n   \n   * \n\n * 表单属性选择器\n   \n   * $("A:enabled") 获取可用元素\n   * $("A:disable") 获取不可用元素\n   * $("A:checked") 获取单选/复选框被选中的元素\n   * $("A:selected") 获取下拉框选中的元素\n\n\n# DOM\n\n# 操作文本\n\n * html() 获取标签的文本\n * html(value) 设置标签的文本内容, 解析标签\n\n# 操作对象\n\n * $("元素") 创建指定元素 如不存在则创建 如存在是获取元素\n   \n   * let span = $("<span>文本</span>")\n     \n     \n     1\n     \n\n * append(element) 添加为最后一个子元素 添加对象为 element\n\n * appendTo(element) 添加为最后一个子元素 添加对象为 调用对象\n\n * prepend(element) 添加为第一个子元素 添加对象为 element\n\n * prependTo(element) 添加为第一个子元素 添加对象为 调用对象\n\n * before(element) 添加到当前元素的前面 由添加对象调用 兄弟关系\n\n * after(element) 添加到当前元素的后面 兄弟关系\n\n * remove() 删除指定元素 可自己删自己\n\n * empty() 清空指定元素的所有子元素 但调用对象还是存在的\n\n# 操作样式\n\n * css(name) 根据样式名称获取css样式\n * css(name,value) 设置css样式\n * addClass(value) 添加类名\n * removeClass(value) 移除类名\n * toggleClass(value) 如没有指定类名则添加 如有则删除\n * hide() 隐藏元素\n * show() 显示元素\n\n# 操作属性\n\n * attr(name,[value]) 获取/设置属性的值 如只传name则为获取\n * prop(name,[value]) 获取/设置属性的值(checked,selected)\n\n\n# BOM\n\n * $(function(){}) 页面加载事件\n   \n   * $(windows) 获取当前窗口对象\n     \n     * scroll() 鼠标滚动事件\n     * height() 当前窗口的高度\n     * scrollTop() 滚动条上下滚动的距离\n\n * $(document).height() 当前文档的高度\n\n\n\n\n# AJAX\n\nAJAX(Asynchronous JavaScript And XML) 异步的JavaScript和XML 是多个技术综合 用于快速创建动态网页的技术\n\n\n# JS实现AJAX\n\n//创建 XMLHttpRequest核心对象\nlet xmlHttp = new XMLHttpRequest()\n\n//打开链接     请求方式   url        是否异步\nxmlHttp.open(\'GET\', \'userServlet\', true)\n\n//发送请求\nxmlHttp.send()\n\n//处理响应\nxmlHttp.onreadystatechange = function(){\n    if(xmlHttp.readyState === 4 && xmlHttp.status === 200){\n        //判断请求和响应是否成功\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n * XMLHttpRequest 核心对象 用于在后台和服务器交换数据 可以在不重新加载整个网页的情况下,对网页的某部分进行更新\n   * open(method,url,async) 打开链接\n     * method 请求的类型 GET或者POST\n     * url 请求资源的路径\n     * async true为异步 false为同步\n   * send(String params) 发送请求\n     * params 请求的参数(POST请求专用)\n   * onreadystatechange 处理响应\n   * readyState\n     * 0 -请求未初始化 1-服务器连接已建立 2-请求已接受 3-请求处理中 4-请求已完成,且响应已经就绪\n   * status\n     * 200-响应已经全部OK\n   * responseText 获取字符串形式的响应数据\n   * responseXML 获取XML形式的响应数据\n\n\n# jQuery 实现AJAX\n\n# GET方式\n\n * $.get(url,[data],[callback],[type])\n   * url 请求的资源路径\n   * data 发送给服务器端的请求参数 可以是键值对 或 js对象\n   * callback 当请求成功后的回调函数 一般写逻辑代码\n   * type 预期的返回数据的类型 xml,html,js,json,text等\n\n    $(\'#username\').blur(function () {\n      $.get(\n        \'url\',\n        \'username=\' + username,\n        function (data) {\n                        //data为服务端传回来的数据\n          document.write(data)\n        },\n        \'text\'\n      )\n    })\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# POST 方法\n\n * $.post(url,[data],[callback],[type])\n   * url 请求的资源路径\n   * data 发送给服务器端的请求参数 可以是键值对 或 js对象\n   * callback 当请求成功后的回调函数 一般写逻辑代码\n   * type 预期的返回数据的类型 xml,html,js,json,text等\n\n\n# 通用方式\n\n * $.ajax({name:value,name:value,...})\n   * url 请求的资源路径\n   * async 是否异步请求 默认为ture异步\n   * data 发送到服务器的数据\n   * type 请求方式 post或get 默认为get\n   * dataType 返回数据的类型\n   * success 请求成功的回调函数\n   * error 请求失败时的回调函数\n\n$.ajax({\n  url: \'url地址\',\n  async: true,\n  data: \'username=\' + username,\n  type: \'get\',\n  dataType: \'text\',\n  success: function (data) {\n    print(data)\n  },\n  error: function () {\n    console.error()\n  },\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n',normalizedContent:'# javascript\n\njavascript 是ecmascript+bom+dom\n\necmascript:客户端脚本语言的标准 脚本语言:不需要编译,就可以被浏览器解析解析执行\n\n\n# 引入方式\n\n * 在html内部 由<script>标签引入\n * 外部引入 由<script src="js路径">标签引入\n\n\n# 注释\n\n * 单行注释 // 内容\n * 多行注释 /* 内容 */\n\n\n# 输入输出语句\n\n * 输入框 prompt("提示内容")\n * 弹出警告框 alert("提示内容")\n * 控制台输出 console.log("显示内容")\n * 页面内容输出 document.write("显示内容")\n * 弹出确认框 确认返ture 取消返false confirm("提示内容")\n\n\n# 变量和常量\n\njavascript属于弱类型语言,定义变量时不区分具体的数据类型\n\n * 定义局部变量 let 变量名 = 值\n * 定义全局变量 变量名 = 值\n * 定义常量 const 常量名 = 值\n\n\n# 原始数据类型\n\n * boolean 布尔类型\n * null 声明null值的特殊关键字\n * undefined 代表变量未定义\n * numbner 整数或者浮点数\n * string 字符串\n * bigint 大整数 如: let num =10n\n\n\n# typeof 方法\n\ntpyeof() 用于判断变量的数据类型\n\n\n# 算数运算符\n\n\n\n\n\n与java没有太大差别 多个全等于 ===\n\n字符串 做 + 运算时 都是拼接\n\n而与数字做 - * % 等运算时会自动转换类型\n\n\n# 流程控制和循环语句\n\n * if语句\n * switch语句\n * for循环\n * while循环 与java都差不多\n\n\n# 数组\n\n与java数组基本一致 , 但数组类型和长度都没有限制\n\n * let 数组名 =[元素1 , 元素2, ...]\n * 索引 从0开始 最大到数组长度-1 没有索引越界 而且长度没有限制 可以直接越界赋值\n * 数组长度 数组名.length\n * 数组高级运算符 ...\n   * 数组复制 let arr2=[...arr]\n   * 合并数组 let arr4 =[...arr2,...arr3]\n   * 字符串转数组 let arr5=[..."hello"]\n\n\n# 函数\n\n类似于方法\n\n * function 方法名(参数) { return 返回值}\n * 可变参数 function 方法名(...参数名) { return 返回值}\n * 匿名函数 function(参数列表) {方法体 }\n\n\n# dom\n\ndom(document object model) 文档对象模型\n\n将html 文档的各个组成方法 , 封装成对象 . 借助这些东西,可以对html文档进行增删改查的动态操作\n\n\n\n * document 文档对象\n * element 元素对象\n * attribute 属性对象\n * text 文本对象\n\n\n# element 元素操作\n\n# 获取元素\n\n * getelmentbyid(id属性值) 根据id获取一个元素\n * getelmentbytagname(标签名称) 标签标签名称获取多个元素\n * getelmentbyname(name属性值) 根据name属性获取多个元素\n * getelmentbyclassname(class属性值) 根据class属性获取多个元素\n * 子元素对象.parentelemnet属性 获取当前元素的父元素\n\n# 操作元素\n\n * createelemnt(标签名) 创建一个标签 创建后需要配合添加使用\n * appendchild(子元素对象) 将指定子元素添加到父元素中\n * removechild(子元素对象) 用父元素删除指定子元素\n * replacechild(新元素对象,旧元素对象) 用新元素替换旧元素\n\n\n# attribute 属性的操作\n\n * setattribute(属性名,属性值) 设置属性\n * getattribute(属性名) 根据属性名获取属性值\n * removeattribute(属性名) 根据筛选名移除指定的属性\n * style属性 为元素添加样式 如: div.style.color = "red"\n * classname属性 添加指定样式 (其实是为元素添加class属性) 如: div.classname = "div"\n\n\n# text 文本操作\n\n * innertext 只添加文本内容,不解析标签\n * innerhtml 添加文本内容,并解析标签 如<a> 解析为标签而不是文本\n\n\n# 事件\n\n事件指当某些组件执行了某些操作后,会触发某些代码的执行\n\n * onload 某个页面或者图像被完成加载\n * onsubmit 当表单提交时触发\n * onclick 鼠标点击事件\n * ondblclick 鼠标双击双击\n * onblur 元素失去焦点\n * onfocus 元素获得焦点\n * onchange 用户改变域的内容\n\n----------------------------------------\n\n * onkeydown 某个键盘的键被按下\n * onkeypress 某个键盘的键被按下或者按住\n * onkeyup 某个键盘的键被松开\n * onmouserdown 某个鼠标按键被按下\n * onmouseup 某个鼠标按键被松开\n * onmouseover 鼠标被移到某元素之上\n * onmouserout 鼠标从某元素移开\n\n\n# 绑定事件\n\n * 通过标签中事件属性绑定\n   \n   * <button id="up" onclick="up()"></button>\n     \n     \n     1\n     \n\n * 通过dom元素属性绑定\n   \n   * document.getelementbyid("down").onclick = function(){\n             let img =  document.getelementbyid("img")\n             img.setattribute("src","../../品优购/img/arrow-prev.png")\n         }\n     \n     \n     1\n     2\n     3\n     4\n     \n\n\n# 面向对象\n\n\n# 定义和使用\n\nclass 类名{\n    //构造方法\n    constructor(变量列表){\n        变量赋值\n    }\n\n    方法名(参数列表){\n        方法体\n        return 返回值\n    }\n}\n\n//使用\nlet 对象名 = new 类名(实际变量值)\n对象名.方法名()\n\n\nclass person {\n  constructor(name, age) {\n    this.name = name\n    this.age = age\n  }\n\n  show(){\n    document.write(this.name + " " + this.age)\n  }\n}\n\nlet p = new person("zz",19)\np.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 继承\n\nextends关键字 顶级父类 object\n\nclass worker extends person {\n  constructor(name, age, salary) {\n    super(name, age)\n    this.salary = salary\n  }\n\n  show() {\n    document.write(this.name + \' \' + this.age + \' \' + this.salary)\n  }\n}\n\nlet w =new worker("qq",22,33)\nw.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 内置对象\n\n * number\n   * parsefloat() 将传入的字符串浮点数转为浮点数\n   * parseint() 将传入的字符串整数转为整数 从左到右转化 一直转换到有非数字字符停止 如200a13bc 则转为200 后面的忽略\n * math\n   * ceil(x) 向上取整\n   * floor(x) 向下取整\n   * round(x) 四舍五入最接近的整数\n   * random() 随机数返回 [0.0 , 1.0) 之间的数\n   * pow(x,y) 幂运算x的y次方\n * date\n   * date() 根据当前时间创建对象\n   * date(value) 根据指定毫秒值创建对象\n   * date(year,month,[day,hours,minutes,seconds,milliseconds])\n     * getfullyear() 获取年\n     * getmonth() 获取月 从0-11\n     * getdate() 获取天\n     * gethours() 获取小时\n     * getminutes() 获取秒\n     * gettime() 返回时间戳 1970年1月1日到now的毫秒数\n     * tolocalestring() 返回本地日期格式的字符串\n * string\n   * string(value) 根据指定字符串创建对象\n   * let s = "字符串" 赋值\n     * length属性 长度\n     * charat(index) 获取指定索引的字符\n     * indexof(value) 查找指定字符的索引 没有为-1\n     * substring(start,end) 截取字符串 (含头不含尾)\n     * split(value) 切割字符串 返回数组\n     * replace(old,new) 字符串替换\n * regexp 正则表达式\n   * regexp(规则) 根据规则创建对象\n   * let reg = /^规则$/ 赋值\n     * test(匹配的字符串) 根据指定规则验证字符串是否符合 返回布尔值\n   * \n * array\n   * push(元素) 添加元素到末尾\n   * pop() 删除末尾的元素\n   * shift() 删除数组最前面的元素\n   * includes(元素) 判断数组是否包含此元素\n   * reverse() 反转数组\n   * sort() 排序数组\n * set 元素唯一 存取顺序一致\n   * set() 创建set对象\n     * add(元素) 添加元素\n     * size顺序 获取长度\n     * keys() 获取迭代器对象\n       * next().value 获取迭代器元素\n     * delete(元素) 删除指定元素\n * map 存取顺序一致\n   * map() 创建map对象\n     * set(key,value) 添加元素\n     * size属性 获取长度\n     * get(key) 根据key获取value\n     * entries() 获取迭代器对象\n       * next().value 获取迭代器元素 键值对\n     * delete(key) 根据key删除键值对\n * json(javascript object notation)\n   * stringify(对象) 将指定对象转为json字符串\n   * parse(字符串) 将json字符串 转为js对象\n * decodeuricomponent(string s) 将url字符转为utf-8\n\n\n# bom\n\nbom(browser object model) 浏览器对象模型 将浏览器的各个组成部分封装成不同的对象\n\n * windows 窗口对象\n   \n   * 定时器\n     * settimeout(功能,毫秒值) 设置一次性定时器 返回一个唯一标识\n     * cleartimeout(标识) 取消一次性定时器\n     * setinterval(功能,毫秒值) 设置循环定时器 返回一个唯一标识\n     * clearinterval(标识) 取消循环定时器\n   * 加载事件\n     * window.onload 在页面加载完毕后触发此事件\n\n * location 地址栏对象\n   \n   * href属性 设置新的url 使浏览器读取并显示新的url\n\n * history 历史记录对象\n\n * navigator 浏览器对象\n\n * screen 显示器屏幕对象\n\n\n# jquery\n\n\n# jquery对象\n\n * $(js的 dom 对象) js对象和jquery对象转换\n * jquery 对象[索引] jquery对象转为js对象\n * jquery 对象.get(索引) jquery对象转为js对象\n\n\n# 事件\n\n * 去掉js中.on的语句 如: onclick ===> click jquery对象.click(功能)\n\n * 绑定事件 jquery对象.on(事件名称,执行的功能)\n\n * 解绑事件 jquery对象.off(事件名称) 如果不指定事件名称,则会将此对象所有事件都解绑\n\n# 事件切换\n\n需要给同一个对象绑定多个事件,并且多个事件有先后顺序关系\n\n * 单独定义\n   $(元素).事件方法名1(功能) $(元素).事件方法名2(功能)\n * 链式定义 $(元素).事件方法名1(功能).事件方法名2(功能)\n\n\n# 遍历\n\n * for循环\n\n * 对象.each(function(index,ele)) 方法 index为索引 ele为元素\n\n * $.each(容器对象,function(index,ele){}) 方法 先 $(元素) 获取容器对象后使用\n\n * for(ele of 容器对象){} 方法 增强for 先 $(元素) 获取容器对象后使用\n\n\n# 选择器\n\n * 基本选择器\n   \n   * $("元素名称") 根据元素获取元素对象数组\n   * $("#id的属性值") 根据id获取元素对象\n   * $(".class的属性值") 根据class获取元素对象数组\n\n * 层级选择器\n   \n   * $("a b") 后代选择器 a元素下面所有的b元素 (包含b的子元素)\n   * $("a > b") 子代选择器 a下的所有b (不包含b的子级)\n   * $("a + b") 兄弟选择器 a下相邻最近的b\n   * $("a ~ b") 兄弟选择器 a相邻的所有b\n\n * 属性选择器\n   \n   * $("a[属性名]") 属性选择器 根据指定属性名获取元素对象数组\n   * $("a[属性名]=属性值") 根据指定属性名和值获取元素数组\n\n * 过滤器选择器\n   \n   * \n\n * 表单属性选择器\n   \n   * $("a:enabled") 获取可用元素\n   * $("a:disable") 获取不可用元素\n   * $("a:checked") 获取单选/复选框被选中的元素\n   * $("a:selected") 获取下拉框选中的元素\n\n\n# dom\n\n# 操作文本\n\n * html() 获取标签的文本\n * html(value) 设置标签的文本内容, 解析标签\n\n# 操作对象\n\n * $("元素") 创建指定元素 如不存在则创建 如存在是获取元素\n   \n   * let span = $("<span>文本</span>")\n     \n     \n     1\n     \n\n * append(element) 添加为最后一个子元素 添加对象为 element\n\n * appendto(element) 添加为最后一个子元素 添加对象为 调用对象\n\n * prepend(element) 添加为第一个子元素 添加对象为 element\n\n * prependto(element) 添加为第一个子元素 添加对象为 调用对象\n\n * before(element) 添加到当前元素的前面 由添加对象调用 兄弟关系\n\n * after(element) 添加到当前元素的后面 兄弟关系\n\n * remove() 删除指定元素 可自己删自己\n\n * empty() 清空指定元素的所有子元素 但调用对象还是存在的\n\n# 操作样式\n\n * css(name) 根据样式名称获取css样式\n * css(name,value) 设置css样式\n * addclass(value) 添加类名\n * removeclass(value) 移除类名\n * toggleclass(value) 如没有指定类名则添加 如有则删除\n * hide() 隐藏元素\n * show() 显示元素\n\n# 操作属性\n\n * attr(name,[value]) 获取/设置属性的值 如只传name则为获取\n * prop(name,[value]) 获取/设置属性的值(checked,selected)\n\n\n# bom\n\n * $(function(){}) 页面加载事件\n   \n   * $(windows) 获取当前窗口对象\n     \n     * scroll() 鼠标滚动事件\n     * height() 当前窗口的高度\n     * scrolltop() 滚动条上下滚动的距离\n\n * $(document).height() 当前文档的高度\n\n\n\n\n# ajax\n\najax(asynchronous javascript and xml) 异步的javascript和xml 是多个技术综合 用于快速创建动态网页的技术\n\n\n# js实现ajax\n\n//创建 xmlhttprequest核心对象\nlet xmlhttp = new xmlhttprequest()\n\n//打开链接     请求方式   url        是否异步\nxmlhttp.open(\'get\', \'userservlet\', true)\n\n//发送请求\nxmlhttp.send()\n\n//处理响应\nxmlhttp.onreadystatechange = function(){\n    if(xmlhttp.readystate === 4 && xmlhttp.status === 200){\n        //判断请求和响应是否成功\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n * xmlhttprequest 核心对象 用于在后台和服务器交换数据 可以在不重新加载整个网页的情况下,对网页的某部分进行更新\n   * open(method,url,async) 打开链接\n     * method 请求的类型 get或者post\n     * url 请求资源的路径\n     * async true为异步 false为同步\n   * send(string params) 发送请求\n     * params 请求的参数(post请求专用)\n   * onreadystatechange 处理响应\n   * readystate\n     * 0 -请求未初始化 1-服务器连接已建立 2-请求已接受 3-请求处理中 4-请求已完成,且响应已经就绪\n   * status\n     * 200-响应已经全部ok\n   * responsetext 获取字符串形式的响应数据\n   * responsexml 获取xml形式的响应数据\n\n\n# jquery 实现ajax\n\n# get方式\n\n * $.get(url,[data],[callback],[type])\n   * url 请求的资源路径\n   * data 发送给服务器端的请求参数 可以是键值对 或 js对象\n   * callback 当请求成功后的回调函数 一般写逻辑代码\n   * type 预期的返回数据的类型 xml,html,js,json,text等\n\n    $(\'#username\').blur(function () {\n      $.get(\n        \'url\',\n        \'username=\' + username,\n        function (data) {\n                        //data为服务端传回来的数据\n          document.write(data)\n        },\n        \'text\'\n      )\n    })\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# post 方法\n\n * $.post(url,[data],[callback],[type])\n   * url 请求的资源路径\n   * data 发送给服务器端的请求参数 可以是键值对 或 js对象\n   * callback 当请求成功后的回调函数 一般写逻辑代码\n   * type 预期的返回数据的类型 xml,html,js,json,text等\n\n\n# 通用方式\n\n * $.ajax({name:value,name:value,...})\n   * url 请求的资源路径\n   * async 是否异步请求 默认为ture异步\n   * data 发送到服务器的数据\n   * type 请求方式 post或get 默认为get\n   * datatype 返回数据的类型\n   * success 请求成功的回调函数\n   * error 请求失败时的回调函数\n\n$.ajax({\n  url: \'url地址\',\n  async: true,\n  data: \'username=\' + username,\n  type: \'get\',\n  datatype: \'text\',\n  success: function (data) {\n    print(data)\n  },\n  error: function () {\n    console.error()\n  },\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n',charsets:{cjk:!0}},{title:"安装Vue CLI",frontmatter:{title:"安装Vue CLI",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/758bea/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/01.%E5%AE%89%E8%A3%85Vue%20CLI.html",relativePath:"前端/02.Vue2/01.安装Vue CLI.md",key:"v-64afecb3",path:"/pages/758bea/",headers:[{level:2,title:"创建Vue CLI 项目",slug:"创建vue-cli-项目",normalizedTitle:"创建vue cli 项目",charIndex:72},{level:2,title:"启动Vue CLI项目",slug:"启动vue-cli项目",normalizedTitle:"启动vue cli项目",charIndex:124},{level:2,title:"安装 Element",slug:"安装-element",normalizedTitle:"安装 element",charIndex:169}],headersStr:"创建Vue CLI 项目 启动Vue CLI项目 安装 Element",content:"# 安装Vue CLI\n\nnpm install -g @vue/cli\n\n\n1\n\n\n查看版本\n\nvue --version\n\n\n1\n\n\n\n# 创建Vue CLI 项目\n\n进入到要放在项目的文件夹\n\nvue create 项目名\n\n\n1\n\n\n\n# 启动Vue CLI项目\n\n进入项目文件夹\n\nnpm run serve\n\n\n1\n\n\n\n# 安装 Element\n\n用cmd打开项目文件夹中安装\n\nnpm i element-ui -S\n\n\n1\n",normalizedContent:"# 安装vue cli\n\nnpm install -g @vue/cli\n\n\n1\n\n\n查看版本\n\nvue --version\n\n\n1\n\n\n\n# 创建vue cli 项目\n\n进入到要放在项目的文件夹\n\nvue create 项目名\n\n\n1\n\n\n\n# 启动vue cli项目\n\n进入项目文件夹\n\nnpm run serve\n\n\n1\n\n\n\n# 安装 element\n\n用cmd打开项目文件夹中安装\n\nnpm i element-ui -s\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Elment",frontmatter:{title:"Elment",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/cb74f3/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/03.Elment.html",relativePath:"前端/02.Vue2/03.Elment.md",key:"v-97656b46",path:"/pages/cb74f3/",headers:[{level:2,title:"表单验证",slug:"表单验证",normalizedTitle:"表单验证",charIndex:13},{level:2,title:"给表单添加验证属性",slug:"给表单添加验证属性",normalizedTitle:"给表单添加验证属性",charIndex:22},{level:2,title:"验证规则",slug:"验证规则",normalizedTitle:"验证规则",charIndex:87},{level:2,title:"trigger验证时机",slug:"trigger验证时机",normalizedTitle:"trigger验证时机",charIndex:702},{level:2,title:"手动触发验证表单",slug:"手动触发验证表单",normalizedTitle:"手动触发验证表单",charIndex:759},{level:2,title:"自定义定义规则",slug:"自定义定义规则",normalizedTitle:"自定义定义规则",charIndex:1110}],headersStr:"表单验证 给表单添加验证属性 验证规则 trigger验证时机 手动触发验证表单 自定义定义规则",content:"# Elment\n\n\n# 表单验证\n\n\n# 给表单添加验证属性\n\n:rules 定义开始表单验证 如: :rules=\"formRules\"\n\n需要绑定vue中的data数据验证规则,并且form表单必须要有:model绑定值\n\n验证子元素绑定为子元素的prop名称\n\n 1. 必须给 el-from 组件绑定 model 为表单数据对象\n 2. 给校园验证的表单项 el-form-item 绑定 prop 属性 需要自定义名称\n 3. 通过 el-from 组件的 rules 属性配置验证规则\n\n\n# 验证规则\n\n验证规则配置常用的有以下几个\n\n * required 必须的 不能为空\n\n * pattern 正则表达式 ^以什么开头的 $以什么结尾的 \\d{n}多少位数字 如:/^1[3|5|7|8|9]\\d{9}$/ 以1开头 第二位为2、5、7、8、9的 9位长度 以空为结尾的\n   \n   { pattern: /^1[3|5|7|8|9]\\d{9}$/, message: '请输入正确的号码格式', trigger: 'change' }\n   \n   // $以什么结束   如:以abc结束   abc$  或者 [abc]$\n   \n   \n   1\n   2\n   3\n   \n\n * range 使用min和max定义范围 如: { min: 6, max: 6, message: '请输入正确的验证码格式', trigger: 'blur' } 这个比较特殊不用指定type类型\n\n * validator 自定义 如:validator:自定义函数名\n\n\n\n\n# trigger验证时机\n\n * blur 当输入框失去焦点时验证\n * change 当输入框改变后验证\n\n\n# 手动触发验证表单\n\n 1. 给el-form 设置 ref 起名字\n 2. 通过 ref 获取 el-form 组织,调用组件的validate进行验证\n\n this.$refs.loginForm.validate((valid, err) => {\n        // 返回两个参数  valid 一个布尔值是否通过验证   err 验证的具体配置\n        // 所以我们可以通过返回的布尔值来判断表单是否通过验证\n        if (!valid) {\n          return  //不通过返回\n        }\n     //通过调用api接口验证用户信息\n        this.login()\n      })\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 自定义定义规则\n\n{\n          validator: (rule, value, callback) => {\n          // 定义验证规则  rule为表单的详细信息  value为布尔\n          // 验证通过: callback()   验证失败:callback(new Error('自定义错误消息提示'))\n            // console.log(value)\n            if (value) {\n              callback()\n            } else {\n              callback(new Error('请同意用户协议'))\n            }\n          },\n          trigger: 'change'\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n",normalizedContent:"# elment\n\n\n# 表单验证\n\n\n# 给表单添加验证属性\n\n:rules 定义开始表单验证 如: :rules=\"formrules\"\n\n需要绑定vue中的data数据验证规则,并且form表单必须要有:model绑定值\n\n验证子元素绑定为子元素的prop名称\n\n 1. 必须给 el-from 组件绑定 model 为表单数据对象\n 2. 给校园验证的表单项 el-form-item 绑定 prop 属性 需要自定义名称\n 3. 通过 el-from 组件的 rules 属性配置验证规则\n\n\n# 验证规则\n\n验证规则配置常用的有以下几个\n\n * required 必须的 不能为空\n\n * pattern 正则表达式 ^以什么开头的 $以什么结尾的 \\d{n}多少位数字 如:/^1[3|5|7|8|9]\\d{9}$/ 以1开头 第二位为2、5、7、8、9的 9位长度 以空为结尾的\n   \n   { pattern: /^1[3|5|7|8|9]\\d{9}$/, message: '请输入正确的号码格式', trigger: 'change' }\n   \n   // $以什么结束   如:以abc结束   abc$  或者 [abc]$\n   \n   \n   1\n   2\n   3\n   \n\n * range 使用min和max定义范围 如: { min: 6, max: 6, message: '请输入正确的验证码格式', trigger: 'blur' } 这个比较特殊不用指定type类型\n\n * validator 自定义 如:validator:自定义函数名\n\n\n\n\n# trigger验证时机\n\n * blur 当输入框失去焦点时验证\n * change 当输入框改变后验证\n\n\n# 手动触发验证表单\n\n 1. 给el-form 设置 ref 起名字\n 2. 通过 ref 获取 el-form 组织,调用组件的validate进行验证\n\n this.$refs.loginform.validate((valid, err) => {\n        // 返回两个参数  valid 一个布尔值是否通过验证   err 验证的具体配置\n        // 所以我们可以通过返回的布尔值来判断表单是否通过验证\n        if (!valid) {\n          return  //不通过返回\n        }\n     //通过调用api接口验证用户信息\n        this.login()\n      })\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 自定义定义规则\n\n{\n          validator: (rule, value, callback) => {\n          // 定义验证规则  rule为表单的详细信息  value为布尔\n          // 验证通过: callback()   验证失败:callback(new error('自定义错误消息提示'))\n            // console.log(value)\n            if (value) {\n              callback()\n            } else {\n              callback(new error('请同意用户协议'))\n            }\n          },\n          trigger: 'change'\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Axios",frontmatter:{title:"Axios",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/e17f8f/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/04.Axios.html",relativePath:"前端/02.Vue2/04.Axios.md",key:"v-128ad24a",path:"/pages/e17f8f/",headers:[{level:2,title:"安装 axios",slug:"安装-axios",normalizedTitle:"安装 axios",charIndex:12},{level:2,title:"封装请求模块",slug:"封装请求模块",normalizedTitle:"封装请求模块",charIndex:66},{level:2,title:"解决axios中转成js对象后int超出范围",slug:"解决axios中转成js对象后int超出范围",normalizedTitle:"解决axios中转成js对象后int超出范围",charIndex:670},{level:2,title:"常见的axios配置",slug:"常见的axios配置",normalizedTitle:"常见的axios配置",charIndex:1185}],headersStr:"安装 axios 封装请求模块 解决axios中转成js对象后int超出范围 常见的axios配置",content:"# Axios\n\n\n# 安装 axios\n\n同样在项目文件夹中用node安装axios\n\nnpm i axios\n\n\n1\n\n\n\n# 封装请求模块\n\n为了方便,我们把axios单独封装一个模块用于项目中请求操作\n\n// 基于 axios 封装的请求模块\nimport axios from 'axios'\n\n// 创建一个axios实例  我们通过这个实例去罚请求,把需要的配置配置发给这个实例来处理\nconst request = axios.create({\n  baseURL: 'http://api-toutiao-web.itheima.net' // 请求的基本路径 前缀\n})\n\n// 导出请求方法\nexport default request\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n在外部调用\n\n//导入模块\nimport requset from '@/utils/request'\n\n//定义方法\n requset({\n        method: '请求方式',\n        url: '请求地址后缀',\n        data: '传入参数'\n      }).then(res => {\n     //成功时返回结果\n        console.log(res)\n      }).catch(err => {\n     //失败结果\n        console.log(err)\n      })\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 解决axios中转成js对象后int超出范围\n\n我们接口返回的文章数据id 是json字符串 axios默认将json字符串转成js对象来方便使用,但是转了之后 id超出了js中的int范围值发生了改变\n\n我们可以使用json-bigint一个包来解决这个问题\n\n# 安装\nnpm install json-bigint\n\n\n1\n2\n\n\n# parse 将json字符串转为json-bigint类型\n\n将json字符串转为json-bigint类型\n\nconst str = '{'id':12354564698748972134}'\n// 转换\nJSONbig.parse(str)\n\n//读取并转为字符串\nJSONbig.parse(str).id.toString()\n\n\n1\n2\n3\n4\n5\n6\n\n\n# stringfy 把json-bigint类型转回js对象\n\n把json-bigint类型转回js对象\n\nconst data = JSONbig.parse('{'id':12354564698748972134}')\n\n// 转换\nJSONbig.stringifly(data)\n\n\n1\n2\n3\n4\n\n\n\n# 常见的axios配置\n\naxios({\n    method:'方法',\n    path:'url',\n    \n    headers: {\n    '名字' : '值'\n},\n    data : {\n        '名字' : '值'\n    },\n    parms : {\n        '名字' : '值'\n    }\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * method:方法\n   * GET 一般用于查询\n   * POST 一般用于添加\n   * PUT 一般用于修改,完整替换\n   * DELETE 一般用于删除\n   * PATCH 一般用于修改,局部修改\n * path:请求路径\n * 返回状态码\n   * 200 ok\n   * 400 请求参数错误\n   * 403 用户非认证,无权限登陆\n   * 507 服务器数据库异常\n * 请求参数\n   * 请求头 Headers : 使用 headers: { 名: 值 }\n   * 查询参数 Query : 使用 parms: { } Query参数axios会转成为 key=value&key=value 的数据格式以 ? 分割 url 后面传递给接口\n   * 请求体 Body : axioos 通过 data : { }\n   * 路径参数 : 请求路由中的 冒号: xxx 表示路径参数,使用时把:xxx替换掉,如: url/:xxx 传123参就写 url/123 字面意思上拼接url路径",normalizedContent:"# axios\n\n\n# 安装 axios\n\n同样在项目文件夹中用node安装axios\n\nnpm i axios\n\n\n1\n\n\n\n# 封装请求模块\n\n为了方便,我们把axios单独封装一个模块用于项目中请求操作\n\n// 基于 axios 封装的请求模块\nimport axios from 'axios'\n\n// 创建一个axios实例  我们通过这个实例去罚请求,把需要的配置配置发给这个实例来处理\nconst request = axios.create({\n  baseurl: 'http://api-toutiao-web.itheima.net' // 请求的基本路径 前缀\n})\n\n// 导出请求方法\nexport default request\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n在外部调用\n\n//导入模块\nimport requset from '@/utils/request'\n\n//定义方法\n requset({\n        method: '请求方式',\n        url: '请求地址后缀',\n        data: '传入参数'\n      }).then(res => {\n     //成功时返回结果\n        console.log(res)\n      }).catch(err => {\n     //失败结果\n        console.log(err)\n      })\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 解决axios中转成js对象后int超出范围\n\n我们接口返回的文章数据id 是json字符串 axios默认将json字符串转成js对象来方便使用,但是转了之后 id超出了js中的int范围值发生了改变\n\n我们可以使用json-bigint一个包来解决这个问题\n\n# 安装\nnpm install json-bigint\n\n\n1\n2\n\n\n# parse 将json字符串转为json-bigint类型\n\n将json字符串转为json-bigint类型\n\nconst str = '{'id':12354564698748972134}'\n// 转换\njsonbig.parse(str)\n\n//读取并转为字符串\njsonbig.parse(str).id.tostring()\n\n\n1\n2\n3\n4\n5\n6\n\n\n# stringfy 把json-bigint类型转回js对象\n\n把json-bigint类型转回js对象\n\nconst data = jsonbig.parse('{'id':12354564698748972134}')\n\n// 转换\njsonbig.stringifly(data)\n\n\n1\n2\n3\n4\n\n\n\n# 常见的axios配置\n\naxios({\n    method:'方法',\n    path:'url',\n    \n    headers: {\n    '名字' : '值'\n},\n    data : {\n        '名字' : '值'\n    },\n    parms : {\n        '名字' : '值'\n    }\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * method:方法\n   * get 一般用于查询\n   * post 一般用于添加\n   * put 一般用于修改,完整替换\n   * delete 一般用于删除\n   * patch 一般用于修改,局部修改\n * path:请求路径\n * 返回状态码\n   * 200 ok\n   * 400 请求参数错误\n   * 403 用户非认证,无权限登陆\n   * 507 服务器数据库异常\n * 请求参数\n   * 请求头 headers : 使用 headers: { 名: 值 }\n   * 查询参数 query : 使用 parms: { } query参数axios会转成为 key=value&key=value 的数据格式以 ? 分割 url 后面传递给接口\n   * 请求体 body : axioos 通过 data : { }\n   * 路径参数 : 请求路由中的 冒号: xxx 表示路径参数,使用时把:xxx替换掉,如: url/:xxx 传123参就写 url/123 字面意思上拼接url路径",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"在Vue CLI中导入 Elment",frontmatter:{title:"在Vue CLI中导入 Elment",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/3b8d98/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/02.%E5%9C%A8Vue%20CLI%E4%B8%AD%E5%AF%BC%E5%85%A5%20Elment.html",relativePath:"前端/02.Vue2/02.在Vue CLI中导入 Elment.md",key:"v-7e57676a",path:"/pages/3b8d98/",headersStr:null,content:"# 在Vue CLI中导入 Elment\n\n在Vue项目中的main.js中加入\n\nimport ElementUI from 'element-ui';\nimport 'element-ui/lib/theme-chalk/index.css';\n\nVue.use(ElementUI);\n\n\n\n1\n2\n3\n4\n5\n",normalizedContent:"# 在vue cli中导入 elment\n\n在vue项目中的main.js中加入\n\nimport elementui from 'element-ui';\nimport 'element-ui/lib/theme-chalk/index.css';\n\nvue.use(elementui);\n\n\n\n1\n2\n3\n4\n5\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"github 推送",frontmatter:{title:"github 推送",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/56d54a/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/06.github%20%E6%8E%A8%E9%80%81.html",relativePath:"前端/02.Vue2/06.github 推送.md",key:"v-c9e2e0d2",path:"/pages/56d54a/",headersStr:null,content:"# github 推送\n\n# 初始化本库\ngit init\n\n# 把文件添加到暂存区\ngit add 文件夹(.为当前打开的目录)\n\n# 把暂存区的文件提交到本地库\ngit commit -m 'first commit'\n\n# 添加远程库起别名\ngit remote add 别名 github库连接\n\n# 推送到远程库\ngit push -u 别名 master(分支名)\n\n# 记住本次推送的配置 下一次可以直接 git push\ngit push -u 别名 分支\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n",normalizedContent:"# github 推送\n\n# 初始化本库\ngit init\n\n# 把文件添加到暂存区\ngit add 文件夹(.为当前打开的目录)\n\n# 把暂存区的文件提交到本地库\ngit commit -m 'first commit'\n\n# 添加远程库起别名\ngit remote add 别名 github库连接\n\n# 推送到远程库\ngit push -u 别名 master(分支名)\n\n# 记住本次推送的配置 下一次可以直接 git push\ngit push -u 别名 分支\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"配置Vue路由",frontmatter:{title:"配置Vue路由",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/efa853/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/05.%E9%85%8D%E7%BD%AEVue%E8%B7%AF%E7%94%B1.html",relativePath:"前端/02.Vue2/05.配置Vue路由.md",key:"v-68e1016a",path:"/pages/efa853/",headersStr:null,content:"# 配置Vue路由\n\n1.先定义一个新的页面\n\n<template>\n  <div class=\"login-container\">\n      <h2>登陆页面</h2>\n    <router-view/>\n  </div>\n</template>\n<script>\nexport default {\n  name: 'LoginIndex',\n  data () {\n    return {\n      num: 1\n    }\n  }\n}\n<\/script>\n\n<style lang=\"less\"></style>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n2.在router的index.js中 引用 并配置\n\nimport Login from '@/views/login'   //引用页面文件\n//@ 在vue cli中表示src中的文件  注意要@后面加斜杠\n\n//在routes数组中添加路由器地址 名字 元件\nconst routes = [{\n  path: '/login',\n  name: 'login',\n  component: Login\n}\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n 3. 手动跳转访问测试",normalizedContent:"# 配置vue路由\n\n1.先定义一个新的页面\n\n<template>\n  <div class=\"login-container\">\n      <h2>登陆页面</h2>\n    <router-view/>\n  </div>\n</template>\n<script>\nexport default {\n  name: 'loginindex',\n  data () {\n    return {\n      num: 1\n    }\n  }\n}\n<\/script>\n\n<style lang=\"less\"></style>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n2.在router的index.js中 引用 并配置\n\nimport login from '@/views/login'   //引用页面文件\n//@ 在vue cli中表示src中的文件  注意要@后面加斜杠\n\n//在routes数组中添加路由器地址 名字 元件\nconst routes = [{\n  path: '/login',\n  name: 'login',\n  component: login\n}\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n 3. 手动跳转访问测试",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"安装 elment-tiptap",frontmatter:{title:"安装 elment-tiptap",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/93608e/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/07.%E5%AE%89%E8%A3%85%20elment-tiptap.html",relativePath:"前端/02.Vue2/07.安装 elment-tiptap.md",key:"v-3e34a8f6",path:"/pages/93608e/",headers:[{level:2,title:"全局引用",slug:"全局引用",normalizedTitle:"全局引用",charIndex:132},{level:2,title:"局部引用",slug:"局部引用",normalizedTitle:"局部引用",charIndex:1470}],headersStr:"全局引用 局部引用",content:"# 安装 elment-tiptap\n\nnpm install --save element-tiptap\n\n\n1\n\n\nhttps://github.com/Leecason/element-tiptap/blob/master/README_ZH.md\n\n\n# 全局引用\n\nimport { ElementTiptapPlugin } from 'element-tiptap';\nimport 'element-tiptap/lib/index.css';\n\n\nVue.use(ElementTiptapPlugin, {\n  lang: \"zh\", // 见 i18n   必须设置lang否则报错\n  spellcheck: true, // 可被 editor 同名 prop 重写\n});\n// 现在你已经在全局注册了 `el-tiptap` 组件。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n<template>\n  <div>\n    <el-tiptap\n      v-model=\"content\"\n      :extensions=\"extensions\"\n    />\n  </div>\n</template>\n\n<script>\nimport {\n  // 需要的 extensions\n  Doc,\n  Text,\n  Paragraph,\n  Heading,\n  Bold,\n  Underline,\n  Italic,\n  Strike,\n  ListItem,\n  BulletList,\n  OrderedList,\n} from 'element-tiptap';\n\nexport default {\n  data () {\n    // 编辑器的 extensions\n    // 它们将会按照你声明的顺序被添加到菜单栏和气泡菜单中\n    return {\n      extensions: [\n        new Doc(),\n        new Text(),\n        new Paragraph(),\n        new Heading({ level: 5 }),\n        new Bold({ bubble: true }), // 在气泡菜单中渲染菜单按钮\n        new Underline({ bubble: true, menubar: false }), // 在气泡菜单而不在菜单栏中渲染菜单按钮\n        new Italic(),\n        new Strike(),\n        new ListItem(),\n        new BulletList(),\n        new OrderedList(),\n      ],\n      // 编辑器的内容\n      content: `\n        <h1>Heading</h1>\n        <p>This Editor is awesome!</p>\n      `,\n    };\n  },\n},\n<\/script>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 局部引用\n\n<template>\n  <div>\n    <el-tiptap\n      v-model=\"content\"\n      :extensions=\"extensions\"\n\t\t lang: 'zh',\n    />\n  </div>\n</template>\n\n<script>\nimport {\n  // 需要的 extensions\n  ElementTiptap ,\n  Doc,\n  Text,\n  Paragraph,\n  Heading,\n  Bold,\n  Underline,\n  Italic,\n  Strike,\n  ListItem,\n  BulletList,\n  OrderedList,\n} from 'element-tiptap';\nimport 'element-tiptap/lib/index.css';\n\nexport default {\n  data () {\n    // 编辑器的 extensions\n    // 它们将会按照你声明的顺序被添加到菜单栏和气泡菜单中\n    return {\n      extensions: [\n        new Doc(),\n        new Text(),\n        new Paragraph(),\n        new Heading({ level: 5 }),\n        new Bold({ bubble: true }), // 在气泡菜单中渲染菜单按钮\n        new Underline({ bubble: true, menubar: false }), // 在气泡菜单而不在菜单栏中渲染菜单按钮\n        new Italic(),\n        new Strike(),\n        new ListItem(),\n        new BulletList(),\n        new OrderedList(),\n      ],\n      // 编辑器的内容\n      content: `\n        <h1>Heading</h1>\n        <p>This Editor is awesome!</p>\n      `,\n    };\n  },\n},\n<\/script>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n",normalizedContent:"# 安装 elment-tiptap\n\nnpm install --save element-tiptap\n\n\n1\n\n\nhttps://github.com/leecason/element-tiptap/blob/master/readme_zh.md\n\n\n# 全局引用\n\nimport { elementtiptapplugin } from 'element-tiptap';\nimport 'element-tiptap/lib/index.css';\n\n\nvue.use(elementtiptapplugin, {\n  lang: \"zh\", // 见 i18n   必须设置lang否则报错\n  spellcheck: true, // 可被 editor 同名 prop 重写\n});\n// 现在你已经在全局注册了 `el-tiptap` 组件。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n<template>\n  <div>\n    <el-tiptap\n      v-model=\"content\"\n      :extensions=\"extensions\"\n    />\n  </div>\n</template>\n\n<script>\nimport {\n  // 需要的 extensions\n  doc,\n  text,\n  paragraph,\n  heading,\n  bold,\n  underline,\n  italic,\n  strike,\n  listitem,\n  bulletlist,\n  orderedlist,\n} from 'element-tiptap';\n\nexport default {\n  data () {\n    // 编辑器的 extensions\n    // 它们将会按照你声明的顺序被添加到菜单栏和气泡菜单中\n    return {\n      extensions: [\n        new doc(),\n        new text(),\n        new paragraph(),\n        new heading({ level: 5 }),\n        new bold({ bubble: true }), // 在气泡菜单中渲染菜单按钮\n        new underline({ bubble: true, menubar: false }), // 在气泡菜单而不在菜单栏中渲染菜单按钮\n        new italic(),\n        new strike(),\n        new listitem(),\n        new bulletlist(),\n        new orderedlist(),\n      ],\n      // 编辑器的内容\n      content: `\n        <h1>heading</h1>\n        <p>this editor is awesome!</p>\n      `,\n    };\n  },\n},\n<\/script>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 局部引用\n\n<template>\n  <div>\n    <el-tiptap\n      v-model=\"content\"\n      :extensions=\"extensions\"\n\t\t lang: 'zh',\n    />\n  </div>\n</template>\n\n<script>\nimport {\n  // 需要的 extensions\n  elementtiptap ,\n  doc,\n  text,\n  paragraph,\n  heading,\n  bold,\n  underline,\n  italic,\n  strike,\n  listitem,\n  bulletlist,\n  orderedlist,\n} from 'element-tiptap';\nimport 'element-tiptap/lib/index.css';\n\nexport default {\n  data () {\n    // 编辑器的 extensions\n    // 它们将会按照你声明的顺序被添加到菜单栏和气泡菜单中\n    return {\n      extensions: [\n        new doc(),\n        new text(),\n        new paragraph(),\n        new heading({ level: 5 }),\n        new bold({ bubble: true }), // 在气泡菜单中渲染菜单按钮\n        new underline({ bubble: true, menubar: false }), // 在气泡菜单而不在菜单栏中渲染菜单按钮\n        new italic(),\n        new strike(),\n        new listitem(),\n        new bulletlist(),\n        new orderedlist(),\n      ],\n      // 编辑器的内容\n      content: `\n        <h1>heading</h1>\n        <p>this editor is awesome!</p>\n      `,\n    };\n  },\n},\n<\/script>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"让两个组件之间通讯",frontmatter:{title:"让两个组件之间通讯",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/e5e482/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/09.%E8%AE%A9%E4%B8%A4%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B9%8B%E9%97%B4%E9%80%9A%E8%AE%AF.html",relativePath:"前端/02.Vue2/09.让两个组件之间通讯.md",key:"v-0b21a7f6",path:"/pages/e5e482/",headersStr:null,content:"# 让两个组件之间通讯\n\n// 全局通信总线\n// 可以让任何组件之间相互通信\n\nimport Vue from 'vue'\n\nexport default new Vue()\n\n// a 组件 要给 b 组件 发送数据\n\n// b 注册通信事件\n// import globalBus  from './global-bus'\n// globalBus.$on('自定义名称', () => {\n// 内容\n// })\n\n// a 发布通信事件\n// import globalBus from './global-bus'\n// globalBus.$emit('自定义名称',传送的数据)\n\n// 两端的事件名称必须一致\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n",normalizedContent:"# 让两个组件之间通讯\n\n// 全局通信总线\n// 可以让任何组件之间相互通信\n\nimport vue from 'vue'\n\nexport default new vue()\n\n// a 组件 要给 b 组件 发送数据\n\n// b 注册通信事件\n// import globalbus  from './global-bus'\n// globalbus.$on('自定义名称', () => {\n// 内容\n// })\n\n// a 发布通信事件\n// import globalbus from './global-bus'\n// globalbus.$emit('自定义名称',传送的数据)\n\n// 两端的事件名称必须一致\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"安装echart",frontmatter:{title:"安装echart",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/812e04/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/10.%E5%AE%89%E8%A3%85echart.html",relativePath:"前端/02.Vue2/10.安装echart.md",key:"v-bb4076c0",path:"/pages/812e04/",headers:[{level:2,title:"引入",slug:"引入",normalizedTitle:"引入",charIndex:48}],headersStr:"引入",content:"# 安装echart\n\nnpm install echarts --save\n\n\n1\n\n\n\n# 引入\n\n//import echarts from 'echarts'  会报错不清楚什么情况\nimport * as echarts from 'echarts'\n\n\n1\n2\n",normalizedContent:"# 安装echart\n\nnpm install echarts --save\n\n\n1\n\n\n\n# 引入\n\n//import echarts from 'echarts'  会报错不清楚什么情况\nimport * as echarts from 'echarts'\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"安装 cropperjs 图片裁切工具",frontmatter:{title:"安装 cropperjs 图片裁切工具",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/f0d787/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/08.%E5%AE%89%E8%A3%85%20cropperjs%20%E5%9B%BE%E7%89%87%E8%A3%81%E5%88%87%E5%B7%A5%E5%85%B7.html",relativePath:"前端/02.Vue2/08.安装 cropperjs 图片裁切工具.md",key:"v-e39cdcd2",path:"/pages/f0d787/",headersStr:null,content:"# 安装 cropperjs 图片裁切工具\n\n npm install cropperjs\n\n\n1\n",normalizedContent:"# 安装 cropperjs 图片裁切工具\n\n npm install cropperjs\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"文件对象",frontmatter:{title:"文件对象",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/2bf2c3/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/11.%E6%96%87%E4%BB%B6%E5%AF%B9%E8%B1%A1.html",relativePath:"前端/02.Vue2/11.文件对象.md",key:"v-fbf30870",path:"/pages/2bf2c3/",headersStr:null,content:"# 文件对象\n\n\x3c!-- 默认是单文件上传  赋予 multile属性可以多文件上传 --\x3e\n<input type='file' id='file'  multile>  \n<button onclick='sendFile()'>上传文件</button>\n<img id='img' src='' widhth='200'>\n\n\n1\n2\n3\n4\n\n\nfunction SsendFile() {\n    const file = document.querySelector('#file')\n    console.log(file)   //获取input按钮\n    console.log(file.files)   // 获得文件对象 用户上传的文件都在里面\n    const fileObj =file.files[0]  //只有单文件则为第0个 如有多个需要遍历 files伪数组\n    \n    //预览 如是图片文件,可以处理图片预览\n    const blob = windows.URL.createObjectURL(fileObj)\n    console.log(blob)  //返回的图片url 直接用img 赋予src\n    document.querySelector('#img').src = blob\n    \n    \n    // 发送请求\n    // 然后把文件对象 放到 FormData 中提交给后端\n    const fd = new FormData()\n    fd.append('接口要求的名词',fileObj)\n    //用ajax 或者 axios 发起请求\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n",normalizedContent:"# 文件对象\n\n\x3c!-- 默认是单文件上传  赋予 multile属性可以多文件上传 --\x3e\n<input type='file' id='file'  multile>  \n<button onclick='sendfile()'>上传文件</button>\n<img id='img' src='' widhth='200'>\n\n\n1\n2\n3\n4\n\n\nfunction ssendfile() {\n    const file = document.queryselector('#file')\n    console.log(file)   //获取input按钮\n    console.log(file.files)   // 获得文件对象 用户上传的文件都在里面\n    const fileobj =file.files[0]  //只有单文件则为第0个 如有多个需要遍历 files伪数组\n    \n    //预览 如是图片文件,可以处理图片预览\n    const blob = windows.url.createobjecturl(fileobj)\n    console.log(blob)  //返回的图片url 直接用img 赋予src\n    document.queryselector('#img').src = blob\n    \n    \n    // 发送请求\n    // 然后把文件对象 放到 formdata 中提交给后端\n    const fd = new formdata()\n    fd.append('接口要求的名词',fileobj)\n    //用ajax 或者 axios 发起请求\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"打包发布",frontmatter:{title:"打包发布",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/fc097d/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/12.%E6%89%93%E5%8C%85%E5%8F%91%E5%B8%83.html",relativePath:"前端/02.Vue2/12.打包发布.md",key:"v-332cf74c",path:"/pages/fc097d/",headers:[{level:2,title:"本地预览测试打包结果",slug:"本地预览测试打包结果",normalizedTitle:"本地预览测试打包结果",charIndex:138},{level:2,title:"GitHub Pages",slug:"github-pages",normalizedTitle:"github pages",charIndex:513},{level:2,title:"GitHub 域名",slug:"github-域名",normalizedTitle:"github 域名",charIndex:530},{level:3,title:"自定义域名",slug:"自定义域名",normalizedTitle:"自定义域名",charIndex:682},{level:2,title:"将我们的项目部署到 GitHub pages",slug:"将我们的项目部署到-github-pages",normalizedTitle:"将我们的项目部署到 github pages",charIndex:745}],headersStr:"本地预览测试打包结果 GitHub Pages GitHub 域名 自定义域名 将我们的项目部署到 GitHub pages",content:"# 打包发布\n\n在发布上线前,我们需要执行构建打包,把.less、.vue、.js 相关资源进行编译打包,转成浏览器可以直接识别运行的普通css、js、html\n\n# yarn run build 或者 yarn build\nnpm run build\n\n\n1\n2\n\n\n\n# 本地预览测试打包结果\n\n不能直接本地双击打开index.html运行\n\n将dist放到一个web服务器中运行测试如\n\n * Ngxin\n * Apache\n * tomacat\n * IIS\n * Node.js\n\n这里我们使用Vue官方推荐的Serve https://cli.vuejs.org/zh/guide/deployment.html#%E9%80%9A%E7%94%A8%E6%8C%87%E5%8D%97\n\n必须要打包后并且本地有dist目录\n\nnpm install -g serve # 全局安装的安装后不用安装\n# -s 参数的意思是将其架设在 Single-Page Application 模式下\n# 这个模式会处理即将提到的路由问题\nserve -s dist  # 以管理员身份进行运行\n\n\n1\n2\n3\n4\n\n\n\n# GitHub Pages\n\n\n# GitHub 域名\n\n每个用户提供了免费的域名\n\n * 用户名.github.io/仓库名/\n\n如果想要忽略后面的仓库名参数 如:用户.github.io,则需要把仓库名命名为用户名.github.io，GitHub默认会自动给这个仓库托管，有且只能有一个，这个仓库，因为每个用户只有一个域名。\n\n\n# 自定义域名\n\n 1. 域名后台添加一个CNAME 记录\n 2. 在项目中添加一个CNAME文件，并填入自定义域名地址\n\n\n# 将我们的项目部署到 GitHub pages\n\n 1. 因为项目中接口都是http协议,而GitHub上强制开启https,在https中无法发出http请求,所以我们要准备一个自己的域名才能使用htpp或htpps\n\n 2. 映射CNAME记录到github项目中\n\n 3. 生成令牌\n    \n    1. 用户设置\n    2. 开发者选项\n    3. 生成密钥\n    4. \n    5. 生成 复制token\n\n 4. 推送并在项目中设置中设置secret 把生成的令牌复制到新建的secret中\n    \n    1. \n    2. NAME: ACCESS_TOKEN TOKEN: TOKEN\n\n 5. 在项目中添加 .github/workflow/mian.yml\n    \n    \n    name: build and deploy\n    \n    # 当 master 分支 push 代码的时候触发 workflow\n    on:\n      push:\n        branches:\n        - master\n    \n    jobs:\n      build-deploy:\n        runs-on: ubuntu-latest\n        steps:\n        # 下载仓库代码\n        - uses: actions/checkout@v2\n    \n        # 缓存依赖\n        - name: Get yarn cache\n          id: yarn-cache\n          run: echo \"::set-output name=dir::$(yarn cache dir)\"\n        - uses: actions/cache@v1\n          with:\n            path: ${{ steps.yarn-cache.outputs.dir }}\n            key: ${{ runner.os }}-yarn-${{ hashFiles('**/yarn.lock') }}\n            restore-keys: |\n              ${{ runner.os }}-yarn-\n    \n        # 安装依赖\n        - run: yarn\n    \n        # 打包构建\n        - run: yarn build\n    \n        # 发布到 GitHub Pages\n        - name: Deploy\n          uses: peaceiris/actions-gh-pages@v2\n          env:\n            PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }} # 访问秘钥\n            PUBLISH_BRANCH: gh-pages # 推送分支\n            PUBLISH_DIR: ./dist # 部署目录\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    \n    \n    > GitHub Action 工作流\n\n 6. public 中 添加 CNAME 自定义域名的域名",normalizedContent:"# 打包发布\n\n在发布上线前,我们需要执行构建打包,把.less、.vue、.js 相关资源进行编译打包,转成浏览器可以直接识别运行的普通css、js、html\n\n# yarn run build 或者 yarn build\nnpm run build\n\n\n1\n2\n\n\n\n# 本地预览测试打包结果\n\n不能直接本地双击打开index.html运行\n\n将dist放到一个web服务器中运行测试如\n\n * ngxin\n * apache\n * tomacat\n * iis\n * node.js\n\n这里我们使用vue官方推荐的serve https://cli.vuejs.org/zh/guide/deployment.html#%e9%80%9a%e7%94%a8%e6%8c%87%e5%8d%97\n\n必须要打包后并且本地有dist目录\n\nnpm install -g serve # 全局安装的安装后不用安装\n# -s 参数的意思是将其架设在 single-page application 模式下\n# 这个模式会处理即将提到的路由问题\nserve -s dist  # 以管理员身份进行运行\n\n\n1\n2\n3\n4\n\n\n\n# github pages\n\n\n# github 域名\n\n每个用户提供了免费的域名\n\n * 用户名.github.io/仓库名/\n\n如果想要忽略后面的仓库名参数 如:用户.github.io,则需要把仓库名命名为用户名.github.io，github默认会自动给这个仓库托管，有且只能有一个，这个仓库，因为每个用户只有一个域名。\n\n\n# 自定义域名\n\n 1. 域名后台添加一个cname 记录\n 2. 在项目中添加一个cname文件，并填入自定义域名地址\n\n\n# 将我们的项目部署到 github pages\n\n 1. 因为项目中接口都是http协议,而github上强制开启https,在https中无法发出http请求,所以我们要准备一个自己的域名才能使用htpp或htpps\n\n 2. 映射cname记录到github项目中\n\n 3. 生成令牌\n    \n    1. 用户设置\n    2. 开发者选项\n    3. 生成密钥\n    4. \n    5. 生成 复制token\n\n 4. 推送并在项目中设置中设置secret 把生成的令牌复制到新建的secret中\n    \n    1. \n    2. name: access_token token: token\n\n 5. 在项目中添加 .github/workflow/mian.yml\n    \n    \n    name: build and deploy\n    \n    # 当 master 分支 push 代码的时候触发 workflow\n    on:\n      push:\n        branches:\n        - master\n    \n    jobs:\n      build-deploy:\n        runs-on: ubuntu-latest\n        steps:\n        # 下载仓库代码\n        - uses: actions/checkout@v2\n    \n        # 缓存依赖\n        - name: get yarn cache\n          id: yarn-cache\n          run: echo \"::set-output name=dir::$(yarn cache dir)\"\n        - uses: actions/cache@v1\n          with:\n            path: ${{ steps.yarn-cache.outputs.dir }}\n            key: ${{ runner.os }}-yarn-${{ hashfiles('**/yarn.lock') }}\n            restore-keys: |\n              ${{ runner.os }}-yarn-\n    \n        # 安装依赖\n        - run: yarn\n    \n        # 打包构建\n        - run: yarn build\n    \n        # 发布到 github pages\n        - name: deploy\n          uses: peaceiris/actions-gh-pages@v2\n          env:\n            personal_token: ${{ secrets.access_token }} # 访问秘钥\n            publish_branch: gh-pages # 推送分支\n            publish_dir: ./dist # 部署目录\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    \n    \n    > github action 工作流\n\n 6. public 中 添加 cname 自定义域名的域名",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"webpack",frontmatter:{title:"webpack",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/b60752/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/13.webpack.html",relativePath:"前端/02.Vue2/13.webpack.md",key:"v-56d6aafb",path:"/pages/b60752/",headers:[{level:2,title:"webpack 入门",slug:"webpack-入门",normalizedTitle:"webpack 入门",charIndex:251},{level:3,title:"引用",slug:"引用",normalizedTitle:"引用",charIndex:15},{level:3,title:"导出",slug:"导出",normalizedTitle:"导出",charIndex:307},{level:3,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:363},{level:3,title:"webpack.config.js 配置 代码分离",slug:"webpack-config-js-配置-代码分离",normalizedTitle:"webpack.config.js 配置 代码分离",charIndex:897},{level:2,title:"webpack 支持好几种模块",slug:"webpack-支持好几种模块",normalizedTitle:"webpack 支持好几种模块",charIndex:3299},{level:2,title:"自动清除dist目录",slug:"自动清除dist目录",normalizedTitle:"自动清除dist目录",charIndex:3371},{level:2,title:"ES6 转 ES5",slug:"es6-转-es5",normalizedTitle:"es6 转 es5",charIndex:3535},{level:3,title:"API 兼容处理",slug:"api-兼容处理",normalizedTitle:"api 兼容处理",charIndex:3955},{level:3,title:"开启缓存",slug:"开启缓存",normalizedTitle:"开启缓存",charIndex:4268},{level:2,title:"Using source maps 源代码地图导航",slug:"using-source-maps-源代码地图导航",normalizedTitle:"using source maps 源代码地图导航",charIndex:4584},{level:2,title:"watch 监听自动打包",slug:"watch-监听自动打包",normalizedTitle:"watch 监听自动打包",charIndex:4898},{level:2,title:"webpack-dev-server 监听自动打包并自动刷新",slug:"webpack-dev-server-监听自动打包并自动刷新",normalizedTitle:"webpack-dev-server 监听自动打包并自动刷新",charIndex:5064},{level:2,title:"热更新",slug:"热更新",normalizedTitle:"热更新",charIndex:5407},{level:2,title:"打包 Vue",slug:"打包-vue",normalizedTitle:"打包 vue",charIndex:5625},{level:2,title:"resolve.extensions 按顺序加载解析这些后缀名",slug:"resolve-extensions-按顺序加载解析这些后缀名",normalizedTitle:"resolve.extensions 按顺序加载解析这些后缀名",charIndex:6261},{level:2,title:"在webpack中不能使用@定向于src文件夹",slug:"在webpack中不能使用-定向于src文件夹",normalizedTitle:"在webpack中不能使用@定向于src文件夹",charIndex:6503},{level:2,title:"使用 ESLint",slug:"使用-eslint",normalizedTitle:"使用 eslint",charIndex:6730},{level:3,title:"选择ESLint 的代码规范",slug:"选择eslint-的代码规范",normalizedTitle:"选择eslint 的代码规范",charIndex:7939}],headersStr:"webpack 入门 引用 导出 安装 webpack.config.js 配置 代码分离 webpack 支持好几种模块 自动清除dist目录 ES6 转 ES5 API 兼容处理 开启缓存 Using source maps 源代码地图导航 watch 监听自动打包 webpack-dev-server 监听自动打包并自动刷新 热更新 打包 Vue resolve.extensions 按顺序加载解析这些后缀名 在webpack中不能使用@定向于src文件夹 使用 ESLint 选择ESLint 的代码规范",content:"# webpack\n\n以前我们引用外部资源都是用<script>标签引入\n\n<script src='vue.js'><\/script>\n\n\n1\n\n 1. 引用了没有使用 浪费资源\n 2. 没有引用 并创建实例 报错不明显 并且 网页无法加载\n 3. 引用太多笼统\n\n现在我们使用webpack我们选择直接使用 关键字 import\n\nimport 'vue' from './vue'\n\n\n1\n\n\n并拥有导包,构建等强大的功能\n\nhttps://webpack.docschina.org/\n\n\n# webpack 入门\n\n\n# 引用\n\nimport bar from './bar.js';\n\n\n1\n\n\n\n# 导出\n\nexport default function bar() {\n  //\n}\n\n\n1\n2\n3\n\n\n\n# 安装\n\n# 生成的默认的package.json\n\n在项目目录下\n\nnpm init -y\n\n\n1\n\n\n# 安装 webpack\n\nnpm install --save-dev webpack-cli webpack\n\n\n1\n\n\n# 生成main.js 打包\n\n.\\node_modules\\.bin\\webpack\n\n\n1\n\n\n# 修改package.json 简化打包命令\n\n//修改\"script中的对象\"\n\"scripts\":{\n    \"build\":\".\\node_modules\\.bin\\webpack\"  //也可以直接写为 \"build\":\"webpack\"  build自定义名称\n}\n\n\n1\n2\n3\n4\n\n\n#之后我们可以使用简化命令打包了\nnpm run build\n\n\n1\n2\n\n\n * # 还有一种方法是package.json中的script为空,会自动查找项目中的webpack\n\n# 然后用以下代码打包\nnpx webpack\n\n\n1\n2\n\n\n之后我们在index中直接加载main.js就好了,webpack已经把webpack文件夹中的打包了\n\n<script src='main.js'><\/script>\n\n\n1\n\n\n\n# webpack.config.js 配置 代码分离\n\n# 打包的入口\n\n设置预要打包的文件,webpack会根据里面的内容逐层引入\n\n module.exports = {\n     entry: '要打包的文件',\n }\n\n\n1\n2\n3\n\n\n# 打包的出口\n\n设置打包后的代码存放的路径\n\nmodule.exports = {\n\toutput: {\n   \t filename: 'main.js',  //打包后的文件名,相对路径\n    \tpath: path.resolve(__dirname, 'dist'),  \n        // 存放到当前项目的dist目录,必须得是一个绝对路径,否则报错\n        //path.resovle(__dirname,'dist') 获取当前文件所属目录的绝对路径再拼接后面的\n   \t}\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 打包模式\n\ndevelopmemt: 开发环境构建,编译速度快,没有压缩\n\nproduction:生产环境构建,编译速度慢,质量比较好,压缩体积\n\nmodule.exports = {\n    entry: '要打包的文件',\n\toutput: {\n   \t filename: 'main.js',  //打包后的文件名,相对路径\n    \tpath: path.resolve(__dirname, 'dist'),  \n        // 存放到当前项目的dist目录,必须得是一个绝对路径,否则报错\n        //path.resovle(__dirname,'dist') 获取当前文件所属目录的绝对路径再拼接后面的\n   \t},\n    mode:'development'  //打包模式,默认为生产环境\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 打包CSS\n\nwebpack本身只嫩理解 JavaScript 和 JSON 文件\n\n需要安装 style-loader 和 css-loader\n\nnpm install --save-dev style-loader css-loader\n\n\n1\n\n\n并在module.exports中追加以下代码,并打包成为JavaScript模块中\n\nmodule: {\n    rules: [\n        // 当匹配到css文件的时候,使用style-loader和css-loader处理,自动插入到index中<head>里面\n      {\n        test: /\\.css$/i,\n        use: ['style-loader', 'css-loader'],\n      },\n    ],\n  },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n其他类型文件模块更多请查询官方手册 https://webpack.docschina.org/loaders/\n\n# 打包 less\n\n安装\n\nnpm install less less-loader --save-dev\n\n\n1\n\n\n追加 module 对象\n\nrules: [\n      {\n        test: /\\.less$/i,\n        loader: [ 'style-loader','css-loader', 'less-loader']\n          //1. less-loader 把less 编译为 css\n          //2. css-loader 把css 转为 JavaScript 模块\n          //3. style-loader 在代码运行期间把css 插入页面的head中\n      },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 打包图片\n\n追加到 module webpack5内置这个loader\n\n      {\n        test: /\\.(png|svg|jpg|jpeg|gif)$/i,\n        type: 'asset/resource',\n      },\n\n\n1\n2\n3\n4\n\n\n引用\n\nimport Icon from './icon.png';\n\n  // 将图像添加到我们已经存在的 div 中。\n  const myIcon = new Image();\n  myIcon.src = Icon;\n\n  element.appendChild(myIcon);\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n或者打index也打包\n\n# 打包网页\n\n安装模块\n\nnpm install --save-dev html-webpack-plugin\n\n\n1\n\n\n追加moudule 文件属性\n\n  plugins: [\n    new HtmlWebpackPlugin({\n      title: '自定义名称',   // 自动生成一个新的html文件  并且自动引用打包的js文件\n    }),\n  ]\n\n\n1\n2\n3\n4\n5\n\n\n  plugins: [\n    new HtmlWebpackPlugin({\n      title: './index.html',   // 基于我们指定的html生成一个打包后的html,不用引用打包的js文件,打包后webpack会自动导入\n    }),\n  ]\n\n\n1\n2\n3\n4\n5\n\n\n# 打包字体\n\n追加moudule 文件属性\n\n      {\n        test: /\\.(woff|woff2|eot|ttf|otf)$/i,\n        type: 'asset/resource',\n      },\n\n\n1\n2\n3\n4\n\n\n\n# webpack 支持好几种模块\n\n * ECMAScript 6 模块\n * CommonJS 模块\n * node\n * AMD模块\n\n\n# 自动清除dist目录\n\n打包的过程并不会清除原来原有的文件,如有相同文件则覆盖\n\n\n\n module.exports = {\n   output: {\n    clean: true,  // 我们只需要在output中追一个参数既可 每次打包自动清除dist目录\n   },\n };\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# ES6 转 ES5\n\nIE中不支持 ES6 某些语法 所以要转为ES5\n\nbabel是一款专门将ES6转ES5 的 编译工具\n\n1.安装\n\n# 如果已经安装过webpack,把后面的webpack去掉\nnpm install -D babel-loader @babel/core @babel/preset-env webpack\n\n\n1\n2\n\n\n2.追加module\n\n {\n      test: /\\.m?js$/,\n      exclude: /(node_modules|bower_components)/,\n      use: {\n        loader: 'babel-loader',\n        options: {\n          presets: ['@babel/preset-env']\n        }\n      }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# API 兼容处理\n\nBabel 默认只能转换基本的 ESC 语法 如:const let 箭头函数 解构赋值\n\n它不会处理 ECMAScript 6 中 新增的API方法,如数组中的 includes方法 字符串的raw方法\n\nhttps://www.babeljs.cn/docs/babel-polyfill\n\n安装\n\nnpm install --save @babel/polyfill\n\n\n1\n\n\n在webpack.config中module.export追加\n\nmodule.exports = {\n  entry: [\"@babel/polyfill\", \"./app/js\"],\n};\n\n\n1\n2\n3\n\n\n\n# 开启缓存\n\nbabel 打包非常耗时,建议开启缓存用以提高打包的效率\n\n {\n      test: /\\.m?js$/,\n      exclude: /(node_modules|bower_components)/,\n      use: {\n        loader: 'babel-loader',\n        options: {\n          presets: ['@babel/preset-env'],\n              cacheDirectory:true  // 开启缓存\n        }\n      }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# Using source maps 源代码地图导航\n\n我们打包后的调试显示的行数与未打包的文件行数不匹配,我们可以添加源代码地图导航\n\n module.exports = {\n   devtool: 'inline-source-map',  //追加,源代码地图导航\n }\n\n\n1\n2\n3\n\n\ninline-source-map 把 source-map 数据和打包结果 存储在同一文件下,不太推荐,体积过大\n\nsource-map: 把 source-map 数据 生成到独立的.map文件中\n\n module.exports = {\n   devtool: 'source-map',  \n }\n\n\n1\n2\n3\n\n\n\n# watch 监听自动打包\n\n当项目中有修改后自动打包\n\n在packge.json文件中追加\n\n\"scripts\":{\n\t\"build-watch\":\"自定义名称 --watch --config webpack.config.js\"\n}\n\n\n1\n2\n3\n\n\n然后打包一下\n\nnpm run build-watch\n\n\n1\n\n\n\n# webpack-dev-server 监听自动打包并自动刷新\n\n开启一个web服务器,当项目中有修改后自动打包并自动刷新浏览器\n\nmodle.experts = {\n     devServer: {\n    contentBase: './dist',  // devServer服务监听这个打包目录\n  },\n}\n\n\n1\n2\n3\n4\n5\n\n\n并在packge.json追加\n\n\"start\": \"webpack serve --open --config webpack.config.js\",  //--open 是自动打开浏览器 可以省参数   --config webpack.config.js 如果配置项为webpack.config.js则不需要这个\n\n\n1\n\n\n\n# 热更新\n\nwebpack-dev-server默认是刷新整个页面实现更新。我们有一种更好的方式 热更新, 可以在不刷新页面的情况下更新内容变化\n\n只需要在devserver中添加hot:true即可\n\nmodle.experts = {\n     devServer: {\n    contentBase: './dist', \n         hot:true // 开启热更新\n  },\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 打包 Vue\n\n项目中安装\n\nnpm i vue\n\n\n1\n\n\n在webpack安装vue-loader 和 vue-template-compiler vue打包模块\n\nnpm install -D vue-loader vue-template-compiler\n\n\n1\n\n\n在webpack配置中引用 https://vue-loader.vuejs.org/zh/guide/#%E6%89%8B%E5%8A%A8%E8%AE%BE%E7%BD%AE\n\nconst { VueLoaderPlugin } = require('vue-loader')\n\n// 并在module.exports中追加\n plugins: [\n    // 请确保引入这个插件！\n    new VueLoaderPlugin()\n  ]\n\n// 并在rules添加文件匹配规则\n  {\n        test: /\\.vue$/,\n        loader: 'vue-loader'\n      }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n 1. 在webpack中的Vue组件 import要加上后缀,无法像cli中忽略后缀\n 2. 在vue使用less 也需要进行配置\n 3. 把loader换成use:['style-loader','css-loader', 'less-loader','vue-loader'] 顺序是从后到前\n\n\n# resolve.extensions 按顺序加载解析这些后缀名\n\nmodule.exports = {\n  //...\n  resolve: {\n    // extensions: ['.js', '.json', '.wasm'],  // 默认值\n      extensions:['.js', '.json','.vue','.css']\n  },\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n配置完后 import时扩展名可以忽略 按你自定义的顺序来解析 从后到前\n\n\n# 在webpack中不能使用@定向于src文件夹\n\n我们可以使用webpack中的resolve.alias创建 import 或 require 的别名，来确保模块引入变得更简单\n\n resolve: {\n    alias: {\n      //别名 : 路径   自定义名称是特殊字符要用引号括起\n      '@': path.resolve(__dirname, './src'),\n    },\n  },\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 使用 ESLint\n\nESLint 是用于JavaScript代码格式校验工具\n\n 1. 安装\n\nnpm install eslint-loader --save-dev\nnpm install eslint --save-dev\n\n\n1\n2\n\n 2. 追加webpack.json\n\nmodule.exports = {\n  // ...\n  module: {\n    rules: [\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'eslint-loader',\n        options: {\n          // eslint options (if necessary)\n        },\n      },\n    ],\n  },\n  // ...\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n如果使用了ES6 转 ES5 模块 会与 ESLint 产生冲突,我们想要优先把ESLint 校验放在前面,但配置选项不是按照书写顺序来执行的,所以我们需要把test 为 js 文件的 loader配置 换use配置,use:['babel-loader', ,'eslint-loader']\n\nmodule.exports = {\n  // ...\n  module: {\n    rules: [\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        use: ['babel-loader', 'eslint-loader'],\n      },\n    ],\n  },\n  // ...\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n也可拆分书写\n\nmodule.exports = {\n  // ...\n  module: {\n    rules: [\n      {\n        enforce: 'pre',   // 强制 提前\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'eslint-loader',\n      },\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'babel-loader',\n      },\n    ],\n  },\n  // ...\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 选择ESLint 的代码规范\n\n打开eslint命令符\n\n.\\node_modules\\.bin\\eslint.cmd --init\n# 或者 npx eslint --init 详情简写请看 安装中的修改package.json 简化打包命令\n\n\n1\n2\n",normalizedContent:"# webpack\n\n以前我们引用外部资源都是用<script>标签引入\n\n<script src='vue.js'><\/script>\n\n\n1\n\n 1. 引用了没有使用 浪费资源\n 2. 没有引用 并创建实例 报错不明显 并且 网页无法加载\n 3. 引用太多笼统\n\n现在我们使用webpack我们选择直接使用 关键字 import\n\nimport 'vue' from './vue'\n\n\n1\n\n\n并拥有导包,构建等强大的功能\n\nhttps://webpack.docschina.org/\n\n\n# webpack 入门\n\n\n# 引用\n\nimport bar from './bar.js';\n\n\n1\n\n\n\n# 导出\n\nexport default function bar() {\n  //\n}\n\n\n1\n2\n3\n\n\n\n# 安装\n\n# 生成的默认的package.json\n\n在项目目录下\n\nnpm init -y\n\n\n1\n\n\n# 安装 webpack\n\nnpm install --save-dev webpack-cli webpack\n\n\n1\n\n\n# 生成main.js 打包\n\n.\\node_modules\\.bin\\webpack\n\n\n1\n\n\n# 修改package.json 简化打包命令\n\n//修改\"script中的对象\"\n\"scripts\":{\n    \"build\":\".\\node_modules\\.bin\\webpack\"  //也可以直接写为 \"build\":\"webpack\"  build自定义名称\n}\n\n\n1\n2\n3\n4\n\n\n#之后我们可以使用简化命令打包了\nnpm run build\n\n\n1\n2\n\n\n * # 还有一种方法是package.json中的script为空,会自动查找项目中的webpack\n\n# 然后用以下代码打包\nnpx webpack\n\n\n1\n2\n\n\n之后我们在index中直接加载main.js就好了,webpack已经把webpack文件夹中的打包了\n\n<script src='main.js'><\/script>\n\n\n1\n\n\n\n# webpack.config.js 配置 代码分离\n\n# 打包的入口\n\n设置预要打包的文件,webpack会根据里面的内容逐层引入\n\n module.exports = {\n     entry: '要打包的文件',\n }\n\n\n1\n2\n3\n\n\n# 打包的出口\n\n设置打包后的代码存放的路径\n\nmodule.exports = {\n\toutput: {\n   \t filename: 'main.js',  //打包后的文件名,相对路径\n    \tpath: path.resolve(__dirname, 'dist'),  \n        // 存放到当前项目的dist目录,必须得是一个绝对路径,否则报错\n        //path.resovle(__dirname,'dist') 获取当前文件所属目录的绝对路径再拼接后面的\n   \t}\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 打包模式\n\ndevelopmemt: 开发环境构建,编译速度快,没有压缩\n\nproduction:生产环境构建,编译速度慢,质量比较好,压缩体积\n\nmodule.exports = {\n    entry: '要打包的文件',\n\toutput: {\n   \t filename: 'main.js',  //打包后的文件名,相对路径\n    \tpath: path.resolve(__dirname, 'dist'),  \n        // 存放到当前项目的dist目录,必须得是一个绝对路径,否则报错\n        //path.resovle(__dirname,'dist') 获取当前文件所属目录的绝对路径再拼接后面的\n   \t},\n    mode:'development'  //打包模式,默认为生产环境\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 打包css\n\nwebpack本身只嫩理解 javascript 和 json 文件\n\n需要安装 style-loader 和 css-loader\n\nnpm install --save-dev style-loader css-loader\n\n\n1\n\n\n并在module.exports中追加以下代码,并打包成为javascript模块中\n\nmodule: {\n    rules: [\n        // 当匹配到css文件的时候,使用style-loader和css-loader处理,自动插入到index中<head>里面\n      {\n        test: /\\.css$/i,\n        use: ['style-loader', 'css-loader'],\n      },\n    ],\n  },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n其他类型文件模块更多请查询官方手册 https://webpack.docschina.org/loaders/\n\n# 打包 less\n\n安装\n\nnpm install less less-loader --save-dev\n\n\n1\n\n\n追加 module 对象\n\nrules: [\n      {\n        test: /\\.less$/i,\n        loader: [ 'style-loader','css-loader', 'less-loader']\n          //1. less-loader 把less 编译为 css\n          //2. css-loader 把css 转为 javascript 模块\n          //3. style-loader 在代码运行期间把css 插入页面的head中\n      },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 打包图片\n\n追加到 module webpack5内置这个loader\n\n      {\n        test: /\\.(png|svg|jpg|jpeg|gif)$/i,\n        type: 'asset/resource',\n      },\n\n\n1\n2\n3\n4\n\n\n引用\n\nimport icon from './icon.png';\n\n  // 将图像添加到我们已经存在的 div 中。\n  const myicon = new image();\n  myicon.src = icon;\n\n  element.appendchild(myicon);\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n或者打index也打包\n\n# 打包网页\n\n安装模块\n\nnpm install --save-dev html-webpack-plugin\n\n\n1\n\n\n追加moudule 文件属性\n\n  plugins: [\n    new htmlwebpackplugin({\n      title: '自定义名称',   // 自动生成一个新的html文件  并且自动引用打包的js文件\n    }),\n  ]\n\n\n1\n2\n3\n4\n5\n\n\n  plugins: [\n    new htmlwebpackplugin({\n      title: './index.html',   // 基于我们指定的html生成一个打包后的html,不用引用打包的js文件,打包后webpack会自动导入\n    }),\n  ]\n\n\n1\n2\n3\n4\n5\n\n\n# 打包字体\n\n追加moudule 文件属性\n\n      {\n        test: /\\.(woff|woff2|eot|ttf|otf)$/i,\n        type: 'asset/resource',\n      },\n\n\n1\n2\n3\n4\n\n\n\n# webpack 支持好几种模块\n\n * ecmascript 6 模块\n * commonjs 模块\n * node\n * amd模块\n\n\n# 自动清除dist目录\n\n打包的过程并不会清除原来原有的文件,如有相同文件则覆盖\n\n\n\n module.exports = {\n   output: {\n    clean: true,  // 我们只需要在output中追一个参数既可 每次打包自动清除dist目录\n   },\n };\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# es6 转 es5\n\nie中不支持 es6 某些语法 所以要转为es5\n\nbabel是一款专门将es6转es5 的 编译工具\n\n1.安装\n\n# 如果已经安装过webpack,把后面的webpack去掉\nnpm install -d babel-loader @babel/core @babel/preset-env webpack\n\n\n1\n2\n\n\n2.追加module\n\n {\n      test: /\\.m?js$/,\n      exclude: /(node_modules|bower_components)/,\n      use: {\n        loader: 'babel-loader',\n        options: {\n          presets: ['@babel/preset-env']\n        }\n      }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# api 兼容处理\n\nbabel 默认只能转换基本的 esc 语法 如:const let 箭头函数 解构赋值\n\n它不会处理 ecmascript 6 中 新增的api方法,如数组中的 includes方法 字符串的raw方法\n\nhttps://www.babeljs.cn/docs/babel-polyfill\n\n安装\n\nnpm install --save @babel/polyfill\n\n\n1\n\n\n在webpack.config中module.export追加\n\nmodule.exports = {\n  entry: [\"@babel/polyfill\", \"./app/js\"],\n};\n\n\n1\n2\n3\n\n\n\n# 开启缓存\n\nbabel 打包非常耗时,建议开启缓存用以提高打包的效率\n\n {\n      test: /\\.m?js$/,\n      exclude: /(node_modules|bower_components)/,\n      use: {\n        loader: 'babel-loader',\n        options: {\n          presets: ['@babel/preset-env'],\n              cachedirectory:true  // 开启缓存\n        }\n      }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# using source maps 源代码地图导航\n\n我们打包后的调试显示的行数与未打包的文件行数不匹配,我们可以添加源代码地图导航\n\n module.exports = {\n   devtool: 'inline-source-map',  //追加,源代码地图导航\n }\n\n\n1\n2\n3\n\n\ninline-source-map 把 source-map 数据和打包结果 存储在同一文件下,不太推荐,体积过大\n\nsource-map: 把 source-map 数据 生成到独立的.map文件中\n\n module.exports = {\n   devtool: 'source-map',  \n }\n\n\n1\n2\n3\n\n\n\n# watch 监听自动打包\n\n当项目中有修改后自动打包\n\n在packge.json文件中追加\n\n\"scripts\":{\n\t\"build-watch\":\"自定义名称 --watch --config webpack.config.js\"\n}\n\n\n1\n2\n3\n\n\n然后打包一下\n\nnpm run build-watch\n\n\n1\n\n\n\n# webpack-dev-server 监听自动打包并自动刷新\n\n开启一个web服务器,当项目中有修改后自动打包并自动刷新浏览器\n\nmodle.experts = {\n     devserver: {\n    contentbase: './dist',  // devserver服务监听这个打包目录\n  },\n}\n\n\n1\n2\n3\n4\n5\n\n\n并在packge.json追加\n\n\"start\": \"webpack serve --open --config webpack.config.js\",  //--open 是自动打开浏览器 可以省参数   --config webpack.config.js 如果配置项为webpack.config.js则不需要这个\n\n\n1\n\n\n\n# 热更新\n\nwebpack-dev-server默认是刷新整个页面实现更新。我们有一种更好的方式 热更新, 可以在不刷新页面的情况下更新内容变化\n\n只需要在devserver中添加hot:true即可\n\nmodle.experts = {\n     devserver: {\n    contentbase: './dist', \n         hot:true // 开启热更新\n  },\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 打包 vue\n\n项目中安装\n\nnpm i vue\n\n\n1\n\n\n在webpack安装vue-loader 和 vue-template-compiler vue打包模块\n\nnpm install -d vue-loader vue-template-compiler\n\n\n1\n\n\n在webpack配置中引用 https://vue-loader.vuejs.org/zh/guide/#%e6%89%8b%e5%8a%a8%e8%ae%be%e7%bd%ae\n\nconst { vueloaderplugin } = require('vue-loader')\n\n// 并在module.exports中追加\n plugins: [\n    // 请确保引入这个插件！\n    new vueloaderplugin()\n  ]\n\n// 并在rules添加文件匹配规则\n  {\n        test: /\\.vue$/,\n        loader: 'vue-loader'\n      }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n 1. 在webpack中的vue组件 import要加上后缀,无法像cli中忽略后缀\n 2. 在vue使用less 也需要进行配置\n 3. 把loader换成use:['style-loader','css-loader', 'less-loader','vue-loader'] 顺序是从后到前\n\n\n# resolve.extensions 按顺序加载解析这些后缀名\n\nmodule.exports = {\n  //...\n  resolve: {\n    // extensions: ['.js', '.json', '.wasm'],  // 默认值\n      extensions:['.js', '.json','.vue','.css']\n  },\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n配置完后 import时扩展名可以忽略 按你自定义的顺序来解析 从后到前\n\n\n# 在webpack中不能使用@定向于src文件夹\n\n我们可以使用webpack中的resolve.alias创建 import 或 require 的别名，来确保模块引入变得更简单\n\n resolve: {\n    alias: {\n      //别名 : 路径   自定义名称是特殊字符要用引号括起\n      '@': path.resolve(__dirname, './src'),\n    },\n  },\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 使用 eslint\n\neslint 是用于javascript代码格式校验工具\n\n 1. 安装\n\nnpm install eslint-loader --save-dev\nnpm install eslint --save-dev\n\n\n1\n2\n\n 2. 追加webpack.json\n\nmodule.exports = {\n  // ...\n  module: {\n    rules: [\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'eslint-loader',\n        options: {\n          // eslint options (if necessary)\n        },\n      },\n    ],\n  },\n  // ...\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n如果使用了es6 转 es5 模块 会与 eslint 产生冲突,我们想要优先把eslint 校验放在前面,但配置选项不是按照书写顺序来执行的,所以我们需要把test 为 js 文件的 loader配置 换use配置,use:['babel-loader', ,'eslint-loader']\n\nmodule.exports = {\n  // ...\n  module: {\n    rules: [\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        use: ['babel-loader', 'eslint-loader'],\n      },\n    ],\n  },\n  // ...\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n也可拆分书写\n\nmodule.exports = {\n  // ...\n  module: {\n    rules: [\n      {\n        enforce: 'pre',   // 强制 提前\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'eslint-loader',\n      },\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'babel-loader',\n      },\n    ],\n  },\n  // ...\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 选择eslint 的代码规范\n\n打开eslint命令符\n\n.\\node_modules\\.bin\\eslint.cmd --init\n# 或者 npx eslint --init 详情简写请看 安装中的修改package.json 简化打包命令\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"vue 版本",frontmatter:{title:"vue 版本",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/0a8d91/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/14.vue%20%E7%89%88%E6%9C%AC.html",relativePath:"前端/02.Vue2/14.vue 版本.md",key:"v-5e457954",path:"/pages/0a8d91/",headers:[{level:2,title:"查看当前项目vue cli版本",slug:"查看当前项目vue-cli版本",normalizedTitle:"查看当前项目vue cli版本",charIndex:13},{level:2,title:"升级当前 vue cli 版本",slug:"升级当前-vue-cli-版本",normalizedTitle:"升级当前 vue cli 版本",charIndex:53}],headersStr:"查看当前项目vue cli版本 升级当前 vue cli 版本",content:"# vue 版本\n\n\n# 查看当前项目vue cli版本\n\nvue --version\n\n\n1\n\n\n\n# 升级当前 vue cli 版本\n\n记得先存储到本地库再升级\n\nvue upgrade\n\n\n1\n",normalizedContent:"# vue 版本\n\n\n# 查看当前项目vue cli版本\n\nvue --version\n\n\n1\n\n\n\n# 升级当前 vue cli 版本\n\n记得先存储到本地库再升级\n\nvue upgrade\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"优化打包",frontmatter:{title:"优化打包",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/b8f860/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/15.%E4%BC%98%E5%8C%96%E6%89%93%E5%8C%85.html",relativePath:"前端/02.Vue2/15.优化打包.md",key:"v-060fb40c",path:"/pages/b8f860/",headers:[{level:2,title:"通过 report 查看不同包打包耗时",slug:"通过-report-查看不同包打包耗时",normalizedTitle:"通过 report 查看不同包打包耗时",charIndex:11},{level:2,title:"Gzip压缩",slug:"gzip压缩",normalizedTitle:"gzip压缩",charIndex:192},{level:2,title:"不打包第三方包",slug:"不打包第三方包",normalizedTitle:"不打包第三方包",charIndex:620}],headersStr:"通过 report 查看不同包打包耗时 Gzip压缩 不打包第三方包",content:"# 优化打包\n\n\n# 通过 report 查看不同包打包耗时\n\n在package.json 中scripts的build:'vue-cli-service build --report' 加上--report配置\n\nnpm run build  # 打包\n\n\n1\n\n\n打包成功后在dist文件夹的report.html并在浏览器中打开,通过图形的分析,可以看到每个包的占比.\n\n\n# Gzip压缩\n\nHTTP协议上的GZIP编码能帮助我们压缩网站资源文件,减少要传输的文件大小,使流量减少加载更快\n\n如何开启?它需要前后端配置才能使用.后端要把服务器软件开启Gzip压缩功能(主流的服务器软件默认都是开启Gzip),客户端不需要做任何处理,取决于浏览器(太旧的浏览器可能不支持)\n\n如何检测内容是否已开启了Gzip压缩?可以查看响应头是否有以下配置\n\nContent-Encoding: gzip\n\n\n1\n\n\n使用 Vue CLI 官方推荐的serve 命令行工具\n\n#1. 安装 工具\nnpm install --global serve\n\n# 查看版本\nserve --version\n\n# 在打包结果目录执行下面命令启动一个 http 静态服务(默认开启 Gzip 压缩启动服务)\nserve -s ./\n\n# 禁用gzip 使用 -u参数\nserve -s -u ./\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 不打包第三方包\n\n通过<script src=''>标签引用的第三方,wepack不做任何处理\n\n所以我们可以通过cdn来加载第三方,优化打包速度.\n\nhttps://www.jsdelivr.com/\n\n注意模块版本 详细模块信息package.json\n\n<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/element-ui@2.15.3/lib/theme-chalk/index.css'>\n    \x3c!-- 先加载css样式 再引入vue 再引入 element --\x3e\n<script src='https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js'><\/script>\n<script src='https://cdn.jsdelivr.net/npm/element-ui@2.15.3/lib/index.js'><\/script>\n\n\n1\n2\n3\n4\n\n\n加载完后,是无法直接使用的 因为我们通过import导入\n\n所以我得再在vue.config.js中追加\n\nmodule.exports = {\n    configureWebpack:{\nextenals:{\n    // 因为在index.html 加载的 所以得全局暴露\n    // 属性名 : 加载的包名\n    \t'vue' : 'Vue',\n    \t'elment-ui': 'ELEMENT'\n\t\t}\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n",normalizedContent:"# 优化打包\n\n\n# 通过 report 查看不同包打包耗时\n\n在package.json 中scripts的build:'vue-cli-service build --report' 加上--report配置\n\nnpm run build  # 打包\n\n\n1\n\n\n打包成功后在dist文件夹的report.html并在浏览器中打开,通过图形的分析,可以看到每个包的占比.\n\n\n# gzip压缩\n\nhttp协议上的gzip编码能帮助我们压缩网站资源文件,减少要传输的文件大小,使流量减少加载更快\n\n如何开启?它需要前后端配置才能使用.后端要把服务器软件开启gzip压缩功能(主流的服务器软件默认都是开启gzip),客户端不需要做任何处理,取决于浏览器(太旧的浏览器可能不支持)\n\n如何检测内容是否已开启了gzip压缩?可以查看响应头是否有以下配置\n\ncontent-encoding: gzip\n\n\n1\n\n\n使用 vue cli 官方推荐的serve 命令行工具\n\n#1. 安装 工具\nnpm install --global serve\n\n# 查看版本\nserve --version\n\n# 在打包结果目录执行下面命令启动一个 http 静态服务(默认开启 gzip 压缩启动服务)\nserve -s ./\n\n# 禁用gzip 使用 -u参数\nserve -s -u ./\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 不打包第三方包\n\n通过<script src=''>标签引用的第三方,wepack不做任何处理\n\n所以我们可以通过cdn来加载第三方,优化打包速度.\n\nhttps://www.jsdelivr.com/\n\n注意模块版本 详细模块信息package.json\n\n<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/element-ui@2.15.3/lib/theme-chalk/index.css'>\n    \x3c!-- 先加载css样式 再引入vue 再引入 element --\x3e\n<script src='https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js'><\/script>\n<script src='https://cdn.jsdelivr.net/npm/element-ui@2.15.3/lib/index.js'><\/script>\n\n\n1\n2\n3\n4\n\n\n加载完后,是无法直接使用的 因为我们通过import导入\n\n所以我得再在vue.config.js中追加\n\nmodule.exports = {\n    configurewebpack:{\nextenals:{\n    // 因为在index.html 加载的 所以得全局暴露\n    // 属性名 : 加载的包名\n    \t'vue' : 'vue',\n    \t'elment-ui': 'element'\n\t\t}\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"vue 图形界面",frontmatter:{title:"vue 图形界面",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/c64d65/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/16.vue%20%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2.html",relativePath:"前端/02.Vue2/16.vue 图形界面.md",key:"v-c91d0ee8",path:"/pages/c64d65/",headersStr:null,content:"# vue 图形界面\n\nvue ui\n\n\n1\n\n\n自动在8000端口启动一个vue的ui图形界面 可以创建项目 管理项目 导入项目",normalizedContent:"# vue 图形界面\n\nvue ui\n\n\n1\n\n\n自动在8000端口启动一个vue的ui图形界面 可以创建项目 管理项目 导入项目",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"路由懒加载",frontmatter:{title:"路由懒加载",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/27ac6a/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/17.%E8%B7%AF%E7%94%B1%E6%87%92%E5%8A%A0%E8%BD%BD.html",relativePath:"前端/02.Vue2/17.路由懒加载.md",key:"v-6cbdabde",path:"/pages/27ac6a/",headers:[{level:2,title:"把组件按组分块",slug:"把组件按组分块",normalizedTitle:"把组件按组分块",charIndex:305}],headersStr:"把组件按组分块",content:"# 路由懒加载\n\n路由懒加载 主要针对于我们自己的代码,默认情况下我们写的代码会被打包到app.xxx.js中,如果我们的代码非常多,打包的文件越来越大,太大会影响首次加载速度.所以我们可以通过路由懒加载来优化它\n\n//我们默认是import 自定义名称 from 组件路径 只针对本地加载的引用\nimport Login from '@/views/login/'\n\n\n1\n2\n\n\n// 我们通过箭头函数来引用  webpack 就会把所有箭头函数引用 加载的资源分割成一个个独立的代码文件块\nconst Login = () => import ('@/views/login/')\n\n\n1\n2\n\n\n\n# 把组件按组分块\n\n// /* webpackChunkName: \"group-foo\" */ 为分组关键字  group-foo 为组名\nconst Login = () => import(/* webpackChunkName: \"group-foo\" */ '@/views/login/')\n\n\n1\n2\n\n\n当这个组某一个模块加载了则此组所有模块都加载了",normalizedContent:"# 路由懒加载\n\n路由懒加载 主要针对于我们自己的代码,默认情况下我们写的代码会被打包到app.xxx.js中,如果我们的代码非常多,打包的文件越来越大,太大会影响首次加载速度.所以我们可以通过路由懒加载来优化它\n\n//我们默认是import 自定义名称 from 组件路径 只针对本地加载的引用\nimport login from '@/views/login/'\n\n\n1\n2\n\n\n// 我们通过箭头函数来引用  webpack 就会把所有箭头函数引用 加载的资源分割成一个个独立的代码文件块\nconst login = () => import ('@/views/login/')\n\n\n1\n2\n\n\n\n# 把组件按组分块\n\n// /* webpackchunkname: \"group-foo\" */ 为分组关键字  group-foo 为组名\nconst login = () => import(/* webpackchunkname: \"group-foo\" */ '@/views/login/')\n\n\n1\n2\n\n\n当这个组某一个模块加载了则此组所有模块都加载了",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"缓存和并行处理",frontmatter:{title:"缓存和并行处理",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/14f647/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/19.%E7%BC%93%E5%AD%98%E5%92%8C%E5%B9%B6%E8%A1%8C%E5%A4%84%E7%90%86.html",relativePath:"前端/02.Vue2/19.缓存和并行处理.md",key:"v-1b1a2fda",path:"/pages/14f647/",headersStr:null,content:"# 缓存和并行处理\n\nVue CLI 内置了\n\ncache-loader 会默认为vue/bable/typescrip 开启,文件缓存会在 node_modules/.cache 中,如编译预到问题可以尝试先删除缓存目录\n\nthread-loader 会在多核cpu的机器上为babel/typescript 转译开启",normalizedContent:"# 缓存和并行处理\n\nvue cli 内置了\n\ncache-loader 会默认为vue/bable/typescrip 开启,文件缓存会在 node_modules/.cache 中,如编译预到问题可以尝试先删除缓存目录\n\nthread-loader 会在多核cpu的机器上为babel/typescript 转译开启",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"element 按需引用",frontmatter:{title:"element 按需引用",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/1bc0dd/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/18.element%20%E6%8C%89%E9%9C%80%E5%BC%95%E7%94%A8.html",relativePath:"前端/02.Vue2/18.element 按需引用.md",key:"v-6d96d148",path:"/pages/1bc0dd/",headersStr:null,content:'# element 按需引用\n\n安装\n\nnpm install babel-plugin-component -D\n\n\n1\n\n\n然后，将 .babelrc 修改为：\n\n{\n  "presets": [["es2015", { "modules": false }]],\n  "plugins": [\n    [\n      "component",\n      {\n        "libraryName": "element-ui",\n        "styleLibraryName": "theme-chalk"\n      }\n    ]\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n按组件引用\n\nimport { Button, Select } from \'element-ui\';\n\nVue.component(Button.name, Button);\nVue.component(Select.name, Select);\n\n\n1\n2\n3\n4\n',normalizedContent:'# element 按需引用\n\n安装\n\nnpm install babel-plugin-component -d\n\n\n1\n\n\n然后，将 .babelrc 修改为：\n\n{\n  "presets": [["es2015", { "modules": false }]],\n  "plugins": [\n    [\n      "component",\n      {\n        "libraryname": "element-ui",\n        "stylelibraryname": "theme-chalk"\n      }\n    ]\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n按组件引用\n\nimport { button, select } from \'element-ui\';\n\nvue.component(button.name, button);\nvue.component(select.name, select);\n\n\n1\n2\n3\n4\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"JavaScript 异步编程",frontmatter:{title:"JavaScript 异步编程",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/195534/",categories:["前端","Vue2"],tags:[null]},regularPath:"/%E5%89%8D%E7%AB%AF/02.Vue2/20.JavaScript%20%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B.html",relativePath:"前端/02.Vue2/20.JavaScript 异步编程.md",key:"v-49d4e880",path:"/pages/195534/",headers:[{level:3,title:"定时器",slug:"定时器",normalizedTitle:"定时器",charIndex:22},{level:3,title:"ajax请求",slug:"ajax请求",normalizedTitle:"ajax请求",charIndex:185},{level:2,title:"1. callback 回调函数",slug:"_1-callback-回调函数",normalizedTitle:"1. callback 回调函数",charIndex:490},{level:2,title:"2. Promise",slug:"_2-promise",normalizedTitle:"2. promise",charIndex:717},{level:2,title:"3.Generator 生成器函数(淘汰了)",slug:"_3-generator-生成器函数-淘汰了",normalizedTitle:"3.generator 生成器函数(淘汰了)",charIndex:870},{level:2,title:"4. Async 函数 (推荐)",slug:"_4-async-函数-推荐",normalizedTitle:"4. async 函数 (推荐)",charIndex:897},{level:3,title:"Async 的 返回值",slug:"async-的-返回值",normalizedTitle:"async 的 返回值",charIndex:1595},{level:2,title:"async 函数的错误处理",slug:"async-函数的错误处理",normalizedTitle:"async 函数的错误处理",charIndex:2136}],headersStr:"定时器 ajax请求 1. callback 回调函数 2. Promise 3.Generator 生成器函数(淘汰了) 4. Async 函数 (推荐) Async 的 返回值 async 函数的错误处理",content:"# JavaScript 异步编程\n\n\n# 定时器\n\nconsole.log(1)\n\nsetTimeout() => {\n    console.log(2)\n}\n\nconsole.log(3)\n\n// 输出结果是 1 3 2  \n// 往下执行时 不会等待setTimeout定时器的结束 而是直接向下继续执行\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# ajax请求\n\n//无论是使用原生的 XMLHttpRequest 还是 Jquery 的ajax 还是是 axios 都是异步的\n\n$.ajax({\n    method:'',\n    url:'',\n    success: function(data) {\n        // data 就是响应结果\n    }\n})\n\n// 异步代码 往往都伴随一个回调函数来接受结果  回头调用\naxios({\n    methos:'',\n    url:''\n}).then(res => {\n    \n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 1. callback 回调函数\n\n有时候,我们需要在异步操作的结果中执行另外一个异步操作\n\naxios({\n    methos:'',\n    url:''\n}).then(res => {\n    axios({\n    methos:'',\n    url:''\n\t\t}).then(res => {\n \n\t\t\t})\n})\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n当出现类型上面的嵌套异步时,非常不便于阅读和理解\n\n\n# 2. Promise\n\nnew Promist((resovle,reject) => {\n    setTimeout(() => {\n        resolve()  // 在Promise中 成功resolve 失败 reject\n    },1000)\n})\n\n\n1\n2\n3\n4\n5\n\n\n\n# 3.Generator 生成器函数(淘汰了)\n\n\n# 4. Async 函数 (推荐)\n\n有了 Async 函数,可以极大了简化我们的异步操作,前提是你的异步操作支持Promise\n\n如axios支持Promiose,所以我们可以使用async函数调用优化它, 任何异步如果支持promise则都可以使用Async函数来使用\n\naxios({\n    methos:'',\n    url:''\n}).then(res => {\n    \n})\n\n// async 只能用于函数 只要是函数都可以被标记为async,无论这个函数是什么形式\nasync function main () {\n    // 通过 async-await 我们可以像屑同步一样来写异步代码 \n    \t\t\t\t\t\t// 使用await关键字的父函数必须加async\n    const res =await axios({  //await等待,只有 await 后面的操作结束了,代码才会继续往后执行\n         methos:'',\n   \t\t url:''\n    })\n    \n    console.log(res)\n    \n    \n     const res2 =await axios({  //res2不会马上执行 要等 res 请求结束后才会执行\n         methos:'',\n   \t\t url:''\n    })\n     \n     console.log(res2)\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# Async 的 返回值\n\nasync function main() {\n    return 123\n    //相对应 async 不会把不是promise对象的返回值返回\n    //return new Promise(resolve => {\n    //resolve(123)\n//})\n}\n\nconst data = main()\nconsole.log(data)   //返回为一个 Promise对象\n// async 始终返回一个Promise对象 如果这个返回值不是Promise对象则封装成Promise对象\n\n//第一种方式获取值 用then获取\nmian().then(data => {\n    console.log(data)  //promise对象用then返回结果\n})\n\n//第二种方式获取值  用await获取\nasync function main2(){\n    const data = await mian()\n    console.log(data)\n}\n\nmain2()\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# async 函数的错误处理\n\n使用catch捕获异常\n\nasync function main() {\n    JSON.parse('123124')\n    return 123\n}\n\nmain().then(res => {\n    console.log(123)\n}).catch(err => {\n    console.log('发生异常',errw)\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n推荐使用 try-catch 捕获\n\nasync function main2() {\n    try{\n        // 如果try 里面的代码遇到异常,则停止执行try里面下面的代码,进入catch\n        const data =await main()\n        console.log(data)\n    } catch (err) {\n        console.log('异常')\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n",normalizedContent:"# javascript 异步编程\n\n\n# 定时器\n\nconsole.log(1)\n\nsettimeout() => {\n    console.log(2)\n}\n\nconsole.log(3)\n\n// 输出结果是 1 3 2  \n// 往下执行时 不会等待settimeout定时器的结束 而是直接向下继续执行\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# ajax请求\n\n//无论是使用原生的 xmlhttprequest 还是 jquery 的ajax 还是是 axios 都是异步的\n\n$.ajax({\n    method:'',\n    url:'',\n    success: function(data) {\n        // data 就是响应结果\n    }\n})\n\n// 异步代码 往往都伴随一个回调函数来接受结果  回头调用\naxios({\n    methos:'',\n    url:''\n}).then(res => {\n    \n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 1. callback 回调函数\n\n有时候,我们需要在异步操作的结果中执行另外一个异步操作\n\naxios({\n    methos:'',\n    url:''\n}).then(res => {\n    axios({\n    methos:'',\n    url:''\n\t\t}).then(res => {\n \n\t\t\t})\n})\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n当出现类型上面的嵌套异步时,非常不便于阅读和理解\n\n\n# 2. promise\n\nnew promist((resovle,reject) => {\n    settimeout(() => {\n        resolve()  // 在promise中 成功resolve 失败 reject\n    },1000)\n})\n\n\n1\n2\n3\n4\n5\n\n\n\n# 3.generator 生成器函数(淘汰了)\n\n\n# 4. async 函数 (推荐)\n\n有了 async 函数,可以极大了简化我们的异步操作,前提是你的异步操作支持promise\n\n如axios支持promiose,所以我们可以使用async函数调用优化它, 任何异步如果支持promise则都可以使用async函数来使用\n\naxios({\n    methos:'',\n    url:''\n}).then(res => {\n    \n})\n\n// async 只能用于函数 只要是函数都可以被标记为async,无论这个函数是什么形式\nasync function main () {\n    // 通过 async-await 我们可以像屑同步一样来写异步代码 \n    \t\t\t\t\t\t// 使用await关键字的父函数必须加async\n    const res =await axios({  //await等待,只有 await 后面的操作结束了,代码才会继续往后执行\n         methos:'',\n   \t\t url:''\n    })\n    \n    console.log(res)\n    \n    \n     const res2 =await axios({  //res2不会马上执行 要等 res 请求结束后才会执行\n         methos:'',\n   \t\t url:''\n    })\n     \n     console.log(res2)\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# async 的 返回值\n\nasync function main() {\n    return 123\n    //相对应 async 不会把不是promise对象的返回值返回\n    //return new promise(resolve => {\n    //resolve(123)\n//})\n}\n\nconst data = main()\nconsole.log(data)   //返回为一个 promise对象\n// async 始终返回一个promise对象 如果这个返回值不是promise对象则封装成promise对象\n\n//第一种方式获取值 用then获取\nmian().then(data => {\n    console.log(data)  //promise对象用then返回结果\n})\n\n//第二种方式获取值  用await获取\nasync function main2(){\n    const data = await mian()\n    console.log(data)\n}\n\nmain2()\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# async 函数的错误处理\n\n使用catch捕获异常\n\nasync function main() {\n    json.parse('123124')\n    return 123\n}\n\nmain().then(res => {\n    console.log(123)\n}).catch(err => {\n    console.log('发生异常',errw)\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n推荐使用 try-catch 捕获\n\nasync function main2() {\n    try{\n        // 如果try 里面的代码遇到异常,则停止执行try里面下面的代码,进入catch\n        const data =await main()\n        console.log(data)\n    } catch (err) {\n        console.log('异常')\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"JRE(Java Runtime Enviroment)",frontmatter:{title:"JRE(Java Runtime Enviroment)",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/0c8879/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/02.JRE(Java%20Runtime%20Enviroment).html",relativePath:"后端/01.JavaSE/02.JRE(Java Runtime Enviroment).md",key:"v-58b13ad2",path:"/pages/0c8879/",headersStr:null,content:"# JRE(Java Runtime Enviroment)\n\n是Java 程序的运行时环境 ，包含Jvm 和运行时所需要的核心类库。",normalizedContent:"# jre(java runtime enviroment)\n\n是java 程序的运行时环境 ，包含jvm 和运行时所需要的核心类库。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"java跨平台原理",frontmatter:{title:"java跨平台原理",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/df8281/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/01.java%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%8E%9F%E7%90%86.html",relativePath:"后端/01.JavaSE/01.java跨平台原理.md",key:"v-4574a8d8",path:"/pages/df8281/",headersStr:null,content:"# java跨平台原理\n\n使用不同平台的jvm(Jvm Java Virtual Machine) java虚拟机即可以在不同平台运行",normalizedContent:"# java跨平台原理\n\n使用不同平台的jvm(jvm java virtual machine) java虚拟机即可以在不同平台运行",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"JDK(Java Development Kit)",frontmatter:{title:"JDK(Java Development Kit)",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/d217f4/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/03.JDK(Java%20Development%20Kit).html",relativePath:"后端/01.JavaSE/03.JDK(Java Development Kit).md",key:"v-42fdff6b",path:"/pages/d217f4/",headersStr:null,content:"# JDK(Java Development Kit)\n\n是java程序开发工具包,包含JRE和开发人员使用的工具。\n\n其中的开发工具:编译工具(javac.exe)和运行工具(java.exe)。\n\n如果要开发一个全新的java程序，那么必须安装JDK\n\n",normalizedContent:"# jdk(java development kit)\n\n是java程序开发工具包,包含jre和开发人员使用的工具。\n\n其中的开发工具:编译工具(javac.exe)和运行工具(java.exe)。\n\n如果要开发一个全新的java程序，那么必须安装jdk\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"常用dos命令",frontmatter:{title:"常用dos命令",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/47da21/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/04.%E5%B8%B8%E7%94%A8dos%E5%91%BD%E4%BB%A4.html",relativePath:"后端/01.JavaSE/04.常用dos命令.md",key:"v-3191e504",path:"/pages/47da21/",headersStr:null,content:"# 常用dos命令\n\n切换盘符 d:\n\n查看当前路径的所有文件夹和文件 dir\n\n进入当前路径的文件夹 cd 文件夹名\n\n返回路径的上一个文件夹 cd ..\n\n进入多级目录 cd 文件夹名\\文件夹名\n\n回退到盘符目录 cd\\\n\n清屏 cls\n\n退出 exit",normalizedContent:"# 常用dos命令\n\n切换盘符 d:\n\n查看当前路径的所有文件夹和文件 dir\n\n进入当前路径的文件夹 cd 文件夹名\n\n返回路径的上一个文件夹 cd ..\n\n进入多级目录 cd 文件夹名\\文件夹名\n\n回退到盘符目录 cd\\\n\n清屏 cls\n\n退出 exit",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"配置系统path环境变量",frontmatter:{title:"配置系统path环境变量",date:"2022-03-18T00:48:41.000Z",permalink:"/pages/3ee21a/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/05.%E9%85%8D%E7%BD%AE%E7%B3%BB%E7%BB%9Fpath%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F.html",relativePath:"后端/01.JavaSE/05.配置系统path环境变量.md",key:"v-471ff6aa",path:"/pages/3ee21a/",headersStr:null,content:"# 配置系统path环境变量\n\n 1. 在path变量中新建 JAVA_HOME 变量 值为 java安装下的bin\n 2. 新建 CLASSPATH 变量 .;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar",normalizedContent:"# 配置系统path环境变量\n\n 1. 在path变量中新建 java_home 变量 值为 java安装下的bin\n 2. 新建 classpath 变量 .;%java_home%\\lib;%java_home%\\lib\\tools.jar",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"开发运行流程",frontmatter:{title:"开发运行流程",date:"2022-03-18T00:48:41.000Z",permalink:"/pages/417c2d/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/06.%E5%BC%80%E5%8F%91%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B.html",relativePath:"后端/01.JavaSE/06.开发运行流程.md",key:"v-7d9a1627",path:"/pages/417c2d/",headersStr:null,content:"# 开发运行流程\n\n1.编写程序\n\n2.编译程序 命令符下使用 javac 文件名.java 进行编译 编译完成后自动生成class文件\n\n3.运行程序 java 类名 即可完成运行",normalizedContent:"# 开发运行流程\n\n1.编写程序\n\n2.编译程序 命令符下使用 javac 文件名.java 进行编译 编译完成后自动生成class文件\n\n3.运行程序 java 类名 即可完成运行",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"基础语法",frontmatter:{title:"基础语法",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/97d4f2/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/08.%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95.html",relativePath:"后端/01.JavaSE/08.基础语法.md",key:"v-d7039bcc",path:"/pages/97d4f2/",headers:[{level:2,title:"JAVA程序中最基本的组成单位是类.",slug:"java程序中最基本的组成单位是类",normalizedTitle:"java程序中最基本的组成单位是类.",charIndex:55}],headersStr:"JAVA程序中最基本的组成单位是类.",content:"# 基础语法\n\n注释 单行注释// 多行注释 以/*开始 以*/结束 文档注释 /**开始 */结束\n\n\n# JAVA程序中最基本的组成单位是类.\n\n类的定义格式：\n\npublic class 类名{\n    //mian方法是程序的人口方法,代码的执行是从main方法开始\n    public static void main(){\n        \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n",normalizedContent:"# 基础语法\n\n注释 单行注释// 多行注释 以/*开始 以*/结束 文档注释 /**开始 */结束\n\n\n# java程序中最基本的组成单位是类.\n\n类的定义格式：\n\npublic class 类名{\n    //mian方法是程序的人口方法,代码的执行是从main方法开始\n    public static void main(){\n        \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"编写规范问题",frontmatter:{title:"编写规范问题",date:"2022-03-18T00:48:41.000Z",permalink:"/pages/977c49/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/07.%E7%BC%96%E5%86%99%E8%A7%84%E8%8C%83%E9%97%AE%E9%A2%98.html",relativePath:"后端/01.JavaSE/07.编写规范问题.md",key:"v-a07ea332",path:"/pages/977c49/",headersStr:null,content:"# 编写规范问题\n\n1.中英文符合问题 引号问题！与python不同java中的字符串用双引号引用不能使用单引号引号 “ ”\n\n2.单词拼写问题",normalizedContent:"# 编写规范问题\n\n1.中英文符合问题 引号问题！与python不同java中的字符串用双引号引用不能使用单引号引号 “ ”\n\n2.单词拼写问题",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"关键字",frontmatter:{title:"关键字",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/d2d164/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/09.%E5%85%B3%E9%94%AE%E5%AD%97.html",relativePath:"后端/01.JavaSE/09.关键字.md",key:"v-66e73294",path:"/pages/d2d164/",headersStr:null,content:"# 关键字\n\n关键字的字母全部小写",normalizedContent:"# 关键字\n\n关键字的字母全部小写",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"常量",frontmatter:{title:"常量",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/10baa9/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/10.%E5%B8%B8%E9%87%8F.html",relativePath:"后端/01.JavaSE/10.常量.md",key:"v-2474d88f",path:"/pages/10baa9/",headersStr:null,content:"# 常量\n\n常量的分类\n\n1.字符串常量 用双引号括起来的内容 ‘’‘Hello World’\n\n2.整数常量 整数 1,12,34\n\n3.小数常量 带小数的数字 13.1,3.14\n\n4.字符常量 用单引号括起来的内容 ‘a’ ,’0’\n\n5.布尔常量 布尔值，表示真假 只有两个值:true,false\n\n6.空常量 一个特殊的值，空值 值是：null\n\n空常量是不能直接输出的\n\n在变量前面加上final关键字即引用为常量 无法更改数值\n\nfinal int r=5;\n\nfinal修饰基本数据类型的变量时，必须赋予初始值且不能被改变，修饰引用变量时，该引用变量不能再指向其他对象",normalizedContent:"# 常量\n\n常量的分类\n\n1.字符串常量 用双引号括起来的内容 ‘’‘hello world’\n\n2.整数常量 整数 1,12,34\n\n3.小数常量 带小数的数字 13.1,3.14\n\n4.字符常量 用单引号括起来的内容 ‘a’ ,’0’\n\n5.布尔常量 布尔值，表示真假 只有两个值:true,false\n\n6.空常量 一个特殊的值，空值 值是：null\n\n空常量是不能直接输出的\n\n在变量前面加上final关键字即引用为常量 无法更改数值\n\nfinal int r=5;\n\nfinal修饰基本数据类型的变量时，必须赋予初始值且不能被改变，修饰引用变量时，该引用变量不能再指向其他对象",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数据类型",frontmatter:{title:"数据类型",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/ec81f9/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/11.%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html",relativePath:"后端/01.JavaSE/11.数据类型.md",key:"v-951a5208",path:"/pages/ec81f9/",headersStr:null,content:"# 数据类型\n\n一、基本数据类型\n\n 1. 数值型\n    * 整数(byte,short,int,long)\n    * 浮点数(float,double)\n    * 字符(char)\n 2. 非数值型\n    * 布尔(boolean)\n\n二、引用数据类型\n\n 1. 类(class)\n 2. 接口(interface)\n 3. 数组([])",normalizedContent:"# 数据类型\n\n一、基本数据类型\n\n 1. 数值型\n    * 整数(byte,short,int,long)\n    * 浮点数(float,double)\n    * 字符(char)\n 2. 非数值型\n    * 布尔(boolean)\n\n二、引用数据类型\n\n 1. 类(class)\n 2. 接口(interface)\n 3. 数组([])",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数值型内容占用和取整范围",frontmatter:{title:"数值型内容占用和取整范围",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/3f6e03/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/12.%E6%95%B0%E5%80%BC%E5%9E%8B%E5%86%85%E5%AE%B9%E5%8D%A0%E7%94%A8%E5%92%8C%E5%8F%96%E6%95%B4%E8%8C%83%E5%9B%B4.html",relativePath:"后端/01.JavaSE/12.数值型内容占用和取整范围.md",key:"v-ca23bd78",path:"/pages/3f6e03/",headers:[{level:2,title:"整数",slug:"整数",normalizedTitle:"整数",charIndex:19},{level:2,title:"浮点数",slug:"浮点数",normalizedTitle:"浮点数",charIndex:110},{level:2,title:"字符",slug:"字符",normalizedTitle:"字符",charIndex:293},{level:2,title:"布尔",slug:"布尔",normalizedTitle:"布尔",charIndex:327}],headersStr:"整数 浮点数 字符 布尔",content:"# 数值型内容占用和取整范围\n\n\n# 整数\n\nbyte 1 -128~127\n\nshort 2 -32768~32767\n\nint（默认） 4 -2^31~2^31-1\n\nlong 8 -2^63~2^63-1\n\n\n# 浮点数\n\nfloat 4 -3.402823E+38~-1.401298E-45\n\n1.401298E-45~3.402823E+38\n\ndouble（默认） 8 -1.797693E+308~-4.9000000E-324\n\n4.9000000E-324~1.797693E+308\n\n//E+38表示乘以10的38次方，E-45表示乘以10的负45次方\n\n\n# 字符\n\nchar 2 0-65535 此为unicode码\n\n\n# 布尔\n\nboolean 1 true,flase",normalizedContent:"# 数值型内容占用和取整范围\n\n\n# 整数\n\nbyte 1 -128~127\n\nshort 2 -32768~32767\n\nint（默认） 4 -2^31~2^31-1\n\nlong 8 -2^63~2^63-1\n\n\n# 浮点数\n\nfloat 4 -3.402823e+38~-1.401298e-45\n\n1.401298e-45~3.402823e+38\n\ndouble（默认） 8 -1.797693e+308~-4.9000000e-324\n\n4.9000000e-324~1.797693e+308\n\n//e+38表示乘以10的38次方，e-45表示乘以10的负45次方\n\n\n# 字符\n\nchar 2 0-65535 此为unicode码\n\n\n# 布尔\n\nboolean 1 true,flase",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"标识符",frontmatter:{title:"标识符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/fcb871/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/14.%E6%A0%87%E8%AF%86%E7%AC%A6.html",relativePath:"后端/01.JavaSE/14.标识符.md",key:"v-d8c50dce",path:"/pages/fcb871/",headersStr:null,content:"# 标识符\n\n标识符：给类，方法，变量起名字的符号\n\n由数字、字母、下划线、美元符($)组成的\n\n不能以数字开头，不能为关键字，区分大小写\n\n小驼峰命名法(针对方法、变量命名)\n\n1.标识符是一个单词时，首字母小写 name\n\n2.由多个单词组成时，第一个单词首字母小写，其他的首字母大写\n\nfirstName\n\n大驼峰命名法(针对类)\n\n1.首字母大 Student\n\n2.每个单词首字母大写 GoodStudent",normalizedContent:"# 标识符\n\n标识符：给类，方法，变量起名字的符号\n\n由数字、字母、下划线、美元符($)组成的\n\n不能以数字开头，不能为关键字，区分大小写\n\n小驼峰命名法(针对方法、变量命名)\n\n1.标识符是一个单词时，首字母小写 name\n\n2.由多个单词组成时，第一个单词首字母小写，其他的首字母大写\n\nfirstname\n\n大驼峰命名法(针对类)\n\n1.首字母大 student\n\n2.每个单词首字母大写 goodstudent",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"算术运算符",frontmatter:{title:"算术运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/a3cd43/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/16.%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/01.JavaSE/16.算术运算符.md",key:"v-32a3ec34",path:"/pages/a3cd43/",headersStr:null,content:"# 算术运算符\n\n运算符：对常量或者变量进行操作的符号\n\n表达式：用运算符把常量或者变量连接起来\n\n+ - * / %\n\n整数相除只能得到整数，必须转要有一个浮点小数才能得出小数结果",normalizedContent:"# 算术运算符\n\n运算符：对常量或者变量进行操作的符号\n\n表达式：用运算符把常量或者变量连接起来\n\n+ - * / %\n\n整数相除只能得到整数，必须转要有一个浮点小数才能得出小数结果",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"类型转换",frontmatter:{title:"类型转换",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/b2c2df/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/15.%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2.html",relativePath:"后端/01.JavaSE/15.类型转换.md",key:"v-49a02e0f",path:"/pages/b2c2df/",headersStr:null,content:"# 类型转换\n\n 1. 自动类型转换\n    \n    把一个表示数据范围小的数值或者变量赋值给另一个表示数据范围大的变量\n    \n    double d = 10;\n\n2.强制类型转换\n\nint k = (int)88.88;  //java: 不兼容的类型: 从double转换到int可能会有损失  需在数值前面添加(欲转换的类型名)\nSystem.out.println(k);  //88  强制转换成int类型 丢失了小数点后的数字\n\n\n1\n2\n",normalizedContent:"# 类型转换\n\n 1. 自动类型转换\n    \n    把一个表示数据范围小的数值或者变量赋值给另一个表示数据范围大的变量\n    \n    double d = 10;\n\n2.强制类型转换\n\nint k = (int)88.88;  //java: 不兼容的类型: 从double转换到int可能会有损失  需在数值前面添加(欲转换的类型名)\nsystem.out.println(k);  //88  强制转换成int类型 丢失了小数点后的数字\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"自增自减运算符",frontmatter:{title:"自增自减运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/a4bf30/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/17.%E8%87%AA%E5%A2%9E%E8%87%AA%E5%87%8F%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/01.JavaSE/17.自增自减运算符.md",key:"v-c93d1656",path:"/pages/a4bf30/",headersStr:null,content:"# 自增自减运算符\n\ni++; ++i;\n\ni-–; -–i;\n\n变量自增/自减 1\n\n如果要赋值给变量实现自增自减 ++得放前面",normalizedContent:"# 自增自减运算符\n\ni++; ++i;\n\ni-–; -–i;\n\n变量自增/自减 1\n\n如果要赋值给变量实现自增自减 ++得放前面",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"关系运算符",frontmatter:{title:"关系运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/a5f46a/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/18.%E5%85%B3%E7%B3%BB%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/01.JavaSE/18.关系运算符.md",key:"v-bc7f093e",path:"/pages/a5f46a/",headersStr:null,content:"# 关系运算符\n\n * == 判断2个对象的值是否相等\n * != 不等于\n * > 大于\n * >= 大于等于\n * ＜ 小于\n * <= 小于等于",normalizedContent:"# 关系运算符\n\n * == 判断2个对象的值是否相等\n * != 不等于\n * > 大于\n * >= 大于等于\n * ＜ 小于\n * <= 小于等于",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"短路逻辑运算符",frontmatter:{title:"短路逻辑运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/5cf2b5/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/20.%E7%9F%AD%E8%B7%AF%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/01.JavaSE/20.短路逻辑运算符.md",key:"v-6d95328a",path:"/pages/5cf2b5/",headersStr:null,content:"# 短路逻辑运算符\n\n&& 短路与，检索一个为false的值，如果找到则false后面的表达式都不会继续执行，只会执行false前面的\n\n|| 短路或，检索一个为true的值，如果找到则ture后面的表达式都不会继续执行，只会执行ture前面的",normalizedContent:"# 短路逻辑运算符\n\n&& 短路与，检索一个为false的值，如果找到则false后面的表达式都不会继续执行，只会执行false前面的\n\n|| 短路或，检索一个为true的值，如果找到则ture后面的表达式都不会继续执行，只会执行ture前面的",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"三元运算符",frontmatter:{title:"三元运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ad2097/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/21.%E4%B8%89%E5%85%83%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/01.JavaSE/21.三元运算符.md",key:"v-1a9db526",path:"/pages/ad2097/",headersStr:null,content:"# 三元运算符\n\n格式：关系表达式？表达式1：表达式2；\n\na > b ? a : b;\n\n如表达式为true，则表达式1为运算结果\n\n如表达式为flase，则表达式2为运算结果",normalizedContent:"# 三元运算符\n\n格式：关系表达式？表达式1：表达式2；\n\na > b ? a : b;\n\n如表达式为true，则表达式1为运算结果\n\n如表达式为flase，则表达式2为运算结果",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数据输入",frontmatter:{title:"数据输入",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/9ca53c/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/22.%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5.html",relativePath:"后端/01.JavaSE/22.数据输入.md",key:"v-06fb6608",path:"/pages/9ca53c/",headersStr:null,content:"# 数据输入\n\n 1. 导包\n    \n    import java.util.Scanner;\n    \n    导包的动作必须出现在类定义的上边\n\n 2. 创建对象\n    \n    Scanner sc =new Scanner(System.in);\n    \n    只有sc是变量名，可以变，其他的都不允许变\n\n 3. 接受数据\n    \n    int i = sc.nextInt();\n    \n    只有i是变量名，可以变，其他的也不允许变",normalizedContent:"# 数据输入\n\n 1. 导包\n    \n    import java.util.scanner;\n    \n    导包的动作必须出现在类定义的上边\n\n 2. 创建对象\n    \n    scanner sc =new scanner(system.in);\n    \n    只有sc是变量名，可以变，其他的都不允许变\n\n 3. 接受数据\n    \n    int i = sc.nextint();\n    \n    只有i是变量名，可以变，其他的也不允许变",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"逻辑运算符",frontmatter:{title:"逻辑运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ae5f03/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/19.%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/01.JavaSE/19.逻辑运算符.md",key:"v-4f1cfdc2",path:"/pages/ae5f03/",headersStr:null,content:"# 逻辑运算符\n\n逻辑运算符是用来连接关系表达式的运算符\n\n& 逻辑与\n\n| 逻辑或\n\n^ 逻辑异或\n\n! 逻辑非 感叹号要使用在表达式前面如(! (l > k) );",normalizedContent:"# 逻辑运算符\n\n逻辑运算符是用来连接关系表达式的运算符\n\n& 逻辑与\n\n| 逻辑或\n\n^ 逻辑异或\n\n! 逻辑非 感叹号要使用在表达式前面如(! (l > k) );",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"流程控制",frontmatter:{title:"流程控制",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/fc3701/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/23.%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html",relativePath:"后端/01.JavaSE/23.流程控制.md",key:"v-631835ec",path:"/pages/fc3701/",headers:[{level:2,title:"顺序结构",slug:"顺序结构",normalizedTitle:"顺序结构",charIndex:11},{level:2,title:"if语句",slug:"if语句",normalizedTitle:"if语句",charIndex:45},{level:2,title:"switch语句",slug:"switch语句",normalizedTitle:"switch语句",charIndex:376},{level:3,title:"case穿透",slug:"case穿透",normalizedTitle:"case穿透",charIndex:667}],headersStr:"顺序结构 if语句 switch语句 case穿透",content:'# 流程控制\n\n\n# 顺序结构\n\n没有特定的语法结构，按照代码的编写顺序依次执行\n\n\n# if语句\n\n格式:\n\nif(关系表达式){\n\n语句体;\n\n}\n\n关系表达式值为true就执行语句体\n\n关系表达式值为false就不执行语句体，继续执行if语句后的语句\n\nif 语句格式2\n\nif(关系表达式){\n\n语句体;\n\n} else {\n\n语句体2;\n\n}\n\n关系表达式值为true就执行语句体1\n\n关系表达式值为false就执行语句体2\n\nif 语句格式3\n\nif(关系表达式1){\n\n语句体1;\n\n} else if (关系表达式2){\n\n语句体2;\n\n}\n\n…\n\nelse{\n\n语句体n\n\n}\n\n首先计算关系表达式1的值\n\n如为true则执行语句体1，如为false则判断关系表达式2，直到elseif全为false才执行else中的语句体\n\n\n# switch语句\n\n格式：\n\nswitch(表达式){\n\ncase 值1：\n\n语句体1；\n\nbreak；\n\ncase 值2：\n\n语句体1；\n\nbreak；\n\ncase 值n：\n\n语句体n；\n\nbreak；\n\n…\n\ndefault:\n\n语句体n+1;\n\n[break];\n\n}\n\n表达式:取值为byte、short、int、char，jdk5以后可以是枚举，jdk7以后可以是String。\n\ncase：后面跟的是要和表达式进行比较的值\n\nbreak：表示中断/结束，用来结束switch语句\n\ndefault：表示所有case都不匹配的时候执行该内容，跟else差不多\n\n\n\n\n# case穿透\n\n如果case的语句体后面不写break，将会出现穿透现象，在不判断下一个case值的情况下，继续往 下执行直到遇到break或者switc语句结束\n\n如:\n\nswitch (n){\n\ncase 1:\n\ncase 2:\n\ncase 3:\n\nSystem.out.println("123");\n\nbreak;\n\n}\n\n如果case1成立则往下执行case2、case3里的语句体然后break结束',normalizedContent:'# 流程控制\n\n\n# 顺序结构\n\n没有特定的语法结构，按照代码的编写顺序依次执行\n\n\n# if语句\n\n格式:\n\nif(关系表达式){\n\n语句体;\n\n}\n\n关系表达式值为true就执行语句体\n\n关系表达式值为false就不执行语句体，继续执行if语句后的语句\n\nif 语句格式2\n\nif(关系表达式){\n\n语句体;\n\n} else {\n\n语句体2;\n\n}\n\n关系表达式值为true就执行语句体1\n\n关系表达式值为false就执行语句体2\n\nif 语句格式3\n\nif(关系表达式1){\n\n语句体1;\n\n} else if (关系表达式2){\n\n语句体2;\n\n}\n\n…\n\nelse{\n\n语句体n\n\n}\n\n首先计算关系表达式1的值\n\n如为true则执行语句体1，如为false则判断关系表达式2，直到elseif全为false才执行else中的语句体\n\n\n# switch语句\n\n格式：\n\nswitch(表达式){\n\ncase 值1：\n\n语句体1；\n\nbreak；\n\ncase 值2：\n\n语句体1；\n\nbreak；\n\ncase 值n：\n\n语句体n；\n\nbreak；\n\n…\n\ndefault:\n\n语句体n+1;\n\n[break];\n\n}\n\n表达式:取值为byte、short、int、char，jdk5以后可以是枚举，jdk7以后可以是string。\n\ncase：后面跟的是要和表达式进行比较的值\n\nbreak：表示中断/结束，用来结束switch语句\n\ndefault：表示所有case都不匹配的时候执行该内容，跟else差不多\n\n\n\n\n# case穿透\n\n如果case的语句体后面不写break，将会出现穿透现象，在不判断下一个case值的情况下，继续往 下执行直到遇到break或者switc语句结束\n\n如:\n\nswitch (n){\n\ncase 1:\n\ncase 2:\n\ncase 3:\n\nsystem.out.println("123");\n\nbreak;\n\n}\n\n如果case1成立则往下执行case2、case3里的语句体然后break结束',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"变量",frontmatter:{title:"变量",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/558212/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/13.%E5%8F%98%E9%87%8F.html",relativePath:"后端/01.JavaSE/13.变量.md",key:"v-5ef65d74",path:"/pages/558212/",headers:[{level:2,title:"变量的定义",slug:"变量的定义",normalizedTitle:"变量的定义",charIndex:9},{level:2,title:"变量的使用",slug:"变量的使用",normalizedTitle:"变量的使用",charIndex:47}],headersStr:"变量的定义 变量的使用",content:"# 变量\n\n\n# 变量的定义\n\n数据类型 变量名 = 变量值\n\nint a = 10\n\n\n# 变量的使用\n\n取值和修改值\n\n取值格式： 变量名\n\na\n\n修改值格式：变量名=变量值\n\na = 20",normalizedContent:"# 变量\n\n\n# 变量的定义\n\n数据类型 变量名 = 变量值\n\nint a = 10\n\n\n# 变量的使用\n\n取值和修改值\n\n取值格式： 变量名\n\na\n\n修改值格式：变量名=变量值\n\na = 20",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"idea中的辅助键",frontmatter:{title:"idea中的辅助键",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/3455ab/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/25.idea%E4%B8%AD%E7%9A%84%E8%BE%85%E5%8A%A9%E9%94%AE.html",relativePath:"后端/01.JavaSE/25.idea中的辅助键.md",key:"v-d2053d44",path:"/pages/3455ab/",headers:[{level:2,title:"类中创建方法",slug:"类中创建方法",normalizedTitle:"类中创建方法",charIndex:182}],headersStr:"类中创建方法",content:"# idea中的辅助键\n\n快速生成语句\n\n 1. 快速生成mian()方法：psvm 回车\n 2. 快速生成输出语句：sout 回车\n\n内容辅助键\n\nCtrl+Alt+space (内容提示/代码补全)\n\n快捷键\n\n单行注释 ctrl+/\n\n多行注释 ctrl+shift+/\n\n格式化 ctrl+alt+l\n\n快速补全类型和变量名 ctrl+alt+v\n\n\n# 类中创建方法\n\nalt + insert 可快速创建构造方法、set和get方法",normalizedContent:"# idea中的辅助键\n\n快速生成语句\n\n 1. 快速生成mian()方法：psvm 回车\n 2. 快速生成输出语句：sout 回车\n\n内容辅助键\n\nctrl+alt+space (内容提示/代码补全)\n\n快捷键\n\n单行注释 ctrl+/\n\n多行注释 ctrl+shift+/\n\n格式化 ctrl+alt+l\n\n快速补全类型和变量名 ctrl+alt+v\n\n\n# 类中创建方法\n\nalt + insert 可快速创建构造方法、set和get方法",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"面向对象基础",frontmatter:{title:"面向对象基础",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/0a58cf/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/28.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%9F%BA%E7%A1%80.html",relativePath:"后端/01.JavaSE/28.面向对象基础.md",key:"v-3ec3ce6e",path:"/pages/0a58cf/",headers:[{level:2,title:"类和对象",slug:"类和对象",normalizedTitle:"类和对象",charIndex:13},{level:2,title:"类的定义",slug:"类的定义",normalizedTitle:"类的定义",charIndex:187},{level:2,title:"对象的使用",slug:"对象的使用",normalizedTitle:"对象的使用",charIndex:286},{level:2,title:"对象内存图",slug:"对象内存图",normalizedTitle:"对象内存图",charIndex:465},{level:3,title:"单个对象 new 一个内存地址（独有的） 指向类",slug:"单个对象-new-一个内存地址-独有的-指向类",normalizedTitle:"单个对象 new 一个内存地址（独有的） 指向类",charIndex:475},{level:3,title:"多个对象 第一个对象new一个内存地址",slug:"多个对象-第一个对象new一个内存地址",normalizedTitle:"多个对象 第一个对象new一个内存地址",charIndex:564},{level:3,title:"多个对象指向相同",slug:"多个对象指向相同",normalizedTitle:"多个对象指向相同",charIndex:661},{level:2,title:"成员变量和局部变量",slug:"成员变量和局部变量",normalizedTitle:"成员变量和局部变量",charIndex:859},{level:2,title:"封装",slug:"封装",normalizedTitle:"封装",charIndex:875},{level:2,title:"this关键字使用",slug:"this关键字使用",normalizedTitle:"this关键字使用",charIndex:1158},{level:2,title:"封装概念",slug:"封装概念",normalizedTitle:"封装概念",charIndex:1295},{level:2,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:1480}],headersStr:"类和对象 类的定义 对象的使用 对象内存图 单个对象 new 一个内存地址（独有的） 指向类 多个对象 第一个对象new一个内存地址 多个对象指向相同 成员变量和局部变量 封装 this关键字使用 封装概念 构造方法",content:"# 面向对象基础\n\n\n# 类和对象\n\n 1. 面向对象 指计算机关注的目标\n\n 2. 类\n    \n    类是对象的数据类型\n    \n    类是具有相同属性和行为的一组对象的集合\n\n 3. 对象的属性\n    \n    对象具有的各种特征，每个属性大都拥有特定的值\n\n 4. 对象的行为\n    \n    对象能够执行的操作\n\n类是对象的抽象，对象是类的实体\n\n\n# 类的定义\n\n类是java程序的基本组成单位\n\npublic class 类名{\n\n//成员变量\n\n变量1的数据类型 变量1;\n\n变量2的数据类型 变量2;\n\n//成员方法\n\n方法1;\n\n}\n\n\n# 对象的使用\n\n创建对象\n\n类名 对象名 = new 类名();\n\nObjectTest01 P =new ObjectTest01();\n\n使用对象\n\n 1. 使用成员变量\n    \n    格式：对象名.变量名\n    \n    p.barand\n\n 2. 使用成员方法\n    \n    格式：对象名.方法名\n    \n    p.call()\n\n\n# 对象内存图\n\n\n# 单个对象 new 一个内存地址（独有的） 指向类\n\n成员变量 和 成员方法都是独用的不与其他发生冲突，只是指向调用\n\n成员方法 对象调用执行完后 会从栈内存消失（清空）\n\n\n# 多个对象 第一个对象new一个内存地址\n\n第二个对象 new一个内存地址\n\n两个对象分别指向不同的内存地址但方法和代码相同，互相之前无影响\n\n如还有对象申请新的内存还是无影响，各走各的\n\n\n# 多个对象指向相同\n\n第一个对象new一个内存地址\n\n将第一个对象的地址赋值给第二个对象\n\n两个对象指向的内存地址都是相同的，因为第二个对象的内存是由第一个对象申请的内存赋值的\n\n第一个对象 申请新的内存地址\n\nObjectTest01 P =new ObjectTest01();\n\n第二个对象 将第一个对象申请的内存地址赋值给第二个对象使用\n\nObjectTest01 P2 = p;\n\n\n# 成员变量和局部变量\n\n\n\n\n# 封装\n\nprivate关键字\n\n是一个权限修饰符，可以修饰成员(成员变量和成员方法)\n\n作用是保护成员不被别的类使用，被private修饰的成员只在本类中才能访问\n\n如需要类外部使用则需要定义java内部方法 get/set方法\n\nset 被修饰的变量名 首字母要大写如setAge\n\n格式: pulic void setXxx(数据类型 变量){\n\n\n\n}\n\nset是传递参数\n\nget是返回参数\n\nget 被修饰的变量名 首字母要大写如getAge\n\npulic 返回值的类型 getXxx (){\n\nreturn 被修饰的变量名\n\n}\n\n\n\n类外直接\n\n\n# this关键字使用\n\n用this修饰的变量用于指代成员变量\n\n如在类方法中 参数和成员变量名称一致则系统认为代码指代的为方法中的局部变量（形参），需要加this修饰指代为成员变量\n\nthis 解决局部变量隐藏成员变量\n\n方法被哪个对象调用了，this就代表哪个对象\n\n\n# 封装概念\n\n封装是面向对象三大特征之一（封装，继承，多态）\n\n封装的原则\n\n将类的某些信息隐藏在类内部，不允许外部直接访问，而是通过类提供的方法来实现对隐藏信息的操作和访问\n\n成员变量private，提供对应的getXxx()/setXxx()方法\n\n封装的好处\n\n通过方法来控制成员变量的操作，提高了代码的安全性\n\n把代码用方法进行封装，提高了代码的复用性。\n\n\n# 构造方法\n\n构造方法是一种特殊的方法\n\n作用：创建对象 主要是完成对象数据的初始化\n\npublic class 类名{\n\n修饰符 类名(参数){\n\n}\n\n}\n\n注意事项\n\n构造方法的创建\n\n当类中没有创建无参构造方法系统默认的自动生成无参构造方法\n\n如果定义了构造方法，系统将不再提供默认的构造方法\n\npubilc 类名(参数){\n\n}\n\n构造方法的重载\n\n如果定义了带参数的构造方法，还要使用无参构造方法，必须再写一个无参数构造方法，因为系统不再提供默认的无参数构造方法\n\n建议无论是否使用，都手工书写无参构造方法",normalizedContent:"# 面向对象基础\n\n\n# 类和对象\n\n 1. 面向对象 指计算机关注的目标\n\n 2. 类\n    \n    类是对象的数据类型\n    \n    类是具有相同属性和行为的一组对象的集合\n\n 3. 对象的属性\n    \n    对象具有的各种特征，每个属性大都拥有特定的值\n\n 4. 对象的行为\n    \n    对象能够执行的操作\n\n类是对象的抽象，对象是类的实体\n\n\n# 类的定义\n\n类是java程序的基本组成单位\n\npublic class 类名{\n\n//成员变量\n\n变量1的数据类型 变量1;\n\n变量2的数据类型 变量2;\n\n//成员方法\n\n方法1;\n\n}\n\n\n# 对象的使用\n\n创建对象\n\n类名 对象名 = new 类名();\n\nobjecttest01 p =new objecttest01();\n\n使用对象\n\n 1. 使用成员变量\n    \n    格式：对象名.变量名\n    \n    p.barand\n\n 2. 使用成员方法\n    \n    格式：对象名.方法名\n    \n    p.call()\n\n\n# 对象内存图\n\n\n# 单个对象 new 一个内存地址（独有的） 指向类\n\n成员变量 和 成员方法都是独用的不与其他发生冲突，只是指向调用\n\n成员方法 对象调用执行完后 会从栈内存消失（清空）\n\n\n# 多个对象 第一个对象new一个内存地址\n\n第二个对象 new一个内存地址\n\n两个对象分别指向不同的内存地址但方法和代码相同，互相之前无影响\n\n如还有对象申请新的内存还是无影响，各走各的\n\n\n# 多个对象指向相同\n\n第一个对象new一个内存地址\n\n将第一个对象的地址赋值给第二个对象\n\n两个对象指向的内存地址都是相同的，因为第二个对象的内存是由第一个对象申请的内存赋值的\n\n第一个对象 申请新的内存地址\n\nobjecttest01 p =new objecttest01();\n\n第二个对象 将第一个对象申请的内存地址赋值给第二个对象使用\n\nobjecttest01 p2 = p;\n\n\n# 成员变量和局部变量\n\n\n\n\n# 封装\n\nprivate关键字\n\n是一个权限修饰符，可以修饰成员(成员变量和成员方法)\n\n作用是保护成员不被别的类使用，被private修饰的成员只在本类中才能访问\n\n如需要类外部使用则需要定义java内部方法 get/set方法\n\nset 被修饰的变量名 首字母要大写如setage\n\n格式: pulic void setxxx(数据类型 变量){\n\n\n\n}\n\nset是传递参数\n\nget是返回参数\n\nget 被修饰的变量名 首字母要大写如getage\n\npulic 返回值的类型 getxxx (){\n\nreturn 被修饰的变量名\n\n}\n\n\n\n类外直接\n\n\n# this关键字使用\n\n用this修饰的变量用于指代成员变量\n\n如在类方法中 参数和成员变量名称一致则系统认为代码指代的为方法中的局部变量（形参），需要加this修饰指代为成员变量\n\nthis 解决局部变量隐藏成员变量\n\n方法被哪个对象调用了，this就代表哪个对象\n\n\n# 封装概念\n\n封装是面向对象三大特征之一（封装，继承，多态）\n\n封装的原则\n\n将类的某些信息隐藏在类内部，不允许外部直接访问，而是通过类提供的方法来实现对隐藏信息的操作和访问\n\n成员变量private，提供对应的getxxx()/setxxx()方法\n\n封装的好处\n\n通过方法来控制成员变量的操作，提高了代码的安全性\n\n把代码用方法进行封装，提高了代码的复用性。\n\n\n# 构造方法\n\n构造方法是一种特殊的方法\n\n作用：创建对象 主要是完成对象数据的初始化\n\npublic class 类名{\n\n修饰符 类名(参数){\n\n}\n\n}\n\n注意事项\n\n构造方法的创建\n\n当类中没有创建无参构造方法系统默认的自动生成无参构造方法\n\n如果定义了构造方法，系统将不再提供默认的构造方法\n\npubilc 类名(参数){\n\n}\n\n构造方法的重载\n\n如果定义了带参数的构造方法，还要使用无参构造方法，必须再写一个无参数构造方法，因为系统不再提供默认的无参数构造方法\n\n建议无论是否使用，都手工书写无参构造方法",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"循环语句",frontmatter:{title:"循环语句",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/db81b2/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/24.%E5%BE%AA%E7%8E%AF%E8%AF%AD%E5%8F%A5.html",relativePath:"后端/01.JavaSE/24.循环语句.md",key:"v-6cdcad60",path:"/pages/db81b2/",headers:[{level:2,title:"for循环",slug:"for循环",normalizedTitle:"for循环",charIndex:11},{level:2,title:"while循环",slug:"while循环",normalizedTitle:"while循环",charIndex:189},{level:2,title:"do…while循环",slug:"do-while循环",normalizedTitle:"do…while循环",charIndex:248},{level:2,title:"跳转控制语句",slug:"跳转控制语句",normalizedTitle:"跳转控制语句",charIndex:374},{level:2,title:"循环嵌套",slug:"循环嵌套",normalizedTitle:"循环嵌套",charIndex:447},{level:2,title:"Random",slug:"random",normalizedTitle:"random",charIndex:490}],headersStr:"for循环 while循环 do…while循环 跳转控制语句 循环嵌套 Random",content:"# 循环语句\n\n\n# for循环\n\n格式:\n\nfor(初始化语句;条件判断语句;条件控制语句){\n\n循环体语句;\n\n}\n\n初始化语句：可是一条或多条语句，这些语句可以完成一些初始化操作\n\n条件判断语句：这里使用一个结果值为boolean类型的表达式\n\n条件控制语句：这里通常使用一条语句来改变变量的值，从而达到控制循环是否继续向下执行结果。常见i++，i–这样的操作。\n\n\n# while循环\n\n格式:\n\nwhile (条件判断语句){\n\n循环体语句;\n\n可省略（条件控制语句;）\n\n}\n\n\n# do…while循环\n\n格式:\n\ndo{\n\n循环体语句;\n\n可省略(条件控制语句);\n\n}while(条件判断语句);\n\ndo…while循环 首先会执行一次do里面的语句，然后判断while的条件，如为true则返回到do，false则结束\n\n\n# 跳转控制语句\n\ncontunue 用于循环中，基于条件控制，跳过某次循环体内容的执行，继续下一次执行\n\nbreak 用于结束当前整个循环\n\n\n# 循环嵌套\n\nfor(){\n\nfor(){\n\n}\n\n}\n\n跟python差不多\n\n\n# Random\n\n产生一个随机数\n\nimport java.util.Random; 导入random的包\n\nRandom r =new Random(); 创建一个对象\n\nint number = r.nextInt(10); 获取0-9的随机数，不包含10",normalizedContent:"# 循环语句\n\n\n# for循环\n\n格式:\n\nfor(初始化语句;条件判断语句;条件控制语句){\n\n循环体语句;\n\n}\n\n初始化语句：可是一条或多条语句，这些语句可以完成一些初始化操作\n\n条件判断语句：这里使用一个结果值为boolean类型的表达式\n\n条件控制语句：这里通常使用一条语句来改变变量的值，从而达到控制循环是否继续向下执行结果。常见i++，i–这样的操作。\n\n\n# while循环\n\n格式:\n\nwhile (条件判断语句){\n\n循环体语句;\n\n可省略（条件控制语句;）\n\n}\n\n\n# do…while循环\n\n格式:\n\ndo{\n\n循环体语句;\n\n可省略(条件控制语句);\n\n}while(条件判断语句);\n\ndo…while循环 首先会执行一次do里面的语句，然后判断while的条件，如为true则返回到do，false则结束\n\n\n# 跳转控制语句\n\ncontunue 用于循环中，基于条件控制，跳过某次循环体内容的执行，继续下一次执行\n\nbreak 用于结束当前整个循环\n\n\n# 循环嵌套\n\nfor(){\n\nfor(){\n\n}\n\n}\n\n跟python差不多\n\n\n# random\n\n产生一个随机数\n\nimport java.util.random; 导入random的包\n\nrandom r =new random(); 创建一个对象\n\nint number = r.nextint(10); 获取0-9的随机数，不包含10",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"标准类",frontmatter:{title:"标准类",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ab994e/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/29.%E6%A0%87%E5%87%86%E7%B1%BB.html",relativePath:"后端/01.JavaSE/29.标准类.md",key:"v-9df1bfda",path:"/pages/ab994e/",headersStr:null,content:"# 标准类\n\n 1. 成员变量\n    \n    使用private修饰\n\n 2. 构造方法\n    \n    提供一个无参构造方法\n    \n    提供一个带多个参数的构造方法\n\n 3. 成员方法\n    \n    提供每一个成员变量对应的setXxx()/getXxx()\n    \n    提供一个显示对象信息的show()\n\n 4. 创建对象并为其成员变量赋值的两种方式\n    \n    无参构造方法创建对象后使用setXxx()赋值\n    \n    使用带参构造方法直接创建带有属性值的对象",normalizedContent:"# 标准类\n\n 1. 成员变量\n    \n    使用private修饰\n\n 2. 构造方法\n    \n    提供一个无参构造方法\n    \n    提供一个带多个参数的构造方法\n\n 3. 成员方法\n    \n    提供每一个成员变量对应的setxxx()/getxxx()\n    \n    提供一个显示对象信息的show()\n\n 4. 创建对象并为其成员变量赋值的两种方式\n    \n    无参构造方法创建对象后使用setxxx()赋值\n    \n    使用带参构造方法直接创建带有属性值的对象",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"方法",frontmatter:{title:"方法",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/8361e2/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/27.%E6%96%B9%E6%B3%95.html",relativePath:"后端/01.JavaSE/27.方法.md",key:"v-e78fc5d0",path:"/pages/8361e2/",headers:[{level:2,title:"方法定义和调用",slug:"方法定义和调用",normalizedTitle:"方法定义和调用",charIndex:105},{level:2,title:"带参数方法定义和调用",slug:"带参数方法定义和调用",normalizedTitle:"带参数方法定义和调用",charIndex:170},{level:2,title:"形参和实参",slug:"形参和实参",normalizedTitle:"形参和实参",charIndex:368},{level:2,title:"带返回值方法的定义和调用",slug:"带返回值方法的定义和调用",normalizedTitle:"带返回值方法的定义和调用",charIndex:404},{level:2,title:"方法的注意事项",slug:"方法的注意事项",normalizedTitle:"方法的注意事项",charIndex:528},{level:2,title:"方法的通用格式",slug:"方法的通用格式",normalizedTitle:"方法的通用格式",charIndex:619},{level:2,title:"方法重载",slug:"方法重载",normalizedTitle:"方法重载",charIndex:743},{level:2,title:"方法的参数传递",slug:"方法的参数传递",normalizedTitle:"方法的参数传递",charIndex:881}],headersStr:"方法定义和调用 带参数方法定义和调用 形参和实参 带返回值方法的定义和调用 方法的注意事项 方法的通用格式 方法重载 方法的参数传递",content:"# 方法\n\n方法(method)是将具有独立功能的代码块组织成为一个整体，使其具有特殊功能的代码集\n\n方法必须先创建才能使用，称为方法定义\n\n方法创建后并不是直接运行的，需要手动使用后才执行，称为调用\n\n\n# 方法定义和调用\n\n格式\n\npublic static void 方法名(){\n\n//方法体\n\n}\n\n调用\n\n方法名();\n\n\n# 带参数方法定义和调用\n\n格式:public static void 方法名(参数){…}\n\n单个参数:public static void 方法名(数据类型 变量名){…}\n\n多个参数:public static void 方法名(数据类型 变量名1,数据类型 变量名2,…){…}\n\n注意：参数中的数据类型与变量名都不能缺少，多个参数之间使用逗号分隔\n\n调用\n\n方法名(参数,参数2)\n\n\n# 形参和实参\n\n形参：方法定义中的参数\n\n实参：方法调用中的参数\n\n\n# 带返回值方法的定义和调用\n\npubilc static 数据类型 方法名(参数){\n\nreturn 数据;\n\n}\n\n方法定义是return后面的返回值要与方法定义上的数据类型一致，否则会报错\n\n调用\n\n数据类型 变量名 =方法名(参数);\n\n\n# 方法的注意事项\n\n方法不能嵌套定义\n\nvoid表示无返回值，可以省略return，也可单独书写return,后面不加数据\n\n执行return后方法中后面的代码将不再向下执行\n\n\n# 方法的通用格式\n\npubilc static 返回值类型 方法名(参数){\n\n方法体;\n\nreturn 数据;\n\n}\n\npubic static 修饰符，目前先记住这个格式\n\n定义方法时，要明确返回值类型如没写void，明确参数类型和数量\n\n\n# 方法重载\n\n方法重载指同一个类中定义多个方法之间的关系，满足以下条件则构成重载\n\n 1. 多个方法在同一个类中\n 2. 多个方法具有相同的方法\n 3. 多个方法参数不同，类型不同或者数量不同\n\n与返回值无关\n\n在调用的时候，java虚拟机会通过参数的不同来区分同名的\n\n\n# 方法的参数传递\n\n对于基本数据类型的参数，形式参数的改变，不影响实际参数的值\n\n方法中的变量为局部变量，方法内值的改变不影响实际外的值\n\n引用类型\n\n对于引用类型的参数，形式参数的改变，影响实际参数的值\n\nSystem.out.println(“内容”); 输出内容并换\n\nSystem.out.print(“内容”); 输出内容不换行\n\nSystem.out.println(); 起到换行的作业",normalizedContent:"# 方法\n\n方法(method)是将具有独立功能的代码块组织成为一个整体，使其具有特殊功能的代码集\n\n方法必须先创建才能使用，称为方法定义\n\n方法创建后并不是直接运行的，需要手动使用后才执行，称为调用\n\n\n# 方法定义和调用\n\n格式\n\npublic static void 方法名(){\n\n//方法体\n\n}\n\n调用\n\n方法名();\n\n\n# 带参数方法定义和调用\n\n格式:public static void 方法名(参数){…}\n\n单个参数:public static void 方法名(数据类型 变量名){…}\n\n多个参数:public static void 方法名(数据类型 变量名1,数据类型 变量名2,…){…}\n\n注意：参数中的数据类型与变量名都不能缺少，多个参数之间使用逗号分隔\n\n调用\n\n方法名(参数,参数2)\n\n\n# 形参和实参\n\n形参：方法定义中的参数\n\n实参：方法调用中的参数\n\n\n# 带返回值方法的定义和调用\n\npubilc static 数据类型 方法名(参数){\n\nreturn 数据;\n\n}\n\n方法定义是return后面的返回值要与方法定义上的数据类型一致，否则会报错\n\n调用\n\n数据类型 变量名 =方法名(参数);\n\n\n# 方法的注意事项\n\n方法不能嵌套定义\n\nvoid表示无返回值，可以省略return，也可单独书写return,后面不加数据\n\n执行return后方法中后面的代码将不再向下执行\n\n\n# 方法的通用格式\n\npubilc static 返回值类型 方法名(参数){\n\n方法体;\n\nreturn 数据;\n\n}\n\npubic static 修饰符，目前先记住这个格式\n\n定义方法时，要明确返回值类型如没写void，明确参数类型和数量\n\n\n# 方法重载\n\n方法重载指同一个类中定义多个方法之间的关系，满足以下条件则构成重载\n\n 1. 多个方法在同一个类中\n 2. 多个方法具有相同的方法\n 3. 多个方法参数不同，类型不同或者数量不同\n\n与返回值无关\n\n在调用的时候，java虚拟机会通过参数的不同来区分同名的\n\n\n# 方法的参数传递\n\n对于基本数据类型的参数，形式参数的改变，不影响实际参数的值\n\n方法中的变量为局部变量，方法内值的改变不影响实际外的值\n\n引用类型\n\n对于引用类型的参数，形式参数的改变，影响实际参数的值\n\nsystem.out.println(“内容”); 输出内容并换\n\nsystem.out.print(“内容”); 输出内容不换行\n\nsystem.out.println(); 起到换行的作业",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数组",frontmatter:{title:"数组",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/add390/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/26.%E6%95%B0%E7%BB%84.html",relativePath:"后端/01.JavaSE/26.数组.md",key:"v-5aca27e4",path:"/pages/add390/",headers:[{level:2,title:"格式",slug:"格式",normalizedTitle:"格式",charIndex:40},{level:2,title:"数组初始化",slug:"数组初始化",normalizedTitle:"数组初始化",charIndex:143},{level:2,title:"动态初始化",slug:"动态初始化",normalizedTitle:"动态初始化",charIndex:212},{level:2,title:"数组元素访问",slug:"数组元素访问",normalizedTitle:"数组元素访问",charIndex:313},{level:2,title:"内存分配",slug:"内存分配",normalizedTitle:"内存分配",charIndex:387},{level:2,title:"静态初始化",slug:"静态初始化",normalizedTitle:"静态初始化",charIndex:601},{level:2,title:"数据操作中的注意事项",slug:"数据操作中的注意事项",normalizedTitle:"数据操作中的注意事项",charIndex:758},{level:2,title:"获取数组元素数量",slug:"获取数组元素数量",normalizedTitle:"获取数组元素数量",charIndex:879},{level:2,title:"获取最值",slug:"获取最值",normalizedTitle:"获取最值",charIndex:931}],headersStr:"格式 数组初始化 动态初始化 数组元素访问 内存分配 静态初始化 数据操作中的注意事项 获取数组元素数量 获取最值",content:"# 数组\n\n数组(array)是一种用于存储多个相同类型数据的存储模型\n\n\n# 格式\n\n数据类型[] 变量名 如:int[] arr\n\n定义了一个int类型的数组，数组名是arr\n\n数据类型 变量名[] 如:int arr[]\n\n定义了一个int类型的变量，变量名是arr数组\n\n\n# 数组初始化\n\njava中的数组必须先初始化，然后才能使用\n\n所谓初始化：就是为数组中的数组元素分配内存空间，并为每个数组元素赋值\n\n\n# 动态初始化\n\n初始化时只指定数组长度，由系统为数组分配初始值，默认值为0\n\n格式：\n\n数据类型[] 变量名 = new 数据类型[数组长度];\n\nint[] arr = new int[3];\n\n\n# 数组元素访问\n\n数组变量访问方式\n\n格式:数组名\n\n数组内容保存的数据的访问方式\n\n格式:数组名[索引]\n\n索引是数组中数据的编号（下标）\n\n\n# 内存分配\n\n数组在初始化时，会为存储空间添加默认值\n\n整数：默认值0\n\n浮点数：默认值0.0\n\n布尔值：false\n\n字符串：默认为空字符\n\n引用数据类型：默认值是null\n\n栈内存：存储局部变量\n\n即定义在方法中的变量，例如：arr 使用完毕，立即消失\n\n堆内存：存储new出来的内容(实体，对象)\n\n数组在初始化时，会为存储空间添加默认值\n\n每个new出来的东西都有一个地址值，使用完毕，会在垃圾回收器空闲时被回收\n\n\n# 静态初始化\n\n初始化时指定每个数组元素的初始值，由系统决定数组长度\n\n格式:数据类型[] 变量名 =new 数据类型[]{数据1，数据2，…};\n\nint[] arr = new int[]{1,2,3}\n\n简化格式:数据类型[]变量名 ={数据1，数据2，…};\n\nint[] arr ={1,2,3}\n\n\n# 数据操作中的注意事项\n\n 1. 索引越界 即下标超长数组的长度\n 2. 空指针异常 访问的数组已经不再指向堆内存的数据，造成空指针异常 即修改了原arr的内存为null，null为空值，为引用数据类型的默认值，表示不指向任何有效对象\n\n\n# 获取数组元素数量\n\n格式：数组名.length\n\n如 arr.length\n\n获取的是元素数量\n\n\n# 获取最值\n\n使用遍历和if语句来实现\n\n定义一个缓存变量起始值为数组的第0个，因为数组遍历从第0个开始遍历\n\nif 判断循环遍历的元素进行 比较运算 符合条件则赋值给缓存变量\n\n循环一直到结束，输出缓存变量",normalizedContent:"# 数组\n\n数组(array)是一种用于存储多个相同类型数据的存储模型\n\n\n# 格式\n\n数据类型[] 变量名 如:int[] arr\n\n定义了一个int类型的数组，数组名是arr\n\n数据类型 变量名[] 如:int arr[]\n\n定义了一个int类型的变量，变量名是arr数组\n\n\n# 数组初始化\n\njava中的数组必须先初始化，然后才能使用\n\n所谓初始化：就是为数组中的数组元素分配内存空间，并为每个数组元素赋值\n\n\n# 动态初始化\n\n初始化时只指定数组长度，由系统为数组分配初始值，默认值为0\n\n格式：\n\n数据类型[] 变量名 = new 数据类型[数组长度];\n\nint[] arr = new int[3];\n\n\n# 数组元素访问\n\n数组变量访问方式\n\n格式:数组名\n\n数组内容保存的数据的访问方式\n\n格式:数组名[索引]\n\n索引是数组中数据的编号（下标）\n\n\n# 内存分配\n\n数组在初始化时，会为存储空间添加默认值\n\n整数：默认值0\n\n浮点数：默认值0.0\n\n布尔值：false\n\n字符串：默认为空字符\n\n引用数据类型：默认值是null\n\n栈内存：存储局部变量\n\n即定义在方法中的变量，例如：arr 使用完毕，立即消失\n\n堆内存：存储new出来的内容(实体，对象)\n\n数组在初始化时，会为存储空间添加默认值\n\n每个new出来的东西都有一个地址值，使用完毕，会在垃圾回收器空闲时被回收\n\n\n# 静态初始化\n\n初始化时指定每个数组元素的初始值，由系统决定数组长度\n\n格式:数据类型[] 变量名 =new 数据类型[]{数据1，数据2，…};\n\nint[] arr = new int[]{1,2,3}\n\n简化格式:数据类型[]变量名 ={数据1，数据2，…};\n\nint[] arr ={1,2,3}\n\n\n# 数据操作中的注意事项\n\n 1. 索引越界 即下标超长数组的长度\n 2. 空指针异常 访问的数组已经不再指向堆内存的数据，造成空指针异常 即修改了原arr的内存为null，null为空值，为引用数据类型的默认值，表示不指向任何有效对象\n\n\n# 获取数组元素数量\n\n格式：数组名.length\n\n如 arr.length\n\n获取的是元素数量\n\n\n# 获取最值\n\n使用遍历和if语句来实现\n\n定义一个缓存变量起始值为数组的第0个，因为数组遍历从第0个开始遍历\n\nif 判断循环遍历的元素进行 比较运算 符合条件则赋值给缓存变量\n\n循环一直到结束，输出缓存变量",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"字符串",frontmatter:{title:"字符串",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/8a171b/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/30.%E5%AD%97%E7%AC%A6%E4%B8%B2.html",relativePath:"后端/01.JavaSE/30.字符串.md",key:"v-1f373500",path:"/pages/8a171b/",headers:[{level:2,title:"API",slug:"api",normalizedTitle:"api",charIndex:10},{level:2,title:"String",slug:"string",normalizedTitle:"string",charIndex:96},{level:2,title:"String 构造方法",slug:"string-构造方法",normalizedTitle:"string 构造方法",charIndex:320},{level:2,title:"String 对象的特点",slug:"string-对象的特点",normalizedTitle:"string 对象的特点",charIndex:498},{level:2,title:"字符串的比较",slug:"字符串的比较",normalizedTitle:"字符串的比较",charIndex:616},{level:2,title:"遍历字符串",slug:"遍历字符串",normalizedTitle:"遍历字符串",charIndex:772},{level:2,title:"统计字符次数",slug:"统计字符次数",normalizedTitle:"统计字符次数",charIndex:937},{level:2,title:"StringBuilder",slug:"stringbuilder",normalizedTitle:"stringbuilder",charIndex:1082},{level:3,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:327},{level:3,title:"添加和反转方法",slug:"添加和反转方法",normalizedTitle:"添加和反转方法",charIndex:1313},{level:3,title:"链式编程",slug:"链式编程",normalizedTitle:"链式编程",charIndex:1416},{level:2,title:"String与Stringbuilder相互转换",slug:"string与stringbuilder相互转换",normalizedTitle:"string与stringbuilder相互转换",charIndex:1581}],headersStr:"API String String 构造方法 String 对象的特点 字符串的比较 遍历字符串 统计字符次数 StringBuilder 构造方法 添加和反转方法 链式编程 String与Stringbuilder相互转换",content:"# 字符串\n\n\n# API\n\n概述API(Application Progarmming Interface)：应用程序编程接口\n\nsc.nextLine()返回是一个字符串类型的值\n\n\n# String\n\nString 类在java.lang包下，使用的时候不需要导包\n\nString类代表字符串，java中所有的字符串文字都被为此类的实例，java中所有双引号字符串的偶数String类的对象\n\n字符串不可变，它们的值在创建后不能被更改\n\n虽然String的值是不可变，但它们可以被共享\n\n字符串效果上相当于字符串数据(char[]),但是底层原理是字节数据(byte[])\n\nJDK8以前是字符数据，JDK9及以后是字节数组\n\n\n# String 构造方法\n\npubilic String() 创建一个空白字符串对象，不含有任何内容\n\npubilc String(char[] chs) 根据字符数组的内容，来创建字符串对象\n\npubilc String(byte[] bys) 根据字节数组的内容，来创建字符串对象\n\nString s =“abc” 直接以赋值方式创建字符串对象\n\n\n# String 对象的特点\n\n通过new创建的字符串对象，每次new都会申请一个内存空间，虽然内存相同但内容地址不相同\n\n以“”赋值方式创建的字符串对象，只有字符序列相同，jvm都只会建立一个String对象，并在字符串池中维护\n\n\n# 字符串的比较\n\n使用==作比较\n\n基本类型：比较的是数据值是否相同\n\n引用类型：比较的是地址值是否相同\n\n字符串是对象，它比较内容是否相同，是通过一个方法来实现的，这个方法叫:equals()\n\npublic boolean equals(Object anObject)：将此字符串与指定对象进行比较\n\n\n# 遍历字符串\n\n根据索引获取字符串的指定字符\n\npublic char charAt(int index);\n\n返回指定索引出的char值，字符串的索引也从0开始\n\n获取字符串的长度\n\npublic int lengh(); 返回此字符串的长度\n\n数组的长度：数组名.length\n\n字符串的长度：字符串对象.length\n\n\n# 统计字符次数\n\n假如ch是一个字符，我要判断它属于大写字母，小写字母，还是数字，直接判断该字符是否在对应的范围即可\n大写字母：ch>='A' && ch<='Z'\n小写字母： ch>='a' && ch<='z'\n数字： ch>='0' && ch<='9'\n\n\n1\n2\n3\n4\n\n\n\n# StringBuilder\n\n如果对字符串进行拼接操作，因为字符串是不可变类型，每次拼接都会构建一个新的string对象，我们可以使用StringBuilder类来解决浪费内存空间问题\n\nStringBuilder是一个可变的字符串类\n\n\n# 构造方法\n\npubilc StringBuilder() 创建一个空白的可变字符串对象，没有任何内容\n\npubilc StringBuilder(String str) 根据字符串的内容，来创建可变字符串对象\n\n\n# 添加和反转方法\n\npublic StringBuilder append(任意类型);添加数据，并返回对象本身\n\npublic StringBuilder reverse(); 返回相反的字符序列\n\n\n# 链式编程\n\n如果一个对象被调用后返回对象自己本身，那么可以根据返回的对象继续调用方法\n\n如str.append().append().append().append();\n\n因为str是 StringBuilder对象，而 StringBuilder对象的append方法是添加数据后并返回对象本身，所以可以写为链式编程\n\n\n# String与Stringbuilder相互转换\n\npublic String toString() 通过tostring() 可以把stringbuilder 转换为string\n\npubilc stringbuilder(string s) 通过构造方法可以实现把string转换为stringbuilder",normalizedContent:"# 字符串\n\n\n# api\n\n概述api(application progarmming interface)：应用程序编程接口\n\nsc.nextline()返回是一个字符串类型的值\n\n\n# string\n\nstring 类在java.lang包下，使用的时候不需要导包\n\nstring类代表字符串，java中所有的字符串文字都被为此类的实例，java中所有双引号字符串的偶数string类的对象\n\n字符串不可变，它们的值在创建后不能被更改\n\n虽然string的值是不可变，但它们可以被共享\n\n字符串效果上相当于字符串数据(char[]),但是底层原理是字节数据(byte[])\n\njdk8以前是字符数据，jdk9及以后是字节数组\n\n\n# string 构造方法\n\npubilic string() 创建一个空白字符串对象，不含有任何内容\n\npubilc string(char[] chs) 根据字符数组的内容，来创建字符串对象\n\npubilc string(byte[] bys) 根据字节数组的内容，来创建字符串对象\n\nstring s =“abc” 直接以赋值方式创建字符串对象\n\n\n# string 对象的特点\n\n通过new创建的字符串对象，每次new都会申请一个内存空间，虽然内存相同但内容地址不相同\n\n以“”赋值方式创建的字符串对象，只有字符序列相同，jvm都只会建立一个string对象，并在字符串池中维护\n\n\n# 字符串的比较\n\n使用==作比较\n\n基本类型：比较的是数据值是否相同\n\n引用类型：比较的是地址值是否相同\n\n字符串是对象，它比较内容是否相同，是通过一个方法来实现的，这个方法叫:equals()\n\npublic boolean equals(object anobject)：将此字符串与指定对象进行比较\n\n\n# 遍历字符串\n\n根据索引获取字符串的指定字符\n\npublic char charat(int index);\n\n返回指定索引出的char值，字符串的索引也从0开始\n\n获取字符串的长度\n\npublic int lengh(); 返回此字符串的长度\n\n数组的长度：数组名.length\n\n字符串的长度：字符串对象.length\n\n\n# 统计字符次数\n\n假如ch是一个字符，我要判断它属于大写字母，小写字母，还是数字，直接判断该字符是否在对应的范围即可\n大写字母：ch>='a' && ch<='z'\n小写字母： ch>='a' && ch<='z'\n数字： ch>='0' && ch<='9'\n\n\n1\n2\n3\n4\n\n\n\n# stringbuilder\n\n如果对字符串进行拼接操作，因为字符串是不可变类型，每次拼接都会构建一个新的string对象，我们可以使用stringbuilder类来解决浪费内存空间问题\n\nstringbuilder是一个可变的字符串类\n\n\n# 构造方法\n\npubilc stringbuilder() 创建一个空白的可变字符串对象，没有任何内容\n\npubilc stringbuilder(string str) 根据字符串的内容，来创建可变字符串对象\n\n\n# 添加和反转方法\n\npublic stringbuilder append(任意类型);添加数据，并返回对象本身\n\npublic stringbuilder reverse(); 返回相反的字符序列\n\n\n# 链式编程\n\n如果一个对象被调用后返回对象自己本身，那么可以根据返回的对象继续调用方法\n\n如str.append().append().append().append();\n\n因为str是 stringbuilder对象，而 stringbuilder对象的append方法是添加数据后并返回对象本身，所以可以写为链式编程\n\n\n# string与stringbuilder相互转换\n\npublic string tostring() 通过tostring() 可以把stringbuilder 转换为string\n\npubilc stringbuilder(string s) 通过构造方法可以实现把string转换为stringbuilder",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"ArrayList集合",frontmatter:{title:"ArrayList集合",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ab65fb/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/31.ArrayList%E9%9B%86%E5%90%88.html",relativePath:"后端/01.JavaSE/31.ArrayList集合.md",key:"v-625e23c3",path:"/pages/ab65fb/",headers:[{level:2,title:"ArrayList 构造方法和添加方法",slug:"arraylist-构造方法和添加方法",normalizedTitle:"arraylist 构造方法和添加方法",charIndex:140},{level:2,title:"遍历ArrayList",slug:"遍历arraylist",normalizedTitle:"遍历arraylist",charIndex:306},{level:2,title:"ArrayList 集合常用方法",slug:"arraylist-集合常用方法",normalizedTitle:"arraylist 集合常用方法",charIndex:367}],headersStr:"ArrayList 构造方法和添加方法 遍历ArrayList ArrayList 集合常用方法",content:"# ArrayList集合\n\n集合类的特点：提供一种存储空间可变的存储模型，存储的数据容量可以发生改变\n\nArrayList<E>：\n\n可调整大小的数组实现\n\n<E>:是一种特殊的数据类型，泛型 如ArrayList<String>，ArrayList<Student>\n\n\n# ArrayList 构造方法和添加方法\n\npublic ArrayList() 创建一个空的集合对象\n\npublic boolean add(E e) 将指定的元素追加到此集合的末尾\n\npublic void add(int index，E elemnet) 在此集合中的指定位置插入指定的元素,不可超过集合的长度否则报错\n\n\n# 遍历ArrayList\n\n获取集合的每一个元素 get(int index);\n\n获取集合的长度 size();\n\n\n# ArrayList 集合常用方法\n\npubilc boolean remove(Object o) 删除指定的元素，返回一个布尔值\n\npubilc E remove(int index) 删除指定索引的元素，返回被删除的元素\n\npubilc E set(int index,E elemnt) 修改指定索引处的元素，返回被修改的元素\n\npublic E get(int index) 返回指定索引处的元素\n\npubilc int size() 返回集合中的元素的个数",normalizedContent:"# arraylist集合\n\n集合类的特点：提供一种存储空间可变的存储模型，存储的数据容量可以发生改变\n\narraylist<e>：\n\n可调整大小的数组实现\n\n<e>:是一种特殊的数据类型，泛型 如arraylist<string>，arraylist<student>\n\n\n# arraylist 构造方法和添加方法\n\npublic arraylist() 创建一个空的集合对象\n\npublic boolean add(e e) 将指定的元素追加到此集合的末尾\n\npublic void add(int index，e elemnet) 在此集合中的指定位置插入指定的元素,不可超过集合的长度否则报错\n\n\n# 遍历arraylist\n\n获取集合的每一个元素 get(int index);\n\n获取集合的长度 size();\n\n\n# arraylist 集合常用方法\n\npubilc boolean remove(object o) 删除指定的元素，返回一个布尔值\n\npubilc e remove(int index) 删除指定索引的元素，返回被删除的元素\n\npubilc e set(int index,e elemnt) 修改指定索引处的元素，返回被修改的元素\n\npublic e get(int index) 返回指定索引处的元素\n\npubilc int size() 返回集合中的元素的个数",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"修饰符",frontmatter:{title:"修饰符",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/f3b824/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/33.%E4%BF%AE%E9%A5%B0%E7%AC%A6.html",relativePath:"后端/01.JavaSE/33.修饰符.md",key:"v-7189a78a",path:"/pages/f3b824/",headers:[{level:2,title:"包",slug:"包",normalizedTitle:"包",charIndex:10},{level:3,title:"定义格式",slug:"定义格式",normalizedTitle:"定义格式",charIndex:38}],headersStr:"包 定义格式",content:"# 修饰符\n\n\n# 包\n\n包的概念其实就是文件夹,对类进行分类管理\n\n\n# 定义格式\n\n格式:package 包名;(多级包用.分开) 如果package 包名1.包名2;",normalizedContent:"# 修饰符\n\n\n# 包\n\n包的概念其实就是文件夹,对类进行分类管理\n\n\n# 定义格式\n\n格式:package 包名;(多级包用.分开) 如果package 包名1.包名2;",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"继承",frontmatter:{title:"继承",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/c7fa91/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/32.%E7%BB%A7%E6%89%BF.html",relativePath:"后端/01.JavaSE/32.继承.md",key:"v-5945b915",path:"/pages/c7fa91/",headers:[{level:2,title:"继承的好处与弊端",slug:"继承的好处与弊端",normalizedTitle:"继承的好处与弊端",charIndex:160},{level:2,title:"继承中变量的访问特点",slug:"继承中变量的访问特点",normalizedTitle:"继承中变量的访问特点",charIndex:444},{level:2,title:"super",slug:"super",normalizedTitle:"super",charIndex:539},{level:2,title:"继承中构造方法的访问特点",slug:"继承中构造方法的访问特点",normalizedTitle:"继承中构造方法的访问特点",charIndex:623},{level:2,title:"继承中成员方法的访问特点",slug:"继承中成员方法的访问特点",normalizedTitle:"继承中成员方法的访问特点",charIndex:856},{level:2,title:"方法重写",slug:"方法重写",normalizedTitle:"方法重写",charIndex:938},{level:3,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:947},{level:3,title:"校验是否是方法重写",slug:"校验是否是方法重写",normalizedTitle:"校验是否是方法重写",charIndex:1007},{level:3,title:"注意事项",slug:"注意事项",normalizedTitle:"注意事项",charIndex:1059},{level:2,title:"继承注意事项",slug:"继承注意事项",normalizedTitle:"继承注意事项",charIndex:1180}],headersStr:"继承的好处与弊端 继承中变量的访问特点 super 继承中构造方法的访问特点 继承中成员方法的访问特点 方法重写 概述 校验是否是方法重写 注意事项 继承注意事项",content:"# 继承\n\n继承是面向对象三大特征之一。可以使得子类具有父类的属性和方法，还可以在子类中重新定义，追加属性和方法\n\n格式：pubilc class 子类名 extends 父类名{}\n\n如：public class Zi extends Fu{}\n\nFu：是父类，又称为基类，超类\n\nZi：是子类，又称为派生类\n\n\n# 继承的好处与弊端\n\n好处\n\n提高了代码的复用性(多个类相同的成员或者方法可以放到同一个类中)\n\n提高了代码的维护性（如果方法的代码需要修改，修改一处即可）\n\n弊端\n\n继承让类与类之间产生了关系，类的耦合性增强了，当父类发生变化时子类也不得不跟着变化，削弱了子类的独立性\n\n什么时候使用继承？\n\n继承体现的关系：is a\n\n假设法：我有两个类A和B，如果他们满足A是B的一种，或者B是A的一种，就说明他们存在继承关系，这个时候就可以考虑使用继承，否则不能滥用继承，因为不确定父类什么时候改变，而子类是否需要这些继承的属性或方法。\n\n相对于父类必须是子类的真子集\n\n\n# 继承中变量的访问特点\n\n在子类方法中访问一个变量的访问顺序是\n\n 1. 子类局部范围找\n 2. 子类成员范围找\n 3. 父类成员范围找\n 4. 如果都没有则报错（不考虑父类的父类）\n\n\n# super\n\nsuper关键字用法与this关键字的用法相似\n\nthis:代表本类对象的引用\n\nsuper:代表父类存储空间的标识（也可以理解为父类对象引用）\n\n\n# 继承中构造方法的访问特点\n\n子类中所有的构造方法默认都会访问父类中无参的构造方法\n\n因为子类会继承父类中的数据，可能还会使用父类的数据。所以在子类初始化前，一定要先完成父类数据的初始化\n\n每一个子类构造方法的第一条语句默认都是：super()，如果父类中并没有无参构造方法则报错\n\nsuper()调用的是父类的构造方法\n\n如果父类中没有无参构造方法\n\n只有带参构造方法则通过使用super关键字去调用父类中的代词构造方法\n\n建议在父类中提供一个无参构造方法\n\n\n# 继承中成员方法的访问特点\n\n通过子类对象访问一个方法顺序\n\n 1. 子类成员范围找\n 2. 父类成员范围找\n 3. 如果都没有则报错（不考虑父类的父类j）\n\n\n# 方法重写\n\n\n# 概述\n\n子类中出现了和父类一模一样的方法声明\n\n并在子类重写方法可以在方法中使用super关键字调用父类的方法\n\n\n# 校验是否是方法重写\n\n在重写构造方法上方加上@ovrride 如果报错则此子类构造方法不是重写\n\n\n# 注意事项\n\n 1. 父类中的私有方法是不允许重写的\n\n 2. 声明方法父类与子类必须相同 如不加public关键字 系统默认也是有声明关键字,但不用public权限高\n    \n    (public > 默认 > private)\n\n\n# 继承注意事项\n\n 1. 一个子类不能继承多个父类 只有继承一个父类\n 2. 可以使用嵌套子类继承父类,父类继承父类的父类(多层继承)",normalizedContent:"# 继承\n\n继承是面向对象三大特征之一。可以使得子类具有父类的属性和方法，还可以在子类中重新定义，追加属性和方法\n\n格式：pubilc class 子类名 extends 父类名{}\n\n如：public class zi extends fu{}\n\nfu：是父类，又称为基类，超类\n\nzi：是子类，又称为派生类\n\n\n# 继承的好处与弊端\n\n好处\n\n提高了代码的复用性(多个类相同的成员或者方法可以放到同一个类中)\n\n提高了代码的维护性（如果方法的代码需要修改，修改一处即可）\n\n弊端\n\n继承让类与类之间产生了关系，类的耦合性增强了，当父类发生变化时子类也不得不跟着变化，削弱了子类的独立性\n\n什么时候使用继承？\n\n继承体现的关系：is a\n\n假设法：我有两个类a和b，如果他们满足a是b的一种，或者b是a的一种，就说明他们存在继承关系，这个时候就可以考虑使用继承，否则不能滥用继承，因为不确定父类什么时候改变，而子类是否需要这些继承的属性或方法。\n\n相对于父类必须是子类的真子集\n\n\n# 继承中变量的访问特点\n\n在子类方法中访问一个变量的访问顺序是\n\n 1. 子类局部范围找\n 2. 子类成员范围找\n 3. 父类成员范围找\n 4. 如果都没有则报错（不考虑父类的父类）\n\n\n# super\n\nsuper关键字用法与this关键字的用法相似\n\nthis:代表本类对象的引用\n\nsuper:代表父类存储空间的标识（也可以理解为父类对象引用）\n\n\n# 继承中构造方法的访问特点\n\n子类中所有的构造方法默认都会访问父类中无参的构造方法\n\n因为子类会继承父类中的数据，可能还会使用父类的数据。所以在子类初始化前，一定要先完成父类数据的初始化\n\n每一个子类构造方法的第一条语句默认都是：super()，如果父类中并没有无参构造方法则报错\n\nsuper()调用的是父类的构造方法\n\n如果父类中没有无参构造方法\n\n只有带参构造方法则通过使用super关键字去调用父类中的代词构造方法\n\n建议在父类中提供一个无参构造方法\n\n\n# 继承中成员方法的访问特点\n\n通过子类对象访问一个方法顺序\n\n 1. 子类成员范围找\n 2. 父类成员范围找\n 3. 如果都没有则报错（不考虑父类的父类j）\n\n\n# 方法重写\n\n\n# 概述\n\n子类中出现了和父类一模一样的方法声明\n\n并在子类重写方法可以在方法中使用super关键字调用父类的方法\n\n\n# 校验是否是方法重写\n\n在重写构造方法上方加上@ovrride 如果报错则此子类构造方法不是重写\n\n\n# 注意事项\n\n 1. 父类中的私有方法是不允许重写的\n\n 2. 声明方法父类与子类必须相同 如不加public关键字 系统默认也是有声明关键字,但不用public权限高\n    \n    (public > 默认 > private)\n\n\n# 继承注意事项\n\n 1. 一个子类不能继承多个父类 只有继承一个父类\n 2. 可以使用嵌套子类继承父类,父类继承父类的父类(多层继承)",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"权限修饰符",frontmatter:{title:"权限修饰符",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/047301/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/34.%E6%9D%83%E9%99%90%E4%BF%AE%E9%A5%B0%E7%AC%A6.html",relativePath:"后端/01.JavaSE/34.权限修饰符.md",key:"v-71d4b5e7",path:"/pages/047301/",headersStr:null,content:"# 权限修饰符\n\npublic > protected > 默认 > privater",normalizedContent:"# 权限修饰符\n\npublic > protected > 默认 > privater",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"状态修饰符",frontmatter:{title:"状态修饰符",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/392033/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/35.%E7%8A%B6%E6%80%81%E4%BF%AE%E9%A5%B0%E7%AC%A6.html",relativePath:"后端/01.JavaSE/35.状态修饰符.md",key:"v-78697e58",path:"/pages/392033/",headers:[{level:2,title:"final",slug:"final",normalizedTitle:"final",charIndex:12},{level:2,title:"static",slug:"static",normalizedTitle:"static",charIndex:203},{level:3,title:"访问特点",slug:"访问特点",normalizedTitle:"访问特点",charIndex:274}],headersStr:"final static 访问特点",content:"# 状态修饰符\n\n\n# final\n\nfinal关键字是最终意思,可以修饰成员方法,成员变量,类\n\n 1. 被final修饰后的方法 无法被重写\n 2. 被final修饰后的成员变量 无法重新赋值\n 3. 被final修饰后的类无法成为父类被继承\n\n如果final修饰是常规变量,则该变量不能被重新赋值\n\n如果final修饰是引用型变量,则该变量指向的地址值不能被更改,但指向地址的内部能被修改.\n\n\n# static\n\n被static修饰后的变量会被类的所有对象共享\n\n通过类名或者对象来继续 修改值,会一直保存在类中,所有对象都能访问\n\n\n# 访问特点\n\n静态方法 无法访问不是静态的方法和变量\n\n非静态方法 可以访问静态和非静态的方法和变量",normalizedContent:"# 状态修饰符\n\n\n# final\n\nfinal关键字是最终意思,可以修饰成员方法,成员变量,类\n\n 1. 被final修饰后的方法 无法被重写\n 2. 被final修饰后的成员变量 无法重新赋值\n 3. 被final修饰后的类无法成为父类被继承\n\n如果final修饰是常规变量,则该变量不能被重新赋值\n\n如果final修饰是引用型变量,则该变量指向的地址值不能被更改,但指向地址的内部能被修改.\n\n\n# static\n\n被static修饰后的变量会被类的所有对象共享\n\n通过类名或者对象来继续 修改值,会一直保存在类中,所有对象都能访问\n\n\n# 访问特点\n\n静态方法 无法访问不是静态的方法和变量\n\n非静态方法 可以访问静态和非静态的方法和变量",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"抽象",frontmatter:{title:"抽象",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/a12790/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/37.%E6%8A%BD%E8%B1%A1.html",relativePath:"后端/01.JavaSE/37.抽象.md",key:"v-5d5bf6b5",path:"/pages/a12790/",headers:[{level:3,title:"特点",slug:"特点",normalizedTitle:"特点",charIndex:170}],headersStr:"特点",content:"# 抽象\n\n 1. abstract 关键字\n\n 2. 一个没有方法体的方法应该定义为抽象方法,如果类中有抽象方法,则类必须定义为抽象类\n\n 3. 抽象类中不一定有抽象方法\n\n 4. 抽象类不能实例化,但可以参照多态的形式,通过子类对象实例化,称为抽象类多态\n\n 5. 抽象类的子类 要么定义为抽象类 要么重写抽象类中所有抽象方法\n\n\n# 特点\n\n成员变量: 可以是常量和变量\n\n构造方法:有构造方法,但不能实例化,用于子类访问父类数据的初始化\n\n成员方法:可有抽象方法,用于限定子类必须完成某些动作.也可以有非抽象方法,提高代码复用性",normalizedContent:"# 抽象\n\n 1. abstract 关键字\n\n 2. 一个没有方法体的方法应该定义为抽象方法,如果类中有抽象方法,则类必须定义为抽象类\n\n 3. 抽象类中不一定有抽象方法\n\n 4. 抽象类不能实例化,但可以参照多态的形式,通过子类对象实例化,称为抽象类多态\n\n 5. 抽象类的子类 要么定义为抽象类 要么重写抽象类中所有抽象方法\n\n\n# 特点\n\n成员变量: 可以是常量和变量\n\n构造方法:有构造方法,但不能实例化,用于子类访问父类数据的初始化\n\n成员方法:可有抽象方法,用于限定子类必须完成某些动作.也可以有非抽象方法,提高代码复用性",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"接口",frontmatter:{title:"接口",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/db4705/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/38.%E6%8E%A5%E5%8F%A3.html",relativePath:"后端/01.JavaSE/38.接口.md",key:"v-7d483deb",path:"/pages/db4705/",headers:[{level:2,title:"特点",slug:"特点",normalizedTitle:"特点",charIndex:61},{level:2,title:"成员特点",slug:"成员特点",normalizedTitle:"成员特点",charIndex:295}],headersStr:"特点 成员特点",content:"# 接口\n\n概述\n\n接口是一种公共的规范标准,只要符合规范标准,都是通用,java中的接口更多提醒在对行为的抽象\n\n\n# 特点\n\n使用关键字 interface 修饰\n\n如:public interface 接口名{}\n\n类实现接口(类似继承)用implements关键字\n\n如:public class 类名 implements 接口名{}\n\n接口不能被实例化,跟多态的方式一样,通过类对象实例化,称为接口多态\n\n多态的形式:具体类多态,抽象类多态,接口多态\n\n多态的前提:\n\n 1. 有继承或实现关系;\n 2. 有方法重写;\n 3. 有父类或接口引用指向 子类或实现 类对象\n\n\n# 成员特点\n\n成员变量: 只能常量 默认修饰符:public static final\n\n构造方法:接口没有构造方法,因为接口的主要是对行为进行抽象的,是没有具体存在\n\n一个类如果没有父类,默认继承自object类\n\n成员方法:只能是抽象方法 默认修饰符:public abstract",normalizedContent:"# 接口\n\n概述\n\n接口是一种公共的规范标准,只要符合规范标准,都是通用,java中的接口更多提醒在对行为的抽象\n\n\n# 特点\n\n使用关键字 interface 修饰\n\n如:public interface 接口名{}\n\n类实现接口(类似继承)用implements关键字\n\n如:public class 类名 implements 接口名{}\n\n接口不能被实例化,跟多态的方式一样,通过类对象实例化,称为接口多态\n\n多态的形式:具体类多态,抽象类多态,接口多态\n\n多态的前提:\n\n 1. 有继承或实现关系;\n 2. 有方法重写;\n 3. 有父类或接口引用指向 子类或实现 类对象\n\n\n# 成员特点\n\n成员变量: 只能常量 默认修饰符:public static final\n\n构造方法:接口没有构造方法,因为接口的主要是对行为进行抽象的,是没有具体存在\n\n一个类如果没有父类,默认继承自object类\n\n成员方法:只能是抽象方法 默认修饰符:public abstract",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"多态",frontmatter:{title:"多态",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/6407a8/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/36.%E5%A4%9A%E6%80%81.html",relativePath:"后端/01.JavaSE/36.多态.md",key:"v-4e5c8b8d",path:"/pages/6407a8/",headers:[{level:2,title:"多态的前提",slug:"多态的前提",normalizedTitle:"多态的前提",charIndex:31},{level:2,title:"多态中成员访问特点",slug:"多态中成员访问特点",normalizedTitle:"多态中成员访问特点",charIndex:83},{level:2,title:"多态好处和弊端",slug:"多态好处和弊端",normalizedTitle:"多态好处和弊端",charIndex:232},{level:2,title:"多态中的转型",slug:"多态中的转型",normalizedTitle:"多态中的转型",charIndex:302}],headersStr:"多态的前提 多态中成员访问特点 多态好处和弊端 多态中的转型",content:"# 多态\n\n同一个对象,在不同时刻表现出来的不同形态\n\n\n# 多态的前提\n\n 1. 有继承/实现关系\n\n 2. 有方法重写\n\n 3. 有父类引用指向子类对象\n\n\n# 多态中成员访问特点\n\n成员变量:编译看左边,执行看左边\n\n成员变量必须是引用类中有的才能编译 并且输出是引用类的值\n\n成员方法:编译看左边,执行看右边\n\n成员方法必须是引用类中有的才能编译 并且如对象中重写了此方法 则输出对象中的方法 否则输出引用中的方法\n\n成员方法有重写,成员变量没有\n\n\n# 多态好处和弊端\n\n好处是 定义个方法使用父类作为参数,使用时只需将子类传进就可以使用\n\n弊端是 无法使用子类特有而父类中没有的方法\n\n\n# 多态中的转型\n\n向上转型\n\n从子到父 父类引用指向子类对象\n\n向下转型\n\n从父到子 父类引用转为子类对象",normalizedContent:"# 多态\n\n同一个对象,在不同时刻表现出来的不同形态\n\n\n# 多态的前提\n\n 1. 有继承/实现关系\n\n 2. 有方法重写\n\n 3. 有父类引用指向子类对象\n\n\n# 多态中成员访问特点\n\n成员变量:编译看左边,执行看左边\n\n成员变量必须是引用类中有的才能编译 并且输出是引用类的值\n\n成员方法:编译看左边,执行看右边\n\n成员方法必须是引用类中有的才能编译 并且如对象中重写了此方法 则输出对象中的方法 否则输出引用中的方法\n\n成员方法有重写,成员变量没有\n\n\n# 多态好处和弊端\n\n好处是 定义个方法使用父类作为参数,使用时只需将子类传进就可以使用\n\n弊端是 无法使用子类特有而父类中没有的方法\n\n\n# 多态中的转型\n\n向上转型\n\n从子到父 父类引用指向子类对象\n\n向下转型\n\n从父到子 父类引用转为子类对象",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"抽象类与接口的区别",frontmatter:{title:"抽象类与接口的区别",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/102423/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/40.%E6%8A%BD%E8%B1%A1%E7%B1%BB%E4%B8%8E%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%8C%BA%E5%88%AB.html",relativePath:"后端/01.JavaSE/40.抽象类与接口的区别.md",key:"v-38d33902",path:"/pages/102423/",headersStr:null,content:"# 抽象类与接口的区别\n\n成员区别:\n\n抽象类 变量,常量;有构造方法;有抽象方法;也有非抽象方法\n\n接口 常量;抽象方法\n\n关系区别:\n\n类与类 继承,单继承\n\n类与接口 实现,可单实现,也可多实现\n\n接口与接口 继承,单继承,多继承\n\n设计理念区别:\n\n抽象类 对类抽象,包括属性、行为\n\n接口 对行为抽象，主要是行为\n\n抽象类是对事物的抽象,而接口是对行为的抽象",normalizedContent:"# 抽象类与接口的区别\n\n成员区别:\n\n抽象类 变量,常量;有构造方法;有抽象方法;也有非抽象方法\n\n接口 常量;抽象方法\n\n关系区别:\n\n类与类 继承,单继承\n\n类与接口 实现,可单实现,也可多实现\n\n接口与接口 继承,单继承,多继承\n\n设计理念区别:\n\n抽象类 对类抽象,包括属性、行为\n\n接口 对行为抽象，主要是行为\n\n抽象类是对事物的抽象,而接口是对行为的抽象",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"类和接口的关系",frontmatter:{title:"类和接口的关系",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/b47784/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/39.%E7%B1%BB%E5%92%8C%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%85%B3%E7%B3%BB.html",relativePath:"后端/01.JavaSE/39.类和接口的关系.md",key:"v-45335061",path:"/pages/b47784/",headersStr:null,content:"# 类和接口的关系\n\n类和类的关系:\n\n继承关系,只能单继承,但是可以多层继承\n\n类和接口的关系:\n\n实现关系,可以单实现,也可以多实现,还可以再继承一个类同时实现多个接口\n\n接口和接口的关系:\n\n继承关系,可以单继承,也可以多继承",normalizedContent:"# 类和接口的关系\n\n类和类的关系:\n\n继承关系,只能单继承,但是可以多层继承\n\n类和接口的关系:\n\n实现关系,可以单实现,也可以多实现,还可以再继承一个类同时实现多个接口\n\n接口和接口的关系:\n\n继承关系,可以单继承,也可以多继承",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"内部类",frontmatter:{title:"内部类",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/86e52c/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/42.%E5%86%85%E9%83%A8%E7%B1%BB.html",relativePath:"后端/01.JavaSE/42.内部类.md",key:"v-2f94f67c",path:"/pages/86e52c/",headers:[{level:2,title:"成员内部类",slug:"成员内部类",normalizedTitle:"成员内部类",charIndex:128},{level:2,title:"局部内部类",slug:"局部内部类",normalizedTitle:"局部内部类",charIndex:183},{level:2,title:"匿名内部类",slug:"匿名内部类",normalizedTitle:"匿名内部类",charIndex:222},{level:2,title:"lambda表达式",slug:"lambda表达式",normalizedTitle:"lambda表达式",charIndex:1032},{level:3,title:"lambda表达式的省略",slug:"lambda表达式的省略",normalizedTitle:"lambda表达式的省略",charIndex:3120}],headersStr:"成员内部类 局部内部类 匿名内部类 lambda表达式 lambda表达式的省略",content:'# 内部类\n\n一个类中内部中定义了一个类B\n\n如:public class A{\n\npublic class b{\n\n}\n\n}\n\n内部类的访问特点\n\n 1. 内部类可以直接访问外部类的成员,包括私有\n 2. 外部了你要访问内部类成员,必须创建对象\n\n\n# 成员内部类\n\n如是公共则可以创建对象\n\na类名.b类名 i=new a类名().new b类名();\n\n\n# 局部内部类\n\n局部类外界是无法直接访问的\n\n需要在内部创建对象并使用\n\n\n# 匿名内部类\n\n匿名内部类 是一贯继承了该类或者实现了该接口的子类匿名对象\n\nnew 类名或接口名(){\n\n重写方法;\n\n};\n\n匿名内部类可以直接调用方法 只需要在匿名内部类后调用\n\nnew Inter(){\n    @Override\n    public void show() {\n        System.out.println("匿名内部类");\n    }\n}.show();  //匿名内部类需要在匿名内部类后直接调用内部类的方法\n\n\n1\n2\n3\n4\n5\n6\n\n\n匿名内部类本质上也是一个类 但是是匿名无法 直接调用\n\n但我们可以通过多态实现创建对象 来调用\n\nInter i =  new Inter(){  //匿名内部类也可以使用多态创建对象\n     @Override\n     public void show() {\n         System.out.println("匿名内部类");\n     }\n };  //匿名内部类需要在匿名内部类后直接调用内部类的方法\n\n i.show();  //调用方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果一个方法需要提供一个接口并使用它,我们可以在方法中直接new一个匿名内部类并重写接口的方法\n\npublic void in(Inter inter){\n        inter.show();\n    }\n\nOuter op =new Outer();\nop.in(new Inter() {\n            @Override\n            public void show() {\n                System.out.println("在方法中直接new一个匿名内部类");\n            }\n        });\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# lambda表达式\n\nlambda表达式,我们上面使用直接new一个匿名内部类给这个方法,我们也可以通过lambda表达式直接传一个\n\n使用lambda表达式的前提\n\n 1. 有一个接口\n 2. 接口中有且仅有一个抽象方法\n\n语法: (参数) -> {重写方法};\n\npublic class lambda {\n\n\n    public static void main(String[] args) {\n        goswimming(() -> {\n            System.out.println("lambda");\n        });\n    }\n\n    public static void goswimming(Swimming swimming){\n        swimming.swim();\n    }\n\n}\n\ninterface Swimming {  // 要求必须为接口实现\n    void swim();  // 只能有一个抽象方法\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n带参无返回值\n\npublic class lambda2 {\n    public static void main(String[] args) {\n        useStringHandler(new StringHandler() {\n            @Override\n            public void pringMessage(String msg) {\n                System.out.println("new匿名内部类 " + msg);\n            }\n        });\n\n\n        useStringHandler((String msg) -> {\n            System.out.println("lambda传参 " + msg);\n        });\n    }\n\n    public static void useStringHandler(StringHandler stringHandler) {\n        stringHandler.pringMessage("默认值");\n    }\n}\n\ninterface StringHandler {\n    void pringMessage(String msg);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n带返回值的lambda表达式\n\n必须return\n\nimport java.util.Random;\n\npublic class lambda3 {\n    public static void main(String[] args) {\n\n        useRandomNumHandler(() -> {\n            Random r =new Random();\n            int num = r.nextInt(10)+1;\n            return num;\n        });\n    }\n\n    public static void useRandomNumHandler(RandomNumHandler randomNumHandler){\n        int result = randomNumHandler.getNumber();\n        System.out.println(result);\n    }\n\n\n}\n\n\n\ninterface RandomNumHandler{\n    int getNumber();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n带参与带返回值\n\npublic class lambda4 {\n    public static void main(String[] args) {\n        useCalculator(( a , b) -> a+b);\n    }\n\n    public static void useCalculator(Calculator calculator){\n        int result = calculator.calc(10,20);\n        System.out.println(result);\n    }\n\n}\n\ninterface Calculator{\n    int calc(int a,int b);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# lambda表达式的省略\n\n * 参数类型可以省略,但有多个不同的参数类型时不能省略\n\n * 如果参数只有一个,则小括号可以省略,没有参数则不能省略必须书写()\n\n * 如果代码块的语句只有一条,可以省略大括号和分号甚至是return语句\n\n * m m,kkhj ;bnhjuo9p[\'inmji0op];[\n\n * khJU <KINM]\'编译之后会生成一个单独的.class文件\n\n * lambda表达式：编译之后，不会生成.class文件，对应的字节码会在运行时动态生成',normalizedContent:'# 内部类\n\n一个类中内部中定义了一个类b\n\n如:public class a{\n\npublic class b{\n\n}\n\n}\n\n内部类的访问特点\n\n 1. 内部类可以直接访问外部类的成员,包括私有\n 2. 外部了你要访问内部类成员,必须创建对象\n\n\n# 成员内部类\n\n如是公共则可以创建对象\n\na类名.b类名 i=new a类名().new b类名();\n\n\n# 局部内部类\n\n局部类外界是无法直接访问的\n\n需要在内部创建对象并使用\n\n\n# 匿名内部类\n\n匿名内部类 是一贯继承了该类或者实现了该接口的子类匿名对象\n\nnew 类名或接口名(){\n\n重写方法;\n\n};\n\n匿名内部类可以直接调用方法 只需要在匿名内部类后调用\n\nnew inter(){\n    @override\n    public void show() {\n        system.out.println("匿名内部类");\n    }\n}.show();  //匿名内部类需要在匿名内部类后直接调用内部类的方法\n\n\n1\n2\n3\n4\n5\n6\n\n\n匿名内部类本质上也是一个类 但是是匿名无法 直接调用\n\n但我们可以通过多态实现创建对象 来调用\n\ninter i =  new inter(){  //匿名内部类也可以使用多态创建对象\n     @override\n     public void show() {\n         system.out.println("匿名内部类");\n     }\n };  //匿名内部类需要在匿名内部类后直接调用内部类的方法\n\n i.show();  //调用方法\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果一个方法需要提供一个接口并使用它,我们可以在方法中直接new一个匿名内部类并重写接口的方法\n\npublic void in(inter inter){\n        inter.show();\n    }\n\nouter op =new outer();\nop.in(new inter() {\n            @override\n            public void show() {\n                system.out.println("在方法中直接new一个匿名内部类");\n            }\n        });\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# lambda表达式\n\nlambda表达式,我们上面使用直接new一个匿名内部类给这个方法,我们也可以通过lambda表达式直接传一个\n\n使用lambda表达式的前提\n\n 1. 有一个接口\n 2. 接口中有且仅有一个抽象方法\n\n语法: (参数) -> {重写方法};\n\npublic class lambda {\n\n\n    public static void main(string[] args) {\n        goswimming(() -> {\n            system.out.println("lambda");\n        });\n    }\n\n    public static void goswimming(swimming swimming){\n        swimming.swim();\n    }\n\n}\n\ninterface swimming {  // 要求必须为接口实现\n    void swim();  // 只能有一个抽象方法\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n带参无返回值\n\npublic class lambda2 {\n    public static void main(string[] args) {\n        usestringhandler(new stringhandler() {\n            @override\n            public void pringmessage(string msg) {\n                system.out.println("new匿名内部类 " + msg);\n            }\n        });\n\n\n        usestringhandler((string msg) -> {\n            system.out.println("lambda传参 " + msg);\n        });\n    }\n\n    public static void usestringhandler(stringhandler stringhandler) {\n        stringhandler.pringmessage("默认值");\n    }\n}\n\ninterface stringhandler {\n    void pringmessage(string msg);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n带返回值的lambda表达式\n\n必须return\n\nimport java.util.random;\n\npublic class lambda3 {\n    public static void main(string[] args) {\n\n        userandomnumhandler(() -> {\n            random r =new random();\n            int num = r.nextint(10)+1;\n            return num;\n        });\n    }\n\n    public static void userandomnumhandler(randomnumhandler randomnumhandler){\n        int result = randomnumhandler.getnumber();\n        system.out.println(result);\n    }\n\n\n}\n\n\n\ninterface randomnumhandler{\n    int getnumber();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n带参与带返回值\n\npublic class lambda4 {\n    public static void main(string[] args) {\n        usecalculator(( a , b) -> a+b);\n    }\n\n    public static void usecalculator(calculator calculator){\n        int result = calculator.calc(10,20);\n        system.out.println(result);\n    }\n\n}\n\ninterface calculator{\n    int calc(int a,int b);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# lambda表达式的省略\n\n * 参数类型可以省略,但有多个不同的参数类型时不能省略\n\n * 如果参数只有一个,则小括号可以省略,没有参数则不能省略必须书写()\n\n * 如果代码块的语句只有一条,可以省略大括号和分号甚至是return语句\n\n * m m,kkhj ;bnhjuo9p[\'inmji0op];[\n\n * khju <kinm]\'编译之后会生成一个单独的.class文件\n\n * lambda表达式：编译之后，不会生成.class文件，对应的字节码会在运行时动态生成',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Api",frontmatter:{title:"Api",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/a4cd8f/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/43.Api.html",relativePath:"后端/01.JavaSE/43.Api.md",key:"v-35580351",path:"/pages/a4cd8f/",headers:[{level:2,title:"Sytem",slug:"sytem",normalizedTitle:"sytem",charIndex:10},{level:3,title:"获取当前时间戳",slug:"获取当前时间戳",normalizedTitle:"获取当前时间戳",charIndex:20},{level:3,title:"复制数组",slug:"复制数组",normalizedTitle:"复制数组",charIndex:65},{level:2,title:"Objects",slug:"objects",normalizedTitle:"objects",charIndex:125},{level:3,title:"toSting(对象)",slug:"tosting-对象",normalizedTitle:"tosting(对象)",charIndex:137},{level:3,title:"Objects.toSring(对象,默认字符串)",slug:"objects-tosring-对象-默认字符串",normalizedTitle:"objects.tosring(对象,默认字符串)",charIndex:166},{level:3,title:"isNull(对象)",slug:"isnull-对象",normalizedTitle:"isnull(对象)",charIndex:233},{level:3,title:"nonNull(对象)",slug:"nonnull-对象",normalizedTitle:"nonnull(对象)",charIndex:264},{level:2,title:"BigDecimal",slug:"bigdecimal",normalizedTitle:"bigdecimal",charIndex:297},{level:2,title:"Integer",slug:"integer",normalizedTitle:"integer",charIndex:383},{level:2,title:"自动装箱",slug:"自动装箱",normalizedTitle:"自动装箱",charIndex:395},{level:2,title:"自动拆箱",slug:"自动拆箱",normalizedTitle:"自动拆箱",charIndex:472},{level:2,title:"parseInt字符串转Integer",slug:"parseint字符串转integer",normalizedTitle:"parseint字符串转integer",charIndex:533},{level:2,title:"Integer转字符串",slug:"integer转字符串",normalizedTitle:"integer转字符串",charIndex:582},{level:2,title:"SimpleDateFormat",slug:"simpledateformat",normalizedTitle:"simpledateformat",charIndex:626},{level:3,title:"format",slug:"format",normalizedTitle:"format",charIndex:647},{level:3,title:"parse",slug:"parse",normalizedTitle:"parse",charIndex:533},{level:3,title:"DateTimeFormatter",slug:"datetimeformatter",normalizedTitle:"datetimeformatter",charIndex:1094},{level:2,title:"LocalDateTime",slug:"localdatetime",normalizedTitle:"localdatetime",charIndex:1232},{level:3,title:"now()",slug:"now",normalizedTitle:"now()",charIndex:1519},{level:3,title:"of(int year, Month month, int dayOfMonth, int hour, int minute)",slug:"of-int-year-month-month-int-dayofmonth-int-hour-int-minute",normalizedTitle:"of(int year, month month, int dayofmonth, int hour, int minute)",charIndex:1537},{level:2,title:"Period 两个日期的间隔",slug:"period-两个日期的间隔",normalizedTitle:"period 两个日期的间隔",charIndex:1764},{level:2,title:"Duration 两个时间的间隔",slug:"duration-两个时间的间隔",normalizedTitle:"duration 两个时间的间隔",charIndex:2164}],headersStr:"Sytem 获取当前时间戳 复制数组 Objects toSting(对象) Objects.toSring(对象,默认字符串) isNull(对象) nonNull(对象) BigDecimal Integer 自动装箱 自动拆箱 parseInt字符串转Integer Integer转字符串 SimpleDateFormat format parse DateTimeFormatter LocalDateTime now() of(int year, Month month, int dayOfMonth, int hour, int minute) Period 两个日期的间隔 Duration 两个时间的间隔",content:'# Api\n\n\n# Sytem\n\n\n# 获取当前时间戳\n\nSystem.currentTimeMillis()\n\n\n1\n\n\n\n# 复制数组\n\nSystem.arraycopy(源数组,开始索引,目的数组,开始索引,复制元素个数);\n\n\n1\n\n\n\n# Objects\n\n\n# toSting(对象)\n\n将对象以字符串形式显示\n\n\n# Objects.toSring(对象,默认字符串)\n\n返回对象以字符串形式,如果传入的对象为空则返回第二个参数,默认的字符串\n\n\n# isNull(对象)\n\n判断对象是否为空 返回布尔值\n\n\n# nonNull(对象)\n\n判断对象是否不为空 返回布尔值\n\n\n# BigDecimal\n\n 1. 创建BigDecimal建议使用字符串创建,用数字会有偏差\n 2. 如果除不尽的数请用divide(除数,保留多少位,舍入模式);\n\n\n# Integer\n\n\n# 自动装箱\n\n当把一个基本数据类型 赋值 给一个对象时,java底层会自动帮我们调用valuof方法\n\n如:\n\nInteger in = 100;\n\n\n# 自动拆箱\n\n将一个包装类型对象赋值给一个对应的基本对象,java会自动帮我们拆箱,即\n\nint in2= in;\n\n\n# parseInt字符串转Integer\n\nInteger.parseInt("123")\n\n\n# Integer转字符串\n\n使用字符串拼接\n\n或String.valueof()\n\n\n# SimpleDateFormat\n\n\n# format\n\n Date d1 =new Date();\n        SimpleDateFormat sdf =new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");\n\n        String d2= sdf.format(d1);   //将date对象格式化为SimpleDateFormat指定格式\n        System.out.println(d2);\n\n\n1\n2\n3\n4\n5\n\n\n\n# parse\n\ntry {\n            Date d3 =sdf.parse(d2);  // 要使用异常处理包裹  parse将格式化的对象转回date\n            System.out.println(d3);\n        } catch (ParseException e) {\n            e.printStackTrace();\n        }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# DateTimeFormatter\n\nString s ="2021年7月6日 21:45:13";\n\nDateTimeFormatter pattern = DateTimeFormatter.ofPattern("yyyy年M月d日 HH:mm:ss");  //格式化\nLocalDateTime loca = LocalDateTime.parse(s,pattern);  // 将指定的格式化和指定时间 创建一个localdatetime\nLocalDateTime l =loca.plusDays(1);  // 使用locatime 的 plusdays 方法 添加1天\nString result =l.format(pattern);  //再将locatime 格式化为指定格式 返回字符串\nSystem.out.println(result);\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# LocalDateTime\n\n\n# now()\n\n获取当前时间\n\n\n# of(int year, Month month, int dayOfMonth, int hour, int minute)\n\n指定年月日 时分秒\n\n# get\n\n详情看api 获取年 月 日 时 分 秒\n\n# with\n\n直接设置指定 返回一个副本\n\n年 月 日 时 分 秒\n\n# plus\n\n添加指定的时间 年 月 日 时 分 秒 返回一个副本 可以为负数\n\n# min\n\n减少指定的时间 年 月 日 时 分 秒 返回一个副本 可以为负数\n\n\n# Period 两个日期的间隔\n\n LocalDate start = LocalDate.of(2021,7,7);\n        LocalDate end = LocalDate.of(2021,8,27);\n        Period bw= Period.between(start,end);  // 要2个locadate类 即年月日 开始和结束\n        System.out.println(bw); // 总相隔时间\n        System.out.println(bw.getDays());  // 相隔天数\n        System.out.println(bw.getYears());  // 相隔年份\n        System.out.println(bw.getMonths());  // 相隔月份\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Duration 两个时间的间隔\n\nLocalDateTime start = LocalDateTime.of(2021, 7, 7, 11, 22, 55);\n        LocalDateTime end = LocalDateTime.of(2021, 8, 27, 16, 12, 5);\n\n        Duration bw = Duration.between(start, end);\n        System.out.println(bw);  // 相隔多少h多少m多少s\n\n        System.out.println(bw.toSeconds());  // 间隔秒\n        System.out.println(bw.toMillis());  // 间隔毫秒\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',normalizedContent:'# api\n\n\n# sytem\n\n\n# 获取当前时间戳\n\nsystem.currenttimemillis()\n\n\n1\n\n\n\n# 复制数组\n\nsystem.arraycopy(源数组,开始索引,目的数组,开始索引,复制元素个数);\n\n\n1\n\n\n\n# objects\n\n\n# tosting(对象)\n\n将对象以字符串形式显示\n\n\n# objects.tosring(对象,默认字符串)\n\n返回对象以字符串形式,如果传入的对象为空则返回第二个参数,默认的字符串\n\n\n# isnull(对象)\n\n判断对象是否为空 返回布尔值\n\n\n# nonnull(对象)\n\n判断对象是否不为空 返回布尔值\n\n\n# bigdecimal\n\n 1. 创建bigdecimal建议使用字符串创建,用数字会有偏差\n 2. 如果除不尽的数请用divide(除数,保留多少位,舍入模式);\n\n\n# integer\n\n\n# 自动装箱\n\n当把一个基本数据类型 赋值 给一个对象时,java底层会自动帮我们调用valuof方法\n\n如:\n\ninteger in = 100;\n\n\n# 自动拆箱\n\n将一个包装类型对象赋值给一个对应的基本对象,java会自动帮我们拆箱,即\n\nint in2= in;\n\n\n# parseint字符串转integer\n\ninteger.parseint("123")\n\n\n# integer转字符串\n\n使用字符串拼接\n\n或string.valueof()\n\n\n# simpledateformat\n\n\n# format\n\n date d1 =new date();\n        simpledateformat sdf =new simpledateformat("yyyy-mm-dd hh:mm:ss");\n\n        string d2= sdf.format(d1);   //将date对象格式化为simpledateformat指定格式\n        system.out.println(d2);\n\n\n1\n2\n3\n4\n5\n\n\n\n# parse\n\ntry {\n            date d3 =sdf.parse(d2);  // 要使用异常处理包裹  parse将格式化的对象转回date\n            system.out.println(d3);\n        } catch (parseexception e) {\n            e.printstacktrace();\n        }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# datetimeformatter\n\nstring s ="2021年7月6日 21:45:13";\n\ndatetimeformatter pattern = datetimeformatter.ofpattern("yyyy年m月d日 hh:mm:ss");  //格式化\nlocaldatetime loca = localdatetime.parse(s,pattern);  // 将指定的格式化和指定时间 创建一个localdatetime\nlocaldatetime l =loca.plusdays(1);  // 使用locatime 的 plusdays 方法 添加1天\nstring result =l.format(pattern);  //再将locatime 格式化为指定格式 返回字符串\nsystem.out.println(result);\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# localdatetime\n\n\n# now()\n\n获取当前时间\n\n\n# of(int year, month month, int dayofmonth, int hour, int minute)\n\n指定年月日 时分秒\n\n# get\n\n详情看api 获取年 月 日 时 分 秒\n\n# with\n\n直接设置指定 返回一个副本\n\n年 月 日 时 分 秒\n\n# plus\n\n添加指定的时间 年 月 日 时 分 秒 返回一个副本 可以为负数\n\n# min\n\n减少指定的时间 年 月 日 时 分 秒 返回一个副本 可以为负数\n\n\n# period 两个日期的间隔\n\n localdate start = localdate.of(2021,7,7);\n        localdate end = localdate.of(2021,8,27);\n        period bw= period.between(start,end);  // 要2个locadate类 即年月日 开始和结束\n        system.out.println(bw); // 总相隔时间\n        system.out.println(bw.getdays());  // 相隔天数\n        system.out.println(bw.getyears());  // 相隔年份\n        system.out.println(bw.getmonths());  // 相隔月份\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# duration 两个时间的间隔\n\nlocaldatetime start = localdatetime.of(2021, 7, 7, 11, 22, 55);\n        localdatetime end = localdatetime.of(2021, 8, 27, 16, 12, 5);\n\n        duration bw = duration.between(start, end);\n        system.out.println(bw);  // 相隔多少h多少m多少s\n\n        system.out.println(bw.toseconds());  // 间隔秒\n        system.out.println(bw.tomillis());  // 间隔毫秒\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"异常",frontmatter:{title:"异常",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/236d5b/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/44.%E5%BC%82%E5%B8%B8.html",relativePath:"后端/01.JavaSE/44.异常.md",key:"v-4fd19c04",path:"/pages/236d5b/",headers:[{level:2,title:"throws声明异常",slug:"throws声明异常",normalizedTitle:"throws声明异常",charIndex:9},{level:2,title:"throw抛出异常",slug:"throw抛出异常",normalizedTitle:"throw抛出异常",charIndex:231},{level:2,title:"try catch",slug:"try-catch",normalizedTitle:"try catch",charIndex:369},{level:3,title:"多个catch",slug:"多个catch",normalizedTitle:"多个catch",charIndex:666},{level:3,title:"捕抓所有异常 Exception",slug:"捕抓所有异常-exception",normalizedTitle:"捕抓所有异常 exception",charIndex:1087},{level:3,title:"error中的内部方法",slug:"error中的内部方法",normalizedTitle:"error中的内部方法",charIndex:1309},{level:2,title:"自定义异常",slug:"自定义异常",normalizedTitle:"自定义异常",charIndex:1841}],headersStr:"throws声明异常 throw抛出异常 try catch 多个catch 捕抓所有异常 Exception error中的内部方法 自定义异常",content:'# 异常\n\n\n# throws声明异常\n\n在方法定义处 声明 要捕获的异常\n\n使用throws关键字 加 异常名\n\npublic static void method() throws  NullPointerException{\n        int[] arr =null;\n        System.out.println(arr[2]);\n    }\n\n\n1\n2\n3\n4\n\n\n运行时异常可以省略,但编译时异常不能省略声明,否则不能正常执行\n\n\n# throw抛出异常\n\n使用 thorw new 关键字 加 异常 可以在代码块中直接抛出一个异常\n\nSystem.out.println("接下来要new一个异常");\n        throw new NullPointerException();\n\n\n1\n2\n\n\n\n# try catch\n\ntry {\n            pringArr(arr);  // 可能会发生异常的代码块\n            System.out.println("发生异常后异常语句try后面的代码不会继续执行");\n        } catch (NullPointerException a){\n            System.out.println("捕获到异常");  // 如果捕获到异常执行的代码块\n        }\n        System.out.println("捕获到异常后并不会结束虚拟机");\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 多个catch\n\ntry {\n            System.out.println(Integer.parseInt("123"));\n            System.out.println(2 / 0);\n            //catch 可以有多个 ,用来捕获多个异常\n        } catch (NumberFormatException err) {\n            System.out.println("格式化异常");\n        } catch (ArithmeticException err) {\n            System.out.println("数字运算异常");\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n如果多个异常中存在子父类,那么父类异常应该写在最后,因为如果写在最前面会捕抓到所有的子类异常,后面的子类异常语句并没有任何效果.\n\n\n# 捕抓所有异常 Exception\n\ntry {\n    System.out.println(Integer.parseInt("123"));\n    System.out.println(2 / 0);\n} catch (Exception err) {\n    System.out.println("捕抓所有异常");\n    //不推荐使用,因为我们捕获不同异常可能会有不同的处理方式\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# error中的内部方法\n\n * getMessage() 返回异常的信息\n * toString() 返回异常的类型 和信息\n * printStackTrace() 将异常信息以红色字体呈现在控制台中,与默认异常报错不同,这些方法并不会结束虚拟机\n\ntry {\n    int[] arr ={1,2,3,4};\n    System.out.println(arr[10]);\n} catch (ArrayIndexOutOfBoundsException e) {\n    String message = e.getMessage();  // 返回异常的信息\n    System.out.println(message);\n\n    String s = e.toString();  //返回异常的类型 和信息\n    System.out.println(s);\n\n    e.printStackTrace();   // 将异常信息以红色字体呈现在控制台中,与默认异常报错不同,这些方法并不会结束虚拟机\n    System.out.println("123");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 自定义异常\n\n自定义异常 只需要给予一个 无参构造方法 和 带参构造方法 并且继承运行时异常或编译时异常即可\n\npublic class AgeOutofBoundsException extends RuntimeException {\n        public AgeOutofBoundsException() {\n\n        }\n\n        public AgeOutofBoundsException(String message) {\n            super(message);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',normalizedContent:'# 异常\n\n\n# throws声明异常\n\n在方法定义处 声明 要捕获的异常\n\n使用throws关键字 加 异常名\n\npublic static void method() throws  nullpointerexception{\n        int[] arr =null;\n        system.out.println(arr[2]);\n    }\n\n\n1\n2\n3\n4\n\n\n运行时异常可以省略,但编译时异常不能省略声明,否则不能正常执行\n\n\n# throw抛出异常\n\n使用 thorw new 关键字 加 异常 可以在代码块中直接抛出一个异常\n\nsystem.out.println("接下来要new一个异常");\n        throw new nullpointerexception();\n\n\n1\n2\n\n\n\n# try catch\n\ntry {\n            pringarr(arr);  // 可能会发生异常的代码块\n            system.out.println("发生异常后异常语句try后面的代码不会继续执行");\n        } catch (nullpointerexception a){\n            system.out.println("捕获到异常");  // 如果捕获到异常执行的代码块\n        }\n        system.out.println("捕获到异常后并不会结束虚拟机");\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 多个catch\n\ntry {\n            system.out.println(integer.parseint("123"));\n            system.out.println(2 / 0);\n            //catch 可以有多个 ,用来捕获多个异常\n        } catch (numberformatexception err) {\n            system.out.println("格式化异常");\n        } catch (arithmeticexception err) {\n            system.out.println("数字运算异常");\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n如果多个异常中存在子父类,那么父类异常应该写在最后,因为如果写在最前面会捕抓到所有的子类异常,后面的子类异常语句并没有任何效果.\n\n\n# 捕抓所有异常 exception\n\ntry {\n    system.out.println(integer.parseint("123"));\n    system.out.println(2 / 0);\n} catch (exception err) {\n    system.out.println("捕抓所有异常");\n    //不推荐使用,因为我们捕获不同异常可能会有不同的处理方式\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# error中的内部方法\n\n * getmessage() 返回异常的信息\n * tostring() 返回异常的类型 和信息\n * printstacktrace() 将异常信息以红色字体呈现在控制台中,与默认异常报错不同,这些方法并不会结束虚拟机\n\ntry {\n    int[] arr ={1,2,3,4};\n    system.out.println(arr[10]);\n} catch (arrayindexoutofboundsexception e) {\n    string message = e.getmessage();  // 返回异常的信息\n    system.out.println(message);\n\n    string s = e.tostring();  //返回异常的类型 和信息\n    system.out.println(s);\n\n    e.printstacktrace();   // 将异常信息以红色字体呈现在控制台中,与默认异常报错不同,这些方法并不会结束虚拟机\n    system.out.println("123");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 自定义异常\n\n自定义异常 只需要给予一个 无参构造方法 和 带参构造方法 并且继承运行时异常或编译时异常即可\n\npublic class ageoutofboundsexception extends runtimeexception {\n        public ageoutofboundsexception() {\n\n        }\n\n        public ageoutofboundsexception(string message) {\n            super(message);\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"集合",frontmatter:{title:"集合",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/f58543/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/45.%E9%9B%86%E5%90%88.html",relativePath:"后端/01.JavaSE/45.集合.md",key:"v-61452fab",path:"/pages/f58543/",headers:[{level:2,title:"collectio",slug:"collectio",normalizedTitle:"collectio",charIndex:9},{level:3,title:"removeIf",slug:"removeif",normalizedTitle:"removeif",charIndex:23},{level:2,title:"迭代器",slug:"迭代器",normalizedTitle:"迭代器",charIndex:375},{level:3,title:"iterator()",slug:"iterator",normalizedTitle:"iterator()",charIndex:383},{level:3,title:"hasNext()",slug:"hasnext",normalizedTitle:"hasnext()",charIndex:456},{level:3,title:"next()",slug:"next",normalizedTitle:"next()",charIndex:507},{level:2,title:"增强for循环",slug:"增强for循环",normalizedTitle:"增强for循环",charIndex:626},{level:2,title:"List",slug:"list",normalizedTitle:"list",charIndex:139},{level:2,title:"单向链表",slug:"单向链表",normalizedTitle:"单向链表",charIndex:855},{level:2,title:"双向链表",slug:"双向链表",normalizedTitle:"双向链表",charIndex:1051},{level:2,title:"ArrayList 源码",slug:"arraylist-源码",normalizedTitle:"arraylist 源码",charIndex:1167},{level:3,title:"遍历ArrayList",slug:"遍历arraylist",normalizedTitle:"遍历arraylist",charIndex:1380}],headersStr:"collectio removeIf 迭代器 iterator() hasNext() next() 增强for循环 List 单向链表 双向链表 ArrayList 源码 遍历ArrayList",content:'# 集合\n\n\n# collectio\n\n\n# removeIf\n\nremoveIf()方法底层会遍历集合,得到集合中的每一个元素\n\n如果返回一个true则删除该传入的元素删除\n\n可以使用lambda函数\n\nCollection<String> array = new ArrayList<>();\n        array.add("aaa");\n        array.add("bbb");\n        array.add("cccc");\n        array.removeIf(\n                (String s) ->\n                        s.length() == 3\n                \n        );\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 迭代器\n\n\n# iterator()\n\n返回集合中的迭代器对象\n\nIterator<String> it = list.iterator();\n\n\n1\n\n\n\n# hasNext()\n\n判断当前位置是否有元素可被取出\n\n有返回true 没有则返回false\n\n\n# next()\n\n将迭代器对象指向下一个元素,并返回当前元素\n\nwhile (it.hasNext()){\n            System.out.println(it.next());\n        }\n\n\n1\n2\n3\n\n\n\n# 增强for循环\n\n所有的(Collection)单列集合都可以使用迭代器或增强for,双列集合无法使用\n\nfor(String s : list){\n    System.out.println(s);\n}\n\n\n1\n2\n3\n\n\n\n# List\n\n查询数据,数组通过地址值和索引定位元素,查询任意数据耗时相同,查询速度快\n\n删除数据,要将被删数据删除,同时后面个每个数据前移,删除效率低\n\n添加数据,添加位置后的每个数据后移,再添加元素,添加效率极低\n\n\n# 单向链表\n\n链表通过每个结点(元素)指向下一个结点的地址值来形成链表,如只有1个结点则该结点的指向地址为空(结束结点)\n\n查询数据,(无论查询是位置还是元素)只能从头结点一直查询到被查结点,查询效率慢\n\n删除数据,只需要修改被删除结点前一个结点的指向地址为被删结点的后一个结点即可.删除效率快\n\n添加数据,添加结点与删除结点类型,只需在添加位置更改前后结点的指向地址即可.添加效率快\n\n\n# 双向链表\n\n单向链表每个结点只存储 值和下一个结点的地址.\n\n而双向保留3个数据,分别为,前一个结点的地址 值 下一个结点的地址.这样我们可以从后找到前或从前找到后.查询位置的时候会判断离头结点近还是尾结点近来进行链表查询\n\n\n# ArrayList 源码\n\n默认空参构造方法 是创建一个长度为0的数组\n\n当调用add方法会把长度为0的ArrayList初始化为长度为10,并且都为null, 源码方法名为elementData\n\n并且会有一个变量size 标记当前数组元素长度\n\n如果size超出了默认长度10或者当前数组长度,则会自动扩容,扩容为当前数组长度1.5倍的大小,扩容后的数组为空,把原来的数组拷贝到新的数组中,并且size保持不变.\n\n\n# 遍历ArrayList\n\nArrayList中提供了size方法,直接返回当前size(即为最后一个元素的下标),而不是返回ArrayList的长度',normalizedContent:'# 集合\n\n\n# collectio\n\n\n# removeif\n\nremoveif()方法底层会遍历集合,得到集合中的每一个元素\n\n如果返回一个true则删除该传入的元素删除\n\n可以使用lambda函数\n\ncollection<string> array = new arraylist<>();\n        array.add("aaa");\n        array.add("bbb");\n        array.add("cccc");\n        array.removeif(\n                (string s) ->\n                        s.length() == 3\n                \n        );\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 迭代器\n\n\n# iterator()\n\n返回集合中的迭代器对象\n\niterator<string> it = list.iterator();\n\n\n1\n\n\n\n# hasnext()\n\n判断当前位置是否有元素可被取出\n\n有返回true 没有则返回false\n\n\n# next()\n\n将迭代器对象指向下一个元素,并返回当前元素\n\nwhile (it.hasnext()){\n            system.out.println(it.next());\n        }\n\n\n1\n2\n3\n\n\n\n# 增强for循环\n\n所有的(collection)单列集合都可以使用迭代器或增强for,双列集合无法使用\n\nfor(string s : list){\n    system.out.println(s);\n}\n\n\n1\n2\n3\n\n\n\n# list\n\n查询数据,数组通过地址值和索引定位元素,查询任意数据耗时相同,查询速度快\n\n删除数据,要将被删数据删除,同时后面个每个数据前移,删除效率低\n\n添加数据,添加位置后的每个数据后移,再添加元素,添加效率极低\n\n\n# 单向链表\n\n链表通过每个结点(元素)指向下一个结点的地址值来形成链表,如只有1个结点则该结点的指向地址为空(结束结点)\n\n查询数据,(无论查询是位置还是元素)只能从头结点一直查询到被查结点,查询效率慢\n\n删除数据,只需要修改被删除结点前一个结点的指向地址为被删结点的后一个结点即可.删除效率快\n\n添加数据,添加结点与删除结点类型,只需在添加位置更改前后结点的指向地址即可.添加效率快\n\n\n# 双向链表\n\n单向链表每个结点只存储 值和下一个结点的地址.\n\n而双向保留3个数据,分别为,前一个结点的地址 值 下一个结点的地址.这样我们可以从后找到前或从前找到后.查询位置的时候会判断离头结点近还是尾结点近来进行链表查询\n\n\n# arraylist 源码\n\n默认空参构造方法 是创建一个长度为0的数组\n\n当调用add方法会把长度为0的arraylist初始化为长度为10,并且都为null, 源码方法名为elementdata\n\n并且会有一个变量size 标记当前数组元素长度\n\n如果size超出了默认长度10或者当前数组长度,则会自动扩容,扩容为当前数组长度1.5倍的大小,扩容后的数组为空,把原来的数组拷贝到新的数组中,并且size保持不变.\n\n\n# 遍历arraylist\n\narraylist中提供了size方法,直接返回当前size(即为最后一个元素的下标),而不是返回arraylist的长度',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"类名作为形参和返回值",frontmatter:{title:"类名作为形参和返回值",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/384827/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/41.%E7%B1%BB%E5%90%8D%E4%BD%9C%E4%B8%BA%E5%BD%A2%E5%8F%82%E5%92%8C%E8%BF%94%E5%9B%9E%E5%80%BC.html",relativePath:"后端/01.JavaSE/41.类名作为形参和返回值.md",key:"v-44cc2c9e",path:"/pages/384827/",headers:[{level:2,title:"抽象类作为形参和返回值",slug:"抽象类作为形参和返回值",normalizedTitle:"抽象类作为形参和返回值",charIndex:64},{level:2,title:"接口作为形参和返回值",slug:"接口作为形参和返回值",normalizedTitle:"接口作为形参和返回值",charIndex:108}],headersStr:"抽象类作为形参和返回值 接口作为形参和返回值",content:"# 类名作为形参和返回值\n\n形参为类名时 则需要传递一个实例化的对象\n\n返回值为类名时 则return一个实例化的对象\n\n\n# 抽象类作为形参和返回值\n\n抽象类需要用多态形式创建对象 才能传参\n\n返回值一致\n\n\n# 接口作为形参和返回值\n\n需要重写接口的方法 并以多态形式 传参\n\n返回值一致",normalizedContent:"# 类名作为形参和返回值\n\n形参为类名时 则需要传递一个实例化的对象\n\n返回值为类名时 则return一个实例化的对象\n\n\n# 抽象类作为形参和返回值\n\n抽象类需要用多态形式创建对象 才能传参\n\n返回值一致\n\n\n# 接口作为形参和返回值\n\n需要重写接口的方法 并以多态形式 传参\n\n返回值一致",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Set集合",frontmatter:{title:"Set集合",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/5137f9/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/47.Set%E9%9B%86%E5%90%88.html",relativePath:"后端/01.JavaSE/47.Set集合.md",key:"v-83d6ea7a",path:"/pages/5137f9/",headers:[{level:2,title:"treeset集合",slug:"treeset集合",normalizedTitle:"treeset集合",charIndex:49},{level:2,title:"自然排序Comparable的使用",slug:"自然排序comparable的使用",normalizedTitle:"自然排序comparable的使用",charIndex:133},{level:2,title:"比较器排序 Comparator使用",slug:"比较器排序-comparator使用",normalizedTitle:"比较器排序 comparator使用",charIndex:550}],headersStr:"treeset集合 自然排序Comparable的使用 比较器排序 Comparator使用",content:"# Set集合\n\nSet的特点:\n\n 1. 去除重复\n 2. 存储无序\n 3. 没有索引\n\n\n# treeset集合\n\n可以将元素按照规则进行排序,并且拥有set的特征\n\n想要使用自定义类型并保存到treeset必须指定排序规则,否则无法使用treeset\n\n\n# 自然排序Comparable的使用\n\n 1. 如果返回值为负数,表示当前存入元素是较小值,存左边\n 2. 如果返回值为0,表示与当前元素重复了,不存储\n 3. 如果返回值为正数,表示当前是较大值,存右边\n\nTreeset的无参构造方法使用的是自然排序对元素进行排序\n\n自定义类必须继承Comparable接口 泛型为自定义类,重写接口中的compareTo方法 返回一个int类型\n\npublic class Student  implements Comparable<Student>{\n  private int age;\n\n@Override\n    public int compareTo(Student o) {\n        // this 为当前存储的元素  o为已存储集合中的元素\n        return this.age- o.age;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 比较器排序 Comparator使用\n\nTreeset的带参构造方法使用的比较器排序对元素进行排序\n\nTreeSet<Student> ts = new TreeSet<>(new Comparator<Student>() {\n    @Override\n    public int compare(Student o1, Student o2) {\n        // o1为当前将要存入的元素  o2为集合中的元素\n        return o1.getAge() - o2.getAge();\n    }\n});\n\n\n//可以写成lambda表达式\nTreeSet<Student> ts2 = new TreeSet<>((o1, o2) -> {\n            return o1.getAge() - o2.getAge();\n        });\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n",normalizedContent:"# set集合\n\nset的特点:\n\n 1. 去除重复\n 2. 存储无序\n 3. 没有索引\n\n\n# treeset集合\n\n可以将元素按照规则进行排序,并且拥有set的特征\n\n想要使用自定义类型并保存到treeset必须指定排序规则,否则无法使用treeset\n\n\n# 自然排序comparable的使用\n\n 1. 如果返回值为负数,表示当前存入元素是较小值,存左边\n 2. 如果返回值为0,表示与当前元素重复了,不存储\n 3. 如果返回值为正数,表示当前是较大值,存右边\n\ntreeset的无参构造方法使用的是自然排序对元素进行排序\n\n自定义类必须继承comparable接口 泛型为自定义类,重写接口中的compareto方法 返回一个int类型\n\npublic class student  implements comparable<student>{\n  private int age;\n\n@override\n    public int compareto(student o) {\n        // this 为当前存储的元素  o为已存储集合中的元素\n        return this.age- o.age;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 比较器排序 comparator使用\n\ntreeset的带参构造方法使用的比较器排序对元素进行排序\n\ntreeset<student> ts = new treeset<>(new comparator<student>() {\n    @override\n    public int compare(student o1, student o2) {\n        // o1为当前将要存入的元素  o2为集合中的元素\n        return o1.getage() - o2.getage();\n    }\n});\n\n\n//可以写成lambda表达式\ntreeset<student> ts2 = new treeset<>((o1, o2) -> {\n            return o1.getage() - o2.getage();\n        });\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"树",frontmatter:{title:"树",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/6e8f69/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/48.%E6%A0%91.html",relativePath:"后端/01.JavaSE/48.树.md",key:"v-1bb60fa3",path:"/pages/6e8f69/",headers:[{level:2,title:"二叉树",slug:"二叉树",normalizedTitle:"二叉树",charIndex:8},{level:2,title:"二叉查找树",slug:"二叉查找树",normalizedTitle:"二叉查找树",charIndex:107},{level:3,title:"添加节点",slug:"添加节点",normalizedTitle:"添加节点",charIndex:216},{level:2,title:"平衡二叉树",slug:"平衡二叉树",normalizedTitle:"平衡二叉树",charIndex:291},{level:3,title:"左旋",slug:"左旋",normalizedTitle:"左旋",charIndex:347},{level:3,title:"右旋",slug:"右旋",normalizedTitle:"右旋",charIndex:599},{level:2,title:"红黑树",slug:"红黑树",normalizedTitle:"红黑树",charIndex:946},{level:3,title:"红黑规则",slug:"红黑规则",normalizedTitle:"红黑规则",charIndex:1031},{level:3,title:"添加节点",slug:"添加节点-2",normalizedTitle:"添加节点",charIndex:216},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1590}],headersStr:"二叉树 二叉查找树 添加节点 平衡二叉树 左旋 右旋 红黑树 红黑规则 添加节点 总结",content:'# 树\n\n\n# 二叉树\n\n每个个节点存放4个属性分别为 父节点地址 值 左子节点地址 右子节点地址\n\n度: 每个节点的子节点数量 左右子节点的总数\n\n在二叉树中,任意一个节点的度要小于等于2\n\n查找效率慢\n\n\n# 二叉查找树\n\n二叉查找树,又称二叉排序树或二叉搜索树\n\n特点:\n\n 1. 每个节点最多有两个子节点\n 2. 每个节点的左子结点都是小于自身的\n 3. 每个节点的右子结点都是大于自身的\n\n查找效率比普通二叉树快\n\n\n# 添加节点\n\n规则:\n\n 1. 小的存左边\n 2. 大的存右边\n 3. 一样的不存\n\n首先与根节点做比较,再逐层比较\n\n查询效率比二叉查找树快\n\n\n# 平衡二叉树\n\n * 二叉树左右两个子树的高度差不超过1\n * 任意节点的左右两个字树都是一颗平衡二叉树\n\n\n# 左旋\n\n当添加一个节点破坏了平衡二叉树规则时,并且是添加在根节点右边,我们可以通过左旋来恢复平衡二叉树.\n\n只需要把根节点向左下方拉扯,并且连接的所有节点跟着移动即可.\n\n当添加12节点时破坏了平衡二叉树了\n\n\n\n我们通过左旋来恢复平衡,把根节点的右子节点往上拉\n\n\n\n# 如果被提级的节点已有左节点\n\n\n\n先忽略左子节点存在,再提级,然后放置在原先的根节点的右子节点\n\n\n\n左旋:就是将根节点的右侧往左拉,原先的右子节点变成新的父节点,并把多余的左子节点出让,给已经降级的根节点当右子节点\n\n\n\n\n# 右旋\n\n\n\n右旋:将根节点的左侧往右拉,左子节点变成了新的父节点,并把多余的右子节点出让,给已经降级根节点当左子节点\n\n# 左左\n\n当根节点左子树的左子树有节点插入,导致二叉树不平衡\n\n我们只需要将此二叉树右旋既可\n\n\n\n# 右右\n\n当根节点左子树的右子树有节点插入,导致二叉树不平衡\n\n我们发现单单靠一次的右旋是无法恢复平衡状态\n\n\n\n\n\n我们需要将5当为根节点 左旋一次\n\n\n\n再由根节点7 右旋一次\n\n\n\n# 右右\n\n当根节点右子树的右子树有节点插入,导致二叉树不平衡\n\n我们只需要将此二叉树左旋既可\n\n\n\n# 右左\n\n当根节点右子树的左子树有节点插入,导致二叉树不平衡\n\n我们发现单单靠一次的左旋是无法恢复平衡状态\n\n\n\n把10当为根节点,右旋一次\n\n\n\n再以根节点 左旋\n\n\n\n\n# 红黑树\n\n红黑树又称平衡二叉B树\n\n它是一种特色的二叉查找树,红黑树的每个节点上都有存储位表示节点的颜色\n\n每一个节点可以是红或黑;红黑树不是高度平衡的,它的平衡是通过"红黑规则"进行实现的\n\n而平衡二叉树是高度平衡的,当左右子树高度差超过1时则触发旋转\n\n而红黑树是根据定于的红黑规则触发旋转的\n\n\n# 红黑规则\n\n 1. 每一个节点或是红色,或者是黑色\n 2. 根节点必须是黑色\n 3. 如果一个节点没有子节点或者父节点,则该节点相应的指针属性值为Nil,这些Nil视为叶节点,每个叶节点(Nil)是黑色的\n 4. 如果某个节点是红色的,那么它的子节点必须是黑色(不能出现两个红色节点相连的情况)\n 5. 对每个节点,从该节点到其所有后代节点的简单路径上,均包含相同数目的黑色节点\n\n\n# 添加节点\n\n添加节点时,默认为红色,效率比为黑色要高\n\n并遵循二叉查找树的规则\n\n# 当父节点也是红色,叔叔节点也是红色\n\n因为默认是添加红色,如果父节点为红色,叔叔节点也为红色\n\n 1. 父节点改为黑色\n 2. 叔叔(祖父节点的左/右子节点)节点改为黑色\n 3. 将祖父节点改为红色\n 4. 如果祖父节点为根节点则重新变回黑色\n\n# 当父节点也是红色,叔叔节点是黑色\n\n 1. 将父节点改为黑色\n 2. 将祖父节点改为红色\n 3. 以祖父节点为支点进行旋转\n    * 左节点大于则右旋\n    * 右节点大于则左旋\n    * 可以把Nil先忽略,旋转完后再加回叶节点,比较好理解\n\n\n# 总结\n\n',normalizedContent:'# 树\n\n\n# 二叉树\n\n每个个节点存放4个属性分别为 父节点地址 值 左子节点地址 右子节点地址\n\n度: 每个节点的子节点数量 左右子节点的总数\n\n在二叉树中,任意一个节点的度要小于等于2\n\n查找效率慢\n\n\n# 二叉查找树\n\n二叉查找树,又称二叉排序树或二叉搜索树\n\n特点:\n\n 1. 每个节点最多有两个子节点\n 2. 每个节点的左子结点都是小于自身的\n 3. 每个节点的右子结点都是大于自身的\n\n查找效率比普通二叉树快\n\n\n# 添加节点\n\n规则:\n\n 1. 小的存左边\n 2. 大的存右边\n 3. 一样的不存\n\n首先与根节点做比较,再逐层比较\n\n查询效率比二叉查找树快\n\n\n# 平衡二叉树\n\n * 二叉树左右两个子树的高度差不超过1\n * 任意节点的左右两个字树都是一颗平衡二叉树\n\n\n# 左旋\n\n当添加一个节点破坏了平衡二叉树规则时,并且是添加在根节点右边,我们可以通过左旋来恢复平衡二叉树.\n\n只需要把根节点向左下方拉扯,并且连接的所有节点跟着移动即可.\n\n当添加12节点时破坏了平衡二叉树了\n\n\n\n我们通过左旋来恢复平衡,把根节点的右子节点往上拉\n\n\n\n# 如果被提级的节点已有左节点\n\n\n\n先忽略左子节点存在,再提级,然后放置在原先的根节点的右子节点\n\n\n\n左旋:就是将根节点的右侧往左拉,原先的右子节点变成新的父节点,并把多余的左子节点出让,给已经降级的根节点当右子节点\n\n\n\n\n# 右旋\n\n\n\n右旋:将根节点的左侧往右拉,左子节点变成了新的父节点,并把多余的右子节点出让,给已经降级根节点当左子节点\n\n# 左左\n\n当根节点左子树的左子树有节点插入,导致二叉树不平衡\n\n我们只需要将此二叉树右旋既可\n\n\n\n# 右右\n\n当根节点左子树的右子树有节点插入,导致二叉树不平衡\n\n我们发现单单靠一次的右旋是无法恢复平衡状态\n\n\n\n\n\n我们需要将5当为根节点 左旋一次\n\n\n\n再由根节点7 右旋一次\n\n\n\n# 右右\n\n当根节点右子树的右子树有节点插入,导致二叉树不平衡\n\n我们只需要将此二叉树左旋既可\n\n\n\n# 右左\n\n当根节点右子树的左子树有节点插入,导致二叉树不平衡\n\n我们发现单单靠一次的左旋是无法恢复平衡状态\n\n\n\n把10当为根节点,右旋一次\n\n\n\n再以根节点 左旋\n\n\n\n\n# 红黑树\n\n红黑树又称平衡二叉b树\n\n它是一种特色的二叉查找树,红黑树的每个节点上都有存储位表示节点的颜色\n\n每一个节点可以是红或黑;红黑树不是高度平衡的,它的平衡是通过"红黑规则"进行实现的\n\n而平衡二叉树是高度平衡的,当左右子树高度差超过1时则触发旋转\n\n而红黑树是根据定于的红黑规则触发旋转的\n\n\n# 红黑规则\n\n 1. 每一个节点或是红色,或者是黑色\n 2. 根节点必须是黑色\n 3. 如果一个节点没有子节点或者父节点,则该节点相应的指针属性值为nil,这些nil视为叶节点,每个叶节点(nil)是黑色的\n 4. 如果某个节点是红色的,那么它的子节点必须是黑色(不能出现两个红色节点相连的情况)\n 5. 对每个节点,从该节点到其所有后代节点的简单路径上,均包含相同数目的黑色节点\n\n\n# 添加节点\n\n添加节点时,默认为红色,效率比为黑色要高\n\n并遵循二叉查找树的规则\n\n# 当父节点也是红色,叔叔节点也是红色\n\n因为默认是添加红色,如果父节点为红色,叔叔节点也为红色\n\n 1. 父节点改为黑色\n 2. 叔叔(祖父节点的左/右子节点)节点改为黑色\n 3. 将祖父节点改为红色\n 4. 如果祖父节点为根节点则重新变回黑色\n\n# 当父节点也是红色,叔叔节点是黑色\n\n 1. 将父节点改为黑色\n 2. 将祖父节点改为红色\n 3. 以祖父节点为支点进行旋转\n    * 左节点大于则右旋\n    * 右节点大于则左旋\n    * 可以把nil先忽略,旋转完后再加回叶节点,比较好理解\n\n\n# 总结\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"TreeSet遍历",frontmatter:{title:"TreeSet遍历",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/8b6f0f/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/49.TreeSet%E9%81%8D%E5%8E%86.html",relativePath:"后端/01.JavaSE/49.TreeSet遍历.md",key:"v-3336a5d0",path:"/pages/8b6f0f/",headersStr:null,content:"# TreeSet遍历\n\n先获取左边,再获取中间,再获取右边",normalizedContent:"# treeset遍历\n\n先获取左边,再获取中间,再获取右边",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"哈希值",frontmatter:{title:"哈希值",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/1793cd/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/50.%E5%93%88%E5%B8%8C%E5%80%BC.html",relativePath:"后端/01.JavaSE/50.哈希值.md",key:"v-36833f0c",path:"/pages/1793cd/",headersStr:null,content:"# 哈希值\n\n哈希值(哈希码值):是JDK根据对象的地址或者属性值,算出来的int类型的整数\n\nobject类中有一个方法可以获取对象的哈希值:\n\nStudent s1 =new Student();\nint hash = s1.hashCode();\nSystem.out.println(hash);\n\n\n1\n2\n3\n\n\n我们可以通过重写hashCode方法,通过对象的属性值来生成hashcode\n\n@Override\npublic int hashCode() {\n    int result = name != null ? name.hashCode() : 0;\n    result = 31 * result + age;\n    // 如果对象的值全部一致则hashcode也一致\n    return result;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n",normalizedContent:"# 哈希值\n\n哈希值(哈希码值):是jdk根据对象的地址或者属性值,算出来的int类型的整数\n\nobject类中有一个方法可以获取对象的哈希值:\n\nstudent s1 =new student();\nint hash = s1.hashcode();\nsystem.out.println(hash);\n\n\n1\n2\n3\n\n\n我们可以通过重写hashcode方法,通过对象的属性值来生成hashcode\n\n@override\npublic int hashcode() {\n    int result = name != null ? name.hashcode() : 0;\n    result = 31 * result + age;\n    // 如果对象的值全部一致则hashcode也一致\n    return result;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"泛型",frontmatter:{title:"泛型",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/d6e1c4/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/46.%E6%B3%9B%E5%9E%8B.html",relativePath:"后端/01.JavaSE/46.泛型.md",key:"v-4a502c6c",path:"/pages/d6e1c4/",headers:[{level:2,title:"泛型格式",slug:"泛型格式",normalizedTitle:"泛型格式",charIndex:52},{level:2,title:"泛型可以使用在",slug:"泛型可以使用在",normalizedTitle:"泛型可以使用在",charIndex:160},{level:2,title:"使用泛型方法",slug:"使用泛型方法",normalizedTitle:"使用泛型方法",charIndex:1214},{level:2,title:"类型通配符",slug:"类型通配符",normalizedTitle:"类型通配符",charIndex:1563},{level:3,title:"类型通配符上限",slug:"类型通配符上限",normalizedTitle:"类型通配符上限",charIndex:1694},{level:3,title:"类型通配符下限",slug:"类型通配符下限",normalizedTitle:"类型通配符下限",charIndex:1840}],headersStr:"泛型格式 泛型可以使用在 使用泛型方法 类型通配符 类型通配符上限 类型通配符下限",content:'# 泛型\n\n泛型是JDK5中引入的特性,它把运行期间的问题提前到了编译期间,避免了强制类型转换\n\n\n# 泛型格式\n\n<类型> 按照变量规则书写即可 如: <E> <T>\n\n<类型1,类型2...> 多个泛型格式,多个泛型之间用逗号隔开 如:<E,T,Q>\n\n如果该泛型未接收到指定类型,则默认为object类型\n\n\n# 泛型可以使用在\n\n 1. 类后面 泛型类\n    \n    public static class f<e> {\n            int a;\n        }\n    public static void main(String[] args) {\n            f<Integer> fa = new f<>();  // 在类后面加上<> 指定泛型  new出来的类只能使用指定类型对象,否则会出现编译时问题(泛型把运行时问题提前到编译时)\n    \n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 2. 方法申明上 泛型方法\n    \n    方法定义一个泛型 在返回值前面加上泛型\n    \n    public <E> e getA(e E) {\n                return E;\n            }\n    \n    \n    1\n    2\n    3\n    \n\n 3. 接口后面 泛型接口\n    \n    在接口名后面定义泛型\n    \n    public interface in<e>{\n            e show(e E);\n        }\n    \n    \n    1\n    2\n    3\n    \n    \n    类实现\n    \n    // 实现类不给指定类型 此类也需要成为泛型类\n    public class fa1<e> implements in<e>{\n        @Override\n        public e show(e E) {\n            return null;\n        }\n    }\n    \n    // 实现类给予指定数据类型\n    public class fa2 implements in<Integer>{\n        @Override\n        public Integer show(Integer E) {\n            return null;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n\n# 使用泛型方法\n\npublic static void main(String[] args) {\n        ArrayList<String> list =new ArrayList<>();\n        list.add("对对对");\n        list.add("对对对2");\n        list.add("对对对3");\n        String[] strings = list.toArray(list.toArray(new String[0]));  // 需要传一个泛型指定的类型对象\n        System.out.println(Arrays.toString(strings));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 类型通配符\n\n<?> 可以匹配任何类型\n\nArrayList<?>:可以匹配任何类型,但是ArrayList不能添加元素,获取出来的也是父类类型\n\npublic static void method1(ArrayList<?> list){}\n\n\n1\n\n\n\n# 类型通配符上限\n\n<? extends 类型>\n\nArrayList<? extends Number>:表示它的类型是Number或其他子类\n\npublic static void method2(ArrayList<? extends  Number> list){}\n\n\n1\n\n\n\n# 类型通配符下限\n\n<? super 类型>\n\nArrayList<? super Number>:表示它的类型是Nuber或Number父类\n\npublic static void method3(ArrayList<? super Number> list){}\n\n\n1\n\n\n',normalizedContent:'# 泛型\n\n泛型是jdk5中引入的特性,它把运行期间的问题提前到了编译期间,避免了强制类型转换\n\n\n# 泛型格式\n\n<类型> 按照变量规则书写即可 如: <e> <t>\n\n<类型1,类型2...> 多个泛型格式,多个泛型之间用逗号隔开 如:<e,t,q>\n\n如果该泛型未接收到指定类型,则默认为object类型\n\n\n# 泛型可以使用在\n\n 1. 类后面 泛型类\n    \n    public static class f<e> {\n            int a;\n        }\n    public static void main(string[] args) {\n            f<integer> fa = new f<>();  // 在类后面加上<> 指定泛型  new出来的类只能使用指定类型对象,否则会出现编译时问题(泛型把运行时问题提前到编译时)\n    \n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 2. 方法申明上 泛型方法\n    \n    方法定义一个泛型 在返回值前面加上泛型\n    \n    public <e> e geta(e e) {\n                return e;\n            }\n    \n    \n    1\n    2\n    3\n    \n\n 3. 接口后面 泛型接口\n    \n    在接口名后面定义泛型\n    \n    public interface in<e>{\n            e show(e e);\n        }\n    \n    \n    1\n    2\n    3\n    \n    \n    类实现\n    \n    // 实现类不给指定类型 此类也需要成为泛型类\n    public class fa1<e> implements in<e>{\n        @override\n        public e show(e e) {\n            return null;\n        }\n    }\n    \n    // 实现类给予指定数据类型\n    public class fa2 implements in<integer>{\n        @override\n        public integer show(integer e) {\n            return null;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n\n# 使用泛型方法\n\npublic static void main(string[] args) {\n        arraylist<string> list =new arraylist<>();\n        list.add("对对对");\n        list.add("对对对2");\n        list.add("对对对3");\n        string[] strings = list.toarray(list.toarray(new string[0]));  // 需要传一个泛型指定的类型对象\n        system.out.println(arrays.tostring(strings));\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 类型通配符\n\n<?> 可以匹配任何类型\n\narraylist<?>:可以匹配任何类型,但是arraylist不能添加元素,获取出来的也是父类类型\n\npublic static void method1(arraylist<?> list){}\n\n\n1\n\n\n\n# 类型通配符上限\n\n<? extends 类型>\n\narraylist<? extends number>:表示它的类型是number或其他子类\n\npublic static void method2(arraylist<? extends  number> list){}\n\n\n1\n\n\n\n# 类型通配符下限\n\n<? super 类型>\n\narraylist<? super number>:表示它的类型是nuber或number父类\n\npublic static void method3(arraylist<? super number> list){}\n\n\n1\n\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"HashSet集合",frontmatter:{title:"HashSet集合",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/642d0a/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/52.HashSet%E9%9B%86%E5%90%88.html",relativePath:"后端/01.JavaSE/52.HashSet集合.md",key:"v-33880d43",path:"/pages/642d0a/",headers:[{level:2,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:77},{level:3,title:"1.7版本原理",slug:"_1-7版本原理",normalizedTitle:"1.7版本原理",charIndex:84},{level:3,title:"1.8版本 原理",slug:"_1-8版本-原理",normalizedTitle:"1.8版本 原理",charIndex:324}],headersStr:"原理 1.7版本原理 1.8版本 原理",content:"# HashSet集合\n\n特点:\n\n * 底层数据结构是哈希表\n * 不能保证存储和取出顺序完全一致\n * 没有索引,不能使用普通for循环遍历\n\n\n# 原理\n\n\n# 1.7版本原理\n\n底层为哈希表 用数组+链表实现\n\n 1. 创建一个默认长度16,默认加载因 0.75 的数组,数组名为table\n    * 加载因:当数组存了16*0.75=12个元素时,数组会扩容为原先的两倍\n 2. 根据元素的哈希值跟数组的长度计算出应存入的位置\n 3. 判断当前位置是否为null,如果是null直接存入\n 4. 如果不为null,则代表已有元素,则调用equals方法比较属性值\n 5. 如不一致,则存入数组,老元素挂在新元素下面(链表)\n\n\n\n\n# 1.8版本 原理\n\n底层结构为:哈希表 底层为数组、链表和红黑树的结合体\n\n1.8版本优化了,当存放位置一致时,链表过长 需要对链表每个元素进行比较 当链表长度过长效率不太理想\n\n所以在链表中添加红黑树来进行优化, 当链表长度为8时,(> 8)再添加会自动转成为红黑树\n\n\n\n流程图\n\n",normalizedContent:"# hashset集合\n\n特点:\n\n * 底层数据结构是哈希表\n * 不能保证存储和取出顺序完全一致\n * 没有索引,不能使用普通for循环遍历\n\n\n# 原理\n\n\n# 1.7版本原理\n\n底层为哈希表 用数组+链表实现\n\n 1. 创建一个默认长度16,默认加载因 0.75 的数组,数组名为table\n    * 加载因:当数组存了16*0.75=12个元素时,数组会扩容为原先的两倍\n 2. 根据元素的哈希值跟数组的长度计算出应存入的位置\n 3. 判断当前位置是否为null,如果是null直接存入\n 4. 如果不为null,则代表已有元素,则调用equals方法比较属性值\n 5. 如不一致,则存入数组,老元素挂在新元素下面(链表)\n\n\n\n\n# 1.8版本 原理\n\n底层结构为:哈希表 底层为数组、链表和红黑树的结合体\n\n1.8版本优化了,当存放位置一致时,链表过长 需要对链表每个元素进行比较 当链表长度过长效率不太理想\n\n所以在链表中添加红黑树来进行优化, 当链表长度为8时,(> 8)再添加会自动转成为红黑树\n\n\n\n流程图\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"哈希表",frontmatter:{title:"哈希表",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/7525b1/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/51.%E5%93%88%E5%B8%8C%E8%A1%A8.html",relativePath:"后端/01.JavaSE/51.哈希表.md",key:"v-3495bce5",path:"/pages/7525b1/",headersStr:null,content:"# 哈希表\n\nJDK8之前,底层采用 数组+链表 实现\n\nJDK8以后,底层进行优化. 由 数组+链表+红黑树 实现.",normalizedContent:"# 哈希表\n\njdk8之前,底层采用 数组+链表 实现\n\njdk8以后,底层进行优化. 由 数组+链表+红黑树 实现.",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"可变参数",frontmatter:{title:"可变参数",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/4fb557/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/54.%E5%8F%AF%E5%8F%98%E5%8F%82%E6%95%B0.html",relativePath:"后端/01.JavaSE/54.可变参数.md",key:"v-25108381",path:"/pages/4fb557/",headersStr:null,content:"# 可变参数\n\n可变参数:就是形参的个数是可以变换的\n\npublic static void main(String[] args) {\n    int sum = getSum(1,5,5,3135,156,1456,1);\n    System.out.println(sum);\n}\npublic static int getSum(int... arr){\n    int sum = 0;\n    for (int i : arr) {\n        sum +=i;\n    }\n    return sum;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n注意:\n\n * 底层其实就是一个数组存储\n\n * 可变参数必须写在最后",normalizedContent:"# 可变参数\n\n可变参数:就是形参的个数是可以变换的\n\npublic static void main(string[] args) {\n    int sum = getsum(1,5,5,3135,156,1456,1);\n    system.out.println(sum);\n}\npublic static int getsum(int... arr){\n    int sum = 0;\n    for (int i : arr) {\n        sum +=i;\n    }\n    return sum;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n注意:\n\n * 底层其实就是一个数组存储\n\n * 可变参数必须写在最后",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"创建不可变的集合",frontmatter:{title:"创建不可变的集合",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/7db438/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/55.%E5%88%9B%E5%BB%BA%E4%B8%8D%E5%8F%AF%E5%8F%98%E7%9A%84%E9%9B%86%E5%90%88.html",relativePath:"后端/01.JavaSE/55.创建不可变的集合.md",key:"v-241961c2",path:"/pages/7db438/",headersStr:null,content:'# 创建不可变的集合\n\nList Set Map 中使用of方法创建一个不可变化的集合\n\n该集合无法添加元素,更改,删除\n\n List<Integer> integers = List.of(1, 2, 3, 65, 7);\n        System.out.println(integers);\n\n        //可以通过of来批量添加元素\n        ArrayList<Integer> list = new ArrayList<>(List.of(1, 2, 3, 65, 7));\n        list.add(1);\n        System.out.println(list);\n\n        // 不能添加重复的元素否则of会报错\n        Set<String> set = Set.of("a","b");\n        System.out.println(set);\n\n        Map<String, Integer> key1 = Map.of("key1", 123, "key2", 456);\n        System.out.println(key1);\n\n        Map<String, Integer> stringIntegerMap = Map.ofEntries(Map.entry("key1", 444), Map.entry("key2", 4444));\n        System.out.println(stringIntegerMap);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',normalizedContent:'# 创建不可变的集合\n\nlist set map 中使用of方法创建一个不可变化的集合\n\n该集合无法添加元素,更改,删除\n\n list<integer> integers = list.of(1, 2, 3, 65, 7);\n        system.out.println(integers);\n\n        //可以通过of来批量添加元素\n        arraylist<integer> list = new arraylist<>(list.of(1, 2, 3, 65, 7));\n        list.add(1);\n        system.out.println(list);\n\n        // 不能添加重复的元素否则of会报错\n        set<string> set = set.of("a","b");\n        system.out.println(set);\n\n        map<string, integer> key1 = map.of("key1", 123, "key2", 456);\n        system.out.println(key1);\n\n        map<string, integer> stringintegermap = map.ofentries(map.entry("key1", 444), map.entry("key2", 4444));\n        system.out.println(stringintegermap);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Map 集合",frontmatter:{title:"Map 集合",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/b8041f/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/53.Map%20%E9%9B%86%E5%90%88.html",relativePath:"后端/01.JavaSE/53.Map 集合.md",key:"v-58c7430c",path:"/pages/b8041f/",headers:[{level:2,title:"遍历map",slug:"遍历map",normalizedTitle:"遍历map",charIndex:82},{level:3,title:"entrySet",slug:"entryset",normalizedTitle:"entryset",charIndex:244},{level:3,title:"forEach",slug:"foreach",normalizedTitle:"foreach",charIndex:481},{level:2,title:"HashSet 原理",slug:"hashset-原理",normalizedTitle:"hashset 原理",charIndex:591},{level:2,title:"TreeMap 原理",slug:"treemap-原理",normalizedTitle:"treemap 原理",charIndex:618}],headersStr:"遍历map entrySet forEach HashSet 原理 TreeMap 原理",content:'# Map 集合\n\nMap中 key value 为一对,必须存储为键值对\n\nMap中的put方法,如果此key已经有值,则会替换此key中值,并返回旧值\n\n\n# 遍历map\n\n# keySet\n\n通过keySet()获取所有key\n\nSet<String> strings = map.keySet();\nfor (String string : strings) {\n    System.out.println(map.get(string));\n}\n\n\n1\n2\n3\n4\n\n\n\n# entrySet\n\n通过entrySet获取所有键值对\n\nSet<Map.Entry<String, String>> entries = map.entrySet();\nfor (Map.Entry<String, String> entry : entries) {\n    System.out.println(entry.getValue());\n    System.out.println(entry.getKey());\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# forEach\n\nmap.forEach((String key, String value) -> {\n    System.out.println(key+"  "+value);\n});\n\n\n1\n2\n3\n\n\n\n# HashSet 原理\n\n与HashSet一致\n\n\n# TreeMap 原理\n\n与TreeSet 差不多\n\n节点存储的为键值对,并且只对键进排序,值不影响',normalizedContent:'# map 集合\n\nmap中 key value 为一对,必须存储为键值对\n\nmap中的put方法,如果此key已经有值,则会替换此key中值,并返回旧值\n\n\n# 遍历map\n\n# keyset\n\n通过keyset()获取所有key\n\nset<string> strings = map.keyset();\nfor (string string : strings) {\n    system.out.println(map.get(string));\n}\n\n\n1\n2\n3\n4\n\n\n\n# entryset\n\n通过entryset获取所有键值对\n\nset<map.entry<string, string>> entries = map.entryset();\nfor (map.entry<string, string> entry : entries) {\n    system.out.println(entry.getvalue());\n    system.out.println(entry.getkey());\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# foreach\n\nmap.foreach((string key, string value) -> {\n    system.out.println(key+"  "+value);\n});\n\n\n1\n2\n3\n\n\n\n# hashset 原理\n\n与hashset一致\n\n\n# treemap 原理\n\n与treeset 差不多\n\n节点存储的为键值对,并且只对键进排序,值不影响',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Stream流",frontmatter:{title:"Stream流",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/46103a/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/56.Stream%E6%B5%81.html",relativePath:"后端/01.JavaSE/56.Stream流.md",key:"v-88879b54",path:"/pages/46103a/",headers:[{level:2,title:"获取Stream流(创建流)",slug:"获取stream流-创建流",normalizedTitle:"获取stream流(创建流)",charIndex:14},{level:2,title:"filter 过滤 (中间流)",slug:"filter-过滤-中间流",normalizedTitle:"filter 过滤 (中间流)",charIndex:932},{level:3,title:"test方法",slug:"test方法",normalizedTitle:"test方法",charIndex:974},{level:2,title:"limit 截取",slug:"limit-截取",normalizedTitle:"limit 截取",charIndex:1266},{level:2,title:"skip 跳过",slug:"skip-跳过",normalizedTitle:"skip 跳过",charIndex:1361},{level:2,title:"concat 合并 两个stream流",slug:"concat-合并-两个stream流",normalizedTitle:"concat 合并 两个stream流",charIndex:1454},{level:2,title:"distinct 去除流中重复元素",slug:"distinct-去除流中重复元素",normalizedTitle:"distinct 去除流中重复元素",charIndex:1634},{level:2,title:"forEach 对流中每个元素的操作(结束流)",slug:"foreach-对流中每个元素的操作-结束流",normalizedTitle:"foreach 对流中每个元素的操作(结束流)",charIndex:1746},{level:2,title:"count 流中元素的个数",slug:"count-流中元素的个数",normalizedTitle:"count 流中元素的个数",charIndex:1969},{level:2,title:"Stream流 收集方法",slug:"stream流-收集方法",normalizedTitle:"stream流 收集方法",charIndex:2072},{level:3,title:"collect()",slug:"collect",normalizedTitle:"collect()",charIndex:2134}],headersStr:"获取Stream流(创建流) filter 过滤 (中间流) test方法 limit 截取 skip 跳过 concat 合并 两个stream流 distinct 去除流中重复元素 forEach 对流中每个元素的操作(结束流) count 流中元素的个数 Stream流 收集方法 collect()",content:'# Stream流\n\n\n# 获取Stream流(创建流)\n\n能使用stream流的类型\n\n * 单例集合\n   \n   * 使用Collection接口中的 strema()方法\n   \n   * ArrayList<Integer> list = new ArrayList<>(List.of(1, 2, 3, 65, 7));\n     Stream<Integer> stream = list.stream();\n     stream.forEach(s -> System.out.println(s));\n     // 也可使用链式编程\n     list.stream().forEach(s -> System.out.println(s));\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 双列集合\n   \n   * 间接的生成流:先通过keySet或者entrySet获取一个Set集合,再获取Stream流\n   \n   * Map<String, Integer> key1 = Map.of("key1", 123, "key2", 456);\n     key1.entrySet().stream().forEach(s -> System.out.println(s));\n     \n     \n     1\n     2\n     \n\n * 数组\n   \n   * Arrays中的静态方法stream生成流\n   \n   * int[] arr ={1,4,6,7,5,8};\n     Arrays.stream(arr).forEach(s -> System.out.println(s));\n     \n     \n     1\n     2\n     \n\n * 同种数据类型的多个数据\n   \n   * 使用Stream的of方法 与不可变集合类似\n   \n   * Stream.of(1,5,7,9,9).forEach(s -> System.out.println(s));\n     \n     \n     1\n     \n\n\n# filter 过滤 (中间流)\n\nfilter方法会获取流中的 每一个数据\n\n\n# test方法\n\n传递一个Stream流中的元素,返回一个布尔值,如是true则当前数据保留,如为false则数据不保留\n\nlist.stream().filter(new Predicate<String>() {\n    @Override\n    public boolean test(String s) {\n        boolean result = s.startsWith("张");\n        return result;\n    }\n}).forEach(s -> System.out.println(s));\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# limit 截取\n\n截取steam流中前n个元素\n\nlist.stream().limit(2).forEach(s -> System.out.println(s));\n\n\n1\n\n\n\n# skip 跳过\n\n跳过steam流中前n个元素\n\nlist.stream().skip(3).forEach(s -> System.out.println(s));\n\n\n1\n\n\n\n# concat 合并 两个stream流\n\nStream<String> stream = list.stream();\nStream<String> stream2 = list.stream();\nStream.concat(stream,stream2).forEach(s -> System.out.println(s));\n\n\n1\n2\n3\n\n\n\n# distinct 去除流中重复元素\n\n依赖于hashcode和equals方法\n\nlist.stream().distinct().forEach(s -> System.out.println(s));\n\n\n1\n\n\n\n# forEach 对流中每个元素的操作(结束流)\n\nforEach 对此流中每个元素执行指定操作, 只有一个抽象方法accept()\n\nlist.stream().forEach(new Consumer<String>() {\n    @Override\n    public void accept(String s) {\n        System.out.println(s);\n    }\n});\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# count 流中元素的个数\n\n返回一个long类型的数\n\nlong count = list.stream().count();\nSystem.out.println(count);\n\n\n1\n2\n\n\n\n# Stream流 收集方法\n\n在Stream流无法直接修改集合/数组等数据源中数据,创建Stream流不影响原先数据\n\n\n# collect()\n\n# toList\n\nList<String> collect = list.stream().skip(1).collect(Collectors.toList());\n\n\n\n1\n2\n\n\n# toSet\n\nSet<String> collect1 = list.stream().limit(2).collect(Collectors.toSet());\n\n\n\n1\n2\n\n\n# toMap\n\nMap<Character, Character> collect2 = list.stream().limit(2).collect(Collectors.toMap(\n        s -> s.charAt(1) // key\n        , s -> s.charAt(0) // value\n));\n\n\n1\n2\n3\n4\n',normalizedContent:'# stream流\n\n\n# 获取stream流(创建流)\n\n能使用stream流的类型\n\n * 单例集合\n   \n   * 使用collection接口中的 strema()方法\n   \n   * arraylist<integer> list = new arraylist<>(list.of(1, 2, 3, 65, 7));\n     stream<integer> stream = list.stream();\n     stream.foreach(s -> system.out.println(s));\n     // 也可使用链式编程\n     list.stream().foreach(s -> system.out.println(s));\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 双列集合\n   \n   * 间接的生成流:先通过keyset或者entryset获取一个set集合,再获取stream流\n   \n   * map<string, integer> key1 = map.of("key1", 123, "key2", 456);\n     key1.entryset().stream().foreach(s -> system.out.println(s));\n     \n     \n     1\n     2\n     \n\n * 数组\n   \n   * arrays中的静态方法stream生成流\n   \n   * int[] arr ={1,4,6,7,5,8};\n     arrays.stream(arr).foreach(s -> system.out.println(s));\n     \n     \n     1\n     2\n     \n\n * 同种数据类型的多个数据\n   \n   * 使用stream的of方法 与不可变集合类似\n   \n   * stream.of(1,5,7,9,9).foreach(s -> system.out.println(s));\n     \n     \n     1\n     \n\n\n# filter 过滤 (中间流)\n\nfilter方法会获取流中的 每一个数据\n\n\n# test方法\n\n传递一个stream流中的元素,返回一个布尔值,如是true则当前数据保留,如为false则数据不保留\n\nlist.stream().filter(new predicate<string>() {\n    @override\n    public boolean test(string s) {\n        boolean result = s.startswith("张");\n        return result;\n    }\n}).foreach(s -> system.out.println(s));\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# limit 截取\n\n截取steam流中前n个元素\n\nlist.stream().limit(2).foreach(s -> system.out.println(s));\n\n\n1\n\n\n\n# skip 跳过\n\n跳过steam流中前n个元素\n\nlist.stream().skip(3).foreach(s -> system.out.println(s));\n\n\n1\n\n\n\n# concat 合并 两个stream流\n\nstream<string> stream = list.stream();\nstream<string> stream2 = list.stream();\nstream.concat(stream,stream2).foreach(s -> system.out.println(s));\n\n\n1\n2\n3\n\n\n\n# distinct 去除流中重复元素\n\n依赖于hashcode和equals方法\n\nlist.stream().distinct().foreach(s -> system.out.println(s));\n\n\n1\n\n\n\n# foreach 对流中每个元素的操作(结束流)\n\nforeach 对此流中每个元素执行指定操作, 只有一个抽象方法accept()\n\nlist.stream().foreach(new consumer<string>() {\n    @override\n    public void accept(string s) {\n        system.out.println(s);\n    }\n});\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# count 流中元素的个数\n\n返回一个long类型的数\n\nlong count = list.stream().count();\nsystem.out.println(count);\n\n\n1\n2\n\n\n\n# stream流 收集方法\n\n在stream流无法直接修改集合/数组等数据源中数据,创建stream流不影响原先数据\n\n\n# collect()\n\n# tolist\n\nlist<string> collect = list.stream().skip(1).collect(collectors.tolist());\n\n\n\n1\n2\n\n\n# toset\n\nset<string> collect1 = list.stream().limit(2).collect(collectors.toset());\n\n\n\n1\n2\n\n\n# tomap\n\nmap<character, character> collect2 = list.stream().limit(2).collect(collectors.tomap(\n        s -> s.charat(1) // key\n        , s -> s.charat(0) // value\n));\n\n\n1\n2\n3\n4\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"File",frontmatter:{title:"File",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/48b908/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/58.File.html",relativePath:"后端/01.JavaSE/58.File.md",key:"v-96b98cee",path:"/pages/48b908/",headers:[{level:2,title:"创建文件",slug:"创建文件",normalizedTitle:"创建文件",charIndex:309},{level:2,title:"创建文件夹",slug:"创建文件夹",normalizedTitle:"创建文件夹",charIndex:427},{level:2,title:"创建多级文件夹",slug:"创建多级文件夹",normalizedTitle:"创建多级文件夹",charIndex:469},{level:2,title:"删除",slug:"删除",normalizedTitle:"删除",charIndex:502},{level:2,title:"判断是否为目录",slug:"判断是否为目录",normalizedTitle:"判断是否为目录",charIndex:578},{level:2,title:"判断是否为文件",slug:"判断是否为文件",normalizedTitle:"判断是否为文件",charIndex:636},{level:2,title:"判断此路径是否存在",slug:"判断此路径是否存在",normalizedTitle:"判断此路径是否存在",charIndex:686},{level:2,title:"获取file对象的文件名/文件夹名",slug:"获取file对象的文件名-文件夹名",normalizedTitle:"获取file对象的文件名/文件夹名",charIndex:741},{level:2,title:"获取file对象下所有的文件",slug:"获取file对象下所有的文件",normalizedTitle:"获取file对象下所有的文件",charIndex:802}],headersStr:"创建文件 创建文件夹 创建多级文件夹 删除 判断是否为目录 判断是否为文件 判断此路径是否存在 获取file对象的文件名/文件夹名 获取file对象下所有的文件",content:'# File\n\n文件和目录可以通过File封装成对象\n\n三个构造方法\n\n 1. 文件路径\n    \n    File f = new File("C:\\\\a.txt");\n    \n    \n    1\n    \n\n 2. 路径+文件名\n    \n    File f2 = new File("C:\\\\","a.txt");\n    \n    \n    1\n    \n\n 3. file对象+文件名\n    \n    File test = new File("C:\\\\");\n    File f3 = new File(test,"a.txt");\n    \n    \n    1\n    2\n    \n\n\n# 创建文件\n\nboolean newFile = f.createNewFile();\nSystem.out.println(newFile);\n\n\n1\n2\n\n\n在指定路径创建指定文件,如果文件存在则不创建,创建成功返回true\n\n\n# 创建文件夹\n\ntest.mkdir();\n\n\n1\n\n\n同样会返回一个布尔总\n\n\n# 创建多级文件夹\n\ntest.mkdirs();\n\n\n1\n\n\n\n# 删除\n\ntest.delete();\n\n\n1\n\n\n可以删除文件/空文件夹,删除后不经过回收站,如果文件夹内有东西能把内部文件先删除再删除文件夹\n\n\n# 判断是否为目录\n\nSystem.out.println(test.isDirectory());\n\n\n1\n\n\n\n# 判断是否为文件\n\nSystem.out.println(f.isFile());\n\n\n1\n\n\n\n# 判断此路径是否存在\n\nSystem.out.println(test.exists());\n\n\n1\n\n\n\n# 获取file对象的文件名/文件夹名\n\nSystem.out.println(f.getName());\n\n\n1\n\n\n\n# 获取file对象下所有的文件\n\nFile file = new File("D:\\\\");\nFile[] files = file.listFiles();\n\n\n1\n2\n\n\n默认获取所有隐藏的文件,返回一个file数组,如果file对象为文件则会返回null,如需要权限才能访问则无法获取',normalizedContent:'# file\n\n文件和目录可以通过file封装成对象\n\n三个构造方法\n\n 1. 文件路径\n    \n    file f = new file("c:\\\\a.txt");\n    \n    \n    1\n    \n\n 2. 路径+文件名\n    \n    file f2 = new file("c:\\\\","a.txt");\n    \n    \n    1\n    \n\n 3. file对象+文件名\n    \n    file test = new file("c:\\\\");\n    file f3 = new file(test,"a.txt");\n    \n    \n    1\n    2\n    \n\n\n# 创建文件\n\nboolean newfile = f.createnewfile();\nsystem.out.println(newfile);\n\n\n1\n2\n\n\n在指定路径创建指定文件,如果文件存在则不创建,创建成功返回true\n\n\n# 创建文件夹\n\ntest.mkdir();\n\n\n1\n\n\n同样会返回一个布尔总\n\n\n# 创建多级文件夹\n\ntest.mkdirs();\n\n\n1\n\n\n\n# 删除\n\ntest.delete();\n\n\n1\n\n\n可以删除文件/空文件夹,删除后不经过回收站,如果文件夹内有东西能把内部文件先删除再删除文件夹\n\n\n# 判断是否为目录\n\nsystem.out.println(test.isdirectory());\n\n\n1\n\n\n\n# 判断是否为文件\n\nsystem.out.println(f.isfile());\n\n\n1\n\n\n\n# 判断此路径是否存在\n\nsystem.out.println(test.exists());\n\n\n1\n\n\n\n# 获取file对象的文件名/文件夹名\n\nsystem.out.println(f.getname());\n\n\n1\n\n\n\n# 获取file对象下所有的文件\n\nfile file = new file("d:\\\\");\nfile[] files = file.listfiles();\n\n\n1\n2\n\n\n默认获取所有隐藏的文件,返回一个file数组,如果file对象为文件则会返回null,如需要权限才能访问则无法获取',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"方法引用",frontmatter:{title:"方法引用",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/566611/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/57.%E6%96%B9%E6%B3%95%E5%BC%95%E7%94%A8.html",relativePath:"后端/01.JavaSE/57.方法引用.md",key:"v-3d06de88",path:"/pages/566611/",headersStr:null,content:"# 方法引用\n\njdk8中使用了::的用法。就是把方法当做参数传到stream内部，使stream的每个元素都传入到该方法里面执行一下，双冒号运算就是Java中的[方法引用],[方法引用]的格式是\n\n方法引用：若Lamdba体中的内容已经有方法已经实现了，我们可以使用方法引用\n\n三种类型：对象：：实例方法名 ，类：：静态方法名，类：：实例方法名\n\n类型       语法                 对应的LAMBDA表达式\n静态方法引用   类名::staticMethod   (args) -> 类名.staticMethod(args)\n实例方法引用   inst::instMethod   (args) -> inst.instMethod(args)\n对象方法引用   类名::instMethod     (inst,args) -> 类名.instMethod(args)\n构建方法引用   类名::new            (args) -> new 类名(args)\n\nperson -> person.getAge();\n\nPerson：：getAge\n    \nnew HashMap<>()\n//等价于\nHsahMap :: new\n\n\n1\n2\n3\n4\n5\n6\n7\n",normalizedContent:"# 方法引用\n\njdk8中使用了::的用法。就是把方法当做参数传到stream内部，使stream的每个元素都传入到该方法里面执行一下，双冒号运算就是java中的[方法引用],[方法引用]的格式是\n\n方法引用：若lamdba体中的内容已经有方法已经实现了，我们可以使用方法引用\n\n三种类型：对象：：实例方法名 ，类：：静态方法名，类：：实例方法名\n\n类型       语法                 对应的lambda表达式\n静态方法引用   类名::staticmethod   (args) -> 类名.staticmethod(args)\n实例方法引用   inst::instmethod   (args) -> inst.instmethod(args)\n对象方法引用   类名::instmethod     (inst,args) -> 类名.instmethod(args)\n构建方法引用   类名::new            (args) -> new 类名(args)\n\nperson -> person.getage();\n\nperson：：getage\n    \nnew hashmap<>()\n//等价于\nhsahmap :: new\n\n\n1\n2\n3\n4\n5\n6\n7\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"多线程高级",frontmatter:{title:"多线程高级",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/c73ee4/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/60.%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%AB%98%E7%BA%A7.html",relativePath:"后端/01.JavaSE/60.多线程高级.md",key:"v-05bbaf37",path:"/pages/c73ee4/",headers:[{level:2,title:"线程状态",slug:"线程状态",normalizedTitle:"线程状态",charIndex:12},{level:2,title:"线程池",slug:"线程池",normalizedTitle:"线程池",charIndex:23},{level:3,title:"创建线程池",slug:"创建线程池",normalizedTitle:"创建线程池",charIndex:31},{level:3,title:"提交 submit",slug:"提交-submit",normalizedTitle:"提交 submit",charIndex:261},{level:3,title:"销毁 shutdown",slug:"销毁-shutdown",normalizedTitle:"销毁 shutdown",charIndex:288},{level:2,title:"ThreadPoolExecutor  自定义线程池",slug:"threadpoolexecutor-自定义线程池",normalizedTitle:"threadpoolexecutor  自定义线程池",charIndex:null},{level:2,title:"Volatile",slug:"volatile",normalizedTitle:"volatile",charIndex:1186},{level:3,title:"使用Synchronized同步代码块",slug:"使用synchronized同步代码块",normalizedTitle:"使用synchronized同步代码块",charIndex:1428},{level:2,title:"原子性",slug:"原子性",normalizedTitle:"原子性",charIndex:1564},{level:3,title:"原子类 Atomic",slug:"原子类-atomic",normalizedTitle:"原子类 atomic",charIndex:1721},{level:3,title:"Synchronized 与 CAS 的区别(乐观锁和悲观锁)",slug:"synchronized-与-cas-的区别-乐观锁和悲观锁",normalizedTitle:"synchronized 与 cas 的区别(乐观锁和悲观锁)",charIndex:2302},{level:2,title:"Hashtable",slug:"hashtable",normalizedTitle:"hashtable",charIndex:2568},{level:2,title:"ConcurrentHashMap",slug:"concurrenthashmap",normalizedTitle:"concurrenthashmap",charIndex:2745},{level:3,title:"JDK1.7原理",slug:"jdk1-7原理",normalizedTitle:"jdk1.7原理",charIndex:2833},{level:3,title:"JDK1.8原理",slug:"jdk1-8原理",normalizedTitle:"jdk1.8原理",charIndex:3318},{level:2,title:"CountDownLatch",slug:"countdownlatch",normalizedTitle:"countdownlatch",charIndex:3642},{level:2,title:"Semaphore",slug:"semaphore",normalizedTitle:"semaphore",charIndex:3818}],headersStr:"线程状态 线程池 创建线程池 提交 submit 销毁 shutdown ThreadPoolExecutor  自定义线程池 Volatile 使用Synchronized同步代码块 原子性 原子类 Atomic Synchronized 与 CAS 的区别(乐观锁和悲观锁) Hashtable ConcurrentHashMap JDK1.7原理 JDK1.8原理 CountDownLatch Semaphore",content:"# 多线程高级\n\n\n# 线程状态\n\n\n\n\n# 线程池\n\n\n# 创建线程池\n\n创建一关默认的线程池,默认为空,(空参构造方法)默认数量最多可以容纳int的最大值个线程\n\nExecutorService executorService = Executors.newCachedThreadPool(5);\n\n\n1\n\n\n带参构造为该线程池指定数量的线程池,并不是线程池创建就拥有指定数量的线程,而是该线程池可以拥有线程的上限\n\nExecutors类为创建线程池对象类\n\nExecutorService类为控制线程池类\n\n\n# 提交 submit\n\n把线程提交给线程池处理\n\n\n# 销毁 shutdown\n\n关闭线程池\n\nexecutorService.shutdown();\n\n\n1\n\n\n\n# ThreadPoolExecutor 自定义线程池\n\n        ThreadPoolExecutor pool = new ThreadPoolExecutor(2,5,2, TimeUnit.SECONDS,new ArrayBlockingQueue<>(10),Executors.defaultThreadFactory(),new ThreadPoolExecutor.AbortPolicy());\n\n\n\n1\n2\n\n\n共有7个参数\n\n 1. 核心线程数量 int 不能小于0\n 2. 最大线程数 int 不能小于等于0.并且大于等于核心数\n 3. 空闲线程最大存活时间 int 不能小于0\n 4. 时间单位 Enum TimeUnit中的常量\n 5. 任务队列 传递一个阻塞队列 不能为null 如果submit的线程过多则会缓存到队列中\n 6. 创建线程工程 我们使用默认的线程池创建 Executors.defaultThreadFactory() 不能为null\n 7. 任务的拒绝策略(即超出最大线程数如何处理) 我们使用new ThreadPoolExecutor.AbortPolicy()超出则拒绝 不能为null 当submit线程数量超出了最大线程数+任务队列边界时 触发 拒绝策略\n    * ThreadPoolExecutor.AbortPolicy: 丢弃任务并抛出RejectedExecutionException异常. 默认的cl\n    * ThreadPoolExecutor.DiscardPolicy: 丢弃任务 但不抛出异常 不太推荐使用\n    * ThreadPoolExecutor.DiscardOldestPolicy: 抛弃队列中等待最久的任务 然后把当前任务加入队列中\n    * ThreadPoolExecutor.CallerRunsPolicy: 调度任务的run()方法绕过线程池直接执行\n\n\n# Volatile\n\n 1. 堆内存是唯一的,每一个线程都有自己的线程栈\n 2. 每个线程在使用堆里面变量的时候,都会先拷贝一个变量的副本中.\n 3. 在线程中,每一次使用都是从变量副本中获取\n\n如果A线程 修改了堆中共享的变量值,其他线程不一定能及时的使用最新的值\n\n使用Volatile关键字可以解决这个问题,强制线程每次使用时,都会先去看一下共享区域中最新值\n\n只需要共享数据前面加上Volatile关键字\n\nVolatile int count = 100;\n\n\n1\n\n\n\n# 使用Synchronized同步代码块\n\n使用Synchronized同步代码块,也可以解决此问题\n\n 1. 线程获取锁\n 2. 清空变量副本\n 3. 拷贝共享变量中最新的值到副本中\n 4. 执行代码\n 5. 将修改后变量副本中的值赋值给共享数据\n 6. 释放锁\n\n\n# 原子性\n\n原子性指的是在一次或多次操作中,要么所有的操作全部执行并且不会受到任何因素而中断,要么所有的操作都不执行,多个操作是一个不可以分割的整体\n\n我们可以使用Synchronized 锁 来强制协议统一共享数据的唯一性\n\n而Volatile只是让线程在执行时检测在共享数据中检测最新的值并同步 副本中\n\n\n# 原子类 Atomic\n\nJDK1.5提供了一个原子类 Atomic 该类下的的实现类都可以实现原子性,方便程序员在多线程环境下，无锁的进行原子操作。原子变量的底层使用了处理器提供的原子指令，但是不同的CPU架构可能提供的原子指令不一样，也有可能需要某种形式的内部锁,所以该方法不能绝对保证线程不被阻塞。\n\n# AtomicInteger\n\n * AtomicInteger(): 空参构造方法 默认为0\n * AtomicInteger(int initialValue): 带参构造方法 指定值原子型\n * get(): 获取值\n * getAndIncrement(): 以原子方式加+1 并且返回自增前的值\n * incrementAndGet(): 以原子方式加+1 并且返回自增后的值\n * addAndGet(int data): 以原子方式 与指定值相加 并返回结果\n * getAndSet(int value): 以原子方式 设置原子型为指定值 并返回旧的值\n\n原理:\n\n自旋锁 + CAS 算法\n\nCAS算法:有3个操作数(内存值V , 旧的预期值A , 要修改的值B )\n\n当旧的预期A == 内存值 此时修改成功 将V改为B\n\n当旧的预期A != 内存值 修改失败 不做任何操作\n\n并重新获取现在的最新值(这个动作称为自旋)\n\n\n\n\n# Synchronized 与 CAS 的区别(乐观锁和悲观锁)\n\n相同点: 在多线程区块下,都可以保证共享数据的安全性\n\n不同点:\n\n * Synchronized 总是从最坏的角度出发,认为每次获取数据的时候,别人都有可能修改.所以在每次操作共享数据之前,都会上锁.(悲观锁)\n * CAS 是乐观的角度出发,假设每次获取数据别人都不会修改,所以不会上锁.只不过在修改共享数据的时候,会去检查一下,别人有没有修改过这个数据. (乐观锁) 如果别人修改过,那么再次获取最新的值 如果别人没有修改过,那么直接修改共享数据的值\n\n\n# Hashtable\n\n在多线程下使用HashMap不是线程安全的，在多线程并发的环境下，可能会产生死锁等问题。\n\n 1. Hashtable采用悲观锁 Synchronized 的形式保证数据的安全性\n 2. 只有有线程访问,会将整张表全部锁起来,所以Hashtable效率低下\n 3. 底层原理与hashmap一样也是数组+链表实现,其他无异\n\n\n# ConcurrentHashMap\n\n如果map集合要使用多线程我们可以使用ConcurrentHashMap,它线程安全,效率较高\n\nHashtable已经被淘汰了\n\n\n# JDK1.7原理\n\n 1. 创建一个默认长度为16,默认加载因 位0.75的数组 数组名为 Segmewnt 无法扩容\n 2. 再创建一个长度为2的小数组(数组名为HashEntey) 把地址值赋值给Segmewnt数组中的索引0 (模板) 其他索引都为null\n 3. 根据键的哈希值计算出 Segmewnt 数组索引\n 4. 如果此索引为空 就会创建一个长度默认为2的数组 并把这个小数组的地址值 赋值给 该索引\n 5. 再次利用键的哈希值计算出在 小数组 应存入的索引(二次哈希)\n 6. 如果为空,则直接添加 如非空则equals比较 不同则存入 (链表形式挂载在新元素下面)\n 7. 小数组的加载因 同样为0.75 当超过了2*0.75=1.5 强转为int=1 会自动扩容2倍\n 8. Segmewnt数组无法扩容(恒定为16) 因为只有HashEntey小数组在扩容\n 9. ConcurrentHashMap 通过Segmewnt 索引 来加锁(悲观锁 Synchronized ) 所以在JDK1.7 默认情况下,最多允许 16个线程同时访问\n\n\n# JDK1.8原理\n\n底层结构:哈希表(数组 链表 红黑树结合体)\n\n结合CAS机制 + Synchronized 同步代码块形式来保证线程安全\n\n 1. 如果使用空参构造方法创建ConcurrentHashMap,则什么事情都不做. 只有在第一次添加元素时候创建哈希表\n 2. 计算出当前元素应存入的索引\n 3. 如果该索引为为null,则利用CAS算法,将本结点添加到数组中\n 4. 如果该索引不为null,则利用Volatile关键字获取当前位置最新的结点地址,挂载到它的下面,变成链表\n 5. 当链表长度大于等于8时,自动转成红黑树\n 6. 以链表或红黑树头结点为锁对象,配合悲观锁(Synchronized)保证多线程数据的安全性\n\n\n# CountDownLatch\n\n让某一条线程等待其他线程执行完毕之后在执行.\n\n * CountDownLatch(int count): 带参构造方法 传递线程数,表示等待线程数 定义一个计数器\n * await(): 让线程等待 等待其他线程执行完毕后才执行 计数器为0执行\n * countDown(): 当前线程执行完毕 将计数器-1\n\n\n# Semaphore\n\n可以控制访问特定资源的线程数量 在线程类中创建并使用\n\n * Semaphore(int permits): 带参构造方法 最多允许多少条线程同时执行\n * acquire(): 获取通信证\n * release(): 归还通信证",normalizedContent:"# 多线程高级\n\n\n# 线程状态\n\n\n\n\n# 线程池\n\n\n# 创建线程池\n\n创建一关默认的线程池,默认为空,(空参构造方法)默认数量最多可以容纳int的最大值个线程\n\nexecutorservice executorservice = executors.newcachedthreadpool(5);\n\n\n1\n\n\n带参构造为该线程池指定数量的线程池,并不是线程池创建就拥有指定数量的线程,而是该线程池可以拥有线程的上限\n\nexecutors类为创建线程池对象类\n\nexecutorservice类为控制线程池类\n\n\n# 提交 submit\n\n把线程提交给线程池处理\n\n\n# 销毁 shutdown\n\n关闭线程池\n\nexecutorservice.shutdown();\n\n\n1\n\n\n\n# threadpoolexecutor 自定义线程池\n\n        threadpoolexecutor pool = new threadpoolexecutor(2,5,2, timeunit.seconds,new arrayblockingqueue<>(10),executors.defaultthreadfactory(),new threadpoolexecutor.abortpolicy());\n\n\n\n1\n2\n\n\n共有7个参数\n\n 1. 核心线程数量 int 不能小于0\n 2. 最大线程数 int 不能小于等于0.并且大于等于核心数\n 3. 空闲线程最大存活时间 int 不能小于0\n 4. 时间单位 enum timeunit中的常量\n 5. 任务队列 传递一个阻塞队列 不能为null 如果submit的线程过多则会缓存到队列中\n 6. 创建线程工程 我们使用默认的线程池创建 executors.defaultthreadfactory() 不能为null\n 7. 任务的拒绝策略(即超出最大线程数如何处理) 我们使用new threadpoolexecutor.abortpolicy()超出则拒绝 不能为null 当submit线程数量超出了最大线程数+任务队列边界时 触发 拒绝策略\n    * threadpoolexecutor.abortpolicy: 丢弃任务并抛出rejectedexecutionexception异常. 默认的cl\n    * threadpoolexecutor.discardpolicy: 丢弃任务 但不抛出异常 不太推荐使用\n    * threadpoolexecutor.discardoldestpolicy: 抛弃队列中等待最久的任务 然后把当前任务加入队列中\n    * threadpoolexecutor.callerrunspolicy: 调度任务的run()方法绕过线程池直接执行\n\n\n# volatile\n\n 1. 堆内存是唯一的,每一个线程都有自己的线程栈\n 2. 每个线程在使用堆里面变量的时候,都会先拷贝一个变量的副本中.\n 3. 在线程中,每一次使用都是从变量副本中获取\n\n如果a线程 修改了堆中共享的变量值,其他线程不一定能及时的使用最新的值\n\n使用volatile关键字可以解决这个问题,强制线程每次使用时,都会先去看一下共享区域中最新值\n\n只需要共享数据前面加上volatile关键字\n\nvolatile int count = 100;\n\n\n1\n\n\n\n# 使用synchronized同步代码块\n\n使用synchronized同步代码块,也可以解决此问题\n\n 1. 线程获取锁\n 2. 清空变量副本\n 3. 拷贝共享变量中最新的值到副本中\n 4. 执行代码\n 5. 将修改后变量副本中的值赋值给共享数据\n 6. 释放锁\n\n\n# 原子性\n\n原子性指的是在一次或多次操作中,要么所有的操作全部执行并且不会受到任何因素而中断,要么所有的操作都不执行,多个操作是一个不可以分割的整体\n\n我们可以使用synchronized 锁 来强制协议统一共享数据的唯一性\n\n而volatile只是让线程在执行时检测在共享数据中检测最新的值并同步 副本中\n\n\n# 原子类 atomic\n\njdk1.5提供了一个原子类 atomic 该类下的的实现类都可以实现原子性,方便程序员在多线程环境下，无锁的进行原子操作。原子变量的底层使用了处理器提供的原子指令，但是不同的cpu架构可能提供的原子指令不一样，也有可能需要某种形式的内部锁,所以该方法不能绝对保证线程不被阻塞。\n\n# atomicinteger\n\n * atomicinteger(): 空参构造方法 默认为0\n * atomicinteger(int initialvalue): 带参构造方法 指定值原子型\n * get(): 获取值\n * getandincrement(): 以原子方式加+1 并且返回自增前的值\n * incrementandget(): 以原子方式加+1 并且返回自增后的值\n * addandget(int data): 以原子方式 与指定值相加 并返回结果\n * getandset(int value): 以原子方式 设置原子型为指定值 并返回旧的值\n\n原理:\n\n自旋锁 + cas 算法\n\ncas算法:有3个操作数(内存值v , 旧的预期值a , 要修改的值b )\n\n当旧的预期a == 内存值 此时修改成功 将v改为b\n\n当旧的预期a != 内存值 修改失败 不做任何操作\n\n并重新获取现在的最新值(这个动作称为自旋)\n\n\n\n\n# synchronized 与 cas 的区别(乐观锁和悲观锁)\n\n相同点: 在多线程区块下,都可以保证共享数据的安全性\n\n不同点:\n\n * synchronized 总是从最坏的角度出发,认为每次获取数据的时候,别人都有可能修改.所以在每次操作共享数据之前,都会上锁.(悲观锁)\n * cas 是乐观的角度出发,假设每次获取数据别人都不会修改,所以不会上锁.只不过在修改共享数据的时候,会去检查一下,别人有没有修改过这个数据. (乐观锁) 如果别人修改过,那么再次获取最新的值 如果别人没有修改过,那么直接修改共享数据的值\n\n\n# hashtable\n\n在多线程下使用hashmap不是线程安全的，在多线程并发的环境下，可能会产生死锁等问题。\n\n 1. hashtable采用悲观锁 synchronized 的形式保证数据的安全性\n 2. 只有有线程访问,会将整张表全部锁起来,所以hashtable效率低下\n 3. 底层原理与hashmap一样也是数组+链表实现,其他无异\n\n\n# concurrenthashmap\n\n如果map集合要使用多线程我们可以使用concurrenthashmap,它线程安全,效率较高\n\nhashtable已经被淘汰了\n\n\n# jdk1.7原理\n\n 1. 创建一个默认长度为16,默认加载因 位0.75的数组 数组名为 segmewnt 无法扩容\n 2. 再创建一个长度为2的小数组(数组名为hashentey) 把地址值赋值给segmewnt数组中的索引0 (模板) 其他索引都为null\n 3. 根据键的哈希值计算出 segmewnt 数组索引\n 4. 如果此索引为空 就会创建一个长度默认为2的数组 并把这个小数组的地址值 赋值给 该索引\n 5. 再次利用键的哈希值计算出在 小数组 应存入的索引(二次哈希)\n 6. 如果为空,则直接添加 如非空则equals比较 不同则存入 (链表形式挂载在新元素下面)\n 7. 小数组的加载因 同样为0.75 当超过了2*0.75=1.5 强转为int=1 会自动扩容2倍\n 8. segmewnt数组无法扩容(恒定为16) 因为只有hashentey小数组在扩容\n 9. concurrenthashmap 通过segmewnt 索引 来加锁(悲观锁 synchronized ) 所以在jdk1.7 默认情况下,最多允许 16个线程同时访问\n\n\n# jdk1.8原理\n\n底层结构:哈希表(数组 链表 红黑树结合体)\n\n结合cas机制 + synchronized 同步代码块形式来保证线程安全\n\n 1. 如果使用空参构造方法创建concurrenthashmap,则什么事情都不做. 只有在第一次添加元素时候创建哈希表\n 2. 计算出当前元素应存入的索引\n 3. 如果该索引为为null,则利用cas算法,将本结点添加到数组中\n 4. 如果该索引不为null,则利用volatile关键字获取当前位置最新的结点地址,挂载到它的下面,变成链表\n 5. 当链表长度大于等于8时,自动转成红黑树\n 6. 以链表或红黑树头结点为锁对象,配合悲观锁(synchronized)保证多线程数据的安全性\n\n\n# countdownlatch\n\n让某一条线程等待其他线程执行完毕之后在执行.\n\n * countdownlatch(int count): 带参构造方法 传递线程数,表示等待线程数 定义一个计数器\n * await(): 让线程等待 等待其他线程执行完毕后才执行 计数器为0执行\n * countdown(): 当前线程执行完毕 将计数器-1\n\n\n# semaphore\n\n可以控制访问特定资源的线程数量 在线程类中创建并使用\n\n * semaphore(int permits): 带参构造方法 最多允许多少条线程同时执行\n * acquire(): 获取通信证\n * release(): 归还通信证",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"多线程",frontmatter:{title:"多线程",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/606294/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/59.%E5%A4%9A%E7%BA%BF%E7%A8%8B.html",relativePath:"后端/01.JavaSE/59.多线程.md",key:"v-97dc2282",path:"/pages/606294/",headers:[{level:2,title:"并行和并发",slug:"并行和并发",normalizedTitle:"并行和并发",charIndex:10},{level:2,title:"进程和线程",slug:"进程和线程",normalizedTitle:"进程和线程",charIndex:74},{level:3,title:"进程",slug:"进程",normalizedTitle:"进程",charIndex:74},{level:3,title:"线程",slug:"线程",normalizedTitle:"线程",charIndex:3},{level:2,title:"继承Thread",slug:"继承thread",normalizedTitle:"继承thread",charIndex:243},{level:2,title:"Runnable接口",slug:"runnable接口",normalizedTitle:"runnable接口",charIndex:634},{level:2,title:"Callable接口与Future",slug:"callable接口与future",normalizedTitle:"callable接口与future",charIndex:1041},{level:2,title:"Thread",slug:"thread",normalizedTitle:"thread",charIndex:245},{level:3,title:"getName",slug:"getname",normalizedTitle:"getname",charIndex:1807},{level:3,title:"setName",slug:"setname",normalizedTitle:"setname",charIndex:1847},{level:3,title:"currentThread",slug:"currentthread",normalizedTitle:"currentthread",charIndex:1925},{level:2,title:"线程休眠",slug:"线程休眠",normalizedTitle:"线程休眠",charIndex:2077},{level:2,title:"线程调度",slug:"线程调度",normalizedTitle:"线程调度",charIndex:2133},{level:2,title:"线程的优先级",slug:"线程的优先级",normalizedTitle:"线程的优先级",charIndex:2253},{level:3,title:"getProiority",slug:"getproiority",normalizedTitle:"getproiority",charIndex:2264},{level:3,title:"setPriority",slug:"setpriority",normalizedTitle:"setpriority",charIndex:2336},{level:2,title:"后台线程/守护线程",slug:"后台线程-守护线程",normalizedTitle:"后台线程/守护线程",charIndex:2440},{level:3,title:"setDaemon",slug:"setdaemon",normalizedTitle:"setdaemon",charIndex:2454},{level:2,title:"线程的安全问题",slug:"线程的安全问题",normalizedTitle:"线程的安全问题",charIndex:2527},{level:3,title:"同步代码块",slug:"同步代码块",normalizedTitle:"同步代码块",charIndex:2539},{level:3,title:"同步方法",slug:"同步方法",normalizedTitle:"同步方法",charIndex:2798},{level:2,title:"Lock锁",slug:"lock锁",normalizedTitle:"lock锁",charIndex:2891},{level:3,title:"lock",slug:"lock",normalizedTitle:"lock",charIndex:2963},{level:3,title:"unlock",slug:"unlock",normalizedTitle:"unlock",charIndex:3031},{level:2,title:"死锁",slug:"死锁",normalizedTitle:"死锁",charIndex:3068},{level:2,title:"生成者消费者(等待 唤醒机制)",slug:"生成者消费者-等待-唤醒机制",normalizedTitle:"生成者消费者(等待 唤醒机制)",charIndex:3125},{level:3,title:"等待 wait",slug:"等待-wait",normalizedTitle:"等待 wait",charIndex:3145},{level:3,title:"唤醒 notify",slug:"唤醒-notify",normalizedTitle:"唤醒 notify",charIndex:3186},{level:3,title:"唤醒所有  notifyall",slug:"唤醒所有-notifyall",normalizedTitle:"唤醒所有  notifyall",charIndex:null},{level:2,title:"阻塞队列实现等待唤醒机制",slug:"阻塞队列实现等待唤醒机制",normalizedTitle:"阻塞队列实现等待唤醒机制",charIndex:3337},{level:3,title:"ArrayBlockingQueue",slug:"arrayblockingqueue",normalizedTitle:"arrayblockingqueue",charIndex:3354},{level:3,title:"LinkedBlockingQueue",slug:"linkedblockingqueue",normalizedTitle:"linkedblockingqueue",charIndex:3486}],headersStr:"并行和并发 进程和线程 进程 线程 继承Thread Runnable接口 Callable接口与Future Thread getName setName currentThread 线程休眠 线程调度 线程的优先级 getProiority setPriority 后台线程/守护线程 setDaemon 线程的安全问题 同步代码块 同步方法 Lock锁 lock unlock 死锁 生成者消费者(等待 唤醒机制) 等待 wait 唤醒 notify 唤醒所有  notifyall 阻塞队列实现等待唤醒机制 ArrayBlockingQueue LinkedBlockingQueue",content:'# 多线程\n\n\n# 并行和并发\n\n并行:在同一时刻,有多个指令在多个cpu上同时执行\n\n并发:在同一时刻,有多个指令在单个CPU上交替执行\n\n\n# 进程和线程\n\n\n# 进程\n\n进程:是正在运行的软件\n\n独立性:进程是一贯能独立运行的基本单位\n\n动态性:进程的实质是程序的一次执行过程\n\n并发性:任何进程都可以同其他进程一起并发执行\n\n\n# 线程\n\n线程:是进程中的单个顺序控制流,是一条执行路径\n\n * 单线程:一个进程只有一条执行路径\n * 多线程:一个进程拥有多个执行路径\n\n\n# 继承Thread\n\n * 类继承Thread类\n\n * 重写run()方法\n\n * public static class xian extends Thread {\n       @Override\n       public void run() {\n           super.run();\n       }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n * 创建类对象\n\n * 调用用start()方法 启动线程 交由JVM调用此线程的run()方法\n\n * public static void main(String[] args) {\n       xian x = new xian();\n       x.start();\n   }\n   \n   \n   1\n   2\n   3\n   4\n   \n\n\n# Runnable接口\n\n * 类实现Runnable接口\n\n * 重写run方法\n\n * public static class xian2 implements Runnable{\n       @Override\n       public void run() {\n           System.out.println("线程启动");\n       }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n * 创建类对象\n\n * xian2 x =new xian2();\n   \n   \n   1\n   \n\n * 创建Thread类对象,把类对象作为构造方法的参数\n\n * Thread t1 =new Thread(x);\n   \n   \n   1\n   \n\n * 启动线程\n\n * t1.start();\n   \n   \n   1\n   \n\n\n# Callable接口与Future\n\n * 类实现Callable接口\n\n * 重写call()方法\n\n * // 接口的泛型是call方法返回的类型\n       public static class xian3 implements Callable<String> {\n           @Override\n           public String call() throws Exception {\n               //返回的为线程执行完毕的结果,执行语句在方法体写\n               return "你好多线程";\n           }\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   \n\n * 创建类对象\n\n * xian3 x = new xian3();\n   \n   \n   1\n   \n\n * 创建Future的实现类FutureTask对象,并将类对象作为构造方法参数传递\n\n * FutureTask<String> ft = new FutureTask<>(x);\n   \n   \n   1\n   \n\n * 创建Thread类对象,并把FutureTask对象作为构造方法参数传递\n\n * Thread t1 = new Thread(ft);\n   \n   \n   1\n   \n\n * 启动线程\n\n * t1.start();\n   \n   \n   1\n   \n\n// 获取线程执行完毕的结果,get方法一定在线程启动之后,否则get下面语句不执行\nString s = ft.get();\nSystem.out.println(s);\n\n\n1\n2\n3\n\n\n\n# Thread\n\n\n# getName\n\n获取线程名称,线程有默认名称为 Thread-线程数\n\n\n# setName\n\n也可以通过构造方法设置线程名称\n\nThead类中带有带参构造方法,可以给线程设置名称,但是继承的类必须使用super关键字引用.\n\n\n# currentThread\n\n返回当前正在执行的线程对象\n\nString name = Thread.currentThread().getName();\n\n\n1\n\n\n如果Runnable接口或Callble接口想要使用Thread的方法可以\n\n先捕抓到当前执行的线程 再使用Thread的方法\n\n\n# 线程休眠\n\nsleep()方法 让线程休眠指定毫秒\n\nThread.sleep(1000);\n\n\n1\n\n\n\n# 线程调度\n\n * 分时调度模型:所有线程轮流使用cou的使用权,平均分配每个线程占用cpu的时间片\n * 抢占式调度模型:优先让优先级高的线程使用cpu,如果优先级相同,那么会随机选择一个,优先级高的线程获取cpu时间片相对多一些\n\n\n# 线程的优先级\n\n\n# getProiority\n\n获取指定线程的优先级\n\nThread.currentThread().getPriority()\n\n\n1\n\n\n\n# setPriority\n\n设置指定线程的优先级\n\n默认为5 范围1-10\n\nThread.currentThread().setPriority(6);\n\n\n1\n\n\n优先级高不代表运行的时间相对减少\n\n\n# 后台线程/守护线程\n\n\n# setDaemon\n\n需要传递一个布尔值,true为设置为守护线程\n\n当普通线程执行完后,那么守护线程没有继续执行下去的必要(自动结束)\n\n\n# 线程的安全问题\n\n\n# 同步代码块\n\n锁多条语句操作共享数据,可以使用同步代码块实现\n\nObject obj =new Object();\n// 传递任意对象,注意要是唯一的,否则线程认为是不同的同步锁\nsynchronized (obj){\n    // 线程共享的操作数据\n}\n\n\n1\n2\n3\n4\n5\n\n\n默认情况是打开的,只要有一个线程进去执行代码了,锁就会关闭,只有等代码块执行完毕才重新打开\n\n同步的好处:\n\n解决了多线程的数据安全问题\n\n弊端:\n\n当线程过多时,因为每个线程都会判断同步上的锁,耗费系统资源,运行效率降低\n\n\n# 同步方法\n\n在方法返回值前面加上关键字synchronized,该方法的所有代码都加上锁\n\n此同步方法锁对象为this\n\n如果此同步方法 是静态的 则锁对象为 类名.class\n\n\n# Lock锁\n\nLock是接口不能直接实例化,我们通过它的实现类ReentrantLock来实例化\n\nprivate ReentrantLock lock =new ReentrantLock();\n\n\n1\n\n\n\n# lock\n\nlock.lock();\n\n\n1\n\n\n加锁\n\n\n# unlock\n\nlock.unlock();\n\n\n1\n\n\n释放锁\n\n\n# 死锁\n\n线程死锁是指由于两个或者多个线程互相持有对象所需要的资源,导致这些线程处于等待状态,无法前往执行\n\n\n# 生成者消费者(等待 唤醒机制)\n\n\n# 等待 wait\n\n同步代码块中锁的对象是什么则 wait方法在该对象调用\n\n\n# 唤醒 notify\n\n会随机唤醒该进程里的任意线程\n\n\n# 唤醒所有 notifyall\n\n锁的步骤:\n\n 1. while(true) 死循环\n 2. synchronized 同步锁,锁对象要唯一\n 3. 判断,共享数据是否结束 结束的操作\n 4. 判断,共享数据是否结束 没有结束的操作\n\n\n# 阻塞队列实现等待唤醒机制\n\n\n# ArrayBlockingQueue\n\n底层是数组,有界,创建时通过带参构造方法,定义该阻塞队列的边界\n\n# put\n\nput("元素"),存储,底层有lock锁\n\n# take\n\ntake()取出,如果取不出则会一直等待,直到下一关元素put进阻塞队列\n\n\n# LinkedBlockingQueue\n\n底层是链表,无界.但不是真正的无界,默认最大为int的最大值,也可以通过带参构造方法,定义阻塞队列的边界',normalizedContent:'# 多线程\n\n\n# 并行和并发\n\n并行:在同一时刻,有多个指令在多个cpu上同时执行\n\n并发:在同一时刻,有多个指令在单个cpu上交替执行\n\n\n# 进程和线程\n\n\n# 进程\n\n进程:是正在运行的软件\n\n独立性:进程是一贯能独立运行的基本单位\n\n动态性:进程的实质是程序的一次执行过程\n\n并发性:任何进程都可以同其他进程一起并发执行\n\n\n# 线程\n\n线程:是进程中的单个顺序控制流,是一条执行路径\n\n * 单线程:一个进程只有一条执行路径\n * 多线程:一个进程拥有多个执行路径\n\n\n# 继承thread\n\n * 类继承thread类\n\n * 重写run()方法\n\n * public static class xian extends thread {\n       @override\n       public void run() {\n           super.run();\n       }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n * 创建类对象\n\n * 调用用start()方法 启动线程 交由jvm调用此线程的run()方法\n\n * public static void main(string[] args) {\n       xian x = new xian();\n       x.start();\n   }\n   \n   \n   1\n   2\n   3\n   4\n   \n\n\n# runnable接口\n\n * 类实现runnable接口\n\n * 重写run方法\n\n * public static class xian2 implements runnable{\n       @override\n       public void run() {\n           system.out.println("线程启动");\n       }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n * 创建类对象\n\n * xian2 x =new xian2();\n   \n   \n   1\n   \n\n * 创建thread类对象,把类对象作为构造方法的参数\n\n * thread t1 =new thread(x);\n   \n   \n   1\n   \n\n * 启动线程\n\n * t1.start();\n   \n   \n   1\n   \n\n\n# callable接口与future\n\n * 类实现callable接口\n\n * 重写call()方法\n\n * // 接口的泛型是call方法返回的类型\n       public static class xian3 implements callable<string> {\n           @override\n           public string call() throws exception {\n               //返回的为线程执行完毕的结果,执行语句在方法体写\n               return "你好多线程";\n           }\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   \n\n * 创建类对象\n\n * xian3 x = new xian3();\n   \n   \n   1\n   \n\n * 创建future的实现类futuretask对象,并将类对象作为构造方法参数传递\n\n * futuretask<string> ft = new futuretask<>(x);\n   \n   \n   1\n   \n\n * 创建thread类对象,并把futuretask对象作为构造方法参数传递\n\n * thread t1 = new thread(ft);\n   \n   \n   1\n   \n\n * 启动线程\n\n * t1.start();\n   \n   \n   1\n   \n\n// 获取线程执行完毕的结果,get方法一定在线程启动之后,否则get下面语句不执行\nstring s = ft.get();\nsystem.out.println(s);\n\n\n1\n2\n3\n\n\n\n# thread\n\n\n# getname\n\n获取线程名称,线程有默认名称为 thread-线程数\n\n\n# setname\n\n也可以通过构造方法设置线程名称\n\nthead类中带有带参构造方法,可以给线程设置名称,但是继承的类必须使用super关键字引用.\n\n\n# currentthread\n\n返回当前正在执行的线程对象\n\nstring name = thread.currentthread().getname();\n\n\n1\n\n\n如果runnable接口或callble接口想要使用thread的方法可以\n\n先捕抓到当前执行的线程 再使用thread的方法\n\n\n# 线程休眠\n\nsleep()方法 让线程休眠指定毫秒\n\nthread.sleep(1000);\n\n\n1\n\n\n\n# 线程调度\n\n * 分时调度模型:所有线程轮流使用cou的使用权,平均分配每个线程占用cpu的时间片\n * 抢占式调度模型:优先让优先级高的线程使用cpu,如果优先级相同,那么会随机选择一个,优先级高的线程获取cpu时间片相对多一些\n\n\n# 线程的优先级\n\n\n# getproiority\n\n获取指定线程的优先级\n\nthread.currentthread().getpriority()\n\n\n1\n\n\n\n# setpriority\n\n设置指定线程的优先级\n\n默认为5 范围1-10\n\nthread.currentthread().setpriority(6);\n\n\n1\n\n\n优先级高不代表运行的时间相对减少\n\n\n# 后台线程/守护线程\n\n\n# setdaemon\n\n需要传递一个布尔值,true为设置为守护线程\n\n当普通线程执行完后,那么守护线程没有继续执行下去的必要(自动结束)\n\n\n# 线程的安全问题\n\n\n# 同步代码块\n\n锁多条语句操作共享数据,可以使用同步代码块实现\n\nobject obj =new object();\n// 传递任意对象,注意要是唯一的,否则线程认为是不同的同步锁\nsynchronized (obj){\n    // 线程共享的操作数据\n}\n\n\n1\n2\n3\n4\n5\n\n\n默认情况是打开的,只要有一个线程进去执行代码了,锁就会关闭,只有等代码块执行完毕才重新打开\n\n同步的好处:\n\n解决了多线程的数据安全问题\n\n弊端:\n\n当线程过多时,因为每个线程都会判断同步上的锁,耗费系统资源,运行效率降低\n\n\n# 同步方法\n\n在方法返回值前面加上关键字synchronized,该方法的所有代码都加上锁\n\n此同步方法锁对象为this\n\n如果此同步方法 是静态的 则锁对象为 类名.class\n\n\n# lock锁\n\nlock是接口不能直接实例化,我们通过它的实现类reentrantlock来实例化\n\nprivate reentrantlock lock =new reentrantlock();\n\n\n1\n\n\n\n# lock\n\nlock.lock();\n\n\n1\n\n\n加锁\n\n\n# unlock\n\nlock.unlock();\n\n\n1\n\n\n释放锁\n\n\n# 死锁\n\n线程死锁是指由于两个或者多个线程互相持有对象所需要的资源,导致这些线程处于等待状态,无法前往执行\n\n\n# 生成者消费者(等待 唤醒机制)\n\n\n# 等待 wait\n\n同步代码块中锁的对象是什么则 wait方法在该对象调用\n\n\n# 唤醒 notify\n\n会随机唤醒该进程里的任意线程\n\n\n# 唤醒所有 notifyall\n\n锁的步骤:\n\n 1. while(true) 死循环\n 2. synchronized 同步锁,锁对象要唯一\n 3. 判断,共享数据是否结束 结束的操作\n 4. 判断,共享数据是否结束 没有结束的操作\n\n\n# 阻塞队列实现等待唤醒机制\n\n\n# arrayblockingqueue\n\n底层是数组,有界,创建时通过带参构造方法,定义该阻塞队列的边界\n\n# put\n\nput("元素"),存储,底层有lock锁\n\n# take\n\ntake()取出,如果取不出则会一直等待,直到下一关元素put进阻塞队列\n\n\n# linkedblockingqueue\n\n底层是链表,无界.但不是真正的无界,默认最大为int的最大值,也可以通过带参构造方法,定义阻塞队列的边界',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"网络编程",frontmatter:{title:"网络编程",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/fc7fd4/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/61.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B.html",relativePath:"后端/01.JavaSE/61.网络编程.md",key:"v-74f764f4",path:"/pages/fc7fd4/",headers:[{level:2,title:"InetAddress",slug:"inetaddress",normalizedTitle:"inetaddress",charIndex:11},{level:2,title:"IP",slug:"ip",normalizedTitle:"ip",charIndex:142},{level:2,title:"端口",slug:"端口",normalizedTitle:"端口",charIndex:295},{level:2,title:"协议",slug:"协议",normalizedTitle:"协议",charIndex:349},{level:3,title:"UDP协议",slug:"udp协议",normalizedTitle:"udp协议",charIndex:378},{level:3,title:"TCP协议",slug:"tcp协议",normalizedTitle:"tcp协议",charIndex:472},{level:2,title:"UDP通信程序",slug:"udp通信程序",normalizedTitle:"udp通信程序",charIndex:559},{level:3,title:"发送端",slug:"发送端",normalizedTitle:"发送端",charIndex:571},{level:3,title:"接收端",slug:"接收端",normalizedTitle:"接收端",charIndex:1081},{level:3,title:"组播",slug:"组播",normalizedTitle:"组播",charIndex:1660},{level:3,title:"广播",slug:"广播",normalizedTitle:"广播",charIndex:2332},{level:2,title:"TCP通信程序",slug:"tcp通信程序",normalizedTitle:"tcp通信程序",charIndex:2437},{level:3,title:"客户端",slug:"客户端",normalizedTitle:"客户端",charIndex:1674},{level:3,title:"服务端",slug:"服务端",normalizedTitle:"服务端",charIndex:2872},{level:3,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:3280},{level:2,title:"三次握手",slug:"三次握手",normalizedTitle:"三次握手",charIndex:3335},{level:2,title:"四次挥手",slug:"四次挥手",normalizedTitle:"四次挥手",charIndex:3457},{level:2,title:"UUID",slug:"uuid",normalizedTitle:"uuid",charIndex:3623}],headersStr:"InetAddress IP 端口 协议 UDP协议 TCP协议 UDP通信程序 发送端 接收端 组播 广播 TCP通信程序 客户端 服务端 原理 三次握手 四次挥手 UUID",content:'# 网络编程\n\n\n# InetAddress\n\n * getByName(String host): 根据ip地址/主机名 返回一个InetAddress对象\n * getHostName(): 返回主机名\n * getHostAddress(): 以字符串形式返回ip地址\n\n\n# IP\n\nIPv4: 32bit (4字节) 点分十进制表示法\n\nIPV6: 128bit(8字节) 冒分十六进制表示法\n\n冒号后如果连为0可以省略 如 :0025 可以表示为:25\n\n如果中间有多个连续的冒号可以简写为2个冒号 如: FF01:0:0:0:0:1101 写为 FF01::1101\n\n\n# 端口\n\n端口:应用程序在设备中唯一标识\n\n端口号:两个字节表示的整数,取值范围 0 ~ 65535\n\n\n# 协议\n\n协议:连接和通信的规则被称为网络通信协议\n\n\n# UDP协议\n\n * 用户数据报协议(User Datagram Protocol)\n * UDP是面向无连接通信协议 连接快,有大小限制一次最多发生64k,数据不安全,易丢失数据\n\n\n# TCP协议\n\n * 传输控制协议(Transmission Control Protocol)\n * TCP协议是面向连接的通信协议 速度慢,没有大小限制,数据安全\n\n\n# UDP通信程序\n\n\n# 发送端\n\n * DatagramSocket() : 无参构造方法\n * DatagramPacket(byte[] 数据数组, 要传输数组的长度, 目标地址对象 , 端口号):带参构造方法 打包数据\n * send(DatagramPacket 打包的数组): 发送数据\n * close(): 关闭通信\n\n// upd发送\nDatagramSocket ds = new DatagramSocket();\n\n//数据值\nString s = "你好世界";\nbyte[] bytes = s.getBytes();\nInetAddress byName = InetAddress.getByName("127.0.0.1");\nint port = 10000;\n\n// 打包数据\nDatagramPacket dp = new DatagramPacket(bytes, bytes.length, byName, port);\n\n// 发送数据\nds.send(dp);\n\n// 关闭\nds.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 接收端\n\n * DatagramSocket(int 端口号) : 带参构造方法 接收数据的端口\n * DatagramPacket((byte[] 数据数组, 要接收数组的长度): 带参构造方法 接收容器\n * receive(DatagramPacket 接收容器): 接收数据 如果接收不数据会一直等待(阻塞)下面的代码不会执行\n * getData(): 获取要接收到的数据 返回为一个字节数组\n * getLength(): 获取要发送的数据的长度或接收到的数据的长度。\n * close(): 关闭通信\n\n// 接收端建立接收\nDatagramSocket ds =new DatagramSocket(10000);\n\n// 接收容器\nbyte[] bytes =new byte[1024];\nDatagramPacket dp =new DatagramPacket(bytes,bytes.length);\n\n// 接收数据\nds.receive(dp);\nint lenth = dp.getLength();\n\nSystem.out.println(new String(bytes,0,lenth));\n\n// 关闭通信\nds.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 组播\n\n上面1对1 服务器对客户端 称为单播\n\n组播又称多播 指特定某范围内客户端进行发送\n\n组播地址:224.0.0.0 ~ 239.255.255.255 其中 244.0.0.0 ~ 224.0.0.255 为预留的组播地址\n\n * 发送端 与 1对1 无异 注意getByname 为组播地址\n\n * 接收端 创建 MulticastSocket 对象\n   \n   * joinGroup(InetAddress 对象): 把当前客户端加入到组播组中\n\n * MulticastSocket ms = new MulticastSocket(10000);\n   \n   ms.joinGroup(InetAddress.getByName("224.1.1.1"));\n   \n   byte[] bytes =new byte[1024];\n   DatagramPacket dp =new DatagramPacket(bytes,bytes.length);\n   \n   \n   \n   ms.receive(dp);\n   int lenth = dp.getLength();\n   \n   System.out.println(new String(bytes,0,lenth));\n   \n   ms.close();\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   \n\n\n# 广播\n\n将数据发送到所有客户端\n\n广播地址:255.255.255.255\n\n * 发送端 与 1对1 无异 注意getByname 为广播地址\n * 接收端 与 1对1 无异 因为广播是无差别的发送\n\n\n# TCP通信程序\n\nTCP通信协议是一种可靠的网络协议,它在通信的两端各建立一个Socket对象\n\n通信之前要确保连接已经建立\n\n\n# 客户端\n\n * Socket(string host, int port): 带参构造方法\n * getOutputStream(): 获取一个IO流 返回一个OutputStream对象\n   * write(): 写入(传输)数据\n * close(): 在关闭流的时候 会给对方发送一个结束标记动作\n\nSocket sk =new Socket("127.0.0.1",10000);\n\nOutputStream outputStream = sk.getOutputStream();\n\noutputStream.write("hello".getBytes(StandardCharsets.UTF_8));\n\noutputStream.close();\nsk.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 服务端\n\n * ServerSocket(int port ) : 带参构造方法\n * accept(): 返回一个Socket对象 会有阻塞现象\n * getInputStream(): 获取输入流\n * read(): 读取流中的数据 返回值为一个int 会有阻塞现象\n\nServerSocket ss =new ServerSocket(10000);\n\nSocket accept = ss.accept();\n\nInputStream inputStream = accept.getInputStream();\nint b;\nwhile ((b = inputStream.read()) != -1){\n    System.out.println((char) b);\n}\n\naccept.close();\nss.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 原理\n\n 1. accept方法是阻塞的,作用是等待客户端连接\n 2. 客服端创建对象并连接服务器,此时是通过三次握手协议保证跟服务器之间的连接\n 3. 针对客户端来讲,是往外写,所以是输出流 针对服务器来讲,是往里读,所以是输入流\n 4. read方法也是阻塞\n 5. 在关闭流的时候,还多了一个往服务器写结束标记的动作\n 6. 最后一步断开连接,通过四次挥手协议保证连接终止\n\n\n# 三次握手\n\n\n\n 1. 向服务器发出连接请求\n 2. 服务器向客户端返回一个响应\n 3. 客户端再次向服务器发出确认信息\n\n\n# 四次挥手\n\n\n\n 1. 客户端向服务器发出取消请求\n 2. 服务器向客户端返回一个响应\n 3. 服务器向客户端发出确认取消信息\n 4. 客户端再次发送确认消息\n\n\n# UUID\n\njava提供UUID类 可以方便的生成uuid\n\n * randomUUID(): 伪随机 返回一个UUID对象\n * toString(): 将UUID对象转成String',normalizedContent:'# 网络编程\n\n\n# inetaddress\n\n * getbyname(string host): 根据ip地址/主机名 返回一个inetaddress对象\n * gethostname(): 返回主机名\n * gethostaddress(): 以字符串形式返回ip地址\n\n\n# ip\n\nipv4: 32bit (4字节) 点分十进制表示法\n\nipv6: 128bit(8字节) 冒分十六进制表示法\n\n冒号后如果连为0可以省略 如 :0025 可以表示为:25\n\n如果中间有多个连续的冒号可以简写为2个冒号 如: ff01:0:0:0:0:1101 写为 ff01::1101\n\n\n# 端口\n\n端口:应用程序在设备中唯一标识\n\n端口号:两个字节表示的整数,取值范围 0 ~ 65535\n\n\n# 协议\n\n协议:连接和通信的规则被称为网络通信协议\n\n\n# udp协议\n\n * 用户数据报协议(user datagram protocol)\n * udp是面向无连接通信协议 连接快,有大小限制一次最多发生64k,数据不安全,易丢失数据\n\n\n# tcp协议\n\n * 传输控制协议(transmission control protocol)\n * tcp协议是面向连接的通信协议 速度慢,没有大小限制,数据安全\n\n\n# udp通信程序\n\n\n# 发送端\n\n * datagramsocket() : 无参构造方法\n * datagrampacket(byte[] 数据数组, 要传输数组的长度, 目标地址对象 , 端口号):带参构造方法 打包数据\n * send(datagrampacket 打包的数组): 发送数据\n * close(): 关闭通信\n\n// upd发送\ndatagramsocket ds = new datagramsocket();\n\n//数据值\nstring s = "你好世界";\nbyte[] bytes = s.getbytes();\ninetaddress byname = inetaddress.getbyname("127.0.0.1");\nint port = 10000;\n\n// 打包数据\ndatagrampacket dp = new datagrampacket(bytes, bytes.length, byname, port);\n\n// 发送数据\nds.send(dp);\n\n// 关闭\nds.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 接收端\n\n * datagramsocket(int 端口号) : 带参构造方法 接收数据的端口\n * datagrampacket((byte[] 数据数组, 要接收数组的长度): 带参构造方法 接收容器\n * receive(datagrampacket 接收容器): 接收数据 如果接收不数据会一直等待(阻塞)下面的代码不会执行\n * getdata(): 获取要接收到的数据 返回为一个字节数组\n * getlength(): 获取要发送的数据的长度或接收到的数据的长度。\n * close(): 关闭通信\n\n// 接收端建立接收\ndatagramsocket ds =new datagramsocket(10000);\n\n// 接收容器\nbyte[] bytes =new byte[1024];\ndatagrampacket dp =new datagrampacket(bytes,bytes.length);\n\n// 接收数据\nds.receive(dp);\nint lenth = dp.getlength();\n\nsystem.out.println(new string(bytes,0,lenth));\n\n// 关闭通信\nds.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 组播\n\n上面1对1 服务器对客户端 称为单播\n\n组播又称多播 指特定某范围内客户端进行发送\n\n组播地址:224.0.0.0 ~ 239.255.255.255 其中 244.0.0.0 ~ 224.0.0.255 为预留的组播地址\n\n * 发送端 与 1对1 无异 注意getbyname 为组播地址\n\n * 接收端 创建 multicastsocket 对象\n   \n   * joingroup(inetaddress 对象): 把当前客户端加入到组播组中\n\n * multicastsocket ms = new multicastsocket(10000);\n   \n   ms.joingroup(inetaddress.getbyname("224.1.1.1"));\n   \n   byte[] bytes =new byte[1024];\n   datagrampacket dp =new datagrampacket(bytes,bytes.length);\n   \n   \n   \n   ms.receive(dp);\n   int lenth = dp.getlength();\n   \n   system.out.println(new string(bytes,0,lenth));\n   \n   ms.close();\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   \n\n\n# 广播\n\n将数据发送到所有客户端\n\n广播地址:255.255.255.255\n\n * 发送端 与 1对1 无异 注意getbyname 为广播地址\n * 接收端 与 1对1 无异 因为广播是无差别的发送\n\n\n# tcp通信程序\n\ntcp通信协议是一种可靠的网络协议,它在通信的两端各建立一个socket对象\n\n通信之前要确保连接已经建立\n\n\n# 客户端\n\n * socket(string host, int port): 带参构造方法\n * getoutputstream(): 获取一个io流 返回一个outputstream对象\n   * write(): 写入(传输)数据\n * close(): 在关闭流的时候 会给对方发送一个结束标记动作\n\nsocket sk =new socket("127.0.0.1",10000);\n\noutputstream outputstream = sk.getoutputstream();\n\noutputstream.write("hello".getbytes(standardcharsets.utf_8));\n\noutputstream.close();\nsk.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 服务端\n\n * serversocket(int port ) : 带参构造方法\n * accept(): 返回一个socket对象 会有阻塞现象\n * getinputstream(): 获取输入流\n * read(): 读取流中的数据 返回值为一个int 会有阻塞现象\n\nserversocket ss =new serversocket(10000);\n\nsocket accept = ss.accept();\n\ninputstream inputstream = accept.getinputstream();\nint b;\nwhile ((b = inputstream.read()) != -1){\n    system.out.println((char) b);\n}\n\naccept.close();\nss.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 原理\n\n 1. accept方法是阻塞的,作用是等待客户端连接\n 2. 客服端创建对象并连接服务器,此时是通过三次握手协议保证跟服务器之间的连接\n 3. 针对客户端来讲,是往外写,所以是输出流 针对服务器来讲,是往里读,所以是输入流\n 4. read方法也是阻塞\n 5. 在关闭流的时候,还多了一个往服务器写结束标记的动作\n 6. 最后一步断开连接,通过四次挥手协议保证连接终止\n\n\n# 三次握手\n\n\n\n 1. 向服务器发出连接请求\n 2. 服务器向客户端返回一个响应\n 3. 客户端再次向服务器发出确认信息\n\n\n# 四次挥手\n\n\n\n 1. 客户端向服务器发出取消请求\n 2. 服务器向客户端返回一个响应\n 3. 服务器向客户端发出确认取消信息\n 4. 客户端再次发送确认消息\n\n\n# uuid\n\njava提供uuid类 可以方便的生成uuid\n\n * randomuuid(): 伪随机 返回一个uuid对象\n * tostring(): 将uuid对象转成string',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"反射",frontmatter:{title:"反射",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/b3e612/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/63.%E5%8F%8D%E5%B0%84.html",relativePath:"后端/01.JavaSE/63.反射.md",key:"v-54b65e0e",path:"/pages/b3e612/",headers:[{level:2,title:"获取class类的对象",slug:"获取class类的对象",normalizedTitle:"获取class类的对象",charIndex:119},{level:3,title:"Class.forName()",slug:"class-forname",normalizedTitle:"class.forname()",charIndex:135},{level:3,title:"类名.class",slug:"类名-class",normalizedTitle:"类名.class",charIndex:265},{level:3,title:"对象.getclass()",slug:"对象-getclass",normalizedTitle:"对象.getclass()",charIndex:361},{level:2,title:"获取Constructor对象",slug:"获取constructor对象",normalizedTitle:"获取constructor对象",charIndex:490},{level:3,title:"getConstructors()",slug:"getconstructors",normalizedTitle:"getconstructors()",charIndex:545},{level:3,title:"getDeclaredConstructors()",slug:"getdeclaredconstructors",normalizedTitle:"getdeclaredconstructors()",charIndex:822},{level:3,title:"getConstructor()",slug:"getconstructor",normalizedTitle:"getconstructor()",charIndex:1068},{level:3,title:"getDeclaredConstructor()",slug:"getdeclaredconstructor",normalizedTitle:"getdeclaredconstructor()",charIndex:1254},{level:2,title:"newInstance() 创建对象",slug:"newinstance-创建对象",normalizedTitle:"newinstance() 创建对象",charIndex:1301},{level:3,title:"被parivate修饰的情况",slug:"被parivate修饰的情况",normalizedTitle:"被parivate修饰的情况",charIndex:1571},{level:2,title:"获取反射的成员变量",slug:"获取反射的成员变量",normalizedTitle:"获取反射的成员变量",charIndex:2121},{level:3,title:"赋值",slug:"赋值",normalizedTitle:"赋值",charIndex:2672},{level:2,title:"获取成员方法",slug:"获取成员方法",normalizedTitle:"获取成员方法",charIndex:2972}],headersStr:"获取class类的对象 Class.forName() 类名.class 对象.getclass() 获取Constructor对象 getConstructors() getDeclaredConstructors() getConstructor() getDeclaredConstructor() newInstance() 创建对象 被parivate修饰的情况 获取反射的成员变量 赋值 获取成员方法",content:'# 反射\n\n利用反射我们可以使用任意一个类中的,所有方法和属性,无视修饰符\n\n用反射创建对象,反射调用成员变量/方法\n\n利用反射可以无视修饰符获取类里面所有的属性和方法\n\n先获取配置文件中的信息,动态获取信息并创建对象和调用方法\n\n\n# 获取class类的对象\n\n\n# Class.forName()\n\nClass<?> aClass = Class.forName("笔记.反射.Studen");\nSystem.out.println(aClass);\n\n\n1\n2\n\n\n通过forName传递全类名(包名+类名)获取\n\n\n# 类名.class\n\nClass<Studen> studenClass = Studen.class;\nSystem.out.println(studenClass);\n\n\n1\n2\n\n\n\n# 对象.getclass()\n\nStuden s =new Studen();\nClass<? extends Studen> aClass1 = s.getClass();\nSystem.out.println(aClass1);\n\n\n1\n2\n3\n\n\n\n# 获取Constructor对象\n\nConstructor为构造方法对象,在反射中所有方法属性都为对象\n\n\n# getConstructors()\n\n该方法获取该class所有的公共构造方法\n\nClass<?> aClass = Class.forName("笔记.反射.Studen");\n// 获取该class所有的公共构造方法\nConstructor<?>[] constructors = aClass.getConstructors();\nfor (Constructor<?> constructor : constructors) {\n    System.out.println(constructor);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# getDeclaredConstructors()\n\n返回所有的构造方法,无视修饰符\n\nConstructor<?>[] declaredConstructors = aClass.getDeclaredConstructors();\nfor (Constructor<?> declaredConstructor : declaredConstructors) {\n    System.out.println(declaredConstructor);\n}\n\n\n1\n2\n3\n4\n\n\n\n# getConstructor()\n\n返回指定的公共构造方法,必须要与类中的构造方法形参一致,否则无法找到指定构造方法,并抛出异常\n\nConstructor<?> constructor = aClass.getConstructor(String.class,int.class);\nSystem.out.println(constructor);\n\n\n1\n2\n\n\n\n# getDeclaredConstructor()\n\n返回单个指定构造方法 无视修饰符\n\n\n# newInstance() 创建对象\n\n获取到构造方法后我们可以使用 newInstance() 来创建对象 该方法返回一个对象 默认为object类型,需要强转为类的类型\n\nStuden zhangsan = (Studen) declaredConstructor.newInstance("zhangsan", 23);\nSystem.out.println(zhangsan);\n\n\n1\n2\n\n\n如果构造方法为空参 我们可以直接在获取class后 的class对象直接newInstance(),该方法已经过时,不推荐使用\n\n\n# 被parivate修饰的情况\n\n如果该反射对象不是公共修饰符,则创建对象时会发生异常,需要临时取消访问检查\n\n * setAccessible(boolean ); 临时取消访问检查 暴力反射\n\n * // 获取class类\n   Class<?> aClass1 = Class.forName("笔记.反射.Studen");\n   \n   // 获取所有构造方法\n   Constructor<?> declaredConstructor1 = aClass1.getDeclaredConstructor(String.class);\n   \n   // 临时取消访问检查\n   declaredConstructor1.setAccessible(true);\n   \n   // 创建对象 并 强转成Studen类\n   Studen lisi = (Studen) declaredConstructor1.newInstance("lisi");\n   \n   System.out.println(lisi);\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   \n\n\n# 获取反射的成员变量\n\n * getFields(); 返回所有公共的成员变量对象的数组\n\n * getDeclaredFields(): 返回所有成员变量的数组 无视修饰符\n\n * getField(String name); 指定成员变量名字 返回单个公共成员变量对象\n\n * getDeclaredField(String name): 返回单个公共成员变量对象 无视修饰符\n\n * Class<?> aClass1 = Class.forName("笔记.反射.Studen");\n   \n   \n   Field[] fields = aClass1.getFields();\n   for (Field field : fields) {\n       System.out.println(field);\n   }\n   \n   Field age = aClass1.getDeclaredField("age");\n           System.out.println(age);\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   \n\n注意 获取的成员变量必须真实存在 否则会报错 并且注意修饰符\n\n\n# 赋值\n\n * set(object obj,object value): 赋值\n\n * get(object obj): 获取值\n\n * Field w = aClass1.getField("w");\n   \n   w.set(lisi,123);\n   System.out.println(w.get(lisi));\n   \n   \n   1\n   2\n   3\n   4\n   \n\n注意:\n\n 1. 对象参数传递 newinstance()方法创建的对象\n\n 2. 如果为私有属性 必须临时取消访问检查 setAccessible(boolean ) 先获取field对象再取消检查\n\n\n# 获取成员方法\n\n * getMethods(): 返回所有公共成员方法对象的数组,包括继承的\n\n * getDeclaredMethods(): 返回所有的成员方法对象的数组,不包括继承的方法\n\n * getMethod(String name,Class<?>... parmeterTypres): 返回指定名称的公共成员方法 Class为形参 必须传递类型的class对象\n\n * geDeclaredtMethod(String name,Class<?>... parmeterTypres): 返回指定名称的成员方法\n\n * Method setName = aClass1.getMethod("setName", String.class);\n   System.out.println(setName);\n   \n   \n   1\n   2\n   \n\n# 使用反射后方法\n\n * invoke(object obj,object args): 运行指定方法\n   \n   * 参数一: 用obj对象调用该方法 newinstance出来的对象\n   * 参数二:调用方法的传递参数(可省)\n\n * Method setName = aClass1.getMethod("setName", String.class);\n   \n   Studen lisi = (Studen) declaredConstructor1.newInstance("lisi");\n   \n   setName.invoke(lisi,"haha");\n   \n   \n   1\n   2\n   3\n   4\n   5\n   ',normalizedContent:'# 反射\n\n利用反射我们可以使用任意一个类中的,所有方法和属性,无视修饰符\n\n用反射创建对象,反射调用成员变量/方法\n\n利用反射可以无视修饰符获取类里面所有的属性和方法\n\n先获取配置文件中的信息,动态获取信息并创建对象和调用方法\n\n\n# 获取class类的对象\n\n\n# class.forname()\n\nclass<?> aclass = class.forname("笔记.反射.studen");\nsystem.out.println(aclass);\n\n\n1\n2\n\n\n通过forname传递全类名(包名+类名)获取\n\n\n# 类名.class\n\nclass<studen> studenclass = studen.class;\nsystem.out.println(studenclass);\n\n\n1\n2\n\n\n\n# 对象.getclass()\n\nstuden s =new studen();\nclass<? extends studen> aclass1 = s.getclass();\nsystem.out.println(aclass1);\n\n\n1\n2\n3\n\n\n\n# 获取constructor对象\n\nconstructor为构造方法对象,在反射中所有方法属性都为对象\n\n\n# getconstructors()\n\n该方法获取该class所有的公共构造方法\n\nclass<?> aclass = class.forname("笔记.反射.studen");\n// 获取该class所有的公共构造方法\nconstructor<?>[] constructors = aclass.getconstructors();\nfor (constructor<?> constructor : constructors) {\n    system.out.println(constructor);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# getdeclaredconstructors()\n\n返回所有的构造方法,无视修饰符\n\nconstructor<?>[] declaredconstructors = aclass.getdeclaredconstructors();\nfor (constructor<?> declaredconstructor : declaredconstructors) {\n    system.out.println(declaredconstructor);\n}\n\n\n1\n2\n3\n4\n\n\n\n# getconstructor()\n\n返回指定的公共构造方法,必须要与类中的构造方法形参一致,否则无法找到指定构造方法,并抛出异常\n\nconstructor<?> constructor = aclass.getconstructor(string.class,int.class);\nsystem.out.println(constructor);\n\n\n1\n2\n\n\n\n# getdeclaredconstructor()\n\n返回单个指定构造方法 无视修饰符\n\n\n# newinstance() 创建对象\n\n获取到构造方法后我们可以使用 newinstance() 来创建对象 该方法返回一个对象 默认为object类型,需要强转为类的类型\n\nstuden zhangsan = (studen) declaredconstructor.newinstance("zhangsan", 23);\nsystem.out.println(zhangsan);\n\n\n1\n2\n\n\n如果构造方法为空参 我们可以直接在获取class后 的class对象直接newinstance(),该方法已经过时,不推荐使用\n\n\n# 被parivate修饰的情况\n\n如果该反射对象不是公共修饰符,则创建对象时会发生异常,需要临时取消访问检查\n\n * setaccessible(boolean ); 临时取消访问检查 暴力反射\n\n * // 获取class类\n   class<?> aclass1 = class.forname("笔记.反射.studen");\n   \n   // 获取所有构造方法\n   constructor<?> declaredconstructor1 = aclass1.getdeclaredconstructor(string.class);\n   \n   // 临时取消访问检查\n   declaredconstructor1.setaccessible(true);\n   \n   // 创建对象 并 强转成studen类\n   studen lisi = (studen) declaredconstructor1.newinstance("lisi");\n   \n   system.out.println(lisi);\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   \n\n\n# 获取反射的成员变量\n\n * getfields(); 返回所有公共的成员变量对象的数组\n\n * getdeclaredfields(): 返回所有成员变量的数组 无视修饰符\n\n * getfield(string name); 指定成员变量名字 返回单个公共成员变量对象\n\n * getdeclaredfield(string name): 返回单个公共成员变量对象 无视修饰符\n\n * class<?> aclass1 = class.forname("笔记.反射.studen");\n   \n   \n   field[] fields = aclass1.getfields();\n   for (field field : fields) {\n       system.out.println(field);\n   }\n   \n   field age = aclass1.getdeclaredfield("age");\n           system.out.println(age);\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   \n\n注意 获取的成员变量必须真实存在 否则会报错 并且注意修饰符\n\n\n# 赋值\n\n * set(object obj,object value): 赋值\n\n * get(object obj): 获取值\n\n * field w = aclass1.getfield("w");\n   \n   w.set(lisi,123);\n   system.out.println(w.get(lisi));\n   \n   \n   1\n   2\n   3\n   4\n   \n\n注意:\n\n 1. 对象参数传递 newinstance()方法创建的对象\n\n 2. 如果为私有属性 必须临时取消访问检查 setaccessible(boolean ) 先获取field对象再取消检查\n\n\n# 获取成员方法\n\n * getmethods(): 返回所有公共成员方法对象的数组,包括继承的\n\n * getdeclaredmethods(): 返回所有的成员方法对象的数组,不包括继承的方法\n\n * getmethod(string name,class<?>... parmetertypres): 返回指定名称的公共成员方法 class为形参 必须传递类型的class对象\n\n * gedeclaredtmethod(string name,class<?>... parmetertypres): 返回指定名称的成员方法\n\n * method setname = aclass1.getmethod("setname", string.class);\n   system.out.println(setname);\n   \n   \n   1\n   2\n   \n\n# 使用反射后方法\n\n * invoke(object obj,object args): 运行指定方法\n   \n   * 参数一: 用obj对象调用该方法 newinstance出来的对象\n   * 参数二:调用方法的传递参数(可省)\n\n * method setname = aclass1.getmethod("setname", string.class);\n   \n   studen lisi = (studen) declaredconstructor1.newinstance("lisi");\n   \n   setname.invoke(lisi,"haha");\n   \n   \n   1\n   2\n   3\n   4\n   5\n   ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"类加载器",frontmatter:{title:"类加载器",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/a33e90/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/62.%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8.html",relativePath:"后端/01.JavaSE/62.类加载器.md",key:"v-3769da1c",path:"/pages/a33e90/",headers:[{level:2,title:"类加载时机",slug:"类加载时机",normalizedTitle:"类加载时机",charIndex:36},{level:2,title:"类加载的过程",slug:"类加载的过程",normalizedTitle:"类加载的过程",charIndex:196},{level:2,title:"类加载器的分类",slug:"类加载器的分类",normalizedTitle:"类加载器的分类",charIndex:217},{level:2,title:"双亲委派模型",slug:"双亲委派模型",normalizedTitle:"双亲委派模型",charIndex:309},{level:2,title:"ClassLoader",slug:"classloader",normalizedTitle:"classloader",charIndex:322}],headersStr:"类加载时机 类加载的过程 类加载器的分类 双亲委派模型 ClassLoader",content:"# 类加载器\n\n类加载器:负责将.class文件 加载到内存中\n\n\n# 类加载时机\n\n 1. 创建类的实例(对象)\n 2. 调用类的类方法\n 3. 访问类或者接口的类变量,或者为该类变量赋值\n 4. 使用反射方式来强制创建某个类或者接口对应的java.lang.Class对象\n 5. 初始化某个类的子类\n 6. 直接使用java.exe命令来运行某个主类\n\n用到就加载,不用不加载\n\n\n# 类加载的过程\n\n\n\n\n\n\n\n\n\n\n\n\n# 类加载器的分类\n\n启动类加载器: 虚拟机内置的类加载器 底层C++编写的\n\n平台类加载器: 负责加载JDK中一些特殊的模块\n\n系统类加载器: 负责加载用户类路径上所指定的类库\n\n\n# 双亲委派模型\n\n\n\n\n# ClassLoader\n\n注意默认是项目的第一层src\n\n * getSystemClassLoader() : 获取系统类加载器 返回值为ClassLoader\n   \n   * getResourceAsStream(String name): 利用加载器去加载一个指定的文件 参数为路径 返回为字节流",normalizedContent:"# 类加载器\n\n类加载器:负责将.class文件 加载到内存中\n\n\n# 类加载时机\n\n 1. 创建类的实例(对象)\n 2. 调用类的类方法\n 3. 访问类或者接口的类变量,或者为该类变量赋值\n 4. 使用反射方式来强制创建某个类或者接口对应的java.lang.class对象\n 5. 初始化某个类的子类\n 6. 直接使用java.exe命令来运行某个主类\n\n用到就加载,不用不加载\n\n\n# 类加载的过程\n\n\n\n\n\n\n\n\n\n\n\n\n# 类加载器的分类\n\n启动类加载器: 虚拟机内置的类加载器 底层c++编写的\n\n平台类加载器: 负责加载jdk中一些特殊的模块\n\n系统类加载器: 负责加载用户类路径上所指定的类库\n\n\n# 双亲委派模型\n\n\n\n\n# classloader\n\n注意默认是项目的第一层src\n\n * getsystemclassloader() : 获取系统类加载器 返回值为classloader\n   \n   * getresourceasstream(string name): 利用加载器去加载一个指定的文件 参数为路径 返回为字节流",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"XML",frontmatter:{title:"XML",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/6bad64/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/64.XML.html",relativePath:"后端/01.JavaSE/64.XML.md",key:"v-6611a25e",path:"/pages/6bad64/",headers:[{level:2,title:"XML 标签",slug:"xml-标签",normalizedTitle:"xml 标签",charIndex:26},{level:2,title:"文档声明",slug:"文档声明",normalizedTitle:"文档声明",charIndex:195},{level:3,title:"CDATA",slug:"cdata",normalizedTitle:"cdata",charIndex:386},{level:2,title:"解析XML",slug:"解析xml",normalizedTitle:"解析xml",charIndex:437},{level:3,title:"DOM对象模型",slug:"dom对象模型",normalizedTitle:"dom对象模型",charIndex:447},{level:3,title:"DOM4J",slug:"dom4j",normalizedTitle:"dom4j",charIndex:521},{level:2,title:"约束",slug:"约束",normalizedTitle:"约束",charIndex:1368},{level:3,title:"DTD约束",slug:"dtd约束",normalizedTitle:"dtd约束",charIndex:1397},{level:3,title:"schema",slug:"schema",normalizedTitle:"schema",charIndex:1709}],headersStr:"XML 标签 文档声明 CDATA 解析XML DOM对象模型 DOM4J 约束 DTD约束 schema",content:'# XML\n\nXML是一种可扩展的标记语言\n\n\n# XML 标签\n\n * 标签由一对尖括号和合法标识符组成 如: <student>\n * 标签必须成对出现 开始标签与结束标签\n * 特殊标签可以不成对,但必须有结束标签 自闭标签\n * 标签中可以定义属性,属性和标签名之间空格隔开,属性值必须用引号引起来 如: html<student age="12"> </student>\n\n\n# 文档声明\n\n文档声明必须是第一行第一列\n\n<?xml version= "1.0" encoding= "UTF-8" standalone= "yes"?>\n\n\n1\n\n * version: 该属性必须存在\n * encoding: 可省\n * standalone: 可省 描述xml文件是否依赖其他的xml文件,取值为yes/no\n * 一个xml文档只有一个根标签\n\n\n# CDATA\n\n<![CDATA[ 内容 ]]>\n\n\n1\n\n\n该标签可以忽略特殊字符,直接显示\n\n\n# 解析XML\n\n\n# DOM对象模型\n\n就是把文档的各个组成部分看做成对应的对象\n\n会把xml文件全部加载到内存\n\n在内存中形成一个树形结构,再获取对应的值\n\n\n\n\n# DOM4J\n\nDom For Java 是一套XML的解析的API-dom4j\n\n// 获取解析器对象\nSAXReader saxReader = new SAXReader();\n// 解析加载xml文件\nDocument read = saxReader.read(new File("../java\\\\src\\\\笔记\\\\_xml\\\\xml\\\\student.xml"));\n\n// 读取根标签\nElement rootElement = read.getRootElement();\n\n// 获取根标签下 所有student标签\nList<Element> student = rootElement.elements("student");\nfor (Element element : student) {\n    // 获取student标签的属性\n    Attribute id = element.attribute("id");\n\n    //获取id的值\n    String value = id.getValue();\n    System.out.println(value);\n\n    //获取name标签的text文本\n    Element name = element.element("name");\n    String text = name.getText();\n    System.out.println(text);\n\n    //获取age\n    Element age = element.element("age");\n    String text1 = age.getText();\n    System.out.println(text1);\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 约束\n\n用来限定xml文件中可使用的标签以及属性\n\n\n# DTD约束\n\n\n\n# 引入DTD约束\n\n 1. 本地引用 在xml文件声明下面\n    \n    <!DOCTYPE 根元素名称 SYSTEM \'DTD配置文件路径\'>\n    \n    \n    1\n    \n\n 2. xml中引用\n    \n    <!DOCTYPE 根元素名称 [\n       DTD配置属性\n    ]>\n    \n    \n    1\n    2\n    3\n    \n\n 3. 网络中引入\n    \n    <!DOCTYPE 根元素名称 PUBLIC "DTD文件的名称" "DTD文件的url">\n    \n    \n    1\n    \n\n# 语法规则\n\n\n\n# 属性规则\n\n\n\n\n# schema\n\n\n\n\n\n# 引入\n\n\n\n# 定义属性\n\n',normalizedContent:'# xml\n\nxml是一种可扩展的标记语言\n\n\n# xml 标签\n\n * 标签由一对尖括号和合法标识符组成 如: <student>\n * 标签必须成对出现 开始标签与结束标签\n * 特殊标签可以不成对,但必须有结束标签 自闭标签\n * 标签中可以定义属性,属性和标签名之间空格隔开,属性值必须用引号引起来 如: html<student age="12"> </student>\n\n\n# 文档声明\n\n文档声明必须是第一行第一列\n\n<?xml version= "1.0" encoding= "utf-8" standalone= "yes"?>\n\n\n1\n\n * version: 该属性必须存在\n * encoding: 可省\n * standalone: 可省 描述xml文件是否依赖其他的xml文件,取值为yes/no\n * 一个xml文档只有一个根标签\n\n\n# cdata\n\n<![cdata[ 内容 ]]>\n\n\n1\n\n\n该标签可以忽略特殊字符,直接显示\n\n\n# 解析xml\n\n\n# dom对象模型\n\n就是把文档的各个组成部分看做成对应的对象\n\n会把xml文件全部加载到内存\n\n在内存中形成一个树形结构,再获取对应的值\n\n\n\n\n# dom4j\n\ndom for java 是一套xml的解析的api-dom4j\n\n// 获取解析器对象\nsaxreader saxreader = new saxreader();\n// 解析加载xml文件\ndocument read = saxreader.read(new file("../java\\\\src\\\\笔记\\\\_xml\\\\xml\\\\student.xml"));\n\n// 读取根标签\nelement rootelement = read.getrootelement();\n\n// 获取根标签下 所有student标签\nlist<element> student = rootelement.elements("student");\nfor (element element : student) {\n    // 获取student标签的属性\n    attribute id = element.attribute("id");\n\n    //获取id的值\n    string value = id.getvalue();\n    system.out.println(value);\n\n    //获取name标签的text文本\n    element name = element.element("name");\n    string text = name.gettext();\n    system.out.println(text);\n\n    //获取age\n    element age = element.element("age");\n    string text1 = age.gettext();\n    system.out.println(text1);\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 约束\n\n用来限定xml文件中可使用的标签以及属性\n\n\n# dtd约束\n\n\n\n# 引入dtd约束\n\n 1. 本地引用 在xml文件声明下面\n    \n    <!doctype 根元素名称 system \'dtd配置文件路径\'>\n    \n    \n    1\n    \n\n 2. xml中引用\n    \n    <!doctype 根元素名称 [\n       dtd配置属性\n    ]>\n    \n    \n    1\n    2\n    3\n    \n\n 3. 网络中引入\n    \n    <!doctype 根元素名称 public "dtd文件的名称" "dtd文件的url">\n    \n    \n    1\n    \n\n# 语法规则\n\n\n\n# 属性规则\n\n\n\n\n# schema\n\n\n\n\n\n# 引入\n\n\n\n# 定义属性\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"日志",frontmatter:{title:"日志",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/344d91/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/68.%E6%97%A5%E5%BF%97.html",relativePath:"后端/01.JavaSE/68.日志.md",key:"v-642b486a",path:"/pages/344d91/",headers:[{level:2,title:"记录器",slug:"记录器",normalizedTitle:"记录器",charIndex:115},{level:2,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:164},{level:3,title:"根Logger",slug:"根logger",normalizedTitle:"根logger",charIndex:222},{level:3,title:"appender",slug:"appender",normalizedTitle:"appender",charIndex:253},{level:3,title:"Layout",slug:"layout",normalizedTitle:"layout",charIndex:1030}],headersStr:"记录器 配置 根Logger appender Layout",content:"# 日志\n\n * Logger.getLogger(类 字节码): 使用log4j的api获取日志对象 不推荐使用\n * LoggerFactory.getLogger(类 字节码): 使用slf4j的api获取日志对象\n\n\n# 记录器\n\n * DEBUG\n * INFO\n * WARN\n * EEROR\n * FATAL\n\n配置规则:只输出级别不低于设定级别的日志信息\n\n\n# 配置\n\n创建 log4j.properties 文件\n\n\n# 根Logger\n\nlog4j.rootLogger=日志级别,appenderName1,appenderName2,...\n\n日志级别:OFF、FATAL、EEROR、WARN、INFO、DEBUG、ALL或者自定义级别\n\nappenderName1:指定日志信息要输出到哪里,可以同时指定多个输出目的地\n\n如:log4j.rootLogger=info,ca,fa\n\n\n# appender\n\norg.apache.log4j.ConsoleAppender:输出到控制台\n\norg.apache.log4j.FileAppender:输出到文件\n\n如:log4j.appender.ca=org.apache.log4j.ConsoleAppender\n\n# 控制台\n\n * ImmediateFlush=true: 所有消息都会被立即输出,默认为true,false不输出\n * Target=System.err: 默认值为System.out 在控制台打印时显示的颜色级别 err为红色警告 默认为黑色\n\n如:\n\nlog4j.appender.ca.ImmediateFlush=true\nlog4j.appender.ca.Target=System.out\n\n\n1\n2\n\n\n# 文件\n\n * ImmediateFlush=true: 所有消息都会被立即输出,默认为true,false不输出\n * Append=false: 默认值为true, true为将日志追加到文件中,false为覆盖写入\n * File=路径: 将信息输出到指定的logging.log4j文件中\n\n如:\n\nlog4j.appender.ca.ImmediateFlush=true\nlog4j.appender.ca.Append=true\nlog4j.appender.ca.File=D:/\n\n\n1\n2\n3\n\n\n\n# Layout\n\n * org.apache.log4j.PatternLayout: 自定义布局模式 比较常用\n   * ConversionPattern=%m%n : 自定义规则\n   * \n * org.apache.log4j.SimpleLayout: 包含日志信息的级别和信息字符串\n * org.apache.log4j.TTCCLayout: 包含日志产生时间 线程 类别 等信息\n\n如: log4j.appender.ca.layout=org.apache.log4j.PatternLayout",normalizedContent:"# 日志\n\n * logger.getlogger(类 字节码): 使用log4j的api获取日志对象 不推荐使用\n * loggerfactory.getlogger(类 字节码): 使用slf4j的api获取日志对象\n\n\n# 记录器\n\n * debug\n * info\n * warn\n * eeror\n * fatal\n\n配置规则:只输出级别不低于设定级别的日志信息\n\n\n# 配置\n\n创建 log4j.properties 文件\n\n\n# 根logger\n\nlog4j.rootlogger=日志级别,appendername1,appendername2,...\n\n日志级别:off、fatal、eeror、warn、info、debug、all或者自定义级别\n\nappendername1:指定日志信息要输出到哪里,可以同时指定多个输出目的地\n\n如:log4j.rootlogger=info,ca,fa\n\n\n# appender\n\norg.apache.log4j.consoleappender:输出到控制台\n\norg.apache.log4j.fileappender:输出到文件\n\n如:log4j.appender.ca=org.apache.log4j.consoleappender\n\n# 控制台\n\n * immediateflush=true: 所有消息都会被立即输出,默认为true,false不输出\n * target=system.err: 默认值为system.out 在控制台打印时显示的颜色级别 err为红色警告 默认为黑色\n\n如:\n\nlog4j.appender.ca.immediateflush=true\nlog4j.appender.ca.target=system.out\n\n\n1\n2\n\n\n# 文件\n\n * immediateflush=true: 所有消息都会被立即输出,默认为true,false不输出\n * append=false: 默认值为true, true为将日志追加到文件中,false为覆盖写入\n * file=路径: 将信息输出到指定的logging.log4j文件中\n\n如:\n\nlog4j.appender.ca.immediateflush=true\nlog4j.appender.ca.append=true\nlog4j.appender.ca.file=d:/\n\n\n1\n2\n3\n\n\n\n# layout\n\n * org.apache.log4j.patternlayout: 自定义布局模式 比较常用\n   * conversionpattern=%m%n : 自定义规则\n   * \n * org.apache.log4j.simplelayout: 包含日志信息的级别和信息字符串\n * org.apache.log4j.ttcclayout: 包含日志产生时间 线程 类别 等信息\n\n如: log4j.appender.ca.layout=org.apache.log4j.patternlayout",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"枚举",frontmatter:{title:"枚举",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/19426f/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/65.%E6%9E%9A%E4%B8%BE.html",relativePath:"后端/01.JavaSE/65.枚举.md",key:"v-29319666",path:"/pages/19426f/",headersStr:null,content:'# 枚举\n\n是指将遍历的值一一列出来,变量的值只限于列举出来的值的范围内\n\npublic enum s{\n    枚举项1,枚举项2;\n}\n\n\n1\n2\n3\n\n\n使用关键字enum创建一个enum\n\n\n\n * name(); 获取枚举项的名称\n\n * ordinal(); 返回枚举项在枚举类中的索引值\n\n * compareTO(E o); 比较两个枚举项,返回的是索引值的差值\n\n * toString(); 返回枚举常量的名称\n\n * valueOf(class<T> type,String name); 获取指定枚举类中的指定名称的枚举值\n   \n   * Enum.valueOf(s.class,"枚举项名称")\n     \n     \n     1\n     \n\n * values(); 获取该枚举类中的所有枚举项',normalizedContent:'# 枚举\n\n是指将遍历的值一一列出来,变量的值只限于列举出来的值的范围内\n\npublic enum s{\n    枚举项1,枚举项2;\n}\n\n\n1\n2\n3\n\n\n使用关键字enum创建一个enum\n\n\n\n * name(); 获取枚举项的名称\n\n * ordinal(); 返回枚举项在枚举类中的索引值\n\n * compareto(e o); 比较两个枚举项,返回的是索引值的差值\n\n * tostring(); 返回枚举常量的名称\n\n * valueof(class<t> type,string name); 获取指定枚举类中的指定名称的枚举值\n   \n   * enum.valueof(s.class,"枚举项名称")\n     \n     \n     1\n     \n\n * values(); 获取该枚举类中的所有枚举项',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"单元测试",frontmatter:{title:"单元测试",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/7ba344/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/67.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95.html",relativePath:"后端/01.JavaSE/67.单元测试.md",key:"v-ea37ced6",path:"/pages/7ba344/",headersStr:null,content:"# 单元测试\n\n * 我们借用Junit工具使用,将junit的jar包导入到工程中\n * 编写测试方法必须是公共的无参数无返回值的非静态方法\n * 通过@Test注解 来标记该方法是测试方法\n   * @Test: 测试该方法\n   * @Before: 在测试的方法前运行\n   * @After: 在测试的方法后运行\n * 右键通过junit运行该方法",normalizedContent:"# 单元测试\n\n * 我们借用junit工具使用,将junit的jar包导入到工程中\n * 编写测试方法必须是公共的无参数无返回值的非静态方法\n * 通过@test注解 来标记该方法是测试方法\n   * @test: 测试该方法\n   * @before: 在测试的方法前运行\n   * @after: 在测试的方法后运行\n * 右键通过junit运行该方法",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"HTTP协议",frontmatter:{title:"HTTP协议",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/a7a164/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/69.HTTP%E5%8D%8F%E8%AE%AE.html",relativePath:"后端/01.JavaSE/69.HTTP协议.md",key:"v-068038d1",path:"/pages/a7a164/",headers:[{level:2,title:"请求组成部分",slug:"请求组成部分",normalizedTitle:"请求组成部分",charIndex:13},{level:2,title:"请求方式",slug:"请求方式",normalizedTitle:"请求方式",charIndex:33},{level:2,title:"响应的组成部分",slug:"响应的组成部分",normalizedTitle:"响应的组成部分",charIndex:163}],headersStr:"请求组成部分 请求方式 响应的组成部分",content:"# HTTP协议\n\n\n# 请求组成部分\n\n * 请求行\n   * 请求方式 提交路径(提交参数) HTTP/版本\n * 请求头\n   * \n * 请求空行\n   * 普通换行 区别请求头和请求体\n * 请求体\n   * 只有post请求才有请求体 ,用于显示提交参数\n\n\n# 请求方式\n\n * GET\n * POST\n\n\n# 响应的组成部分\n\n * 响应行\n   * 请求方式 HTTP/版本号 状态码 状态描述\n   * \n * 响应头\n   * \n * 响应空行\n   * 普通换行 区分响应头和响应体\n * 响应体\n   * 将资源文件发送给客户端浏览器进行解析",normalizedContent:"# http协议\n\n\n# 请求组成部分\n\n * 请求行\n   * 请求方式 提交路径(提交参数) http/版本\n * 请求头\n   * \n * 请求空行\n   * 普通换行 区别请求头和请求体\n * 请求体\n   * 只有post请求才有请求体 ,用于显示提交参数\n\n\n# 请求方式\n\n * get\n * post\n\n\n# 响应的组成部分\n\n * 响应行\n   * 请求方式 http/版本号 状态码 状态描述\n   * \n * 响应头\n   * \n * 响应空行\n   * 普通换行 区分响应头和响应体\n * 响应体\n   * 将资源文件发送给客户端浏览器进行解析",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"响应对象",frontmatter:{title:"响应对象",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/d35ebd/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/72.%E5%93%8D%E5%BA%94%E5%AF%B9%E8%B1%A1.html",relativePath:"后端/01.JavaSE/72.响应对象.md",key:"v-24b2db86",path:"/pages/d35ebd/",headers:[{level:2,title:"常见状态码",slug:"常见状态码",normalizedTitle:"常见状态码",charIndex:32},{level:2,title:"字节流响应消息",slug:"字节流响应消息",normalizedTitle:"字节流响应消息",charIndex:139},{level:2,title:"字符流响应消息",slug:"字符流响应消息",normalizedTitle:"字符流响应消息",charIndex:246},{level:2,title:"设置缓存",slug:"设置缓存",normalizedTitle:"设置缓存",charIndex:370},{level:2,title:"定时刷新",slug:"定时刷新",normalizedTitle:"定时刷新",charIndex:639},{level:2,title:"请求重定向",slug:"请求重定向",normalizedTitle:"请求重定向",charIndex:846},{level:2,title:"文件下载",slug:"文件下载",normalizedTitle:"文件下载",charIndex:1094}],headersStr:"常见状态码 字节流响应消息 字符流响应消息 设置缓存 定时刷新 请求重定向 文件下载",content:'# 响应对象\n\n回馈结果,服务器给客服端浏览器反馈结果\n\n\n# 常见状态码\n\n * 200 成功\n * 302 重定向\n * 304 请求资源未改变,使用缓存\n * 400 请求错误\n * 404 请求资源未找到\n * 405 请求方式不支持\n * 500 服务器错误\n\n\n# 字节流响应消息\n\n * getOutpitStream() 获取响应字节输出流对象\n * setContentType("text/html;charset=UTF-8") 设置响应内容类型 以及字符编码\n\n\n# 字符流响应消息\n\n * getWriter() 获取字符流输出对象\n   * write(obj) 写入数据给客户端\n * setContentType("text/html;charset=UTF-8") 设置响应内容类型 以及字符编码\n\n\n# 设置缓存\n\n对于不经常变化的数据,我可以设置合理的缓存时间,以避免浏览器频繁请求服务器\n\n * setDateHeader(String name,long time) 设置消息头添加缓存,时间单位毫秒\n   \n   * response.setDateHeader("Expires",System.currentTimeMillis() + 1*60*60*1000);\n     // Expires为过期时间关键字  time为当前时间 + 需要持续的时间\n     \n     \n     1\n     2\n     \n\n\n# 定时刷新\n\n过了指定时间后,页面自动进行跳转\n\n * setHeader(String name,String value) 设置消息头定时刷新\n   \n   * response.setHeader("Refresh","3;URL=/login.html")\n     // Refresh 刷新关键字   3为3s  URL为虚拟路径\n     \n     \n     1\n     2\n     \n\n\n# 请求重定向\n\n客户端的一次请求到达后,发现需要借助其他Servlet来实现功能\n\n与请求转发不一样,地址栏会改变,两次请求和请求域对象不能共享数据,可以重定向其他服务器\n\n * sendRedirect(String name) 设置重定向\n   \n   * response.sendRedirect(request.getContextPath() + "/demo02");\n     // 虚拟路径或者服务器URL\n     \n     \n     1\n     2\n     \n\n\n# 文件下载\n\n * setHeader(String name,String value)\n   \n   * response.setHeader("Content-Type","application/octet-stream")\n     //  设置响应头支持的类型 Content-Type消息头名称支持的类型\n     // application/octet-stream 消息头参数 应用的类型为字节流\n     response.setHeader("Content-Dispostition","attachment;filename=xxx.zip")\n     // 设置响应头以下载方式打开附件  \n     //  Content-Dispostition  处理形式\n     // attachment;filename=xxx.zip   以附件形式处理 并指定下载文件的名词\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     ',normalizedContent:'# 响应对象\n\n回馈结果,服务器给客服端浏览器反馈结果\n\n\n# 常见状态码\n\n * 200 成功\n * 302 重定向\n * 304 请求资源未改变,使用缓存\n * 400 请求错误\n * 404 请求资源未找到\n * 405 请求方式不支持\n * 500 服务器错误\n\n\n# 字节流响应消息\n\n * getoutpitstream() 获取响应字节输出流对象\n * setcontenttype("text/html;charset=utf-8") 设置响应内容类型 以及字符编码\n\n\n# 字符流响应消息\n\n * getwriter() 获取字符流输出对象\n   * write(obj) 写入数据给客户端\n * setcontenttype("text/html;charset=utf-8") 设置响应内容类型 以及字符编码\n\n\n# 设置缓存\n\n对于不经常变化的数据,我可以设置合理的缓存时间,以避免浏览器频繁请求服务器\n\n * setdateheader(string name,long time) 设置消息头添加缓存,时间单位毫秒\n   \n   * response.setdateheader("expires",system.currenttimemillis() + 1*60*60*1000);\n     // expires为过期时间关键字  time为当前时间 + 需要持续的时间\n     \n     \n     1\n     2\n     \n\n\n# 定时刷新\n\n过了指定时间后,页面自动进行跳转\n\n * setheader(string name,string value) 设置消息头定时刷新\n   \n   * response.setheader("refresh","3;url=/login.html")\n     // refresh 刷新关键字   3为3s  url为虚拟路径\n     \n     \n     1\n     2\n     \n\n\n# 请求重定向\n\n客户端的一次请求到达后,发现需要借助其他servlet来实现功能\n\n与请求转发不一样,地址栏会改变,两次请求和请求域对象不能共享数据,可以重定向其他服务器\n\n * sendredirect(string name) 设置重定向\n   \n   * response.sendredirect(request.getcontextpath() + "/demo02");\n     // 虚拟路径或者服务器url\n     \n     \n     1\n     2\n     \n\n\n# 文件下载\n\n * setheader(string name,string value)\n   \n   * response.setheader("content-type","application/octet-stream")\n     //  设置响应头支持的类型 content-type消息头名称支持的类型\n     // application/octet-stream 消息头参数 应用的类型为字节流\n     response.setheader("content-dispostition","attachment;filename=xxx.zip")\n     // 设置响应头以下载方式打开附件  \n     //  content-dispostition  处理形式\n     // attachment;filename=xxx.zip   以附件形式处理 并指定下载文件的名词\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"注解",frontmatter:{title:"注解",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/1bf317/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/66.%E6%B3%A8%E8%A7%A3.html",relativePath:"后端/01.JavaSE/66.注解.md",key:"v-0c6ac868",path:"/pages/1bf317/",headers:[{level:2,title:"自定义注解",slug:"自定义注解",normalizedTitle:"自定义注解",charIndex:106},{level:2,title:"元注解",slug:"元注解",normalizedTitle:"元注解",charIndex:319},{level:2,title:"获取方法中是否存在指定注解",slug:"获取方法中是否存在指定注解",normalizedTitle:"获取方法中是否存在指定注解",charIndex:609}],headersStr:"自定义注解 元注解 获取方法中是否存在指定注解",content:"# 注解\n\n作用:对我们的程序进行标注和解释\n\n * @Override: 描述子类重写父类的方法\n * @Deprecated: 描述方法已过时\n * @SuppressWarinings: 压制警告\n\n\n# 自定义注解\n\npublic @interface 注解名称{\n    // 类型:基本数据类型 String Class 注解 枚举 以上类型的一维数组如 int数组 枚举数组...等等\n    public 属性类型 属性名 () default 默认值;\n}\n\n\n1\n2\n3\n4\n\n\n如没有给出默认值,则在使用该注解时要传递值\n\n如果该注解只有一个value注解没有默认值,则使用该注解时传递参数可以省略name\n\n\n# 元注解\n\n描述注解的注解\n\n * @Target: 指定了注解能在哪里使用\n   * ElementType.FIELD: 成员变量\n   * ElementType.TYPE: 类\n   * ElementType.METHOD: 方法\n * @Retention: 注解的生命周期(保留时间)\n   * RetentionPolicy.RUNTIME: 如果没有指定时间,则只能在java中存活,编译成class文件后注解消失\n * @Inherited: 表示修饰的自定义注解可以被子类继承\n * @Documented: 表示该自定义注解,会出现在API文档里面\n\n\n# 获取方法中是否存在指定注解\n\n * isAnnotationPresent(注解的字节码): 判断方法中是否有指定注解 返回布尔值 注意使用改方法判断时,注解要设置生命周期保留到运行期间 @Retention(RetentionPolicy.RUNTIME)",normalizedContent:"# 注解\n\n作用:对我们的程序进行标注和解释\n\n * @override: 描述子类重写父类的方法\n * @deprecated: 描述方法已过时\n * @suppresswarinings: 压制警告\n\n\n# 自定义注解\n\npublic @interface 注解名称{\n    // 类型:基本数据类型 string class 注解 枚举 以上类型的一维数组如 int数组 枚举数组...等等\n    public 属性类型 属性名 () default 默认值;\n}\n\n\n1\n2\n3\n4\n\n\n如没有给出默认值,则在使用该注解时要传递值\n\n如果该注解只有一个value注解没有默认值,则使用该注解时传递参数可以省略name\n\n\n# 元注解\n\n描述注解的注解\n\n * @target: 指定了注解能在哪里使用\n   * elementtype.field: 成员变量\n   * elementtype.type: 类\n   * elementtype.method: 方法\n * @retention: 注解的生命周期(保留时间)\n   * retentionpolicy.runtime: 如果没有指定时间,则只能在java中存活,编译成class文件后注解消失\n * @inherited: 表示修饰的自定义注解可以被子类继承\n * @documented: 表示该自定义注解,会出现在api文档里面\n\n\n# 获取方法中是否存在指定注解\n\n * isannotationpresent(注解的字节码): 判断方法中是否有指定注解 返回布尔值 注意使用改方法判断时,注解要设置生命周期保留到运行期间 @retention(retentionpolicy.runtime)",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"请求对象",frontmatter:{title:"请求对象",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/83bc98/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/71.%E8%AF%B7%E6%B1%82%E5%AF%B9%E8%B1%A1.html",relativePath:"后端/01.JavaSE/71.请求对象.md",key:"v-3c977991",path:"/pages/83bc98/",headers:[{level:2,title:"获取路径",slug:"获取路径",normalizedTitle:"获取路径",charIndex:42},{level:2,title:"获取请求头",slug:"获取请求头",normalizedTitle:"获取请求头",charIndex:230},{level:2,title:"获取请求参数信息",slug:"获取请求参数信息",normalizedTitle:"获取请求参数信息",charIndex:358},{level:2,title:"获取请求参数并封装为对象",slug:"获取请求参数并封装为对象",normalizedTitle:"获取请求参数并封装为对象",charIndex:518},{level:2,title:"通过流对象获取请求信息",slug:"通过流对象获取请求信息",normalizedTitle:"通过流对象获取请求信息",charIndex:730},{level:2,title:"中文乱码问题",slug:"中文乱码问题",normalizedTitle:"中文乱码问题",charIndex:811},{level:2,title:"请求域",slug:"请求域",normalizedTitle:"请求域",charIndex:962},{level:3,title:"请求转发",slug:"请求转发",normalizedTitle:"请求转发",charIndex:999},{level:3,title:"请求包含",slug:"请求包含",normalizedTitle:"请求包含",charIndex:1341}],headersStr:"获取路径 获取请求头 获取请求参数信息 获取请求参数并封装为对象 通过流对象获取请求信息 中文乱码问题 请求域 请求转发 请求包含",content:'# 请求对象\n\n获取资源,在BS 架构中就是客户端浏览器向服务器端发出询问\n\n\n# 获取路径\n\n * getContextPath() 获取虚拟目录名称\n * getServletPath() 获取Servlet映射路径\n * getRemoteAddr() 获取访问者ip\n * getQueryString() 获取请求的消息数据\n * getRequestURI() 获取统一资源标识符\n * getRequestURL() 获取统一资源定位符\n\n\n# 获取请求头\n\n * getHeader(string name) 根据请求头名称获取值\n * getHeaders(string name) 根据请求头名称获取多个值 返回枚举\n * getHeaderNames() 获取所有请求头名称 返回枚举\n\n\n# 获取请求参数信息\n\n * getParameter(string name) 根据名称获取数据\n * getParameterValues(String name) 根据名称获取所有数据\n * getParameterNames() 获取所有名称\n * getParameterMap() 获取所有参数的键值对\n\n\n# 获取请求参数并封装为对象\n\n * 手动封装对象 new一个\n * 反射封装对象\n   * pd = new PropertyDescriptor(name,类对象.class) 根据name获取该对象的成员变量方法\n   * pd.getWriteMethod().invoke(类对象,传递的参数)\n * 工具类封装对象 Beanutils\n   * Beanutils.popilate(类对象,传递的参数);\n\n\n# 通过流对象获取请求信息\n\n * 只支持post请求\n * getReader() 获取字符输入流\n * getInputStream() 获取字节输入流\n\n\n# 中文乱码问题\n\n * GET 方式 在Tomcat8已经解决\n\n * POST 通过 setChararerEnocding("UTF-8")解决\n   \n   * request.setCharacterEncoding("UTF-8");\n     \n     \n     1\n     \n\n\n# 请求域\n\n又称request域 可以在一次请求范围内进行共享数据,一般用于请求转发的多个资源中共享数据\n\n * setAttribute(String name , object vlaue) 向请求域对象中存储数据\n * getAttribute(string name) 根据名称获取请求域的数据\n * removeAttribute(String name) 根据名称删除请求域的数据\n\n\n# 请求转发\n\n客服端的请求到达后,发现需要其他Servlet来实现功能,则需要转发数据\n\n * getRequestDispatcher(String name 虚拟路径) 获取请求调度对象\n   \n   * forward(HttpServletRequest request, HttpServletResponse response) 实现转发\n\n\n# 请求包含\n\n可以合并其他Servlet中的功能一起响应给客户端\n\n * getRequestDispatcher(String name 虚拟路径) 获取请求调度对象\n   * include(HttpServletRequest request, HttpServletResponse response) 实现包含',normalizedContent:'# 请求对象\n\n获取资源,在bs 架构中就是客户端浏览器向服务器端发出询问\n\n\n# 获取路径\n\n * getcontextpath() 获取虚拟目录名称\n * getservletpath() 获取servlet映射路径\n * getremoteaddr() 获取访问者ip\n * getquerystring() 获取请求的消息数据\n * getrequesturi() 获取统一资源标识符\n * getrequesturl() 获取统一资源定位符\n\n\n# 获取请求头\n\n * getheader(string name) 根据请求头名称获取值\n * getheaders(string name) 根据请求头名称获取多个值 返回枚举\n * getheadernames() 获取所有请求头名称 返回枚举\n\n\n# 获取请求参数信息\n\n * getparameter(string name) 根据名称获取数据\n * getparametervalues(string name) 根据名称获取所有数据\n * getparameternames() 获取所有名称\n * getparametermap() 获取所有参数的键值对\n\n\n# 获取请求参数并封装为对象\n\n * 手动封装对象 new一个\n * 反射封装对象\n   * pd = new propertydescriptor(name,类对象.class) 根据name获取该对象的成员变量方法\n   * pd.getwritemethod().invoke(类对象,传递的参数)\n * 工具类封装对象 beanutils\n   * beanutils.popilate(类对象,传递的参数);\n\n\n# 通过流对象获取请求信息\n\n * 只支持post请求\n * getreader() 获取字符输入流\n * getinputstream() 获取字节输入流\n\n\n# 中文乱码问题\n\n * get 方式 在tomcat8已经解决\n\n * post 通过 setchararerenocding("utf-8")解决\n   \n   * request.setcharacterencoding("utf-8");\n     \n     \n     1\n     \n\n\n# 请求域\n\n又称request域 可以在一次请求范围内进行共享数据,一般用于请求转发的多个资源中共享数据\n\n * setattribute(string name , object vlaue) 向请求域对象中存储数据\n * getattribute(string name) 根据名称获取请求域的数据\n * removeattribute(string name) 根据名称删除请求域的数据\n\n\n# 请求转发\n\n客服端的请求到达后,发现需要其他servlet来实现功能,则需要转发数据\n\n * getrequestdispatcher(string name 虚拟路径) 获取请求调度对象\n   \n   * forward(httpservletrequest request, httpservletresponse response) 实现转发\n\n\n# 请求包含\n\n可以合并其他servlet中的功能一起响应给客户端\n\n * getrequestdispatcher(string name 虚拟路径) 获取请求调度对象\n   * include(httpservletrequest request, httpservletresponse response) 实现包含',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Cookie",frontmatter:{title:"Cookie",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/133dfe/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/73.Cookie.html",relativePath:"后端/01.JavaSE/73.Cookie.md",key:"v-5d9b9aeb",path:"/pages/133dfe/",headers:[{level:2,title:"添加和获取Cookie",slug:"添加和获取cookie",normalizedTitle:"添加和获取cookie",charIndex:135},{level:2,title:"规则",slug:"规则",normalizedTitle:"规则",charIndex:334}],headersStr:"添加和获取Cookie 规则",content:'# Cookie\n\n会话:浏览器和服务器之间的多次请求和响应\n\nCookie:客户端会话管理技术 把要共享的数据保存到客户端\n\n\n\n * Cookie(String name,String value) 创建Cookie对象\n * 成员变量的get和set方法\n\n\n# 添加和获取Cookie\n\n添加需要借助响应对象来添加\n\n * addCookie(Cookie cookie) 向客户端添加cookie\n\nresponse.addCookie(new Cookie("name","value"));\n\n\n1\n\n\n获取需要借助请求对象来获取\n\n * getCookies() 获取所有的cookie\n\nrequest.getCookies();\n\n\n1\n\n\n\n# 规则\n\n * 数量限制\n   * 每个网站最多只能有20个Cookie,并且大小不能超4KB 所有网站Cookie总数不能超300个\n * 命名限制\n   * 名称只能保护ASCCI码字母 数字字符 不能有逗号 分号 空格 和 以$开头\n   * 值不支持中文\n * 存活时间限制\n   * 负数 当前会话有效 浏览器关闭则清除\n   * 0 立即清除\n   * 正整数 以秒为单位设置的存活时间\n * 访问路径限制\n   * 取自第一访问资源路径前缀 只要以这个路径开头就可以访问到cookie\n   * 自定义路径: setPath() 设置指定路径',normalizedContent:'# cookie\n\n会话:浏览器和服务器之间的多次请求和响应\n\ncookie:客户端会话管理技术 把要共享的数据保存到客户端\n\n\n\n * cookie(string name,string value) 创建cookie对象\n * 成员变量的get和set方法\n\n\n# 添加和获取cookie\n\n添加需要借助响应对象来添加\n\n * addcookie(cookie cookie) 向客户端添加cookie\n\nresponse.addcookie(new cookie("name","value"));\n\n\n1\n\n\n获取需要借助请求对象来获取\n\n * getcookies() 获取所有的cookie\n\nrequest.getcookies();\n\n\n1\n\n\n\n# 规则\n\n * 数量限制\n   * 每个网站最多只能有20个cookie,并且大小不能超4kb 所有网站cookie总数不能超300个\n * 命名限制\n   * 名称只能保护ascci码字母 数字字符 不能有逗号 分号 空格 和 以$开头\n   * 值不支持中文\n * 存活时间限制\n   * 负数 当前会话有效 浏览器关闭则清除\n   * 0 立即清除\n   * 正整数 以秒为单位设置的存活时间\n * 访问路径限制\n   * 取自第一访问资源路径前缀 只要以这个路径开头就可以访问到cookie\n   * 自定义路径: setpath() 设置指定路径',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Servlet",frontmatter:{title:"Servlet",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/9c9dd6/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/70.Servlet.html",relativePath:"后端/01.JavaSE/70.Servlet.md",key:"v-7703b171",path:"/pages/9c9dd6/",headers:[{level:2,title:"继承GenericServlet",slug:"继承genericservlet",normalizedTitle:"继承genericservlet",charIndex:14},{level:2,title:"继承 HttpServlet",slug:"继承-httpservlet",normalizedTitle:"继承 httpservlet",charIndex:667},{level:2,title:"生命周期",slug:"生命周期",normalizedTitle:"生命周期",charIndex:781},{level:2,title:"线程安全问题",slug:"线程安全问题",normalizedTitle:"线程安全问题",charIndex:841},{level:2,title:"映射方式",slug:"映射方式",normalizedTitle:"映射方式",charIndex:958},{level:2,title:"创建时机",slug:"创建时机",normalizedTitle:"创建时机",charIndex:1161},{level:2,title:"ServletConfig",slug:"servletconfig",normalizedTitle:"servletconfig",charIndex:1650},{level:3,title:"获取方法",slug:"获取方法",normalizedTitle:"获取方法",charIndex:2039},{level:2,title:"ServletContext",slug:"servletcontext",normalizedTitle:"servletcontext",charIndex:2374},{level:3,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:84},{level:3,title:"方法",slug:"方法",normalizedTitle:"方法",charIndex:68},{level:3,title:"注解",slug:"注解",normalizedTitle:"注解",charIndex:3055},{level:3,title:"手动创建容器",slug:"手动创建容器",normalizedTitle:"手动创建容器",charIndex:3266}],headersStr:"继承GenericServlet 继承 HttpServlet 生命周期 线程安全问题 映射方式 创建时机 ServletConfig 获取方法 ServletContext 配置 方法 注解 手动创建容器",content:'# Servlet\n\n\n# 继承GenericServlet\n\n * 继承GenericServlet\n\n * 重写 serviece 方法\n\n * 在web.xml中配置 servlet\n   \n   * \x3c!--    声明--\x3e\n     <servlet>\n         \x3c!--        类名--\x3e\n         <servlet-name>servletDemo</servlet-name>\n         \x3c!--        类路径--\x3e\n         <servlet-class>com.example.demo.demo</servlet-class>\n     </servlet>\n     \x3c!--    映射--\x3e\n     <servlet-mapping>\n         \x3c!--    与类名一致--\x3e\n         <servlet-name>servletDemo</servlet-name>\n         \x3c!--        网页访问链接路径--\x3e\n         <url-pattern>/demo</url-pattern>\n     </servlet-mapping>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 重启项目\n\n\n# 继承 HttpServlet\n\n * 继承HttpServlet\n * 重写 doGet 和 doPost 方法 (可以在doPost中调用doGet方法)\n * 在web.xml中配置 servlet\n * 重启项目\n\n\n# 生命周期\n\n * 出生 init()\n * 过程 doGet serviece\n * 销毁 destroy()\n\n\n# 线程安全问题\n\nserviece 采用是单例模式, 整个应用中只有一个实例对象,我们称为单例模式\n\n * 如果只是使用而不修改\n * 将变量级别降到 局部变量 在doget中\n * 使用 synchronized 同步代码块\n\n\n# 映射方式\n\n * 完整名称\n\n * /开头 + 通配符 以什么什么开头\n   \n   * <servlet-name>/demo/*</servlet-name>\n     \n     \n     1\n     \n\n * 通配符 + 固定结尾 以什么什么结尾\n   \n   * <servlet-name>*.qwq</servlet-name>\n     \n     \n     1\n     \n\n\n# 创建时机\n\n * 第一次访问时创建\n   * 减少对服务器内存的浪费,访问时才创建\n * 服务器加载时创建\n   * 提前创建好对象,提高首次执行的效率 对内容占用较大\n\n在web.xml中配置 servlet 的 load-on-startup\n\n<servlet>\n    <servlet-name>servletDemo</servlet-name>\n    <servlet-class>com.example.demo.demo</servlet-class>      \n    \x3c!-- 正整数为加载时创建值越大优先级越高  负数或不写为第一次访问时创建 --\x3e\n    <load-on-startup>1</load-on-startup>\n</servlet>\n<servlet-mapping>\n    <servlet-name>servletDemo</servlet-name>\n    <url-pattern>/demo</url-pattern>\n</servlet-mapping>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# ServletConfig\n\n在servlet标签中 通过init-param标签 来配置\n\n以键值对方式配置 param-name 和 param-value\n\n<servlet>\n    <servlet-name>servletDemo</servlet-name>\n    <servlet-class>com.example.demo.demo</servlet-class>\n    <init-param>\n        <param-name>encoding</param-name>\n        <param-value>UTF-8</param-value>\n    </init-param>\n    <load-on-startup>1</load-on-startup>\n</servlet>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 获取方法\n\n * 获取ServletConfig对象\n   \n   * private ServletConfig config;\n     // 在init方法中\n     public void init(ServletConfig config) { }\n     \n     \n     1\n     2\n     3\n     \n\n * getInitParameter(String name) 根据key获取value\n\n * getInitParmeterNames() 获取所有参数名称的枚举\n\n * getServletName() 获取Servlet,名称\n\n * getServltContext() 获取ServltContext对象\n\n\n# ServletContext\n\nServletContext是应用上下文对象(应用域对象).每个应用只有一个ServletContext对象\n\n\n# 配置\n\n在<web-app> 标签中 通过 <context-param>标签进行配置\n\n以键值对 <param-name> 和 <param-value> 进行配置\n\n<web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee">\n<context-param>\n        <param-name>key</param-name>\n        <param-value>value</param-value>\n    </context-param>\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 方法\n\n * getServletContext() 获取当前应用的ServletContext对象\n * getInitparameter(String name) 根据名称获取全局配置参数\n * getContextPath() 获取当前应用访问的虚拟目录\n * getRealPath(String path) 根据虚拟目录获取绝对路径\n * \n * setAttribute(String name,Object value) 向应用域对象中存储数据\n * getAttribute(string name) 根据名称获取应用域中的数据\n * removeAttribute(string name) 根据名称移除应用域中的数据\n\n\n# 注解\n\nServlet 3.0 支持 注解和XML配置, 要求tomcat 9 和 javaee8\n\n以前我们写一个类需要在xml中配置该类的路径、名称、链接\n\n * @WebServlet 注解配置Servlet\n\n * 在该类上面使用\n\n * @WebServlet(name = "helloServlet", value = "/hello-servlet")\n   \n   \n   1\n   \n\n * \n\n\n# 手动创建容器\n\n * 定义一个类 继承HttpServlet\n * 重写 doget和dopost方法\n * 定义一个类 实现 ServletContainerInitializer 接口\n * 在src目录下创建一个 META-INF 包\n * 在 META-INF 包 创建一个services 包\n * 在 services 包 下创建一个 javax.servlet.ServletContainerInitializer 的文件 无后缀文件\n * 文件中的内容为容器实现类的全类名\n * 在容器实现类中 的 onStarup 方法 中完成注册 Servlet\n   * 创建要注册的servlet对象\n   * res = serveletContext.addServlet(name,对象)\n   * res.addMapping(访问路径)\n * 重启项目',normalizedContent:'# servlet\n\n\n# 继承genericservlet\n\n * 继承genericservlet\n\n * 重写 serviece 方法\n\n * 在web.xml中配置 servlet\n   \n   * \x3c!--    声明--\x3e\n     <servlet>\n         \x3c!--        类名--\x3e\n         <servlet-name>servletdemo</servlet-name>\n         \x3c!--        类路径--\x3e\n         <servlet-class>com.example.demo.demo</servlet-class>\n     </servlet>\n     \x3c!--    映射--\x3e\n     <servlet-mapping>\n         \x3c!--    与类名一致--\x3e\n         <servlet-name>servletdemo</servlet-name>\n         \x3c!--        网页访问链接路径--\x3e\n         <url-pattern>/demo</url-pattern>\n     </servlet-mapping>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 重启项目\n\n\n# 继承 httpservlet\n\n * 继承httpservlet\n * 重写 doget 和 dopost 方法 (可以在dopost中调用doget方法)\n * 在web.xml中配置 servlet\n * 重启项目\n\n\n# 生命周期\n\n * 出生 init()\n * 过程 doget serviece\n * 销毁 destroy()\n\n\n# 线程安全问题\n\nserviece 采用是单例模式, 整个应用中只有一个实例对象,我们称为单例模式\n\n * 如果只是使用而不修改\n * 将变量级别降到 局部变量 在doget中\n * 使用 synchronized 同步代码块\n\n\n# 映射方式\n\n * 完整名称\n\n * /开头 + 通配符 以什么什么开头\n   \n   * <servlet-name>/demo/*</servlet-name>\n     \n     \n     1\n     \n\n * 通配符 + 固定结尾 以什么什么结尾\n   \n   * <servlet-name>*.qwq</servlet-name>\n     \n     \n     1\n     \n\n\n# 创建时机\n\n * 第一次访问时创建\n   * 减少对服务器内存的浪费,访问时才创建\n * 服务器加载时创建\n   * 提前创建好对象,提高首次执行的效率 对内容占用较大\n\n在web.xml中配置 servlet 的 load-on-startup\n\n<servlet>\n    <servlet-name>servletdemo</servlet-name>\n    <servlet-class>com.example.demo.demo</servlet-class>      \n    \x3c!-- 正整数为加载时创建值越大优先级越高  负数或不写为第一次访问时创建 --\x3e\n    <load-on-startup>1</load-on-startup>\n</servlet>\n<servlet-mapping>\n    <servlet-name>servletdemo</servlet-name>\n    <url-pattern>/demo</url-pattern>\n</servlet-mapping>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# servletconfig\n\n在servlet标签中 通过init-param标签 来配置\n\n以键值对方式配置 param-name 和 param-value\n\n<servlet>\n    <servlet-name>servletdemo</servlet-name>\n    <servlet-class>com.example.demo.demo</servlet-class>\n    <init-param>\n        <param-name>encoding</param-name>\n        <param-value>utf-8</param-value>\n    </init-param>\n    <load-on-startup>1</load-on-startup>\n</servlet>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 获取方法\n\n * 获取servletconfig对象\n   \n   * private servletconfig config;\n     // 在init方法中\n     public void init(servletconfig config) { }\n     \n     \n     1\n     2\n     3\n     \n\n * getinitparameter(string name) 根据key获取value\n\n * getinitparmeternames() 获取所有参数名称的枚举\n\n * getservletname() 获取servlet,名称\n\n * getservltcontext() 获取servltcontext对象\n\n\n# servletcontext\n\nservletcontext是应用上下文对象(应用域对象).每个应用只有一个servletcontext对象\n\n\n# 配置\n\n在<web-app> 标签中 通过 <context-param>标签进行配置\n\n以键值对 <param-name> 和 <param-value> 进行配置\n\n<web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee">\n<context-param>\n        <param-name>key</param-name>\n        <param-value>value</param-value>\n    </context-param>\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 方法\n\n * getservletcontext() 获取当前应用的servletcontext对象\n * getinitparameter(string name) 根据名称获取全局配置参数\n * getcontextpath() 获取当前应用访问的虚拟目录\n * getrealpath(string path) 根据虚拟目录获取绝对路径\n * \n * setattribute(string name,object value) 向应用域对象中存储数据\n * getattribute(string name) 根据名称获取应用域中的数据\n * removeattribute(string name) 根据名称移除应用域中的数据\n\n\n# 注解\n\nservlet 3.0 支持 注解和xml配置, 要求tomcat 9 和 javaee8\n\n以前我们写一个类需要在xml中配置该类的路径、名称、链接\n\n * @webservlet 注解配置servlet\n\n * 在该类上面使用\n\n * @webservlet(name = "helloservlet", value = "/hello-servlet")\n   \n   \n   1\n   \n\n * \n\n\n# 手动创建容器\n\n * 定义一个类 继承httpservlet\n * 重写 doget和dopost方法\n * 定义一个类 实现 servletcontainerinitializer 接口\n * 在src目录下创建一个 meta-inf 包\n * 在 meta-inf 包 创建一个services 包\n * 在 services 包 下创建一个 javax.servlet.servletcontainerinitializer 的文件 无后缀文件\n * 文件中的内容为容器实现类的全类名\n * 在容器实现类中 的 onstarup 方法 中完成注册 servlet\n   * 创建要注册的servlet对象\n   * res = serveletcontext.addservlet(name,对象)\n   * res.addmapping(访问路径)\n * 重启项目',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"JDBC",frontmatter:{title:"JDBC",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/544a3f/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/01.JDBC.html",relativePath:"后端/02.JavaEE/01.JDBC.md",key:"v-7a01132d",path:"/pages/544a3f/",headers:[{level:2,title:"DriverManager 连接",slug:"drivermanager-连接",normalizedTitle:"drivermanager 连接",charIndex:768},{level:2,title:"Connection 连接对象",slug:"connection-连接对象",normalizedTitle:"connection 连接对象",charIndex:999},{level:2,title:"Statement 执行语句",slug:"statement-执行语句",normalizedTitle:"statement 执行语句",charIndex:1224},{level:2,title:"ResultSet 结果",slug:"resultset-结果",normalizedTitle:"resultset 结果",charIndex:1423},{level:2,title:"工具类",slug:"工具类",normalizedTitle:"工具类",charIndex:1568},{level:2,title:"SQL注入攻击",slug:"sql注入攻击",normalizedTitle:"sql注入攻击",charIndex:4350},{level:2,title:"数据库连接池",slug:"数据库连接池",normalizedTitle:"数据库连接池",charIndex:4886},{level:2,title:"自定义数据库连接池",slug:"自定义数据库连接池",normalizedTitle:"自定义数据库连接池",charIndex:4950},{level:3,title:"归还连接",slug:"归还连接",normalizedTitle:"归还连接",charIndex:5759},{level:2,title:"动态代理",slug:"动态代理",normalizedTitle:"动态代理",charIndex:7110},{level:2,title:"以动态代理 规划连接",slug:"以动态代理-规划连接",normalizedTitle:"以动态代理 规划连接",charIndex:8274},{level:2,title:"开源数据库连接池",slug:"开源数据库连接池",normalizedTitle:"开源数据库连接池",charIndex:9208},{level:3,title:"C3P0",slug:"c3p0",normalizedTitle:"c3p0",charIndex:9221},{level:3,title:"Druid",slug:"druid",normalizedTitle:"druid",charIndex:9510},{level:2,title:"自定义JDBC框架",slug:"自定义jdbc框架",normalizedTitle:"自定义jdbc框架",charIndex:9957}],headersStr:"DriverManager 连接 Connection 连接对象 Statement 执行语句 ResultSet 结果 工具类 SQL注入攻击 数据库连接池 自定义数据库连接池 归还连接 动态代理 以动态代理 规划连接 开源数据库连接池 C3P0 Druid 自定义JDBC框架",content:'# JDBC\n\n * 导入mysql-connector-java.jar包\n\n * 注册驱动\n   \n   * Class.forName("com.mysql.jdbc.Driver");\n     \n     \n     1\n     \n\n * 建立连接\n   \n   *  Connection con = DriverManager.getConnection("jdbc:mysql://127.0.0.1:3306/mysql", "root", "123456");\n     \n     \n     1\n     \n\n * 获取对象\n   \n   * Statement stat = con.createStatement();\n     \n     \n     1\n     \n\n * 执行sql语句\n   \n   * String sql ="select * from help_relation";\n     ResultSet result = stat.executeQuery(sql);\n     \n     \n     1\n     2\n     \n\n * 处理结果\n   \n   *  while (result.next()){\n     System.out.println(result.getInt("help_topic_id")+"\\t"+ result.getString("help_keyword_id"));\n     }\n     \n     \n     1\n     2\n     3\n     \n\n * 关闭连接\n   \n   * con.close();\n      stat.close();\n     \n     \n     1\n     2\n     \n\n\n# DriverManager 连接\n\nDriverManager驱动管理对象\n\n * 注册驱动 registerDriver(Driver river) 注册给定的驱动 在上面注册中我们使用class.forname使用了一下Driver这个类,默认被使用就注册驱动 必须是mysql5之后才可以省略这个注册\n * 获取数据连接 getConnection(url , user , password) url为jdbc:mysql://ip:端口/库名\n\n\n# Connection 连接对象\n\nConnection数据库连接对象\n\n * 获取普通执行者对象 createStatement()\n * 获取预编译执行者对象 prepareStatement(String sql)\n * 开启事务 setAutoCommit(boolean autoCommit); 参数为false 则开启事务\n * 提交事务 commit()\n * 回滚事务 rollback()\n * 释放资源 close();\n\n\n# Statement 执行语句\n\nStatement执行sql语句的对象\n\n * 增删改 executeUpdate(String sql) 返回一个int 影响的行数 可以执行 insert、update、delete语句\n * 查询 executeQuery（String sql） 返回值resultset对象 封装查询的结果 可以执行select语句\n * 释放资源 close()\n\n\n# ResultSet 结果\n\nResultSet结果集对象\n\n * 判断结果是否还有数据 next() 有数据返回ture,并索引向下移动一行\n * get数据类型("列名") 返回指定列的指定数据类型结果 如getInt() getString()\n * 释放资源 close()\n\n\n# 工具类\n\n在src下创建config.properties 文件\n\ndriverClass=com.mysql.jdbc.Driver\nurl=jdbc:mysql://127.0.0.1:3306/mysql\nusername=root\npassword=123456\n\n\n1\n2\n3\n4\n\n\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.sql.*;\nimport java.util.Properties;\n\npublic class JDBCUtils {\n    private JDBCUtils() {\n    }\n\n    //2.声明需要的配置变量\n    private static String driverClass;\n    private static String url;\n    private static String usename;\n    private static String password;\n    private static Connection con;\n\n    //3.提供静态代码块 读取配置文件为变量赋值\n    static {\n        try {\n            InputStream is = JDBCUtils.class.getClassLoader().getResourceAsStream("config.properties");\n            Properties prop = new Properties();\n            prop.load(is);\n\n            driverClass = prop.getProperty("driverClass");\n            url = prop.getProperty("url");\n            usename = prop.getProperty("usename");\n            password = prop.getProperty("password");\n\n            //注册驱动\n\n            Class.forName(driverClass);\n\n\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n    }\n\n\n    public static Connection getConnection() {\n        try {\n            con = DriverManager.getConnection(url, usename, password);\n        } catch (SQLException throwables) {\n            throwables.printStackTrace();\n        }\n\n        return con;\n    }\n\n    public static void close(Connection con, Statement stat, ResultSet rs) {\n        if (con != null) {\n            try {\n                con.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n        }\n\n        if (stat != null) {\n            try {\n                stat.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n        }\n\n        if (rs != null) {\n            try {\n                rs.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n        }\n\n    }\n\n    public static void close(Connection con, Statement stat) {\n        if (con != null) {\n            try {\n                con.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n        }\n\n        if (stat != null) {\n            try {\n                stat.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n        }\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n\n\n其实就是封装一个连接和关闭工具类方法\n\n\n# SQL注入攻击\n\n在之前我们使用Statement对象执行sql语句并且sql语句是拼接字符串而成的 不安全\n\n为了解决这个问题我们使用 PreparedStatement预编译执行者对象 并且参数以?形式作为占位符\n\n为占位符 问号 赋值的方法: setXxx(参数1,参数2)\n\n * Xxx 为数据的类型\n * 参数1 为 ?的下标 从1开始\n * 参数2 为 ?的实际参数\n\n内置setObject()\n\nConnection con = DriverManager.getConnection("jdbc:mysql://127.0.0.1:3306/mysql", "root", "123456");\nString sql = "delete from  user where name = ?";\nPreparedStatement pstm = con.prepareStatement(sql);\npstm.setString(1,"张三");\n\n\n1\n2\n3\n4\n\n\n执行SQL语句\n\n * 执行 inser update delete 语句 executeUpdate();\n * 执行 select 语句 executeQuery();\n\n\n# 数据库连接池\n\n数据库连接池负责分配 管理 释放数据库连接 它允许程序重复使用一个现有的数据库连接 而不是重新建立一个\n\n\n# 自定义数据库连接池\n\n * 继承 DataSource 接口\n * 准备一个容器 用于保存多个连接对象\n * 定义静态代码块 获取多个连接对象并放入容器中\n * 重写getConnection()方法 用于获取一个连接对象\n * 定义一个getSize方法 获取连接池容器的大小\n\n// 准备一个容器  用于保存多个连接对象\nprivate static List<Connection> pool = Collections.synchronizedList(new ArrayList<>());\n\n//定义静态代码块  获取多个连接对象并放入容器中\nstatic {\n    for(int i=1;i<=10;i++){\n        Connection con = JDBCUtils.getConnection();\n        pool.add(con);\n\n    }\n}\n\n//重写getConnection()方法  用于获取一个连接对象\n@Override\npublic Connection getConnection() throws SQLException {\n    if(pool.size() > 0 ){\n        Connection con = pool.remove(0);\n        return con;\n    }else {\n        throw new RuntimeException("连接数据已用尽");\n    }\n}\n\n//定义一个getSize方法 获取连接池容器的大小\npublic int getSize(){\n    return pool.size();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 归还连接\n\n把连接对象归还给连接池\n\n# 继承方式\n\n打印通过连接池创建的连对象的 class\n\nSystem.out.println(con.getClass());\n//class com.mysql.cj.jdbc.ConnectionImpl\n\n\n1\n2\n\n\n接下定义一个类 继承 ConnectionImpl 类\n\npublic class return01 extends ConnectionImpl {\n    private Connection con;\n    private List<Connection> pool;\n\n\n    public return01(HostInfo hostInfo, Connection con, List<Connection> pool) throws SQLException {\n        super(hostInfo);\n        this.con = con;\n        this.pool = pool;\n    }\n\n    // 重写close方法 将连接对象 重写归还容器中\n    @Override\n    public void close() throws SQLException {\n        pool.add(con);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n但DriverManager 获取的还是 ConnectionImpl 这个对象, 继承这种方法行不通的\n\n# 装饰设计模式\n\n * 实现 Connection 接口\n\n// 定义连接对象和连接容器的成员变量\n    private Connection con;\n    private List<Connection> pool;\n\n    //通过有参构造方法为成员变量赋值\n    public return02(Connection con,List<Connection> pool){\n        this.con=con;\n        this.pool=pool;\n    }\n\n    // 重写close方法\n    @Override\n    public void close() throws SQLException {\n        pool.add(con);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n其他方法调用原本方法\n\n装饰设计模式就是 重写需要的方法 其他方法则调用原来类的方法\n\n# 适配器设计模式\n\n适配器设计模式 实现接口的所有方法 除了需要的方法 然后再建一个类 继承该实现类 再重新未实现的方法 (使用中间件)\n\n * 定义适配类 实现 Connection 接口 定义为一个抽象类\n * 定义连接对象的成员变量 Connection con\n * 通过有参构造方法赋值\n * 重写所有抽象方法 除了close\n * 继承适配类 成员变量 Connection 和 连接容器\n * 通过有参构造方法赋值\n * 重写close方法\n\n\n# 动态代理\n\n在不改变目标对象方法的情况下对方法进行增强\n\n被代理对象: 真实的对象\n\n代理对象: 内容中的一个对象\n\n代理对象必须和被代理对象实现相同的接口\n\n        Student st = new Student();\n\n    /*\n        参数1 为 类加载器   和被代理对象使用相同的类加载器\n        参数2 为 接口类型Class数组  和被代理对象使用相同的接口\n        参数3 为 代理规则 完成代理增强的功能\n     */\n        StudentInterface proxySt =(StudentInterface) Proxy.newProxyInstance(st.getClass().getClassLoader(), new Class[]{StudentInterface.class}, new InvocationHandler() {\n            /*\n                Student类中所有的方法都经过invoke方法\n                我们通过对方法继续判断\n             */\n            @Override\n            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n                //method为类执行某个方法   args为方法的实际参数\n                if (method.getName().equals("study")) {\n                    // 判断为某个方法\n                    System.out.println("hello world");\n                    return null;\n                } else {\n                    // 如不是则按原本方法执行\n                    return method.invoke(st,args);\n                }\n            }\n        });\n\n        // 代理对象执行方法\n        proxySt.eat("肉");\n        proxySt.study();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 以动态代理 规划连接\n\n //重写getConnection()方法  用于获取一个连接对象\n    @Override\n    public Connection getConnection() throws SQLException {\n        if (pool.size() > 0) {\n            Connection con = pool.remove(0);\n\n            Connection proxycon= (Connection) Proxy.newProxyInstance(con.getClass().getClassLoader(), new Class[]{Connection.class}, new InvocationHandler() {\n                @Override\n                public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n                    if(method.getName().equals("close")){\n                        pool.add(con);\n                        return null;\n                    }else {\n                        return method.invoke(con,args);\n                    }\n                }\n            });\n            return proxycon;\n        } else {\n            throw new RuntimeException("连接数据已用尽");\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 开源数据库连接池\n\n\n# C3P0\n\n 1. 导入jar包\n\n 2. 导入配置文件到src目录下\n\n 3. 创建 C3P0 连接池对象\n    \n    DataSource dataSource=new ComboPooledDataSource();\n    \n    \n    1\n    \n\n 4. 获取数据库连接并使用\n    \n    Connection con = dataSource.getConnection();\n    \n    \n    1\n    \n\n配置文件会自动加载 必须命名为 c3p0-config.xml 或 c3p0-config.properties\n\n\n# Druid\n\n * 导入jar包\n\n * 通过properties集合 加载配置文件\n   \n   InputStream is = druiddemo01.class.getClassLoader().getResourceAsStream("笔记/jdbc/src/druid.properties");\n   Properties prop =new Properties();\n   prop.load(is);\n   \n   \n   1\n   2\n   3\n   \n\n * 通Druid连接池工厂类获取连接池对象\n   \n   DataSource dataSource = DruidDataSourceFactory.createDataSource(prop);\n   \n   \n   1\n   \n\n * 通过连接池获取数据库对象\n   \n   Connection con = dataSource.getConnection();\n   \n   \n   1\n   \n\n\n# 自定义JDBC框架\n\n * DataBaseMetaData 数据库的源信息\n   * getDatabaseProductName() 获取数据库产品的名称\n   * getDatabaseProductVersion() 获取数据库产品的版本号\n * getParameterMetaData() 参数的源信息 预编译对象获取方法\n   * getParameterCount() 获取sql语句中参数的个数\n * getMetaData() 获取结果集的源信息 通过结果集对象获取\n   * getColumnCount() 获取列的总数\n   * getColumnName(int i) 获取列名',normalizedContent:'# jdbc\n\n * 导入mysql-connector-java.jar包\n\n * 注册驱动\n   \n   * class.forname("com.mysql.jdbc.driver");\n     \n     \n     1\n     \n\n * 建立连接\n   \n   *  connection con = drivermanager.getconnection("jdbc:mysql://127.0.0.1:3306/mysql", "root", "123456");\n     \n     \n     1\n     \n\n * 获取对象\n   \n   * statement stat = con.createstatement();\n     \n     \n     1\n     \n\n * 执行sql语句\n   \n   * string sql ="select * from help_relation";\n     resultset result = stat.executequery(sql);\n     \n     \n     1\n     2\n     \n\n * 处理结果\n   \n   *  while (result.next()){\n     system.out.println(result.getint("help_topic_id")+"\\t"+ result.getstring("help_keyword_id"));\n     }\n     \n     \n     1\n     2\n     3\n     \n\n * 关闭连接\n   \n   * con.close();\n      stat.close();\n     \n     \n     1\n     2\n     \n\n\n# drivermanager 连接\n\ndrivermanager驱动管理对象\n\n * 注册驱动 registerdriver(driver river) 注册给定的驱动 在上面注册中我们使用class.forname使用了一下driver这个类,默认被使用就注册驱动 必须是mysql5之后才可以省略这个注册\n * 获取数据连接 getconnection(url , user , password) url为jdbc:mysql://ip:端口/库名\n\n\n# connection 连接对象\n\nconnection数据库连接对象\n\n * 获取普通执行者对象 createstatement()\n * 获取预编译执行者对象 preparestatement(string sql)\n * 开启事务 setautocommit(boolean autocommit); 参数为false 则开启事务\n * 提交事务 commit()\n * 回滚事务 rollback()\n * 释放资源 close();\n\n\n# statement 执行语句\n\nstatement执行sql语句的对象\n\n * 增删改 executeupdate(string sql) 返回一个int 影响的行数 可以执行 insert、update、delete语句\n * 查询 executequery（string sql） 返回值resultset对象 封装查询的结果 可以执行select语句\n * 释放资源 close()\n\n\n# resultset 结果\n\nresultset结果集对象\n\n * 判断结果是否还有数据 next() 有数据返回ture,并索引向下移动一行\n * get数据类型("列名") 返回指定列的指定数据类型结果 如getint() getstring()\n * 释放资源 close()\n\n\n# 工具类\n\n在src下创建config.properties 文件\n\ndriverclass=com.mysql.jdbc.driver\nurl=jdbc:mysql://127.0.0.1:3306/mysql\nusername=root\npassword=123456\n\n\n1\n2\n3\n4\n\n\nimport java.io.ioexception;\nimport java.io.inputstream;\nimport java.sql.*;\nimport java.util.properties;\n\npublic class jdbcutils {\n    private jdbcutils() {\n    }\n\n    //2.声明需要的配置变量\n    private static string driverclass;\n    private static string url;\n    private static string usename;\n    private static string password;\n    private static connection con;\n\n    //3.提供静态代码块 读取配置文件为变量赋值\n    static {\n        try {\n            inputstream is = jdbcutils.class.getclassloader().getresourceasstream("config.properties");\n            properties prop = new properties();\n            prop.load(is);\n\n            driverclass = prop.getproperty("driverclass");\n            url = prop.getproperty("url");\n            usename = prop.getproperty("usename");\n            password = prop.getproperty("password");\n\n            //注册驱动\n\n            class.forname(driverclass);\n\n\n        } catch (exception e) {\n            e.printstacktrace();\n        }\n\n    }\n\n\n    public static connection getconnection() {\n        try {\n            con = drivermanager.getconnection(url, usename, password);\n        } catch (sqlexception throwables) {\n            throwables.printstacktrace();\n        }\n\n        return con;\n    }\n\n    public static void close(connection con, statement stat, resultset rs) {\n        if (con != null) {\n            try {\n                con.close();\n            } catch (sqlexception throwables) {\n                throwables.printstacktrace();\n            }\n        }\n\n        if (stat != null) {\n            try {\n                stat.close();\n            } catch (sqlexception throwables) {\n                throwables.printstacktrace();\n            }\n        }\n\n        if (rs != null) {\n            try {\n                rs.close();\n            } catch (sqlexception throwables) {\n                throwables.printstacktrace();\n            }\n        }\n\n    }\n\n    public static void close(connection con, statement stat) {\n        if (con != null) {\n            try {\n                con.close();\n            } catch (sqlexception throwables) {\n                throwables.printstacktrace();\n            }\n        }\n\n        if (stat != null) {\n            try {\n                stat.close();\n            } catch (sqlexception throwables) {\n                throwables.printstacktrace();\n            }\n        }\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n\n\n其实就是封装一个连接和关闭工具类方法\n\n\n# sql注入攻击\n\n在之前我们使用statement对象执行sql语句并且sql语句是拼接字符串而成的 不安全\n\n为了解决这个问题我们使用 preparedstatement预编译执行者对象 并且参数以?形式作为占位符\n\n为占位符 问号 赋值的方法: setxxx(参数1,参数2)\n\n * xxx 为数据的类型\n * 参数1 为 ?的下标 从1开始\n * 参数2 为 ?的实际参数\n\n内置setobject()\n\nconnection con = drivermanager.getconnection("jdbc:mysql://127.0.0.1:3306/mysql", "root", "123456");\nstring sql = "delete from  user where name = ?";\npreparedstatement pstm = con.preparestatement(sql);\npstm.setstring(1,"张三");\n\n\n1\n2\n3\n4\n\n\n执行sql语句\n\n * 执行 inser update delete 语句 executeupdate();\n * 执行 select 语句 executequery();\n\n\n# 数据库连接池\n\n数据库连接池负责分配 管理 释放数据库连接 它允许程序重复使用一个现有的数据库连接 而不是重新建立一个\n\n\n# 自定义数据库连接池\n\n * 继承 datasource 接口\n * 准备一个容器 用于保存多个连接对象\n * 定义静态代码块 获取多个连接对象并放入容器中\n * 重写getconnection()方法 用于获取一个连接对象\n * 定义一个getsize方法 获取连接池容器的大小\n\n// 准备一个容器  用于保存多个连接对象\nprivate static list<connection> pool = collections.synchronizedlist(new arraylist<>());\n\n//定义静态代码块  获取多个连接对象并放入容器中\nstatic {\n    for(int i=1;i<=10;i++){\n        connection con = jdbcutils.getconnection();\n        pool.add(con);\n\n    }\n}\n\n//重写getconnection()方法  用于获取一个连接对象\n@override\npublic connection getconnection() throws sqlexception {\n    if(pool.size() > 0 ){\n        connection con = pool.remove(0);\n        return con;\n    }else {\n        throw new runtimeexception("连接数据已用尽");\n    }\n}\n\n//定义一个getsize方法 获取连接池容器的大小\npublic int getsize(){\n    return pool.size();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 归还连接\n\n把连接对象归还给连接池\n\n# 继承方式\n\n打印通过连接池创建的连对象的 class\n\nsystem.out.println(con.getclass());\n//class com.mysql.cj.jdbc.connectionimpl\n\n\n1\n2\n\n\n接下定义一个类 继承 connectionimpl 类\n\npublic class return01 extends connectionimpl {\n    private connection con;\n    private list<connection> pool;\n\n\n    public return01(hostinfo hostinfo, connection con, list<connection> pool) throws sqlexception {\n        super(hostinfo);\n        this.con = con;\n        this.pool = pool;\n    }\n\n    // 重写close方法 将连接对象 重写归还容器中\n    @override\n    public void close() throws sqlexception {\n        pool.add(con);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n但drivermanager 获取的还是 connectionimpl 这个对象, 继承这种方法行不通的\n\n# 装饰设计模式\n\n * 实现 connection 接口\n\n// 定义连接对象和连接容器的成员变量\n    private connection con;\n    private list<connection> pool;\n\n    //通过有参构造方法为成员变量赋值\n    public return02(connection con,list<connection> pool){\n        this.con=con;\n        this.pool=pool;\n    }\n\n    // 重写close方法\n    @override\n    public void close() throws sqlexception {\n        pool.add(con);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n其他方法调用原本方法\n\n装饰设计模式就是 重写需要的方法 其他方法则调用原来类的方法\n\n# 适配器设计模式\n\n适配器设计模式 实现接口的所有方法 除了需要的方法 然后再建一个类 继承该实现类 再重新未实现的方法 (使用中间件)\n\n * 定义适配类 实现 connection 接口 定义为一个抽象类\n * 定义连接对象的成员变量 connection con\n * 通过有参构造方法赋值\n * 重写所有抽象方法 除了close\n * 继承适配类 成员变量 connection 和 连接容器\n * 通过有参构造方法赋值\n * 重写close方法\n\n\n# 动态代理\n\n在不改变目标对象方法的情况下对方法进行增强\n\n被代理对象: 真实的对象\n\n代理对象: 内容中的一个对象\n\n代理对象必须和被代理对象实现相同的接口\n\n        student st = new student();\n\n    /*\n        参数1 为 类加载器   和被代理对象使用相同的类加载器\n        参数2 为 接口类型class数组  和被代理对象使用相同的接口\n        参数3 为 代理规则 完成代理增强的功能\n     */\n        studentinterface proxyst =(studentinterface) proxy.newproxyinstance(st.getclass().getclassloader(), new class[]{studentinterface.class}, new invocationhandler() {\n            /*\n                student类中所有的方法都经过invoke方法\n                我们通过对方法继续判断\n             */\n            @override\n            public object invoke(object proxy, method method, object[] args) throws throwable {\n                //method为类执行某个方法   args为方法的实际参数\n                if (method.getname().equals("study")) {\n                    // 判断为某个方法\n                    system.out.println("hello world");\n                    return null;\n                } else {\n                    // 如不是则按原本方法执行\n                    return method.invoke(st,args);\n                }\n            }\n        });\n\n        // 代理对象执行方法\n        proxyst.eat("肉");\n        proxyst.study();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 以动态代理 规划连接\n\n //重写getconnection()方法  用于获取一个连接对象\n    @override\n    public connection getconnection() throws sqlexception {\n        if (pool.size() > 0) {\n            connection con = pool.remove(0);\n\n            connection proxycon= (connection) proxy.newproxyinstance(con.getclass().getclassloader(), new class[]{connection.class}, new invocationhandler() {\n                @override\n                public object invoke(object proxy, method method, object[] args) throws throwable {\n                    if(method.getname().equals("close")){\n                        pool.add(con);\n                        return null;\n                    }else {\n                        return method.invoke(con,args);\n                    }\n                }\n            });\n            return proxycon;\n        } else {\n            throw new runtimeexception("连接数据已用尽");\n        }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 开源数据库连接池\n\n\n# c3p0\n\n 1. 导入jar包\n\n 2. 导入配置文件到src目录下\n\n 3. 创建 c3p0 连接池对象\n    \n    datasource datasource=new combopooleddatasource();\n    \n    \n    1\n    \n\n 4. 获取数据库连接并使用\n    \n    connection con = datasource.getconnection();\n    \n    \n    1\n    \n\n配置文件会自动加载 必须命名为 c3p0-config.xml 或 c3p0-config.properties\n\n\n# druid\n\n * 导入jar包\n\n * 通过properties集合 加载配置文件\n   \n   inputstream is = druiddemo01.class.getclassloader().getresourceasstream("笔记/jdbc/src/druid.properties");\n   properties prop =new properties();\n   prop.load(is);\n   \n   \n   1\n   2\n   3\n   \n\n * 通druid连接池工厂类获取连接池对象\n   \n   datasource datasource = druiddatasourcefactory.createdatasource(prop);\n   \n   \n   1\n   \n\n * 通过连接池获取数据库对象\n   \n   connection con = datasource.getconnection();\n   \n   \n   1\n   \n\n\n# 自定义jdbc框架\n\n * databasemetadata 数据库的源信息\n   * getdatabaseproductname() 获取数据库产品的名称\n   * getdatabaseproductversion() 获取数据库产品的版本号\n * getparametermetadata() 参数的源信息 预编译对象获取方法\n   * getparametercount() 获取sql语句中参数的个数\n * getmetadata() 获取结果集的源信息 通过结果集对象获取\n   * getcolumncount() 获取列的总数\n   * getcolumnname(int i) 获取列名',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Session",frontmatter:{title:"Session",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/83bc22/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/74.Session.html",relativePath:"后端/01.JavaSE/74.Session.md",key:"v-47db4091",path:"/pages/83bc22/",headers:[{level:2,title:"获取 httpSession",slug:"获取-httpsession",normalizedTitle:"获取 httpsession",charIndex:75},{level:2,title:"Session方法",slug:"session方法",normalizedTitle:"session方法",charIndex:194}],headersStr:"获取 httpSession Session方法",content:"# Session\n\n服务器端会话管理技术 本质也是客户端会话管理技术\n\n在客户端保存的是一个特殊标识,而共享的数据保存到服务器端内存对象中\n\n\n# 获取 httpSession\n\n * getSession() 获取 httpSession 对象\n * getSession(boolean create) 获取httpSession 对象,未获取是否自动创建 默认为ture\n\n\n# Session方法\n\n需要借助请求对象中的getSession() 返回一个httpSession 对象\n\n * setAttribute(String,Object value) 设置共享数据\n * getAttribute(String name) 获取共享数据\n * removeAttribute(String name) 移除共享数据\n * getId() 获取唯一标识名称\n * Invalidate() 让session立即失效",normalizedContent:"# session\n\n服务器端会话管理技术 本质也是客户端会话管理技术\n\n在客户端保存的是一个特殊标识,而共享的数据保存到服务器端内存对象中\n\n\n# 获取 httpsession\n\n * getsession() 获取 httpsession 对象\n * getsession(boolean create) 获取httpsession 对象,未获取是否自动创建 默认为ture\n\n\n# session方法\n\n需要借助请求对象中的getsession() 返回一个httpsession 对象\n\n * setattribute(string,object value) 设置共享数据\n * getattribute(string name) 获取共享数据\n * removeattribute(string name) 移除共享数据\n * getid() 获取唯一标识名称\n * invalidate() 让session立即失效",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"MyBatis",frontmatter:{title:"MyBatis",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/c1aa2b/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/02.MyBatis.html",relativePath:"后端/02.JavaEE/02.MyBatis.md",key:"v-60e0d3b1",path:"/pages/c1aa2b/",headers:[{level:2,title:"ORM",slug:"orm",normalizedTitle:"orm",charIndex:163},{level:2,title:"使用",slug:"使用",normalizedTitle:"使用",charIndex:161},{level:2,title:"Resources  加载资源工具类",slug:"resources-加载资源工具类",normalizedTitle:"resources  加载资源工具类",charIndex:null},{level:2,title:"SqlSessionFactoryBuilder 工厂对象功能类",slug:"sqlsessionfactorybuilder-工厂对象功能类",normalizedTitle:"sqlsessionfactorybuilder 工厂对象功能类",charIndex:2589},{level:2,title:"SqlSessionFactory  对象",slug:"sqlsessionfactory-对象",normalizedTitle:"sqlsessionfactory  对象",charIndex:null},{level:2,title:"SqlSession",slug:"sqlsession",normalizedTitle:"sqlsession",charIndex:2172},{level:2,title:"映射配置文件",slug:"映射配置文件",normalizedTitle:"映射配置文件",charIndex:280},{level:2,title:"核心配置文件",slug:"核心配置文件",normalizedTitle:"核心配置文件",charIndex:745},{level:2,title:"数据库连接配置引入",slug:"数据库连接配置引入",normalizedTitle:"数据库连接配置引入",charIndex:5318},{level:2,title:"起别名",slug:"起别名",normalizedTitle:"起别名",charIndex:6787},{level:2,title:"LOG4J",slug:"log4j",normalizedTitle:"log4j",charIndex:7094},{level:2,title:"分层思想",slug:"分层思想",normalizedTitle:"分层思想",charIndex:7217},{level:2,title:"接口代理",slug:"接口代理",normalizedTitle:"接口代理",charIndex:2880},{level:3,title:"源码分析",slug:"源码分析",normalizedTitle:"源码分析",charIndex:7670},{level:2,title:"动态SQL",slug:"动态sql",normalizedTitle:"动态sql",charIndex:7853},{level:2,title:"SQL 片段抽取",slug:"sql-片段抽取",normalizedTitle:"sql 片段抽取",charIndex:8708},{level:2,title:"获取自增的ID 再插入数据",slug:"获取自增的id-再插入数据",normalizedTitle:"获取自增的id 再插入数据",charIndex:9002},{level:2,title:"分页插件",slug:"分页插件",normalizedTitle:"分页插件",charIndex:9433},{level:3,title:"Pageinfo 封装分页相关参数的功能类",slug:"pageinfo-封装分页相关参数的功能类",normalizedTitle:"pageinfo 封装分页相关参数的功能类",charIndex:9867},{level:2,title:"多表操作",slug:"多表操作",normalizedTitle:"多表操作",charIndex:10166},{level:3,title:"一对一",slug:"一对一",normalizedTitle:"一对一",charIndex:10175},{level:3,title:"一对多",slug:"一对多",normalizedTitle:"一对多",charIndex:11526},{level:3,title:"多对多",slug:"多对多",normalizedTitle:"多对多",charIndex:12395},{level:2,title:"注解",slug:"注解",normalizedTitle:"注解",charIndex:58},{level:3,title:"多表操作",slug:"多表操作-2",normalizedTitle:"多表操作",charIndex:10166},{level:2,title:"SQL 构建",slug:"sql-构建",normalizedTitle:"sql 构建",charIndex:16138}],headersStr:"ORM 使用 Resources  加载资源工具类 SqlSessionFactoryBuilder 工厂对象功能类 SqlSessionFactory  对象 SqlSession 映射配置文件 核心配置文件 数据库连接配置引入 起别名 LOG4J 分层思想 接口代理 源码分析 动态SQL SQL 片段抽取 获取自增的ID 再插入数据 分页插件 Pageinfo 封装分页相关参数的功能类 多表操作 一对一 一对多 多对多 注解 多表操作 SQL 构建",content:'# MyBatis\n\nMyBatis 是基于 JAVA 的持久层框架,它内部封装了 JDBC\n\n通过 xml 或者 注解 方式使将要的执行各种 statement 配置起来 , 并通过 java对象和statement中SQL的动态参数进行映射生成最终执行的SQL语句\n\n执行完SQL语句 并将结果映射为java对象返回 使用ORM 思想解决问题\n\n\n# ORM\n\nObject Relational Mapping 对象关系映射 持久化数据和实体对象的映射模式\n\n\n# 使用\n\n * 导入 mybatis.jar 和mysql.jar\n\n * 在src下创建映射配置文件 名字无所谓\n   \n   * <?xml version="1.0" encoding="UTF-8"?>\n     <!DOCTYPE mapper\n             PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"\n             "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n     \n     <mapper namespace="StudentMapper">\n         <select id="any" resultType="com.xxx..xx">\n             select * from mysql\n         </select>\n     </mapper>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     \n\n * 在src下创建核心配置文件\n   \n   * <?xml version="1.0" encoding="UTF-8"?>\n     <!DOCTYPE configuration\n             PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n             "http://mybatis.org/dtd/mybatis-3-config.dtd">\n     \n     <configuration>\n     \x3c!--    默认配置--\x3e\n         <environments default="ie">\n     \x3c!--        配置名--\x3e\n             <environment id="ie">\n                 \n                 <transactionManager type="JDBC"></transactionManager>\n                 <dataSource type="POOLED">\n     \x3c!--                驱动--\x3e\n                     <property name="driver" value="com.mysql.jdbc.Driver"/>\n                     <property name="url" value="jdbc:mysql://127.0.0.1:3306/mysql"/>\n                     <property name="username" value="root"/>\n                     <property name="password" value="123456"/>\n                 </dataSource>\n             </environment>\n         </environments>\n     \n     \x3c!--    映射类--\x3e\n         <mappers>\n     \x3c!--        映射类名称--\x3e\n             <mapper resource="StudentMapper.xml"/> \n         </mappers>\n     </configuration>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     25\n     26\n     27\n     28\n     \n\n * 加核心配置文件\n   \n   * InputStream is = Resources.getResourceAsStream("MyBatisConfig.xml");\n     \n     \n     1\n     \n\n * 获取sqlsession 工厂对象\n   \n   * SqlSessionFactory build = new SqlSessionFactoryBuilder().build(is);\n     \n     \n     1\n     \n\n * 执行映射配置文件的sql语句\n   \n   * List<Object> objects = sqlSession.selectList("StudentMapper.any");\n     \n     \n     1\n     \n\n * 释放资源\n   \n   * sqlSession.close();\n     is.close();\n     \n     \n     1\n     2\n     \n\n\n# Resources 加载资源工具类\n\n * Resources.getResourceAsStream(String fileName) 通过类加载器返回指定资源的字节输入流 与获取类加载 加载指定资源字节输入流一样\n\n\n# SqlSessionFactoryBuilder 工厂对象功能类\n\n * new SqlSessionFactoryBuilder().build(is); 通过指定资源字节输入流获取SqlSessionFactory工厂对象\n\n\n# SqlSessionFactory 对象\n\n * openSession() 获取sqlsession 构建者对象 并开启手动提交事务\n * openSession(boolean auotoCommit) 获取sqlsession 构建者对象 true为自动提交事务\n\n\n# SqlSession\n\n构建者对象接口 用于执行 SQL 管理事务 接口代理\n\n * selectList(String Statement , object parameter) 执行查询语句 并返回list集合\n * selectList(String Statement , object parameter) 执行查询语句 返回一个结果对象\n * inser(String Statement , object parameter) 执行新增语句,返回影响行数\n * update(String Statement , object parameter) 执行修改语句,返回影响行数\n * delete(String Statement , object parameter) 执行删除语句,返回影响行数\n * commit() 提交事务\n * rollback() 回滚事务\n * getMapper(Class<T> cls) 获取指定接口的代理实现类对象\n * close() 释放资源\n\n\n# 映射配置文件\n\n * mapper 核心根标签\n   * namespace属性 名称空间\n * select 查询标签\n   * id属性 唯一标识\n   * resultType属性 指定结果映射对象类型 类路径 增删改可以不指定类因为返回的是一个影响行数\n   * parameterType属性 指定参数映射对象类型 指定执行时传递的parameter类型\n   * SQL语句获取参数 #{属性名}\n\n<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE mapper\n        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n\n<mapper namespace="StudentMapper">\n    <select id="any" resultType="com.xxx..xx" parameterType="java.lang.long">\n        select * from mysql where id = #{id}\n    </select>\n        \n    <insert id="insert" parameterType="笔记.jdbc.src.Student">\n\n        insert into studen value (#{id},#{name},#{age})\n        -- 从Student 中传递 id name age属性\n    </insert>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 核心配置文件\n\n<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE configuration\n        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n\x3c!--        configuration根标签--\x3e\n<configuration>\n\x3c!--  environments配置数据库环境  default属性指定使用哪一个--\x3e\n    <environments default="ie">\n\x3c!--        environment配置数据库环境  id属性唯一标识--\x3e\n        <environment id="ie">\n\x3c!--            transactionManager事务管理  type属性 采用jdbc默认的事务管理--\x3e\n            <transactionManager type="JDBC"></transactionManager>\n\x3c!--           dataSource数据源信息  type属性 连接池    --\x3e\n            <dataSource type="POOLED">\n\x3c!--                property连接数据库的配置信息--\x3e\n                <property name="driver" value="com.mysql.jdbc.Driver"/>\n                <property name="url" value="jdbc:mysql://127.0.0.1:3306/mysql"/>\n                <property name="username" value="root"/>\n                <property name="password" value="123456"/>\n            </dataSource>\n        </environment>\n    </environments>\n\n\x3c!--    mappers引入映射配置文件--\x3e\n    <mappers>\n\x3c!--        mapper 引入指定的映射配置 resource属性 指定映射配置文件的名名称--\x3e\n        <mapper resource="StudentMapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 数据库连接配置引入\n\n * <properties> 引入数据库连接配置文件标签\n   * resource属性 数据库连接配置文件路径\n * 获取连接参数\n   * ${键名}\n\ndriver=com.mysql.jdbc.Driver\nurl=jdbc:mysql://127.0.0.1:3306/mysql\nusername=root\npassword=123456\n\n\n1\n2\n3\n4\n\n\n<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE configuration\n        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n\x3c!--        configuration根标签--\x3e\n<configuration>\n\x3c!--  environments配置数据库环境  default属性指定使用哪一个--\x3e\n    <properties resource="笔记/mybatis/src/config.properties">\n    <environments default="ie">\n\x3c!--        environment配置数据库环境  id属性唯一标识--\x3e\n        <environment id="ie">\n\x3c!--            transactionManager事务管理  type属性 采用jdbc默认的事务管理--\x3e\n            <transactionManager type="JDBC"></transactionManager>\n\x3c!--           dataSource数据源信息  type属性 连接池    --\x3e\n            <dataSource type="POOLED">\n\x3c!--                property连接数据库的配置信息--\x3e\n                <property name="driver" value="${driver}"/>\n                <property name="url" value="${url}"/>\n                <property name="username" value="${username}"/>\n                <property name="password" value="${password}"/>\n            </dataSource>\n        </environment>\n    </environments>\n\n\x3c!--    mappers引入映射配置文件--\x3e\n    <mappers>\n\x3c!--        mapper 引入指定的映射配置 resource属性 指定映射配置文件的名名称--\x3e\n        <mapper resource="StudentMapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 起别名\n\n在映射配置中 我们resultType属性需要提供 类的全路径 可以在核心配置文件中起别名来简写\n\n * <typeAliaser> 为全类名起别名的父标签\n * <typeAlias> 为全类名起步名的子标签\n   * 属性:\n     * type 指定全类名\n     * alias 指定别名\n * <package> 为指定包下所有类起别的子标签 别名就是类名\n\n<typeAliases>\n    <typeAlias type="笔记.jdbc.src.Student" alias="student"></typeAlias>\n</typeAliases>\n\n\n1\n2\n3\n\n\n\n\n\n# LOG4J\n\n在核心配置文件添加\n\n<settings>\n    <setting name="logImpl" value="log4j"/>\n</settings>\n\n\n1\n2\n3\n\n\n并配置好log4j.properties 配置\n\n\n# 分层思想\n\n控制层(controller) ====> 业务层(service) ====> 持久层(dao)\n\n持久层 对接数据库\n\n业务层 处理业务逻辑 此处只是暂时直接调用dao层的方法\n\n控制层 使用 test类\n\n\n# 接口代理\n\n通过接口代理 我们只需要写dao层的接口 由MyBatis 框架根据接口的定义来创建接口的动态代理对象\n\n * 映射配置文件中的名称空间必须与dao层接口的全类名相同\n * 映射配置文件中的增删改查的id属性必须和dao层接口的方法名相同\n * 映射配置文件中的增删改查标签的parameterType属性必须和dao层接口方法的参数相同\n * 映射配置文件中的增删改查标签的resultType属性必须和dao层接口的返回值相同\n\ngetMapper(Class<T> cls) 获取指定接口的代理实现类对象\n\nmybatisdemo01 mapper = sqlSession.getMapper(mybatisdemo01.class);\n\n\n1\n\n\n\n# 源码分析\n\n通过getMapper()方法 获取到 org.apache.ibatis.binding.MapperProxy 代理对象 底层使用 JDK 的动态代理技术 帮我们实现代理实现类对象\n\n执行方法时调用了 mapperMethod.execute()方法 通过switch语句 判断操作类型是增删改查操作\n\n通过SqlSession 方法来执行\n\n\n# 动态SQL\n\n可以根据SQL语句动态根据条件查询\n\n * <where> 条件标签 如果有动态条件 则使用该标签替代where关键字\n * <if> 条件判断标签 test属性 条件控制 如果成立则拼接SQL语句\n\n<select id="dongtaisql" resultType="studen">\n    select * from stden\n    <where>\n        <if test="id != null">\n            id = #{id}\n        </if>\n        <if test="age != null">\n            and age = #{age}\n        </if>\n    </where>\n</select>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * <foreach> 循环遍历标签 适用于多个参数或者的关系\n   * collection属性 参数容器类型(list集合 array数组)\n   * open属性 开始的sql语句\n   * close属性 结束的sql语句\n   * item属性 参数变量名\n   * separator属性 分隔符\n\n\x3c!-- select * forme studen id in(1,2,3)--\x3e\n<select id="selectbyids" resultType="studen" parameterType="list">\n    select * from studen\n    <where>\n        <foreach collection="list" open="id in(" close=")" separator="," item="id">\n            #{id}\n        </foreach>\n    </where>\n</select>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# SQL 片段抽取\n\n将一些重复性的SQL语句进行抽取 达到复用的效果\n\n * <sql> 标签 抽取sql语句标签 id属性唯一标识\n * <include> 引入sql片段标签 refid属性需要引用片段的唯一标识\n\n<sql id="select">select * from studen</sql>\n<select id="qsq" resultType="student" parameterType="student">\n<include refid="select"></include> where id = #{id}\n</select>\n\n\n1\n2\n3\n4\n\n\n\n# 获取自增的ID 再插入数据\n\n先执行LAST_INSERT_ID() 返回一个id并封装在对象 再执行inser语句\n\n<insert id="add" parameterType="com.itheima.pojo.CheckGroup">\n    <selectKey resultType="java.lang.Integer" order="AFTER" keyProperty="id">\n        select LAST_INSERT_ID()\n    </selectKey>\n    insert into t_checkgroup(code, name, sex, helpCode, remark, attention)\n    values (#{code}, #{name}, #{sex}, #{helpCode}, #{remark}, #{attention})\n</insert>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 分页插件\n\nmybatis不带分页功能的 mysql中分页使用 limit 语句 不同的数据库实现的关键字也不同\n\nPageHelper 第三方分页助手\n\n * 导入jar包 pagehelper.jar jsqlparser.jar\n\n * 在核心配置文件中集成分页助手\n   \n   * <plugins>\n             <plugin interceptor="com.github.pagehelper.PageInterceptor"></plugin>\n     \t</plugins>\n     \n     \n     1\n     2\n     3\n     \n\n * 在测试类中使用分页功能\n   \n   * PageHelper.startPage(1,3);\n     // PageHelper.startPage(第几页,每页显示多少个);  设置分页参数\n     \n     \n     1\n     2\n     \n\n\n# Pageinfo 封装分页相关参数的功能类\n\n * getTotal() 获取总条数\n * getPages() 获取总页数\n * getPageNum() 获取当前页\n * getPageSize() 获取每页显示条数\n * getPrePage() 获取上一页\n * getNextPage() 获取下一页\n * islsFiresPage() 获取是否是第一页\n * islsLastPage() 获取是否是最后一页\n\nPageInfo<Student> info =new PageInfo<>(list);\nint total = info.getTotal();\n\n\n1\n2\n\n\n\n# 多表操作\n\n\n# 一对一\n\n<mapper namespace="1v1">\n    \x3c!--\n        resultMap 配置字段和实体对象属性的映射关系\n        id为唯一标识\n        type为映射对象路径\n    --\x3e\n    <resultMap id="oneToOne" type="card">\n        \x3c!--\n            id为主键标签  column为表中列名   property为对象属性名称\n            result为其他列标签\n        --\x3e\n        <id column="cid" property="id"/>\n        <result column="number" property="number" />\n                \x3c!--\n                        association: 配置被包含对象的映射关系   对象内的对象\n                        property: 被包含对象的变量名   对象内的对象变量名是什么\n                        javaType:被包含对象的数据类型\n                --\x3e\n        <association property="prs" javaType="person">\n            <id column="pid" property="id"></id>\n            <result column="name" property="name"/>\n            <result column="age" property="age"/>\n\n        </association>\n    </resultMap>\n        \x3c!--\n            resultMap 为多表操作映射\n         --\x3e\n    <select id="selectall" resultMap="oneToOne">\n        select c.id cid,number,pid,name,age from card c,person p where c.pid=p.id;\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n * <resultMap> 配置字段和实体对象属性的映射关系\n   * id属性 唯一标识\n   * type属性 实体对象类型\n * <id> 配置主键映射关系标签\n * <result> 配置非主键映射关系标签\n   * column属性 表中字段名称\n   * property属性 实体对象变量名称\n * <association> 配置被包含对象的映射关系标签\n   * property属性 被包含对象的变量名\n   * javaType属性 被包含对象的数据类型\n\n\n# 一对多\n\n<mapper>\n    <resultMap id="onetomany" type="classes">\n        <id column="cid" property="id"/>\n        <result column="canem" property="name"/>\n\n        \x3c!--\n            collection: 配置被包含的集合对象映射关系\n            property属性  被包含集合对象的变量名\n            ofType属性  被包含集合对象元素的数据类型\n            --\x3e\n        <collection property="students" ofType="student">\n            <id column="sid" property="id"/>\n            <result column="sname" property="name"/>\n            <result column="sage" property="age"/>\n        </collection>\n    </resultMap>\n    <select id="selectall" resultMap="onetomany">\n        select c.id cid,c.name cname,s.id sid,s.name sname,s.age sage from classes c,student s where c.id=s.id\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * <collection> 配置被包含的集合对象映射关系\n   * property属性 被包含集合对象的变量名\n   * ofType属性 被包含集合对象元素的数据类型\n\n\n# 多对多\n\n<mapper>\n    <resultMap id="manytomany" type="sstdent">\n        <id column="sid" property="id"/>\n        <result column="sname" property="name"/>\n        <result column="sage" property="age"/>\n        <collection property="coures" ofType="corse">\n            <id column="cid" property="id"/>\n            <result column="cname" property="name"/>\n        </collection>\n    </resultMap>\n    <select id="selectall" resultMap="manytomany">\n        select  sc.sid,s.name sname,s.age sage,sc.cid,c.name cname from stdent s,course c,stu,_cr sc where sc.sid=s.id and sc.cid=c.id\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n<collection> 配置被包含的集合对象映射关系\n\n * property属性 被包含集合对象的变量名\n * ofType属性 被包含集合对象元素的数据类型\n\n\n# 注解\n\n * @Select("查询的SQL语句") 指定参数还是#{key}\n * @Insert("新增的SQL语句") 如:@Insert("inser into student value (#{id},#{age},#{name})")\n * @Update("修改的SQL语句")\n * @Delete("删除的SQL语句")\n\n通过注解形式的操作 不需要创建映射配置文件 映射配置内容 写在核心配置文件中\n\n<mappers>\n    \t\x3c!-- name为接口所在的包路径 可以指定类或者指定包下的所有类 --\x3e\n        <package name="com.xxx.xxx.stdentmapper"/>\n</mappers>\n\n\n1\n2\n3\n4\n\n\n\n# 多表操作\n\n# 一对一\n\n@Select("select * fome card")\n    @Results({\n            @Result(column = "id" ,property = "id"),\n            @Result(column = "number" , property = "number"),\n            @Result(\n                    property = "p",   // 被包含对象的变量名\n                    javaType = Person.class,  // 被包含对象的实现数据类型类\n                    column = "pid",  // 根据上面select查询出来表中的哪个字段来查询第二个表\n                    /*\n                     one = @one()  一对一写法\n                     select属性  指定调用哪个接口的哪个方法\n                     */\n                    one = @One(select = "com.xxx.xxx接口.selectByid方法")\n            )\n    })\n    List<Card> selectAll();\n\n//@one注解调用的接口方法\n    @Select("select * from person where id=#{id}")\n    Person selectByid(Integer id);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * @Results 封装映射关系的父注解 Result[] vlue() 定义了Result数组\n   * @Result 封装映射关系的子注解\n     * column属性 查询出的表中字段名称\n     * property属性 实体对象中的属性名称\n     * javaType属性 被包含对象的数据类型\n     * one 属性 一对一查询\n       * @One一对一查询注解\n         * select属性 指向要调用某个接口中的方法\n\n# 一对多\n\n//一对多\n@Select("select * fome classes")\n@Results({\n        @Result(column = "id" ,property = "id"),\n        @Result(column = "number" , property = "number"),\n        @Result(\n                property = "students",   // 被包含对象的变量名\n                javaType = List.class,  // 被包含对象的实现数据类型类\n                column = "id",  // 根据上面select查询出来表中的哪个字段来查询第二个表\n\n        /*                    many = @Many()  一对多写法\n                select属性  指定调用哪个接口的哪个方法*/\n                many= @Many(select = "com.xxx.xxx接口.xxx方法")\n        )\n})\nList<Classes> selectAll();\n\n//@one注解调用的接口方法\n@Select("select * from student where cid=#{cid}")\nList<Student> selectByid(Integer cid);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * many属性 一对多查询\n   * @Many一对多查询注解\n     * select属性 指向要调用某个接口中的方法\n\n# 多对多\n\n//多对多\n@Select("select distinct s.id,s.name fome studebt s,stu_cr sc where sc.sid = s.id")\n@Results({\n        @Result(column = "id" ,property = "id"),\n        @Result(column = "number" , property = "number"),\n        @Result(\n                property = "students",   // 被包含对象的变量名\n                javaType = List.class,  // 被包含对象的实现数据类型类\n                column = "id",  // 根据上面select查询出来表中的哪个字段来查询第二个表\n\n                /*                  many = @Many()  一对多写法\n                select属性  指定调用哪个接口的哪个方法*/\n                        many= @Many(select = "com.xxx.xxx接口.xxx方法")\n        )\n})\nList<Student> selectAll();\n\n//@one注解调用的接口方法\n@Select("select c.id,c.name from stu_cr sc,course c where sc.cid=c.id and sc.sid=#{id}")\nList<Course> selectByid(Integer id);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * many属性 一对多查询\n   * @Many一对多查询注解\n     * select属性 指向要调用某个接口中的方法\n\n\n# SQL 构建\n\n\n\npublic static void main(String[] args) {\n    System.out.println(getSelectall());\n    //SELECT *\n    //FROM student\n}\n\npublic static String getSelectall(){\n    return new SQL(){\n        {\n            SELECT("*");\n            FROM("student");\n        }\n    }.toString();\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n * @SelectProvider(type = SQL构造的类.class , mehod = "要执行类中方法") 查询\n\n * @InsertProvider((type = SQL构造的类.class , mehod = "要执行类中方法") 新增\n   \n   public static String getInsert(Student stu){\n           return new SQL(){\n               {\n                   INSERT_INTO("student");\n                   INTO_VALUES("#{id},#{age},#{name}");\n               }\n           }.toString();\n   \n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   \n\n * @UpdateProvider((type = SQL构造的类.class , mehod = "要执行类中方法") 更新\n\n * @DeleteProvider((type = SQL构造的类.class , mehod = "要执行类中方法") 删除',normalizedContent:'# mybatis\n\nmybatis 是基于 java 的持久层框架,它内部封装了 jdbc\n\n通过 xml 或者 注解 方式使将要的执行各种 statement 配置起来 , 并通过 java对象和statement中sql的动态参数进行映射生成最终执行的sql语句\n\n执行完sql语句 并将结果映射为java对象返回 使用orm 思想解决问题\n\n\n# orm\n\nobject relational mapping 对象关系映射 持久化数据和实体对象的映射模式\n\n\n# 使用\n\n * 导入 mybatis.jar 和mysql.jar\n\n * 在src下创建映射配置文件 名字无所谓\n   \n   * <?xml version="1.0" encoding="utf-8"?>\n     <!doctype mapper\n             public "-//mybatis.org//dtd mapper 3.0//en"\n             "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n     \n     <mapper namespace="studentmapper">\n         <select id="any" resulttype="com.xxx..xx">\n             select * from mysql\n         </select>\n     </mapper>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     \n\n * 在src下创建核心配置文件\n   \n   * <?xml version="1.0" encoding="utf-8"?>\n     <!doctype configuration\n             public "-//mybatis.org//dtd config 3.0//en"\n             "http://mybatis.org/dtd/mybatis-3-config.dtd">\n     \n     <configuration>\n     \x3c!--    默认配置--\x3e\n         <environments default="ie">\n     \x3c!--        配置名--\x3e\n             <environment id="ie">\n                 \n                 <transactionmanager type="jdbc"></transactionmanager>\n                 <datasource type="pooled">\n     \x3c!--                驱动--\x3e\n                     <property name="driver" value="com.mysql.jdbc.driver"/>\n                     <property name="url" value="jdbc:mysql://127.0.0.1:3306/mysql"/>\n                     <property name="username" value="root"/>\n                     <property name="password" value="123456"/>\n                 </datasource>\n             </environment>\n         </environments>\n     \n     \x3c!--    映射类--\x3e\n         <mappers>\n     \x3c!--        映射类名称--\x3e\n             <mapper resource="studentmapper.xml"/> \n         </mappers>\n     </configuration>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     25\n     26\n     27\n     28\n     \n\n * 加核心配置文件\n   \n   * inputstream is = resources.getresourceasstream("mybatisconfig.xml");\n     \n     \n     1\n     \n\n * 获取sqlsession 工厂对象\n   \n   * sqlsessionfactory build = new sqlsessionfactorybuilder().build(is);\n     \n     \n     1\n     \n\n * 执行映射配置文件的sql语句\n   \n   * list<object> objects = sqlsession.selectlist("studentmapper.any");\n     \n     \n     1\n     \n\n * 释放资源\n   \n   * sqlsession.close();\n     is.close();\n     \n     \n     1\n     2\n     \n\n\n# resources 加载资源工具类\n\n * resources.getresourceasstream(string filename) 通过类加载器返回指定资源的字节输入流 与获取类加载 加载指定资源字节输入流一样\n\n\n# sqlsessionfactorybuilder 工厂对象功能类\n\n * new sqlsessionfactorybuilder().build(is); 通过指定资源字节输入流获取sqlsessionfactory工厂对象\n\n\n# sqlsessionfactory 对象\n\n * opensession() 获取sqlsession 构建者对象 并开启手动提交事务\n * opensession(boolean auotocommit) 获取sqlsession 构建者对象 true为自动提交事务\n\n\n# sqlsession\n\n构建者对象接口 用于执行 sql 管理事务 接口代理\n\n * selectlist(string statement , object parameter) 执行查询语句 并返回list集合\n * selectlist(string statement , object parameter) 执行查询语句 返回一个结果对象\n * inser(string statement , object parameter) 执行新增语句,返回影响行数\n * update(string statement , object parameter) 执行修改语句,返回影响行数\n * delete(string statement , object parameter) 执行删除语句,返回影响行数\n * commit() 提交事务\n * rollback() 回滚事务\n * getmapper(class<t> cls) 获取指定接口的代理实现类对象\n * close() 释放资源\n\n\n# 映射配置文件\n\n * mapper 核心根标签\n   * namespace属性 名称空间\n * select 查询标签\n   * id属性 唯一标识\n   * resulttype属性 指定结果映射对象类型 类路径 增删改可以不指定类因为返回的是一个影响行数\n   * parametertype属性 指定参数映射对象类型 指定执行时传递的parameter类型\n   * sql语句获取参数 #{属性名}\n\n<?xml version="1.0" encoding="utf-8"?>\n<!doctype mapper\n        public "-//mybatis.org//dtd mapper 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n\n<mapper namespace="studentmapper">\n    <select id="any" resulttype="com.xxx..xx" parametertype="java.lang.long">\n        select * from mysql where id = #{id}\n    </select>\n        \n    <insert id="insert" parametertype="笔记.jdbc.src.student">\n\n        insert into studen value (#{id},#{name},#{age})\n        -- 从student 中传递 id name age属性\n    </insert>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 核心配置文件\n\n<?xml version="1.0" encoding="utf-8"?>\n<!doctype configuration\n        public "-//mybatis.org//dtd config 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n\x3c!--        configuration根标签--\x3e\n<configuration>\n\x3c!--  environments配置数据库环境  default属性指定使用哪一个--\x3e\n    <environments default="ie">\n\x3c!--        environment配置数据库环境  id属性唯一标识--\x3e\n        <environment id="ie">\n\x3c!--            transactionmanager事务管理  type属性 采用jdbc默认的事务管理--\x3e\n            <transactionmanager type="jdbc"></transactionmanager>\n\x3c!--           datasource数据源信息  type属性 连接池    --\x3e\n            <datasource type="pooled">\n\x3c!--                property连接数据库的配置信息--\x3e\n                <property name="driver" value="com.mysql.jdbc.driver"/>\n                <property name="url" value="jdbc:mysql://127.0.0.1:3306/mysql"/>\n                <property name="username" value="root"/>\n                <property name="password" value="123456"/>\n            </datasource>\n        </environment>\n    </environments>\n\n\x3c!--    mappers引入映射配置文件--\x3e\n    <mappers>\n\x3c!--        mapper 引入指定的映射配置 resource属性 指定映射配置文件的名名称--\x3e\n        <mapper resource="studentmapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 数据库连接配置引入\n\n * <properties> 引入数据库连接配置文件标签\n   * resource属性 数据库连接配置文件路径\n * 获取连接参数\n   * ${键名}\n\ndriver=com.mysql.jdbc.driver\nurl=jdbc:mysql://127.0.0.1:3306/mysql\nusername=root\npassword=123456\n\n\n1\n2\n3\n4\n\n\n<?xml version="1.0" encoding="utf-8"?>\n<!doctype configuration\n        public "-//mybatis.org//dtd config 3.0//en"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n\x3c!--        configuration根标签--\x3e\n<configuration>\n\x3c!--  environments配置数据库环境  default属性指定使用哪一个--\x3e\n    <properties resource="笔记/mybatis/src/config.properties">\n    <environments default="ie">\n\x3c!--        environment配置数据库环境  id属性唯一标识--\x3e\n        <environment id="ie">\n\x3c!--            transactionmanager事务管理  type属性 采用jdbc默认的事务管理--\x3e\n            <transactionmanager type="jdbc"></transactionmanager>\n\x3c!--           datasource数据源信息  type属性 连接池    --\x3e\n            <datasource type="pooled">\n\x3c!--                property连接数据库的配置信息--\x3e\n                <property name="driver" value="${driver}"/>\n                <property name="url" value="${url}"/>\n                <property name="username" value="${username}"/>\n                <property name="password" value="${password}"/>\n            </datasource>\n        </environment>\n    </environments>\n\n\x3c!--    mappers引入映射配置文件--\x3e\n    <mappers>\n\x3c!--        mapper 引入指定的映射配置 resource属性 指定映射配置文件的名名称--\x3e\n        <mapper resource="studentmapper.xml"/>\n    </mappers>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 起别名\n\n在映射配置中 我们resulttype属性需要提供 类的全路径 可以在核心配置文件中起别名来简写\n\n * <typealiaser> 为全类名起别名的父标签\n * <typealias> 为全类名起步名的子标签\n   * 属性:\n     * type 指定全类名\n     * alias 指定别名\n * <package> 为指定包下所有类起别的子标签 别名就是类名\n\n<typealiases>\n    <typealias type="笔记.jdbc.src.student" alias="student"></typealias>\n</typealiases>\n\n\n1\n2\n3\n\n\n\n\n\n# log4j\n\n在核心配置文件添加\n\n<settings>\n    <setting name="logimpl" value="log4j"/>\n</settings>\n\n\n1\n2\n3\n\n\n并配置好log4j.properties 配置\n\n\n# 分层思想\n\n控制层(controller) ====> 业务层(service) ====> 持久层(dao)\n\n持久层 对接数据库\n\n业务层 处理业务逻辑 此处只是暂时直接调用dao层的方法\n\n控制层 使用 test类\n\n\n# 接口代理\n\n通过接口代理 我们只需要写dao层的接口 由mybatis 框架根据接口的定义来创建接口的动态代理对象\n\n * 映射配置文件中的名称空间必须与dao层接口的全类名相同\n * 映射配置文件中的增删改查的id属性必须和dao层接口的方法名相同\n * 映射配置文件中的增删改查标签的parametertype属性必须和dao层接口方法的参数相同\n * 映射配置文件中的增删改查标签的resulttype属性必须和dao层接口的返回值相同\n\ngetmapper(class<t> cls) 获取指定接口的代理实现类对象\n\nmybatisdemo01 mapper = sqlsession.getmapper(mybatisdemo01.class);\n\n\n1\n\n\n\n# 源码分析\n\n通过getmapper()方法 获取到 org.apache.ibatis.binding.mapperproxy 代理对象 底层使用 jdk 的动态代理技术 帮我们实现代理实现类对象\n\n执行方法时调用了 mappermethod.execute()方法 通过switch语句 判断操作类型是增删改查操作\n\n通过sqlsession 方法来执行\n\n\n# 动态sql\n\n可以根据sql语句动态根据条件查询\n\n * <where> 条件标签 如果有动态条件 则使用该标签替代where关键字\n * <if> 条件判断标签 test属性 条件控制 如果成立则拼接sql语句\n\n<select id="dongtaisql" resulttype="studen">\n    select * from stden\n    <where>\n        <if test="id != null">\n            id = #{id}\n        </if>\n        <if test="age != null">\n            and age = #{age}\n        </if>\n    </where>\n</select>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * <foreach> 循环遍历标签 适用于多个参数或者的关系\n   * collection属性 参数容器类型(list集合 array数组)\n   * open属性 开始的sql语句\n   * close属性 结束的sql语句\n   * item属性 参数变量名\n   * separator属性 分隔符\n\n\x3c!-- select * forme studen id in(1,2,3)--\x3e\n<select id="selectbyids" resulttype="studen" parametertype="list">\n    select * from studen\n    <where>\n        <foreach collection="list" open="id in(" close=")" separator="," item="id">\n            #{id}\n        </foreach>\n    </where>\n</select>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# sql 片段抽取\n\n将一些重复性的sql语句进行抽取 达到复用的效果\n\n * <sql> 标签 抽取sql语句标签 id属性唯一标识\n * <include> 引入sql片段标签 refid属性需要引用片段的唯一标识\n\n<sql id="select">select * from studen</sql>\n<select id="qsq" resulttype="student" parametertype="student">\n<include refid="select"></include> where id = #{id}\n</select>\n\n\n1\n2\n3\n4\n\n\n\n# 获取自增的id 再插入数据\n\n先执行last_insert_id() 返回一个id并封装在对象 再执行inser语句\n\n<insert id="add" parametertype="com.itheima.pojo.checkgroup">\n    <selectkey resulttype="java.lang.integer" order="after" keyproperty="id">\n        select last_insert_id()\n    </selectkey>\n    insert into t_checkgroup(code, name, sex, helpcode, remark, attention)\n    values (#{code}, #{name}, #{sex}, #{helpcode}, #{remark}, #{attention})\n</insert>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 分页插件\n\nmybatis不带分页功能的 mysql中分页使用 limit 语句 不同的数据库实现的关键字也不同\n\npagehelper 第三方分页助手\n\n * 导入jar包 pagehelper.jar jsqlparser.jar\n\n * 在核心配置文件中集成分页助手\n   \n   * <plugins>\n             <plugin interceptor="com.github.pagehelper.pageinterceptor"></plugin>\n     \t</plugins>\n     \n     \n     1\n     2\n     3\n     \n\n * 在测试类中使用分页功能\n   \n   * pagehelper.startpage(1,3);\n     // pagehelper.startpage(第几页,每页显示多少个);  设置分页参数\n     \n     \n     1\n     2\n     \n\n\n# pageinfo 封装分页相关参数的功能类\n\n * gettotal() 获取总条数\n * getpages() 获取总页数\n * getpagenum() 获取当前页\n * getpagesize() 获取每页显示条数\n * getprepage() 获取上一页\n * getnextpage() 获取下一页\n * islsfirespage() 获取是否是第一页\n * islslastpage() 获取是否是最后一页\n\npageinfo<student> info =new pageinfo<>(list);\nint total = info.gettotal();\n\n\n1\n2\n\n\n\n# 多表操作\n\n\n# 一对一\n\n<mapper namespace="1v1">\n    \x3c!--\n        resultmap 配置字段和实体对象属性的映射关系\n        id为唯一标识\n        type为映射对象路径\n    --\x3e\n    <resultmap id="onetoone" type="card">\n        \x3c!--\n            id为主键标签  column为表中列名   property为对象属性名称\n            result为其他列标签\n        --\x3e\n        <id column="cid" property="id"/>\n        <result column="number" property="number" />\n                \x3c!--\n                        association: 配置被包含对象的映射关系   对象内的对象\n                        property: 被包含对象的变量名   对象内的对象变量名是什么\n                        javatype:被包含对象的数据类型\n                --\x3e\n        <association property="prs" javatype="person">\n            <id column="pid" property="id"></id>\n            <result column="name" property="name"/>\n            <result column="age" property="age"/>\n\n        </association>\n    </resultmap>\n        \x3c!--\n            resultmap 为多表操作映射\n         --\x3e\n    <select id="selectall" resultmap="onetoone">\n        select c.id cid,number,pid,name,age from card c,person p where c.pid=p.id;\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n * <resultmap> 配置字段和实体对象属性的映射关系\n   * id属性 唯一标识\n   * type属性 实体对象类型\n * <id> 配置主键映射关系标签\n * <result> 配置非主键映射关系标签\n   * column属性 表中字段名称\n   * property属性 实体对象变量名称\n * <association> 配置被包含对象的映射关系标签\n   * property属性 被包含对象的变量名\n   * javatype属性 被包含对象的数据类型\n\n\n# 一对多\n\n<mapper>\n    <resultmap id="onetomany" type="classes">\n        <id column="cid" property="id"/>\n        <result column="canem" property="name"/>\n\n        \x3c!--\n            collection: 配置被包含的集合对象映射关系\n            property属性  被包含集合对象的变量名\n            oftype属性  被包含集合对象元素的数据类型\n            --\x3e\n        <collection property="students" oftype="student">\n            <id column="sid" property="id"/>\n            <result column="sname" property="name"/>\n            <result column="sage" property="age"/>\n        </collection>\n    </resultmap>\n    <select id="selectall" resultmap="onetomany">\n        select c.id cid,c.name cname,s.id sid,s.name sname,s.age sage from classes c,student s where c.id=s.id\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * <collection> 配置被包含的集合对象映射关系\n   * property属性 被包含集合对象的变量名\n   * oftype属性 被包含集合对象元素的数据类型\n\n\n# 多对多\n\n<mapper>\n    <resultmap id="manytomany" type="sstdent">\n        <id column="sid" property="id"/>\n        <result column="sname" property="name"/>\n        <result column="sage" property="age"/>\n        <collection property="coures" oftype="corse">\n            <id column="cid" property="id"/>\n            <result column="cname" property="name"/>\n        </collection>\n    </resultmap>\n    <select id="selectall" resultmap="manytomany">\n        select  sc.sid,s.name sname,s.age sage,sc.cid,c.name cname from stdent s,course c,stu,_cr sc where sc.sid=s.id and sc.cid=c.id\n    </select>\n</mapper>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n<collection> 配置被包含的集合对象映射关系\n\n * property属性 被包含集合对象的变量名\n * oftype属性 被包含集合对象元素的数据类型\n\n\n# 注解\n\n * @select("查询的sql语句") 指定参数还是#{key}\n * @insert("新增的sql语句") 如:@insert("inser into student value (#{id},#{age},#{name})")\n * @update("修改的sql语句")\n * @delete("删除的sql语句")\n\n通过注解形式的操作 不需要创建映射配置文件 映射配置内容 写在核心配置文件中\n\n<mappers>\n    \t\x3c!-- name为接口所在的包路径 可以指定类或者指定包下的所有类 --\x3e\n        <package name="com.xxx.xxx.stdentmapper"/>\n</mappers>\n\n\n1\n2\n3\n4\n\n\n\n# 多表操作\n\n# 一对一\n\n@select("select * fome card")\n    @results({\n            @result(column = "id" ,property = "id"),\n            @result(column = "number" , property = "number"),\n            @result(\n                    property = "p",   // 被包含对象的变量名\n                    javatype = person.class,  // 被包含对象的实现数据类型类\n                    column = "pid",  // 根据上面select查询出来表中的哪个字段来查询第二个表\n                    /*\n                     one = @one()  一对一写法\n                     select属性  指定调用哪个接口的哪个方法\n                     */\n                    one = @one(select = "com.xxx.xxx接口.selectbyid方法")\n            )\n    })\n    list<card> selectall();\n\n//@one注解调用的接口方法\n    @select("select * from person where id=#{id}")\n    person selectbyid(integer id);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * @results 封装映射关系的父注解 result[] vlue() 定义了result数组\n   * @result 封装映射关系的子注解\n     * column属性 查询出的表中字段名称\n     * property属性 实体对象中的属性名称\n     * javatype属性 被包含对象的数据类型\n     * one 属性 一对一查询\n       * @one一对一查询注解\n         * select属性 指向要调用某个接口中的方法\n\n# 一对多\n\n//一对多\n@select("select * fome classes")\n@results({\n        @result(column = "id" ,property = "id"),\n        @result(column = "number" , property = "number"),\n        @result(\n                property = "students",   // 被包含对象的变量名\n                javatype = list.class,  // 被包含对象的实现数据类型类\n                column = "id",  // 根据上面select查询出来表中的哪个字段来查询第二个表\n\n        /*                    many = @many()  一对多写法\n                select属性  指定调用哪个接口的哪个方法*/\n                many= @many(select = "com.xxx.xxx接口.xxx方法")\n        )\n})\nlist<classes> selectall();\n\n//@one注解调用的接口方法\n@select("select * from student where cid=#{cid}")\nlist<student> selectbyid(integer cid);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * many属性 一对多查询\n   * @many一对多查询注解\n     * select属性 指向要调用某个接口中的方法\n\n# 多对多\n\n//多对多\n@select("select distinct s.id,s.name fome studebt s,stu_cr sc where sc.sid = s.id")\n@results({\n        @result(column = "id" ,property = "id"),\n        @result(column = "number" , property = "number"),\n        @result(\n                property = "students",   // 被包含对象的变量名\n                javatype = list.class,  // 被包含对象的实现数据类型类\n                column = "id",  // 根据上面select查询出来表中的哪个字段来查询第二个表\n\n                /*                  many = @many()  一对多写法\n                select属性  指定调用哪个接口的哪个方法*/\n                        many= @many(select = "com.xxx.xxx接口.xxx方法")\n        )\n})\nlist<student> selectall();\n\n//@one注解调用的接口方法\n@select("select c.id,c.name from stu_cr sc,course c where sc.cid=c.id and sc.sid=#{id}")\nlist<course> selectbyid(integer id);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n * many属性 一对多查询\n   * @many一对多查询注解\n     * select属性 指向要调用某个接口中的方法\n\n\n# sql 构建\n\n\n\npublic static void main(string[] args) {\n    system.out.println(getselectall());\n    //select *\n    //from student\n}\n\npublic static string getselectall(){\n    return new sql(){\n        {\n            select("*");\n            from("student");\n        }\n    }.tostring();\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n * @selectprovider(type = sql构造的类.class , mehod = "要执行类中方法") 查询\n\n * @insertprovider((type = sql构造的类.class , mehod = "要执行类中方法") 新增\n   \n   public static string getinsert(student stu){\n           return new sql(){\n               {\n                   insert_into("student");\n                   into_values("#{id},#{age},#{name}");\n               }\n           }.tostring();\n   \n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   \n\n * @updateprovider((type = sql构造的类.class , mehod = "要执行类中方法") 更新\n\n * @deleteprovider((type = sql构造的类.class , mehod = "要执行类中方法") 删除',charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"Jedis",frontmatter:{title:"Jedis",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/40c5ff/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/04.Jedis.html",relativePath:"后端/02.JavaEE/04.Jedis.md",key:"v-6a96a0b1",path:"/pages/40c5ff/",headers:[{level:2,title:"连接池",slug:"连接池",normalizedTitle:"连接池",charIndex:268},{level:2,title:"持久化",slug:"持久化",normalizedTitle:"持久化",charIndex:1164},{level:3,title:"RDB",slug:"rdb",normalizedTitle:"rdb",charIndex:1174},{level:3,title:"AOF",slug:"aof",normalizedTitle:"aof",charIndex:1613}],headersStr:"连接池 持久化 RDB AOF",content:'# Jedis\n\nJava 连接 redis 服务 Jedis、SpringData Redis、Lettuce\n\n * 导入jar包\n\n//获取连接\nJedis jedis =new Jedis("192.168.130.128",6379);\njedis.auth("123456");\n\n//执行\njedis.set("name2","123");\nSystem.out.println(jedis.get("name"));\n\n//关闭连接\njedis.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 连接池\n\n导入jedis.jar commons-pool.jar\n\nprivate static int MaxTotal, maxIdel, port;\nprivate static String host;\nprivate static JedisPoolConfig jpc;\nprivate static JedisPool jp;\n\nstatic {\n    ResourceBundle bundle = ResourceBundle.getBundle("笔记/redis/src/redis");\n    MaxTotal = Integer.parseInt(bundle.getString("redis.maxTotal"));\n    maxIdel = Integer.parseInt(bundle.getString("redis.maxIdel"));\n    host = bundle.getString("redis.host");\n    port = Integer.parseInt(bundle.getString("redis.port"));\n    jpc = new JedisPoolConfig();\n    jpc.setMaxTotal(MaxTotal);\n    jpc.setMaxIdle(maxIdel);\n    jp = new JedisPool(jpc, host, port, 2000, "123456");\n\n\n\n}\n\npublic static Jedis getJedis() {\n    return jp.getResource();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n创建 redis.properties\n\nredis.maxTotal=50\nredis.maxIdel=10\nredis.host=192.168.130.128\nredis.port=6379\n\n\n1\n2\n3\n4\n\n\n\n# 持久化\n\n\n\n\n# RDB\n\n * save 保存\n\ndbfilename "dump-6379.rdb"  #rdb快照保存的文件名  不设置也有默认名当save执行时\nrdbcopression yes  #存储到本地是否压缩 默认为yes no则不压缩\nrdbchecksum yes #设置读写文件过程是否进行RDB格式校验 默认为yes  \n\n\n1\n2\n3\n\n\nsave指令会阻塞当前服务器直到RDB完成 线上不建议使用\n\n * bgsave 后台保存\n\n\n\n * save second changes 设置自动持久化 满足限定时间内key的变化数据则进行save\n\n#在redis配置文件中设置\nsave second changes  #设置自动持久化  如:save 10 2  10秒中2个key发生变化 自动保存\n\n\n1\n2\n\n\n\n\n\n\n * debug reload 服务器运行过程中重启 也会执行一次RDB\n * shutdown save 关闭服务器并save\n\n\n# AOF\n\n配置文件\n\nappendonly yes #开启AOF持久化  默认为no不开启\nappendfilename appendonly-6379.aof # 保存文件名 有默认名\nappendfsync always|everysec|no  #AOF写数据策略 默认为everysec\n#always 每次写入操作同步到AOF中\n#everysec 每秒将缓冲区的同步到AOF\n#no 系统控制 整个过程不可控\n\n\n1\n2\n3\n4\n5\n6\n\n\nRDB和AOF都是影响了数据库的数据才作记录\n\n * bgrewriteaof 后台重写aof 将aof文件进行压缩简化合并\n\n\n\n',normalizedContent:'# jedis\n\njava 连接 redis 服务 jedis、springdata redis、lettuce\n\n * 导入jar包\n\n//获取连接\njedis jedis =new jedis("192.168.130.128",6379);\njedis.auth("123456");\n\n//执行\njedis.set("name2","123");\nsystem.out.println(jedis.get("name"));\n\n//关闭连接\njedis.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 连接池\n\n导入jedis.jar commons-pool.jar\n\nprivate static int maxtotal, maxidel, port;\nprivate static string host;\nprivate static jedispoolconfig jpc;\nprivate static jedispool jp;\n\nstatic {\n    resourcebundle bundle = resourcebundle.getbundle("笔记/redis/src/redis");\n    maxtotal = integer.parseint(bundle.getstring("redis.maxtotal"));\n    maxidel = integer.parseint(bundle.getstring("redis.maxidel"));\n    host = bundle.getstring("redis.host");\n    port = integer.parseint(bundle.getstring("redis.port"));\n    jpc = new jedispoolconfig();\n    jpc.setmaxtotal(maxtotal);\n    jpc.setmaxidle(maxidel);\n    jp = new jedispool(jpc, host, port, 2000, "123456");\n\n\n\n}\n\npublic static jedis getjedis() {\n    return jp.getresource();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n创建 redis.properties\n\nredis.maxtotal=50\nredis.maxidel=10\nredis.host=192.168.130.128\nredis.port=6379\n\n\n1\n2\n3\n4\n\n\n\n# 持久化\n\n\n\n\n# rdb\n\n * save 保存\n\ndbfilename "dump-6379.rdb"  #rdb快照保存的文件名  不设置也有默认名当save执行时\nrdbcopression yes  #存储到本地是否压缩 默认为yes no则不压缩\nrdbchecksum yes #设置读写文件过程是否进行rdb格式校验 默认为yes  \n\n\n1\n2\n3\n\n\nsave指令会阻塞当前服务器直到rdb完成 线上不建议使用\n\n * bgsave 后台保存\n\n\n\n * save second changes 设置自动持久化 满足限定时间内key的变化数据则进行save\n\n#在redis配置文件中设置\nsave second changes  #设置自动持久化  如:save 10 2  10秒中2个key发生变化 自动保存\n\n\n1\n2\n\n\n\n\n\n\n * debug reload 服务器运行过程中重启 也会执行一次rdb\n * shutdown save 关闭服务器并save\n\n\n# aof\n\n配置文件\n\nappendonly yes #开启aof持久化  默认为no不开启\nappendfilename appendonly-6379.aof # 保存文件名 有默认名\nappendfsync always|everysec|no  #aof写数据策略 默认为everysec\n#always 每次写入操作同步到aof中\n#everysec 每秒将缓冲区的同步到aof\n#no 系统控制 整个过程不可控\n\n\n1\n2\n3\n4\n5\n6\n\n\nrdb和aof都是影响了数据库的数据才作记录\n\n * bgrewriteaof 后台重写aof 将aof文件进行压缩简化合并\n\n\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"JSP",frontmatter:{title:"JSP",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/427528/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/75.JSP.html",relativePath:"后端/01.JavaSE/75.JSP.md",key:"v-33e328d1",path:"/pages/427528/",headers:[{level:2,title:"语法",slug:"语法",normalizedTitle:"语法",charIndex:10},{level:2,title:"指令",slug:"指令",normalizedTitle:"指令",charIndex:180},{level:2,title:"细节",slug:"细节",normalizedTitle:"细节",charIndex:340},{level:2,title:"MVC 模型",slug:"mvc-模型",normalizedTitle:"mvc 模型",charIndex:523},{level:2,title:"EL 表达式",slug:"el-表达式",normalizedTitle:"el 表达式",charIndex:536},{level:3,title:"获取数据",slug:"获取数据",normalizedTitle:"获取数据",charIndex:551},{level:3,title:"注意事项",slug:"注意事项",normalizedTitle:"注意事项",charIndex:920},{level:3,title:"运算符",slug:"运算符",normalizedTitle:"运算符",charIndex:983},{level:3,title:"EL 表达式隐式对象",slug:"el-表达式隐式对象",normalizedTitle:"el 表达式隐式对象",charIndex:997},{level:2,title:"JSTL",slug:"jstl",normalizedTitle:"jstl",charIndex:1014},{level:3,title:"核心标签库",slug:"核心标签库",normalizedTitle:"核心标签库",charIndex:1070},{level:2,title:"Filter",slug:"filter",normalizedTitle:"filter",charIndex:1231},{level:3,title:"FilterChain",slug:"filterchain",normalizedTitle:"filterchain",charIndex:1290},{level:3,title:"生命周期",slug:"生命周期",normalizedTitle:"生命周期",charIndex:464},{level:3,title:"FilterConfig",slug:"filterconfig",normalizedTitle:"filterconfig",charIndex:1915},{level:3,title:"过滤器五种拦截行为",slug:"过滤器五种拦截行为",normalizedTitle:"过滤器五种拦截行为",charIndex:2527}],headersStr:"语法 指令 细节 MVC 模型 EL 表达式 获取数据 注意事项 运算符 EL 表达式隐式对象 JSTL 核心标签库 Filter FilterChain 生命周期 FilterConfig 过滤器五种拦截行为",content:'# JSP\n\n\n# 语法\n\n * 注释 <%-- 内容 --%>\n * java代码块 <% java代码 %>\n * jsp表达式 <%= 表达式 %> 相对应out.println()\n * jsp声明 <%! 声明变量或者方法 %> <%! String str ="abc"; %> 如果加! 则声明是成员变量 不加则是局部变量 方法必须加!\n\n\n# 指令\n\n * page指令 配置 <%@ page 属性名=属性值 属性名=属性值 %>\n   * \n * include指令 包含其他页面 <%@ include file=包含的页面 %>\n * taglib指令 可以引入外部标签库 <%@ taglib uri=标签库的地址 prefix=前缀名称 %>\n\n\n# 细节\n\n * 九大隐式对象 不需要创建,jsp已经帮我们创建了\n   \n   * \n\n * PageContext 对象 是JSP独有,在Servlet中没有 是四大域对象中的页面域对象,还可以操作其他三个域对象中的属性 还可以获取其八个隐式对象 生命周期随着JSP 每一个JSP都有一个PageContext 对象\n\n * 四大域对象\n   \n   * \n\n\n# MVC 模型\n\n\n\n\n# EL 表达式\n\n在JSP页面中获取数据 让我们的JSP脱离java代码块和JSP表达式\n\n${表达式内容} 等效于 <% out.println(request.getAttribute("user"))%>和<%=request.getAttribute("user") %>\n\n只要是四大域的内容都可以获取出来\n\n先用setAttribute() 共享数据\n\n\n# 获取数据\n\n * 基本数据\n   * ${name}\n * 自定义对象数据\n   * ${对象名}\n   * ${对象名.属性名} 调用相对应的get方法\n * 数组类型数据\n   * ${arr}\n   * ${arr[0]}\n * List集合数据\n   * ${list}\n   * ${list[0]}\n * Map集合数据\n   * ${map}\n   * ${map.key}\n\n\n# 注意事项\n\n * 没有空指针异常\n * 没有索引越界异常\n * 没有字符串的拼接\n * 获取四大域对象是从小到大查找\n\n\n# 运算符\n\n\n\n\n\n\n\n\n# EL 表达式隐式对象\n\n\n\n\n# JSTL\n\nJSP 标准标签库 可以利用这些标签取代JSP页面上的JAVA 代码 从而提高的可读性\n\n\n\n\n# 核心标签库\n\n\n\n 1. 导入jstl.jar包并添加到项目中\n\n 2. 在jsp中添加\n    \n    <%@ taglib uri="http://java.sun.com/jsp/jst1/core" prefix="自定义标签"%>\n    \n    \n    1\n    \n\n 3. 使用jstl标签\n\n\n# Filter\n\n过滤器, Filter是用来过滤请求资源和资源响应的对象 , 并且按需求来处理不同的请求\n\n\n\n\n# FilterChain\n\n\n\n * 继承javax.servlet.Filter\n\n * 重写doFilter()方法\n\n * 处理逻辑\n\n * 使用FilterChain的dodoFilter放行\n\n * 使用注解 @WebFilter("拦截的路径") 拦截需要拦截请求的路径\n   \n   * 使用配置文件方式 在web.xml 与之前servlet类似\n   \n   * <filter>\n         <filter-name>demo</filter-name>\n         <filter-class>com.example.demo.demo</filter-class>\n     </filter>\n     <filter-mapping>\n         <filter-name>demo</filter-name>\n         <url-pattern>/*</url-pattern>\n     </filter-mapping>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n多个过滤器,取决于过滤器映射的顺序\n\n\n# 生命周期\n\n * 创建 当应用价值实例化对象并执行init方法\n * 过程 执行doFilter方法\n * 销毁 当应用卸载或者服务器停止 执行 destroy 方法\n\n\n# FilterConfig\n\nFilterConfig是一个接口 代表过滤器的配置对象 可以加载一些初始化参数 不需要创建 由服务器通过init方法传递提供\n\n * getFilterName() 获取过滤器名称\n\n * getInitParameter(String key) 根据key获取value\n   \n   * <filter>\n         <filter-name>demo</filter-name>\n         <filter-class>com.example.demo.demo</filter-class>\n         \x3c!-- 需要添加init-param标签--\x3e\n         <init-param>\n             <param-name>key</param-name>\n             <param-value>value</param-value>\n         </init-param>\n     </filter>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     \n\n * getInitParameterNames() 获取所有参数的key\n\n * getrServletContext() 获取应用上下文对象\n\n\n# 过滤器五种拦截行为\n\nFilter过滤器默认拦截的是请求,如需要拦截请求转发 请求包含 需要在配置文件配置\n\n\n\n\x3c!-- 全局错误页面 --\x3e\n<error-page>\n    \t\x3c!-- 错误类型--\x3e\n        <exception-type>java.lang.Exception</exception-type>\n    \t\x3c!-- 发生错误后跳转页面 虚拟路径 --\x3e\n        <location>/err.jsp</location>\n    </error-page>\n    <error-page>\n        \x3c!--  错误代码 --\x3e\n        <error-code>404</error-code>\n        <location>/err.jsp</location>\n    </error-page>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n',normalizedContent:'# jsp\n\n\n# 语法\n\n * 注释 <%-- 内容 --%>\n * java代码块 <% java代码 %>\n * jsp表达式 <%= 表达式 %> 相对应out.println()\n * jsp声明 <%! 声明变量或者方法 %> <%! string str ="abc"; %> 如果加! 则声明是成员变量 不加则是局部变量 方法必须加!\n\n\n# 指令\n\n * page指令 配置 <%@ page 属性名=属性值 属性名=属性值 %>\n   * \n * include指令 包含其他页面 <%@ include file=包含的页面 %>\n * taglib指令 可以引入外部标签库 <%@ taglib uri=标签库的地址 prefix=前缀名称 %>\n\n\n# 细节\n\n * 九大隐式对象 不需要创建,jsp已经帮我们创建了\n   \n   * \n\n * pagecontext 对象 是jsp独有,在servlet中没有 是四大域对象中的页面域对象,还可以操作其他三个域对象中的属性 还可以获取其八个隐式对象 生命周期随着jsp 每一个jsp都有一个pagecontext 对象\n\n * 四大域对象\n   \n   * \n\n\n# mvc 模型\n\n\n\n\n# el 表达式\n\n在jsp页面中获取数据 让我们的jsp脱离java代码块和jsp表达式\n\n${表达式内容} 等效于 <% out.println(request.getattribute("user"))%>和<%=request.getattribute("user") %>\n\n只要是四大域的内容都可以获取出来\n\n先用setattribute() 共享数据\n\n\n# 获取数据\n\n * 基本数据\n   * ${name}\n * 自定义对象数据\n   * ${对象名}\n   * ${对象名.属性名} 调用相对应的get方法\n * 数组类型数据\n   * ${arr}\n   * ${arr[0]}\n * list集合数据\n   * ${list}\n   * ${list[0]}\n * map集合数据\n   * ${map}\n   * ${map.key}\n\n\n# 注意事项\n\n * 没有空指针异常\n * 没有索引越界异常\n * 没有字符串的拼接\n * 获取四大域对象是从小到大查找\n\n\n# 运算符\n\n\n\n\n\n\n\n\n# el 表达式隐式对象\n\n\n\n\n# jstl\n\njsp 标准标签库 可以利用这些标签取代jsp页面上的java 代码 从而提高的可读性\n\n\n\n\n# 核心标签库\n\n\n\n 1. 导入jstl.jar包并添加到项目中\n\n 2. 在jsp中添加\n    \n    <%@ taglib uri="http://java.sun.com/jsp/jst1/core" prefix="自定义标签"%>\n    \n    \n    1\n    \n\n 3. 使用jstl标签\n\n\n# filter\n\n过滤器, filter是用来过滤请求资源和资源响应的对象 , 并且按需求来处理不同的请求\n\n\n\n\n# filterchain\n\n\n\n * 继承javax.servlet.filter\n\n * 重写dofilter()方法\n\n * 处理逻辑\n\n * 使用filterchain的dodofilter放行\n\n * 使用注解 @webfilter("拦截的路径") 拦截需要拦截请求的路径\n   \n   * 使用配置文件方式 在web.xml 与之前servlet类似\n   \n   * <filter>\n         <filter-name>demo</filter-name>\n         <filter-class>com.example.demo.demo</filter-class>\n     </filter>\n     <filter-mapping>\n         <filter-name>demo</filter-name>\n         <url-pattern>/*</url-pattern>\n     </filter-mapping>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n多个过滤器,取决于过滤器映射的顺序\n\n\n# 生命周期\n\n * 创建 当应用价值实例化对象并执行init方法\n * 过程 执行dofilter方法\n * 销毁 当应用卸载或者服务器停止 执行 destroy 方法\n\n\n# filterconfig\n\nfilterconfig是一个接口 代表过滤器的配置对象 可以加载一些初始化参数 不需要创建 由服务器通过init方法传递提供\n\n * getfiltername() 获取过滤器名称\n\n * getinitparameter(string key) 根据key获取value\n   \n   * <filter>\n         <filter-name>demo</filter-name>\n         <filter-class>com.example.demo.demo</filter-class>\n         \x3c!-- 需要添加init-param标签--\x3e\n         <init-param>\n             <param-name>key</param-name>\n             <param-value>value</param-value>\n         </init-param>\n     </filter>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     \n\n * getinitparameternames() 获取所有参数的key\n\n * getrservletcontext() 获取应用上下文对象\n\n\n# 过滤器五种拦截行为\n\nfilter过滤器默认拦截的是请求,如需要拦截请求转发 请求包含 需要在配置文件配置\n\n\n\n\x3c!-- 全局错误页面 --\x3e\n<error-page>\n    \t\x3c!-- 错误类型--\x3e\n        <exception-type>java.lang.exception</exception-type>\n    \t\x3c!-- 发生错误后跳转页面 虚拟路径 --\x3e\n        <location>/err.jsp</location>\n    </error-page>\n    <error-page>\n        \x3c!--  错误代码 --\x3e\n        <error-code>404</error-code>\n        <location>/err.jsp</location>\n    </error-page>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"Listener",frontmatter:{title:"Listener",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/70e34e/",categories:["后端","Java"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/01.JavaSE/76.Listener.html",relativePath:"后端/01.JavaSE/76.Listener.md",key:"v-257128de",path:"/pages/70e34e/",headers:[{level:2,title:"观察者模式",slug:"观察者模式",normalizedTitle:"观察者模式",charIndex:71},{level:2,title:"监听对象",slug:"监听对象",normalizedTitle:"监听对象",charIndex:143},{level:2,title:"监听域对象属性",slug:"监听域对象属性",normalizedTitle:"监听域对象属性",charIndex:642},{level:2,title:"监听会话相关的感知型监听器",slug:"监听会话相关的感知型监听器",normalizedTitle:"监听会话相关的感知型监听器",charIndex:1431},{level:2,title:"配置监听器",slug:"配置监听器",normalizedTitle:"配置监听器",charIndex:1800}],headersStr:"观察者模式 监听对象 监听域对象属性 监听会话相关的感知型监听器 配置监听器",content:'# Listener\n\n监听器 所有监听器都是基于观察者设计模式的\n\n可以对 对象的创建销毁 域对象中属性变化 会话相关内容进行监听\n\n\n# 观察者模式\n\n * 事件源: 触发事件的对象\n * 事件: 触发的动作,封装了事件源\n * 监听器: 当事件源触发事件后,可以完成功能\n\n\n# 监听对象\n\n * ServletContextListener 用于监听ServletContext对象的创建和销毁\n   * contextInitialized(ServletContextEvent sce) 对象创建时执行该方法\n   * contextDestroyed(ServletContextEvent sce) 对象销毁时执行该方法\n * HttpSessionListener 监听HttpSession对象的创建和销毁\n   * sessionCreated(HttpsessionEvent se) 对象创建时执行该方法\n   * sessionDestroyed(HttpsessionEvent se) 对象销毁时执行该方法\n * ServletRequestListener 监听ServletRequest的创建和销毁\n   * requestInitialized(ServletRequestEvent sre) 对象创建时执行该方法\n   * requestDestroyed(ServletRequestEvent sre) 对象销毁时执行该方法\n\n\n# 监听域对象属性\n\n * ServletContextAttributeListener 监听ServletContext应用域中的属性变化\n   * attributeAdded(ServletContextAttributeEvent scae) 域中添加属性执行该方法\n   * attributeRemoved(ServletContextAttributeEvent scae) 域中移除属性执行该方法\n   * attributeReplaced(ServletContextAttributeEvent scae) 域中替换属性执行该方法\n * HttpSessionAttributeListener 监听HttpSession 会话域中的属性变化\n   * attributeAdded(HttpSessionBindingEvent se) 域中添加属性执行该方法\n   * attributeRemoved(HttpSessionBindingEvent se) 域中移除属性执行该方法\n   * attributeReplaced(HttpSessionBindingEvent se) 域中替换属性执行该方法\n * ServletRequestAttributeListener 监听ServletRequest 请求域中的属性变化\n   * attributeAdded(ServletRequestAttributeEvent srae) 域中添加属性执行该方法\n   * attributeRemoved(ServletRequestAttributeEvent srae) 域中移除属性执行该方法\n   * attributeReplaced(ServletRequestAttributeEvent srae) 域中替换属性执行该方法\n\n\n# 监听会话相关的感知型监听器\n\n * HttpSessionBindListener 感知对象和会话域绑定的监听器\n   \n   * valueBound(HttpSessionBindEvent event) 数据添加到会话域中(绑定)执行方法\n   * valueUnbound(HttpSessionBindEvent event) 数据移除到会话域中(解绑)执行方法\n\n * HttpSessionActivationListener 感知会话域中对象钝化和活化的监听器\n   \n   * seesionWillPassivate(HttpSessionEvent se) 会话域中数据钝化时执行该方法\n   * seesionDidActivate(HttpSessionEvent se) 会话域中数据活化时执行该方法\n\n\n# 配置监听器\n\n * 注解标识 @WebListener\n   \n   * 拦截多个指定页面@WebListener (value = {"/xxx","/xxx"})\n\n * web.xml配置\n   \n   * <listener>\n         <listener-class>com.example.demo</listener-class>\n     </listener>\n     \n     \n     1\n     2\n     3\n     ',normalizedContent:'# listener\n\n监听器 所有监听器都是基于观察者设计模式的\n\n可以对 对象的创建销毁 域对象中属性变化 会话相关内容进行监听\n\n\n# 观察者模式\n\n * 事件源: 触发事件的对象\n * 事件: 触发的动作,封装了事件源\n * 监听器: 当事件源触发事件后,可以完成功能\n\n\n# 监听对象\n\n * servletcontextlistener 用于监听servletcontext对象的创建和销毁\n   * contextinitialized(servletcontextevent sce) 对象创建时执行该方法\n   * contextdestroyed(servletcontextevent sce) 对象销毁时执行该方法\n * httpsessionlistener 监听httpsession对象的创建和销毁\n   * sessioncreated(httpsessionevent se) 对象创建时执行该方法\n   * sessiondestroyed(httpsessionevent se) 对象销毁时执行该方法\n * servletrequestlistener 监听servletrequest的创建和销毁\n   * requestinitialized(servletrequestevent sre) 对象创建时执行该方法\n   * requestdestroyed(servletrequestevent sre) 对象销毁时执行该方法\n\n\n# 监听域对象属性\n\n * servletcontextattributelistener 监听servletcontext应用域中的属性变化\n   * attributeadded(servletcontextattributeevent scae) 域中添加属性执行该方法\n   * attributeremoved(servletcontextattributeevent scae) 域中移除属性执行该方法\n   * attributereplaced(servletcontextattributeevent scae) 域中替换属性执行该方法\n * httpsessionattributelistener 监听httpsession 会话域中的属性变化\n   * attributeadded(httpsessionbindingevent se) 域中添加属性执行该方法\n   * attributeremoved(httpsessionbindingevent se) 域中移除属性执行该方法\n   * attributereplaced(httpsessionbindingevent se) 域中替换属性执行该方法\n * servletrequestattributelistener 监听servletrequest 请求域中的属性变化\n   * attributeadded(servletrequestattributeevent srae) 域中添加属性执行该方法\n   * attributeremoved(servletrequestattributeevent srae) 域中移除属性执行该方法\n   * attributereplaced(servletrequestattributeevent srae) 域中替换属性执行该方法\n\n\n# 监听会话相关的感知型监听器\n\n * httpsessionbindlistener 感知对象和会话域绑定的监听器\n   \n   * valuebound(httpsessionbindevent event) 数据添加到会话域中(绑定)执行方法\n   * valueunbound(httpsessionbindevent event) 数据移除到会话域中(解绑)执行方法\n\n * httpsessionactivationlistener 感知会话域中对象钝化和活化的监听器\n   \n   * seesionwillpassivate(httpsessionevent se) 会话域中数据钝化时执行该方法\n   * seesiondidactivate(httpsessionevent se) 会话域中数据活化时执行该方法\n\n\n# 配置监听器\n\n * 注解标识 @weblistener\n   \n   * 拦截多个指定页面@weblistener (value = {"/xxx","/xxx"})\n\n * web.xml配置\n   \n   * <listener>\n         <listener-class>com.example.demo</listener-class>\n     </listener>\n     \n     \n     1\n     2\n     3\n     ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Jackson",frontmatter:{title:"Jackson",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/df2f58/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/03.Jackson.html",relativePath:"后端/02.JavaEE/03.Jackson.md",key:"v-2bbcab5e",path:"/pages/df2f58/",headersStr:null,content:'# Jackson\n\n坐标\n\n    <dependency>\n      <groupId>com.fasterxml.jackson.core</groupId>\n      <artifactId>jackson-databind</artifactId>\n      <version>2.9.0</version>\n    </dependency>\n    <dependency>\n      <groupId>com.fasterxml.jackson.core</groupId>\n      <artifactId>jackson-core</artifactId>\n      <version>2.9.0</version>\n    </dependency>\n    <dependency>\n      <groupId>com.fasterxml.jackson.core</groupId>\n      <artifactId>jackson-annotations</artifactId>\n      <version>2.9.0</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nJackson是把JSON 转为java对象或者集合的一个工具类\n\nSpringMVC 转换默认使用 Jackson\n\n * ObjectMapper 实现JSON字符串和对象之间的转换\n   * writeValueAsString(object obj) 将java对象转为json字符串\n   * <T> T readValue(String json,class<T> valueTType) 将json字符串转为java对象\n   * <T> T readValue(String json,TypeReferncevalueTType) 将json字符串转为java对象\n * TypeReference 对集合泛型的反序列化\n\nprivate ObjectMapper mapper =new ObjectMapper();\nuser user = new user("张三",23);\n//对象转json\nString json = mapper.writeValueAsString(user);\nSystem.out.println(json);\n\n//json转对象\nuser user1 = mapper.readValue(json, user.class);\nSystem.out.println(user1);\n\n//map转json\nHashMap<String,String> map =new HashMap<>();\nmap.put("姓名","张三");\nmap.put("性别","男");\nString s = mapper.writeValueAsString(map);\nSystem.out.println(s);\n\n//json转map\nHashMap<String,String> hashMap = mapper.readValue(s, HashMap.class);\nSystem.out.println(hashMap);\n\n//map<String,user> 转json\nHashMap<String,user> map2 =new HashMap<>();\nmap2.put("一班",new user("张三",23));\nmap2.put("二班",new user("李四",23));\nString s1 = mapper.writeValueAsString(map2);\nSystem.out.println(s1);\n\n\n//json转 map<String,user>\nHashMap<String,user> map3=mapper.readValue(s1,new TypeReference<HashMap<String,user>>(){});\nSystem.out.println(map3);\n\n//List<String> 换json\nArrayList<String> list=new ArrayList<>();\nlist.add("张三");\nlist.add("李四");\nString s2 = mapper.writeValueAsString(list);\nSystem.out.println(s2);\n\n//List<String> 转json\nArrayList arrayList = mapper.readValue(s2, ArrayList.class);\nSystem.out.println(arrayList);\n\n//List<user> 换json\nArrayList<user> list2=new ArrayList<>();\nlist2.add(new user("张三",23));\nlist2.add(new user("王五",22));\nString s3 = mapper.writeValueAsString(list2);\nSystem.out.println(s3);\n\n//List<user> 转json\nArrayList<user> arrayList2 = mapper.readValue(s3, new TypeReference<ArrayList<user>>(){});\nSystem.out.println(arrayList2);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n',normalizedContent:'# jackson\n\n坐标\n\n    <dependency>\n      <groupid>com.fasterxml.jackson.core</groupid>\n      <artifactid>jackson-databind</artifactid>\n      <version>2.9.0</version>\n    </dependency>\n    <dependency>\n      <groupid>com.fasterxml.jackson.core</groupid>\n      <artifactid>jackson-core</artifactid>\n      <version>2.9.0</version>\n    </dependency>\n    <dependency>\n      <groupid>com.fasterxml.jackson.core</groupid>\n      <artifactid>jackson-annotations</artifactid>\n      <version>2.9.0</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\njackson是把json 转为java对象或者集合的一个工具类\n\nspringmvc 转换默认使用 jackson\n\n * objectmapper 实现json字符串和对象之间的转换\n   * writevalueasstring(object obj) 将java对象转为json字符串\n   * <t> t readvalue(string json,class<t> valuettype) 将json字符串转为java对象\n   * <t> t readvalue(string json,typereferncevaluettype) 将json字符串转为java对象\n * typereference 对集合泛型的反序列化\n\nprivate objectmapper mapper =new objectmapper();\nuser user = new user("张三",23);\n//对象转json\nstring json = mapper.writevalueasstring(user);\nsystem.out.println(json);\n\n//json转对象\nuser user1 = mapper.readvalue(json, user.class);\nsystem.out.println(user1);\n\n//map转json\nhashmap<string,string> map =new hashmap<>();\nmap.put("姓名","张三");\nmap.put("性别","男");\nstring s = mapper.writevalueasstring(map);\nsystem.out.println(s);\n\n//json转map\nhashmap<string,string> hashmap = mapper.readvalue(s, hashmap.class);\nsystem.out.println(hashmap);\n\n//map<string,user> 转json\nhashmap<string,user> map2 =new hashmap<>();\nmap2.put("一班",new user("张三",23));\nmap2.put("二班",new user("李四",23));\nstring s1 = mapper.writevalueasstring(map2);\nsystem.out.println(s1);\n\n\n//json转 map<string,user>\nhashmap<string,user> map3=mapper.readvalue(s1,new typereference<hashmap<string,user>>(){});\nsystem.out.println(map3);\n\n//list<string> 换json\narraylist<string> list=new arraylist<>();\nlist.add("张三");\nlist.add("李四");\nstring s2 = mapper.writevalueasstring(list);\nsystem.out.println(s2);\n\n//list<string> 转json\narraylist arraylist = mapper.readvalue(s2, arraylist.class);\nsystem.out.println(arraylist);\n\n//list<user> 换json\narraylist<user> list2=new arraylist<>();\nlist2.add(new user("张三",23));\nlist2.add(new user("王五",22));\nstring s3 = mapper.writevalueasstring(list2);\nsystem.out.println(s3);\n\n//list<user> 转json\narraylist<user> arraylist2 = mapper.readvalue(s3, new typereference<arraylist<user>>(){});\nsystem.out.println(arraylist2);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"Maven",frontmatter:{title:"Maven",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/682f06/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/05.Maven.html",relativePath:"后端/02.JavaEE/05.Maven.md",key:"v-73c846de",path:"/pages/682f06/",headers:[{level:2,title:"环境变量",slug:"环境变量",normalizedTitle:"环境变量",charIndex:95},{level:2,title:"坐标",slug:"坐标",normalizedTitle:"坐标",charIndex:169},{level:2,title:"仓库配置",slug:"仓库配置",normalizedTitle:"仓库配置",charIndex:260},{level:3,title:"远程仓库配置",slug:"远程仓库配置",normalizedTitle:"远程仓库配置",charIndex:474},{level:2,title:"构建",slug:"构建",normalizedTitle:"构建",charIndex:739},{level:2,title:"依赖管理",slug:"依赖管理",normalizedTitle:"依赖管理",charIndex:837},{level:3,title:"可选依赖",slug:"可选依赖",normalizedTitle:"可选依赖",charIndex:1228},{level:3,title:"排除依赖",slug:"排除依赖",normalizedTitle:"排除依赖",charIndex:1529},{level:2,title:"依赖范围",slug:"依赖范围",normalizedTitle:"依赖范围",charIndex:2059},{level:2,title:"生命周期",slug:"生命周期",normalizedTitle:"生命周期",charIndex:2124},{level:3,title:"插件",slug:"插件",normalizedTitle:"插件",charIndex:2255}],headersStr:"环境变量 坐标 仓库配置 远程仓库配置 构建 依赖管理 可选依赖 排除依赖 依赖范围 生命周期 插件",content:'# Maven\n\nMaven 是一个项目管理工具 ,将项目开发和管理过程抽象成一个项目对象模型(POM)\n\nPOM(Project Object Model) 项目对象模型\n\n\n\n\n\n\n# 环境变量\n\n配置 MAVEN_HOME = MAVEN目录\n\n然后在path中 配置 %MAVEN_HOME%\\bin\n\nmvn\n\n\n1\n\n\n\n# 坐标\n\nMaven中的坐标用于描述仓库中资源的位置\n\nhttps://repo1.maven.org/maven2/\n\nhttps://mvnrepository.com/\n\n\n# 仓库配置\n\n自定义本地仓库 默认为 c:/用户/.m2 目录下\n\n创建 repository 目录 并把maven中conf的settings.xml放到同级中\n\n\x3c!-- 配置 settings --\x3e\n<localRepository>D:\\compile\\maven\\repository</localRepository>\n\n\n1\n2\n\n\n然后把修改后的settings同样覆盖掉 maven conf下的文件\n\n\n# 远程仓库配置\n\n默认远程仓库都是在https://repo1.maven.org/maven2/ 我们换成阿里巴巴的镜像\n\n配置同样是 settings.xml 中\n\n<mirror>\n      <id>aliyunmaven</id>\n      <mirrorOf>*</mirrorOf>\n      <name>阿里云公共仓库</name>\n      <url>https://maven.aliyun.com/repository/public</url>\n</mirror>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 构建\n\n * mvn compile 编译\n * mvn clean 清理\n * mvn test 测试\n * mvn package 打包\n * mvn install 安装到本地仓库\n\n\n# 依赖管理\n\n\x3c!-- 依赖父标签 --\x3e\n<dependencies>\n    \x3c!-- 依赖子标签每个依赖需要用dependency包裹 --\x3e\n    <dependency>\n        \x3c!-- 所属群组id --\x3e\n        <groupId>com.alibaba</groupId>\n        \x3c!-- 所属项目id --\x3e\n        <artifactId>fastjson</artifactId>\n        \x3c!-- 版本号 --\x3e\n        <version>1.2.75</version>\n    </dependency>\n</dependencies>    \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n可以进行依赖传递 即依赖可以加载另外一个项目中依赖 只需要提供群组id和项目id\n\n\n\n\n# 可选依赖\n\n可以选择对外隐藏指定的依赖资源 只有依赖传递才能用\n\n<dependencies>\n    <dependency>\n        <groupId>com.alibaba</groupId>\n        <artifactId>fastjson</artifactId>\n        <version>1.2.75</version>\n        \x3c!-- 对外是否隐藏 --\x3e\n        <optional>true</optional>\n    </dependency>\n</dependencies>    \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 排除依赖\n\n排除指定的依赖 黑名单\n\n<dependencies>\n    <dependency>\n        <groupId>依赖传递下级路径</groupId>\n        <artifactId>fastjson</artifactId>\n        <version>1.2.75</version>\n    </dependency>\n        \x3c!-- 排除依赖 --\x3e\n        <exclusions>\n            <exclusion>\n                 \x3c!-- 所属群组id --\x3e\n                <groupId>org.antlr</groupId>\n                 \x3c!-- 所属项目id --\x3e\n                <artifactId>antlr4-runtime</artifactId>\n            </exclusion>\n        </exclusions>    \n</dependencies>  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 依赖范围\n\n依赖的jar默认在任何地方都可以用 使用scope标签设定其作用范围\n\n\n\n依赖传递 作用范围也会有影响\n\n\n\n\n# 生命周期\n\n * clean 清理工作\n   * pre-clean\n   * clean\n   * post-clea\n * default 构建\n   * \n   * 执行哪个构建 就执行到那个构建就结束 下面的不会执行\n * site\n   * \n\n\n# 插件\n\nhttps://maven.apache.org/plugins/index.html\n\n插件与生命周期内的阶段绑定 执行到对应生命周期就执行对应的插件给你\n\n<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n                <executions>\n                    <execution>\n                        \x3c!-- 此插件在什么生命周期运行 --\x3e\n                        <phase>generate-test-resources</phase>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',normalizedContent:'# maven\n\nmaven 是一个项目管理工具 ,将项目开发和管理过程抽象成一个项目对象模型(pom)\n\npom(project object model) 项目对象模型\n\n\n\n\n\n\n# 环境变量\n\n配置 maven_home = maven目录\n\n然后在path中 配置 %maven_home%\\bin\n\nmvn\n\n\n1\n\n\n\n# 坐标\n\nmaven中的坐标用于描述仓库中资源的位置\n\nhttps://repo1.maven.org/maven2/\n\nhttps://mvnrepository.com/\n\n\n# 仓库配置\n\n自定义本地仓库 默认为 c:/用户/.m2 目录下\n\n创建 repository 目录 并把maven中conf的settings.xml放到同级中\n\n\x3c!-- 配置 settings --\x3e\n<localrepository>d:\\compile\\maven\\repository</localrepository>\n\n\n1\n2\n\n\n然后把修改后的settings同样覆盖掉 maven conf下的文件\n\n\n# 远程仓库配置\n\n默认远程仓库都是在https://repo1.maven.org/maven2/ 我们换成阿里巴巴的镜像\n\n配置同样是 settings.xml 中\n\n<mirror>\n      <id>aliyunmaven</id>\n      <mirrorof>*</mirrorof>\n      <name>阿里云公共仓库</name>\n      <url>https://maven.aliyun.com/repository/public</url>\n</mirror>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 构建\n\n * mvn compile 编译\n * mvn clean 清理\n * mvn test 测试\n * mvn package 打包\n * mvn install 安装到本地仓库\n\n\n# 依赖管理\n\n\x3c!-- 依赖父标签 --\x3e\n<dependencies>\n    \x3c!-- 依赖子标签每个依赖需要用dependency包裹 --\x3e\n    <dependency>\n        \x3c!-- 所属群组id --\x3e\n        <groupid>com.alibaba</groupid>\n        \x3c!-- 所属项目id --\x3e\n        <artifactid>fastjson</artifactid>\n        \x3c!-- 版本号 --\x3e\n        <version>1.2.75</version>\n    </dependency>\n</dependencies>    \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n可以进行依赖传递 即依赖可以加载另外一个项目中依赖 只需要提供群组id和项目id\n\n\n\n\n# 可选依赖\n\n可以选择对外隐藏指定的依赖资源 只有依赖传递才能用\n\n<dependencies>\n    <dependency>\n        <groupid>com.alibaba</groupid>\n        <artifactid>fastjson</artifactid>\n        <version>1.2.75</version>\n        \x3c!-- 对外是否隐藏 --\x3e\n        <optional>true</optional>\n    </dependency>\n</dependencies>    \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 排除依赖\n\n排除指定的依赖 黑名单\n\n<dependencies>\n    <dependency>\n        <groupid>依赖传递下级路径</groupid>\n        <artifactid>fastjson</artifactid>\n        <version>1.2.75</version>\n    </dependency>\n        \x3c!-- 排除依赖 --\x3e\n        <exclusions>\n            <exclusion>\n                 \x3c!-- 所属群组id --\x3e\n                <groupid>org.antlr</groupid>\n                 \x3c!-- 所属项目id --\x3e\n                <artifactid>antlr4-runtime</artifactid>\n            </exclusion>\n        </exclusions>    \n</dependencies>  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 依赖范围\n\n依赖的jar默认在任何地方都可以用 使用scope标签设定其作用范围\n\n\n\n依赖传递 作用范围也会有影响\n\n\n\n\n# 生命周期\n\n * clean 清理工作\n   * pre-clean\n   * clean\n   * post-clea\n * default 构建\n   * \n   * 执行哪个构建 就执行到那个构建就结束 下面的不会执行\n * site\n   * \n\n\n# 插件\n\nhttps://maven.apache.org/plugins/index.html\n\n插件与生命周期内的阶段绑定 执行到对应生命周期就执行对应的插件给你\n\n<project xmlns="http://maven.apache.org/pom/4.0.0" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n                <executions>\n                    <execution>\n                        \x3c!-- 此插件在什么生命周期运行 --\x3e\n                        <phase>generate-test-resources</phase>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"POI",frontmatter:{title:"POI",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/528ce7/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/06.POI.html",relativePath:"后端/02.JavaEE/06.POI.md",key:"v-e1af82de",path:"/pages/528ce7/",headersStr:null,content:'# POI\n\n<dependency>\n      <groupId>org.apache.poi</groupId>\n      <artifactId>poi</artifactId>\n      <version>4.0.1</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.poi</groupId>\n      <artifactId>poi-ooxml</artifactId>\n      <version>4.0.1</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.poi</groupId>\n      <artifactId>poi-ooxml-schemas</artifactId>\n      <version>4.0.1</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nEjava操控xcel\n\nJXL: 支持xls文件操作\n\nPOI: 支持xls与xlsx文件操作\n\n//获取对应的excel文件  工作簿文件\nXSSFWorkbook wb =new XSSFWorkbook();\n\n//创建工资表\nXSSFSheet sheet = wb.createSheet();//创建一个空子表\nwb.createSheet("s1"); //创建一个指定子表\n\nXSSFRow row = sheet.createRow(1);//创建工作表的行对象  索引从0开始  返回一个行对象\nXSSFCell cell = row.createCell(1);  //使用行对象创建列对象 索引从0开始 返回单元格对象\ncell.setCellValue("test");   //单元格对象set值\n//获取单元 类型要对应\nString stringCellValue = cell.getStringCellValue();\nSystem.out.println(stringCellValue);\n\n//保存为文件\nFile file =new File("test.xlsx");\nOutputStream os =new FileOutputStream(file);\nwb.write(os);\nwb.close();\nos.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n * 获取execel文件\n   \n   * //留空或者文件不存在则自动创建新文件\n     XSSFWorkbook wb =new XSSFWorkbook("test.xlsx");\n     \n     \n     1\n     2\n     \n\n * 创建工作表\n   \n   * XSSFSheet sheet = wb.createSheet()  //可指定名称\n     \n     \n     1\n     \n\n * 获取行对象 索引从0开始\n   \n   * XSSFRow row = sheet.createRow(0);\n     \n     \n     1\n     \n\n * 获取列(单元格)对象 索引从0开始\n   \n   * XSSFCell cell = row.createCell(0);\n     \n     \n     1\n     \n\n * 设置单元内容\n   \n   * cell.setCellValue("test");   //单元格对象set值\n     \n     \n     1\n     \n\n * 获取单元格内容 内容类型要与方法一致\n   \n   * String stringCellValue = cell.getStringCellValue();\n     \n     \n     1\n     \n\n * 保存为文件\n   \n   * File file =new File("test.xlsx");\n     OutputStream os =new FileOutputStream(file);\n     wb.write(os);\n     \n     \n     1\n     2\n     3\n     \n\n * 关闭 close()方法\n\n * 合并单元格 指定某个表的行列\n   \n   * sheet.addMergedRegion(new CellRangeAddress(1,1,1,12));\n     \n     \n     1\n     \n\n * 设置样式\n   \n   * XSSFCellStyle cellStyle = wb.createCellStyle();  //创建样式对象\n     cellStyle.setAlignment(HorizontalAlignment.CENTER);  //水平对齐\n     cellStyle.setVerticalAlignment(VerticalAlignment.CENTER);  //垂直对齐\n     \n     cell_1.setCellStyle(cellStyle);\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n   \n   * cs_field.setBorderTop(BorderStyle.THIN);  //设置上边框\n     cs_field.setBorderBottom(BorderStyle.THIN);  //下边框\n     cs_field.setBorderLeft(BorderStyle.THIN);  //左边框\n     cs_field.setBorderRight(BorderStyle.THIN);  //右边框\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     ',normalizedContent:'# poi\n\n<dependency>\n      <groupid>org.apache.poi</groupid>\n      <artifactid>poi</artifactid>\n      <version>4.0.1</version>\n    </dependency>\n    <dependency>\n      <groupid>org.apache.poi</groupid>\n      <artifactid>poi-ooxml</artifactid>\n      <version>4.0.1</version>\n    </dependency>\n    <dependency>\n      <groupid>org.apache.poi</groupid>\n      <artifactid>poi-ooxml-schemas</artifactid>\n      <version>4.0.1</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nejava操控xcel\n\njxl: 支持xls文件操作\n\npoi: 支持xls与xlsx文件操作\n\n//获取对应的excel文件  工作簿文件\nxssfworkbook wb =new xssfworkbook();\n\n//创建工资表\nxssfsheet sheet = wb.createsheet();//创建一个空子表\nwb.createsheet("s1"); //创建一个指定子表\n\nxssfrow row = sheet.createrow(1);//创建工作表的行对象  索引从0开始  返回一个行对象\nxssfcell cell = row.createcell(1);  //使用行对象创建列对象 索引从0开始 返回单元格对象\ncell.setcellvalue("test");   //单元格对象set值\n//获取单元 类型要对应\nstring stringcellvalue = cell.getstringcellvalue();\nsystem.out.println(stringcellvalue);\n\n//保存为文件\nfile file =new file("test.xlsx");\noutputstream os =new fileoutputstream(file);\nwb.write(os);\nwb.close();\nos.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n * 获取execel文件\n   \n   * //留空或者文件不存在则自动创建新文件\n     xssfworkbook wb =new xssfworkbook("test.xlsx");\n     \n     \n     1\n     2\n     \n\n * 创建工作表\n   \n   * xssfsheet sheet = wb.createsheet()  //可指定名称\n     \n     \n     1\n     \n\n * 获取行对象 索引从0开始\n   \n   * xssfrow row = sheet.createrow(0);\n     \n     \n     1\n     \n\n * 获取列(单元格)对象 索引从0开始\n   \n   * xssfcell cell = row.createcell(0);\n     \n     \n     1\n     \n\n * 设置单元内容\n   \n   * cell.setcellvalue("test");   //单元格对象set值\n     \n     \n     1\n     \n\n * 获取单元格内容 内容类型要与方法一致\n   \n   * string stringcellvalue = cell.getstringcellvalue();\n     \n     \n     1\n     \n\n * 保存为文件\n   \n   * file file =new file("test.xlsx");\n     outputstream os =new fileoutputstream(file);\n     wb.write(os);\n     \n     \n     1\n     2\n     3\n     \n\n * 关闭 close()方法\n\n * 合并单元格 指定某个表的行列\n   \n   * sheet.addmergedregion(new cellrangeaddress(1,1,1,12));\n     \n     \n     1\n     \n\n * 设置样式\n   \n   * xssfcellstyle cellstyle = wb.createcellstyle();  //创建样式对象\n     cellstyle.setalignment(horizontalalignment.center);  //水平对齐\n     cellstyle.setverticalalignment(verticalalignment.center);  //垂直对齐\n     \n     cell_1.setcellstyle(cellstyle);\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n   \n   * cs_field.setbordertop(borderstyle.thin);  //设置上边框\n     cs_field.setborderbottom(borderstyle.thin);  //下边框\n     cs_field.setborderleft(borderstyle.thin);  //左边框\n     cs_field.setborderright(borderstyle.thin);  //右边框\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Maven 高级",frontmatter:{title:"Maven 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/127f3b/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/09.Maven%20%E9%AB%98%E7%BA%A7.html",relativePath:"后端/02.JavaEE/09.Maven 高级.md",key:"v-1bb1ec60",path:"/pages/127f3b/",headers:[{level:2,title:"分模块开发与设计",slug:"分模块开发与设计",normalizedTitle:"分模块开发与设计",charIndex:15},{level:2,title:"聚合",slug:"聚合",normalizedTitle:"聚合",charIndex:114},{level:2,title:"继承",slug:"继承",normalizedTitle:"继承",charIndex:492},{level:2,title:"属性",slug:"属性",normalizedTitle:"属性",charIndex:1060},{level:2,title:"版本管理",slug:"版本管理",normalizedTitle:"版本管理",charIndex:1539},{level:2,title:"资源文件",slug:"资源文件",normalizedTitle:"资源文件",charIndex:1584},{level:2,title:"多环境开发",slug:"多环境开发",normalizedTitle:"多环境开发",charIndex:2227},{level:2,title:"跳过测试",slug:"跳过测试",normalizedTitle:"跳过测试",charIndex:3025},{level:2,title:"私服",slug:"私服",normalizedTitle:"私服",charIndex:3096},{level:3,title:"私服资源获取",slug:"私服资源获取",normalizedTitle:"私服资源获取",charIndex:3218},{level:3,title:"资源发布",slug:"资源发布",normalizedTitle:"资源发布",charIndex:3231}],headersStr:"分模块开发与设计 聚合 继承 属性 版本管理 资源文件 多环境开发 跳过测试 私服 私服资源获取 资源发布",content:"# Maven 高级\n\n\n# 分模块开发与设计\n\n 1. 将与当前层无关的内容清除掉\n 2. 在pom.xml将另外层的导入坐标\n 3. compile 预编译一下 是否出错\n 4. 然后install 打包成坐标\n\n\n\n\n# 聚合\n\n当一个工程构建 其他分模块的工程都会跟随着构建\n\n在主工程管理 中的pom.xml\n\n    \x3c!--    定义该工程用于进行构建管理--\x3e\n    <packaging>pom</packaging>\n\n    \x3c!--    管理工程的列表--\x3e\n    <modules>\n        \x3c!--        具体的工程名称--\x3e\n        <module>../ssm_dao</module>\n        <module>../ssm_pojo</module>\n        <module>../ssm_service</module>\n        <module>../ssm_controller</module>\n    </modules>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 继承\n\n在父工程中 定义 声明依赖\n\n<dependencyManagement>\n        \x3c!--        所有的依赖包--\x3e\n        <dependencies>\n\n        </dependencies>\n    </dependencyManagement>\n\n\n1\n2\n3\n4\n5\n6\n\n\n插件继承\n\n<pluginManagement>\n            <plugins></plugins>\n        </pluginManagement>\n\n\n1\n2\n3\n\n\n在子工程的pom 中定义父工程\n\n    <parent>\n        <groupId>com.itheima</groupId>\n        <artifactId>springmvc_validator</artifactId>\n        <version>1.0-SNAPSHOT</version>\n        \x3c!--        父工程的pom文件--\x3e\n        <relativePath>../ssm/pom.xml</relativePath>\n    </parent>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n在子工程所有的依赖只有父工程有的 都可以不写版本号\n\n\n# 属性\n\n定义属性\n\n    \x3c!--    定义属性--\x3e\n    <properties>\n        \x3c!--        标签名为属性名  值为属性值--\x3e\n        <spring.version>5.1.9.RELEASE</spring.version>\n    </properties>\n\n\n1\n2\n3\n4\n5\n\n\n使用属性\n\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context</artifactId>\n            \x3c!--            使用${属性名} 来获取值--\x3e\n            <version>${spring.version}</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\n${version} 是maven内部提供的 当前pom工程的版本号 我们在继承\n\n\n# 版本管理\n\n * snapshot(快照版本)\n * release(发布版本)\n\n\n# 资源文件\n\n在POM外的资源文件中使用定义好的属性值\n\n定义属性\n\n    \x3c!--    定义属性--\x3e\n    <properties>\n        \x3c!--        标签名为属性名  值为属性值--\x3e\n        <jdbc.url>jdbc:mysql://localhost:3306/ssm_db</jdbc.url>\n    </properties>\n\n\n1\n2\n3\n4\n5\n\n\n开启资源配置加载 如果是test测试目录下的资源文件在<resource> 包含在内的标签前面都加上test\n\n   <build>  \n\t\t\t<resource>\n                \x3c!--            资源文件的路径--\x3e\n       \t <directory>${project.basedir}/src/main/resources</directory>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在资源文件中引用\n\njdbc.driver=com.mysql.jdbc.Driver\njdbc.url=${jdbc.url}\njdbc.username=root\njdbc.password=123456\n\n\n1\n2\n3\n4\n\n\n\n# 多环境开发\n\n不同环境使用定义好的多个属性值\n\n    \x3c!--创建多环境--\x3e\n    <profiles>\n        <profile>\n            \x3c!--            定义环境对应的唯一名称--\x3e\n            <id>pro_eny</id>\n            \x3c!--定义环境中专用的属性值--\x3e\n            <properties>\n                <jdbc.url>jdbc:mysql://localhost:3306/ssm_db</jdbc.url>\n            </properties>\n            \x3c!--            默认环境 true为默认--\x3e\n            <activation>\n                <activeByDefault>true</activeByDefault>\n            </activation>\n        </profile>\n\n        <profile>\n            <id>dep_env</id>\n            <properties>\n                <jdbc.url>jdbc:mysql://localhost:3307/ssm_db</jdbc.url>\n            </properties>\n        </profile>\n    </profiles>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n使用环境\n\nmvn install -P 对应环境id\n\nmvn install -P dep_env\n\n\n1\n\n\n\n# 跳过测试\n\n * 在IDE的Maven 中 把test 跳过\n * 命令行 mvm install -D skipTests\n * \n\n\n# 私服\n\nNexus\n\nhttps://help.sonatype.com/repomanager3/download\n\n运行bin下的nexus\n\nnexus /run nexus\n\n\n1\n\n\n启动完成后 localhost:8081\n\n\n# 私服资源获取\n\n\n\n\n# 资源发布\n\n\n\n或者IDA中 maven的 deploy\n\nid对应本地的maven对应servers的username和password",normalizedContent:"# maven 高级\n\n\n# 分模块开发与设计\n\n 1. 将与当前层无关的内容清除掉\n 2. 在pom.xml将另外层的导入坐标\n 3. compile 预编译一下 是否出错\n 4. 然后install 打包成坐标\n\n\n\n\n# 聚合\n\n当一个工程构建 其他分模块的工程都会跟随着构建\n\n在主工程管理 中的pom.xml\n\n    \x3c!--    定义该工程用于进行构建管理--\x3e\n    <packaging>pom</packaging>\n\n    \x3c!--    管理工程的列表--\x3e\n    <modules>\n        \x3c!--        具体的工程名称--\x3e\n        <module>../ssm_dao</module>\n        <module>../ssm_pojo</module>\n        <module>../ssm_service</module>\n        <module>../ssm_controller</module>\n    </modules>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 继承\n\n在父工程中 定义 声明依赖\n\n<dependencymanagement>\n        \x3c!--        所有的依赖包--\x3e\n        <dependencies>\n\n        </dependencies>\n    </dependencymanagement>\n\n\n1\n2\n3\n4\n5\n6\n\n\n插件继承\n\n<pluginmanagement>\n            <plugins></plugins>\n        </pluginmanagement>\n\n\n1\n2\n3\n\n\n在子工程的pom 中定义父工程\n\n    <parent>\n        <groupid>com.itheima</groupid>\n        <artifactid>springmvc_validator</artifactid>\n        <version>1.0-snapshot</version>\n        \x3c!--        父工程的pom文件--\x3e\n        <relativepath>../ssm/pom.xml</relativepath>\n    </parent>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n在子工程所有的依赖只有父工程有的 都可以不写版本号\n\n\n# 属性\n\n定义属性\n\n    \x3c!--    定义属性--\x3e\n    <properties>\n        \x3c!--        标签名为属性名  值为属性值--\x3e\n        <spring.version>5.1.9.release</spring.version>\n    </properties>\n\n\n1\n2\n3\n4\n5\n\n\n使用属性\n\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-context</artifactid>\n            \x3c!--            使用${属性名} 来获取值--\x3e\n            <version>${spring.version}</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\n${version} 是maven内部提供的 当前pom工程的版本号 我们在继承\n\n\n# 版本管理\n\n * snapshot(快照版本)\n * release(发布版本)\n\n\n# 资源文件\n\n在pom外的资源文件中使用定义好的属性值\n\n定义属性\n\n    \x3c!--    定义属性--\x3e\n    <properties>\n        \x3c!--        标签名为属性名  值为属性值--\x3e\n        <jdbc.url>jdbc:mysql://localhost:3306/ssm_db</jdbc.url>\n    </properties>\n\n\n1\n2\n3\n4\n5\n\n\n开启资源配置加载 如果是test测试目录下的资源文件在<resource> 包含在内的标签前面都加上test\n\n   <build>  \n\t\t\t<resource>\n                \x3c!--            资源文件的路径--\x3e\n       \t <directory>${project.basedir}/src/main/resources</directory>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n在资源文件中引用\n\njdbc.driver=com.mysql.jdbc.driver\njdbc.url=${jdbc.url}\njdbc.username=root\njdbc.password=123456\n\n\n1\n2\n3\n4\n\n\n\n# 多环境开发\n\n不同环境使用定义好的多个属性值\n\n    \x3c!--创建多环境--\x3e\n    <profiles>\n        <profile>\n            \x3c!--            定义环境对应的唯一名称--\x3e\n            <id>pro_eny</id>\n            \x3c!--定义环境中专用的属性值--\x3e\n            <properties>\n                <jdbc.url>jdbc:mysql://localhost:3306/ssm_db</jdbc.url>\n            </properties>\n            \x3c!--            默认环境 true为默认--\x3e\n            <activation>\n                <activebydefault>true</activebydefault>\n            </activation>\n        </profile>\n\n        <profile>\n            <id>dep_env</id>\n            <properties>\n                <jdbc.url>jdbc:mysql://localhost:3307/ssm_db</jdbc.url>\n            </properties>\n        </profile>\n    </profiles>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n使用环境\n\nmvn install -p 对应环境id\n\nmvn install -p dep_env\n\n\n1\n\n\n\n# 跳过测试\n\n * 在ide的maven 中 把test 跳过\n * 命令行 mvm install -d skiptests\n * \n\n\n# 私服\n\nnexus\n\nhttps://help.sonatype.com/repomanager3/download\n\n运行bin下的nexus\n\nnexus /run nexus\n\n\n1\n\n\n启动完成后 localhost:8081\n\n\n# 私服资源获取\n\n\n\n\n# 资源发布\n\n\n\n或者ida中 maven的 deploy\n\nid对应本地的maven对应servers的username和password",charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"Spring",frontmatter:{title:"Spring",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/17e650/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/07.Spring.html",relativePath:"后端/02.JavaEE/07.Spring.md",key:"v-61c16c55",path:"/pages/17e650/",headers:[{level:2,title:"IoC 反转",slug:"ioc-反转",normalizedTitle:"ioc 反转",charIndex:68},{level:3,title:"耦合与内聚",slug:"耦合与内聚",normalizedTitle:"耦合与内聚",charIndex:174},{level:3,title:"工厂模式",slug:"工厂模式",normalizedTitle:"工厂模式",charIndex:309},{level:3,title:"创建项目",slug:"创建项目",normalizedTitle:"创建项目",charIndex:320},{level:3,title:"bean",slug:"bean",normalizedTitle:"bean",charIndex:780},{level:2,title:"DI 依赖注入",slug:"di-依赖注入",normalizedTitle:"di 依赖注入",charIndex:2786},{level:3,title:"set注入",slug:"set注入",normalizedTitle:"set注入",charIndex:2868},{level:3,title:"构造器注入",slug:"构造器注入",normalizedTitle:"构造器注入",charIndex:4442},{level:3,title:"集合类型数据注入",slug:"集合类型数据注入",normalizedTitle:"集合类型数据注入",charIndex:5292},{level:3,title:"p命名空间 简化配置",slug:"p命名空间-简化配置",normalizedTitle:"p命名空间 简化配置",charIndex:6995},{level:3,title:"SpEL EL表达式",slug:"spel-el表达式",normalizedTitle:"spel el表达式",charIndex:7789},{level:3,title:"properties文件",slug:"properties文件",normalizedTitle:"properties文件",charIndex:7849},{level:3,title:"import 团队开发",slug:"import-团队开发",normalizedTitle:"import 团队开发",charIndex:8965},{level:3,title:"容器运行时加载多个IoC配置",slug:"容器运行时加载多个ioc配置",normalizedTitle:"容器运行时加载多个ioc配置",charIndex:9061},{level:3,title:"bean注意事项",slug:"bean注意事项",normalizedTitle:"bean注意事项",charIndex:9198},{level:3,title:"ApplicationContext对象",slug:"applicationcontext对象",normalizedTitle:"applicationcontext对象",charIndex:9308},{level:3,title:"第三方资源bean配置",slug:"第三方资源bean配置",normalizedTitle:"第三方资源bean配置",charIndex:9335},{level:3,title:"Spring+MyBatis",slug:"spring-mybatis",normalizedTitle:"spring+mybatis",charIndex:10152},{level:2,title:"注解",slug:"注解",normalizedTitle:"注解",charIndex:13492},{level:2,title:"新注解",slug:"新注解",normalizedTitle:"新注解",charIndex:13696},{level:2,title:"加载控制",slug:"加载控制",normalizedTitle:"加载控制",charIndex:14335},{level:2,title:"整合Junit",slug:"整合junit",normalizedTitle:"整合junit",charIndex:14405},{level:2,title:"Ioc底层核心原理",slug:"ioc底层核心原理",normalizedTitle:"ioc底层核心原理",charIndex:15520},{level:2,title:"组件扫描过滤器",slug:"组件扫描过滤器",normalizedTitle:"组件扫描过滤器",charIndex:15542},{level:3,title:"自定义组件过滤器",slug:"自定义组件过滤器",normalizedTitle:"自定义组件过滤器",charIndex:15818},{level:2,title:"自定义导入器",slug:"自定义导入器",normalizedTitle:"自定义导入器",charIndex:16860},{level:2,title:"自定义注册器",slug:"自定义注册器",normalizedTitle:"自定义注册器",charIndex:17664},{level:2,title:"bean初始化过程",slug:"bean初始化过程",normalizedTitle:"bean初始化过程",charIndex:19008},{level:2,title:"AOP",slug:"aop",normalizedTitle:"aop",charIndex:19026},{level:3,title:"AOP的动态代理对象",slug:"aop的动态代理对象",normalizedTitle:"aop的动态代理对象",charIndex:19182},{level:3,title:"XML配置",slug:"xml配置",normalizedTitle:"xml配置",charIndex:19460},{level:3,title:"注解",slug:"注解-2",normalizedTitle:"注解",charIndex:13492},{level:3,title:"静态代理",slug:"静态代理",normalizedTitle:"静态代理",charIndex:26315},{level:3,title:"JDK动态代理",slug:"jdk动态代理",normalizedTitle:"jdk动态代理",charIndex:26766},{level:3,title:"CGLIB",slug:"cglib",normalizedTitle:"cglib",charIndex:27590},{level:3,title:"AOP底层 切换动态代理方式",slug:"aop底层-切换动态代理方式",normalizedTitle:"aop底层 切换动态代理方式",charIndex:28823},{level:3,title:"织入时机",slug:"织入时机",normalizedTitle:"织入时机",charIndex:29135},{level:2,title:"事务",slug:"事务",normalizedTitle:"事务",charIndex:29146},{level:3,title:"事务核心对象",slug:"事务核心对象",normalizedTitle:"事务核心对象",charIndex:29236},{level:3,title:"编程式事务",slug:"编程式事务",normalizedTitle:"编程式事务",charIndex:29249},{level:3,title:"AOP改造编程式事务",slug:"aop改造编程式事务",normalizedTitle:"aop改造编程式事务",charIndex:30085},{level:3,title:"声明式事务",slug:"声明式事务",normalizedTitle:"声明式事务",charIndex:31140},{level:3,title:"事务传播行为",slug:"事务传播行为",normalizedTitle:"事务传播行为",charIndex:33680},{level:3,title:"注解事务",slug:"注解事务",normalizedTitle:"注解事务",charIndex:33872},{level:2,title:"Spring模板对象",slug:"spring模板对象",normalizedTitle:"spring模板对象",charIndex:34546},{level:3,title:"JdbcTemplate",slug:"jdbctemplate",normalizedTitle:"jdbctemplate",charIndex:34561}],headersStr:"IoC 反转 耦合与内聚 工厂模式 创建项目 bean DI 依赖注入 set注入 构造器注入 集合类型数据注入 p命名空间 简化配置 SpEL EL表达式 properties文件 import 团队开发 容器运行时加载多个IoC配置 bean注意事项 ApplicationContext对象 第三方资源bean配置 Spring+MyBatis 注解 新注解 加载控制 整合Junit Ioc底层核心原理 组件扫描过滤器 自定义组件过滤器 自定义导入器 自定义注册器 bean初始化过程 AOP AOP的动态代理对象 XML配置 注解 静态代理 JDK动态代理 CGLIB AOP底层 切换动态代理方式 织入时机 事务 事务核心对象 编程式事务 AOP改造编程式事务 声明式事务 事务传播行为 注解事务 Spring模板对象 JdbcTemplate",content:'# Spring\n\nSpring是分层的JavaSE/EE应用 full-stack轻量级 一站式 开源框架\n\n体系结构:\n\n\n\n\n# IoC 反转\n\nIoC(Inversion Of Control) 控制反转,Spring反向控制应用程序所需要使用的外部资源\n\nSpring控制的资源全部放置在Spring容器中,该容器称为IoC容器\n\n\n# 耦合与内聚\n\n耦合(Coupling): 代码书写过程中所使用技术的结合紧密度,用于衡量软件中各个模块之间的互联程度\n\n内聚(Cohesion):代码书写过程中单个模块内部各组成部分间的联系,用于衡量软件中各个功能模块内部的功能联系\n\n我们追求 高内聚 低耦合\n\n\n# 工厂模式\n\n\n\n\n# 创建项目\n\n * 创建maven项目\n\n * 导入模块\n   \n   *     <dependencies>\n             <dependency>\n                 <groupId>org.springframework</groupId>\n                 <artifactId>spring-context</artifactId>\n                 <version>5.1.9.RELEASE</version>\n             </dependency>\n         </dependencies>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     \n\n * 在resources中创建applicationContext.xml文件\n   \n   * <?xml version="1.0" encoding="UTF-8"?>\n     <beans xmlns="http://www.springframework.org/schema/beans"\n            xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n            xsi:schemaLocation="http://www.springframework.org/schema/beans\n             https://www.springframework.org/schema/beans/spring-beans.xsd">\n     \x3c!--   bean为映射标签 创建spring控制的资源  id为操作空间  class为实现类引用路径--\x3e\n         <bean id="userService" class="com.itheima.service.impl.UserServiceImpl"/>\n     </beans>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n * 使用方法\n   \n   * //2.加载配置文件\n             ApplicationContext ctx =new ClassPathXmlApplicationContext("applicationContext.xml");\n             //3.获取资源\n             UserService userService = (UserService) ctx.getBean("userService");\n     \n             userService.save();\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# bean\n\n * <bean>标签 定义spring中的资源, 此标签定义的资源将受到spring控制\n   \n   * id属性 bean的名词 通过id值获取到bean\n   \n   * class属性 bean的类型\n   \n   * name属性 bean的别名 我们可以通过id或者name来获取到bean 并且name可以起多个别名 别名之间用逗号隔开\n   \n   * scope属性 定义bean的作用范围\n     \n     * singleton值 设置创建出的对象保存在spring容器中 是一个单例对象 默认值 单例时创建spring容器时就加载bean绑定的类\n     * prototype值 设置创建出的对象保存在spring容器中 是一个**非单例对象 ** 非单例则是在getBean时创建类\n     * request、session、application、websocket值 设置创建出的对象放置在web容器对应的位置\n   \n   * init-method bean对象初始化时执行指定方法\n     \n     * 值为bean对应的类中的具体方法名\n   \n   * destroy-method bean对象销毁时执行指定方法\n     \n     * 值为bean对应的类中的具体方法名 只有ClassPathXmlApplicationContext才有close方法 spring默认自动销毁但销毁方法不会执行\n     * \n   \n   * factory-method属性\n     \n     * 值为静态工厂中创建对象的方法\n   \n   * factory-bean属性\n     \n     * 值为实例工厂的id 需要先用一个bean绑定实例工厂类 然后第二个bean的factory-bean为第一个bean的id 然后用factory-method调用其实例工厂的方法\n     \n     * \x3c!--    factory-bea  实例工厂创建对象  需要bean先绑定实例工厂类 然后第二个bean的factory-bea为实例工厂的id 然后factory-method调用其创建对象方法--\x3e\n           <bean id="userService3" class="com.itheima.service.UserServiceFactory2" />\n           <bean factory-bean="userService3" factory-method="getService"/>\n       \n       \n       1\n       2\n       3\n       \n\n\n# DI 依赖注入\n\nDI(Dependency Injection) 依赖注入,应用程序运行依赖的资源由Spring为其提供,资源进入应用程序的方式称为注入\n\n\n# set注入\n\n * <property>标签 使用set方法的形式为bean提供资源 是bean的子标签\n   * name属性\n     * 值为对应bean中的属性名 要该属性必须为私有属性同时提供set方法\n   * value属性\n     * 值为name绑定属性的 值 设定非引用类型属性的值\n   * ref属性\n     * 值为引用类型属性对象bean的id 不能与value同时使用\n\n配置\n\n    <bean id="userService" class="com.itheima.service.impl.UserServiceImpl">\n\x3c!--        将要注入的引用类型变量通过property属性进行注入 对应的name是要注入的变量名  使用ref声明要注入的bean的id  --\x3e\n        <property name="userDao" ref="userDao"></property>\n\x3c!--        如果要注入的变量为一个值 我们通过value来设置 同样需要在类中设置私有变量 和set方法--\x3e\n        <property name="num" value="13"/>\n    </bean>\n\n\x3c!--    将要注入的资源声明为bean--\x3e\n    <bean id="userDao" class="com.itheima.dao.impl.UserDaoImpl"/>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n反转资源注入\n\nimport com.itheima.dao.UserDao;\nimport com.itheima.service.UserService;\n\npublic class UserServiceImpl implements UserService {\n    private UserDao userDao;\n    private int num;\n\n    //对需要进行注入的变量添加set方法\n    public void setUserDao(UserDao userDao) {\n        this.userDao = userDao;\n    }\n\n    public void setNum(int num) {\n        this.num = num;\n    }\n\n    @Override\n    public void save() {\n        userDao.save();\n        System.out.println("running...");\n        System.out.println(num);\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n调用\n\npublic class UserApp {\n    public static void main(String[] args) {\n        //2.加载配置文件\n        ApplicationContext ctx =new ClassPathXmlApplicationContext("applicationContext.xml");\n        //3.获取资源\n        UserService userService = (UserService) ctx.getBean("userService");\n        userService.save();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\n# 构造器注入\n\n * <constructor-arg> 标签 使用构造方法的形式为bean提供资源 是bean的子标签\n   * name属性\n     * 值为对应bean中的属性名 要该属性必须为私有属性同时提供set方法\n   * value属性\n     * 值为name绑定属性的 值 设定非引用类型属性的值\n   * ref属性\n     * 值为引用类型属性对象bean的id 不能与value同时使用\n   * type属性\n     * 值为设定构造方法参数的类型 用于赋值给指定的变量类型 推荐使用nanme指定变量\n   * index属性\n     * 值为设定构造方法参数的位置 用于赋值给指定 构造方法中 变量的index 从0开始 推荐使用name指定变量\n\n被注入类要声明构造方法并赋值\n\nprivate UserDao userDao;\nprivate int num;\npublic UserServiceImpl(UserDao userDao,int num){\n        this.userDao=userDao;\n        this.num=num;\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\nbean配置\n\n\x3c!--    构造方法注入--\x3e\n\x3c!--    注入类--\x3e\n    <bean id="userDao" class="com.itheima.dao.impl.UserDaoImpl"/>\n\x3c!--被注入的类--\x3e\n    <bean id="userService" class="com.itheima.service.impl.UserServiceImpl">\n        <constructor-arg ref="userDao" />\n        <constructor-arg name="num" value="456"/>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 集合类型数据注入\n\n如 <array>、<list>、<set>、<map>、<props>等集合 是归属于property或constructor-arg标签的子标签\n\n * <list>\n   \n   * <value>标签\n     * 值为元素对于的值\n\n * <props>\n   \n   * <prop>标签\n     * key属性\n       * 值为属性名\n     * 值为元素对于的值\n\n * <array>\n\n * <value>标签\n   \n   * 值为元素对于的值\n\n * <set>\n   \n   * <value>标签\n     * 值为元素对于的值\n\n * <map>\n   \n   * <enrty>标签\n     * key属性\n       * 值为key\n     * value属性\n       * 值为值\n\n    \x3c!--    集合注入--\x3e\n    <bean id="userDao" class="com.itheima.dao.impl.UserDaoImpl"/>\n\n    <bean id="userService" class="com.itheima.service.impl.UserServiceImpl">\n        <property name="userDao" ref="userDao"/>\n        <property name="bookDao" ref="bookDao"/>\n    </bean>\n\n    <bean id="bookDao" class="com.itheima.dao.impl.BookDaoImpl">\n        <property name="al">\n            <list>\n                <value>helll</value>\n                <value>world</value>\n            </list>\n        </property>\n        <property name="properties">\n            <props>\n                <prop key="name">age</prop>\n                <prop key="value">19</prop>\n            </props>\n        </property>\n        <property name="arr">\n            <array>\n                <value>123</value>\n                <value>456</value>\n            </array>\n        </property>\n        <property name="hs">\n            <set>\n                <value>111</value>\n                <value>222</value>\n            </set>\n        </property>\n        <property name="hm">\n            <map>\n                <entry key="name" value="zhangsan"/>\n                <entry key="name" value="lisi"/>\n            </map>\n        </property>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# p命名空间 简化配置\n\n增加属性 xmlns:p="http://www.springframework.org/schema/p" 配置p命名空间\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd">\n\n\n1\n2\n3\n4\n5\n\n\np命名空间为bean注入属性值 替代property\n\n\x3c!--    两个bean一样的--\x3e\n   <bean id="userService" class="com.itheima.service.impl.UserServiceImpl" p:bookDao-ref="bookDao" p:userDao-ref="userDao" />\n\n    <bean id="userService" class="com.itheima.service.impl.UserServiceImpl">\n        <property name="userDao" ref="userDao"/>\n        <property name="bookDao" ref="bookDao"/>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# SpEL EL表达式\n\nspring通过EL表达式的支持,统一属性注入格式\n\n\n\n在value中书写el表达式\n\n\n# properties文件\n\n增加属性 xmlns:context="http://www.springframework.org/schema/context"\n\n并加上约束 http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemaLocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n">\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n加context命名空间的支持\n\n    \x3c!--   context命名空间 加载类路径下 所有的properties文件  加载后使用${属性名} --\x3e\n    <context:property-placeholder location="classpath:*.properties"/>\n <bean id="userService" class="com.itheima.service.impl.UserServiceImpl">\n            <constructor-arg ref="userDao" />\n            <constructor-arg name="num" value="${age}"/>\n        </bean>--\x3e\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# import 团队开发\n\nxml中通过<import>标签我们可以引入外部的IoC配置\n\n * <impoprt>标签 引用外部IoC配置\n   * resource属性 配置文件名\n\n\n# 容器运行时加载多个IoC配置\n\nApplicationContext ctx =new ClassPathXmlApplicationContext("applicationContext.xml","applicationContext2.xml");\n\n\n1\n\n\n\n# bean注意事项\n\n * id是唯一的 同一文件中不允许存在相同id 而不同文件中后定义覆盖前面的\n * 导入配置文件可以理解为 将配置文件复制粘贴到对应位置\n * 导入配置文件顺序不同可能导致程序运行结果不同\n\n\n# ApplicationContext对象\n\n\n\n\n# 第三方资源bean配置\n\n添加druid模块 druid是阿里巴巴开发的jdbc连接池组件\n\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n            <version>1.1.21</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n配置文件\n\n    <bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource">\n        <property name="driverClassName" value="com.mysql.jdbc.Driver"/>\n        <property name="url" value="jdbc:mysql://localhost:3306/heima_mm"/>\n        <property name="username" value="root"/>\n        <property name="password" value="123456"/>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n\n\n创建\n\nApplicationContext ctx =new ClassPathXmlApplicationContext("applicationContext.xml");\nDruidDataSource dataSource = (DruidDataSource) ctx.getBean("dataSource");\nSystem.out.println(dataSource);\n\n\n1\n2\n3\n\n\n看清楚 第三方类的类名 属性名 set方法\n\n\n# Spring+MyBatis\n\napplicationContext.xml\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemaLocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n">\n\n    <context:property-placeholder location="classpath:*.properties"/>\n\n    <bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource">\n        <property name="driverClassName" value="${jdbc.driver}"/>\n        <property name="url" value="${jdbc.url}"/>\n        <property name="username" value="${jdbc.username}"/>\n        <property name="password" value="${jdbc.password}"/>\n\n    </bean>\n\n\n    \x3c!--    spring整合mybatis后控制创建连接用的对象--\x3e\n    <bean class="org.mybatis.spring.SqlSessionFactoryBean">\n        <property name="dataSource" ref="dataSource"/>\n        <property name="typeAliasesPackage" value="com.itheima.domain"/>\n    </bean>\n\n    \x3c!--    加载mybatis映射配置扫描 将其作为spring的bean进行管理--\x3e\n    <bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">\n        <property name="basePackage" value="com.itheima.dao"/>\n    </bean>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\npox.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>spring+mybatis</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis</artifactId>\n            <version>3.5.3</version>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.47</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context</artifactId>\n            <version>5.1.9.RELEASE</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-jdbc</artifactId>\n            <version>5.1.9.RELEASE</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n            <version>1.1.21</version>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis-spring</artifactId>\n            <version>2.0.3</version>\n        </dependency>\n    </dependencies>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 注解\n\n注解代替xml配置可以简化配置 提供开发效率\n\nhttps://www.cnblogs.com/alter888/p/9083963.html\n\n\n\n并且要在applicationContext.xml中配置组件扫描\n\n    \x3c!--    配置组件扫描--\x3e\n    <context:component-scan base-package="com.itheima"/>\n\n\n1\n2\n\n\n\n# 新注解\n\n使用上面的注解还不能完全替代xml配置 如第三方的类\n\n\n\n@Bean 标记第三方类\n\npublic class JDBCConfig {\n    @Value("${jdbc.driver}")\n    private String driver;\n    @Value("${jdbc.url}")\n    private String url;\n    @Value("${jdbc.username}")\n    private String userName;\n    @Value("${jdbc.password}")\n    private String password;\n\n    @Bean("dataSource")\n    public DataSource getDataSource(){\n        DruidDataSource ds = new DruidDataSource();\n        ds.setDriverClassName(driver);\n        ds.setUrl(url);\n        ds.setUsername(userName);\n        ds.setPassword(password);\n        return ds;\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n# 加载控制\n\n@DependsOn("classname")\n\n\n\n@Order(n) 控制加载顺序\n\n\n\n@Lazy 延迟加载\n\n\n\n\n# 整合Junit\n\n在Spring中之前我们测试都需要获取容器 然后获取bean\n\n导入坐标\n\n<dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-test</artifactId>\n            <version>5.1.9.RELEASE</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n测试样例\n\n//设定spring专用的类加载器\n@RunWith(SpringJUnit4ClassRunner.class)\n//设定加载的spring上下文对应的配置\n@ContextConfiguration(classes = SpringConfig.class)\npublic class UserServiceTest {\n\n    @Autowired\n    private AccountService accountService;\n\n    @Test\n    public void testFindById(){\n        Account ac = accountService.findById(2);\n//        System.out.println(ac);\n        //assert 预计值  结果值  如果不一致则测试不通过\n        Assert.assertEquals("Jock",ac.getName());\n    }\n\n    @Test\n    public void testFindAll(){\n        List<Account> list = accountService.findAll();\n        Assert.assertEquals(2,list.size());\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# Ioc底层核心原理\n\n\n\n\n\n\n\n\n\n\n# 组件扫描过滤器\n\n@ComponentScan 组件扫描器拥有过滤指定组件功能\n\n * 按注解类型 过滤\n   * excludeFilters 设置排除性过滤器\n   * includeFilters 设置包含性过滤器\n\n//所有的@Service注解被过滤\n@ComponentScan(value = "com.itheima",excludeFilters = @ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Service.class))\n\n\n\n1\n2\n3\n\n\n\n# 自定义组件过滤器\n\n继承TypeFilter 实现match方法 返回false则不过滤 返回true则过滤\n\npackage com.itheima.config.filter;\n\nimport org.springframework.core.type.classreading.MetadataReader;\nimport org.springframework.core.type.classreading.MetadataReaderFactory;\nimport org.springframework.core.type.filter.TypeFilter;\n\nimport java.io.IOException;\n\npublic class MyTypeFilter implements TypeFilter {\n\n\n    public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException {\n        ClassMetadata classMetadata = metadataReader.getClassMetadata();  //获取class的元数据\n        String className = classMetadata.getClassName();  //获取类名\n        System.out.println(className);\n        if(className.equals("com.itheima.service.impl.AccountServiceImpl")){\n            return true;\n        }\n\n        return false;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * 按自定义过滤器 过滤\n\n@ComponentScan(value = "com.itheima",excludeFilters = @ComponentScan.Filter(type = FilterType.CUSTOM,classes = MyTypeFilter.class))\n\n\n1\n\n\n\n# 自定义导入器\n\n继承 ImportSelector 实现selectImports方法\n\npackage com.itheima.config.selector;\n\nimport org.springframework.context.annotation.ImportSelector;\nimport org.springframework.core.type.AnnotationMetadata;\n\nimport java.util.ResourceBundle;\n\npublic class MyImportSelector implements ImportSelector {\n\n    public String[] selectImports(AnnotationMetadata importingClassMetadata) {\n\n        //使用properties文件读取\n        ResourceBundle bundle = ResourceBundle.getBundle("import");  //文件名\n        String className = bundle.getString("className");  //属性名\n        return new String[]{className};\n\n//        return new String[]{"com.itheima.service.impl.AccountServiceImpl"}; //直接返回指定类不推荐使用\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n导入指定的组件 不用需要bean绑定或者注解绑定\n\n@Import(MyImportSelector.class)\n\n\n1\n\n\n\n# 自定义注册器\n\n继承 ImportBeanDefinitionRegistrar 实现 registerBeanDefinitions 方法\n\npackage com.itheima.config.registrar;\n\nimport org.springframework.beans.factory.support.BeanDefinitionRegistry;\nimport org.springframework.context.annotation.ClassPathBeanDefinitionScanner;\nimport org.springframework.context.annotation.ImportBeanDefinitionRegistrar;\nimport org.springframework.core.type.AnnotationMetadata;\nimport org.springframework.core.type.classreading.MetadataReader;\nimport org.springframework.core.type.classreading.MetadataReaderFactory;\nimport org.springframework.core.type.filter.TypeFilter;\n\nimport java.io.IOException;\n\npublic class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar {\n\n    public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {\n        ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(registry,false);\n        scanner.addIncludeFilter(new TypeFilter() {\n            public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException {\n                return true;\n            }\n        });\n        scanner.scan("com.itheima");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n@Import(MyImportBeanDefinitionRegistrar.class)  //自定义注册器\n\n\n1\n\n\n\n# bean初始化过程\n\n\n\n\n\n\n# AOP\n\nAspect Oriented Programming 面向切门编程 通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术\n\nAOP 是 OOP(面向对象) 的延续\n\n\n\n作用:在程序运行期间 在不修改源码的情况下对方法进行功能增强\n\n优势:减少重复代码 提高开发效率 并且便于维护\n\n\n# AOP的动态代理对象\n\n * JDK代理 : 基于接口的动态代理技术\n * cglib代理 : 基于父类的动态代理技术\n\n导入坐标\n\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.4</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n把共性的功能提前出来 并提供类和方法\n\n\n# XML配置\n\n添加aop 以下声明\n\nxmlns:aop="http://www.springframework.org/schema/aop"\nhttp://www.springframework.org/schema/aop\nhttps://www.springframework.org/schema/aop/spring-aop.xsd\n\n\n1\n2\n3\n\n\n标签头\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n绑定\n\n    \x3c!--3.开启AOP命名空间--\x3e\n    <bean id="userService" class="com.itheima.service.impl.UserServiceImpl"/>\n    \x3c!--2.配置共性功能成功spring控制的资源  共性功能的类--\x3e\n    <bean id="myAdvice" class="com.itheima.aop.AOPAdvice"/>\n\n    \x3c!--4.配置AOP--\x3e\n    <aop:config>\n        \x3c!--5.配置切入点--\x3e\n        <aop:pointcut id="pt" expression="execution(* *..*(..))"/>\n        \x3c!--6.配置切面（切入点与通知的关系）--\x3e\n        <aop:aspect ref="myAdvice">\n            \x3c!--7.配置具体的切入点对应通知中那个操作方法  pointcut-ref为pointcut对应的id--\x3e\n            <aop:before method="function" pointcut-ref="pt"/>\n        </aop:aspect>\n    </aop:config>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n * aop:config标签 aop根标签 在beans可以拥有多个\n   * aop:aspect标签 可以在aop:config中配置多个\n     * ref属性 通知(共性类)所在的bean的id\n     * aop:before标签 切面\n       * method属性 通知中具体的方法\n       * pointcut-ref属性 与aop:pointcut中的id要一致\n       * pointcut="execution(* ..(..))" 私有切入点 不能与pointcut-ref共存\n   * aop:pointcut标签 也可以拥有多个 上级未config则为公共切入点 上级未aspect则为局部切入点\n     * id属性 名称 可以自定义\n     * expression属性 切入点模式\n\n# 切入点表达式\n\n\n\n\n\n支持逻辑运算符 和非运算\n\n# 通知类型\n\n * aop:before 前置通知 如果通知中抛出异常 则阻止原始方法运行\n * aop:after 后置通知 无论是否异常 都会执行通知\n * aop:after-running 运行通知 如果抛出异常无法通知\n * aop:after-throwing 异常通知 如果没有抛出异常无法通知\n * aop:around 环绕通知 在原始方法执行前后都有对应执行的执行,还可以阻止原始方法执行\n   * \n\n# 通知顺序\n\n当同一个切入点配置多个通知时,标签配置的顺序就是执行的顺序\n\n# 通知获取数据\n\n * 获取参数 所有的通知都可以获取参数\n   \n   * public void before(JoinPoint jp) throws  Throwable{\n             Object[] args = jp.getArgs();\n         }\n     \n     \n     1\n     2\n     3\n     \n   \n   * 第二个方法\n     \n     * 在通知方法中定义变量\n       \n       * public void before2(int a) {\n                 System.out.println(a);\n             }\n         \n         \n         1\n         2\n         3\n         \n     \n     * 在applicationContext.xml 配置aop传参\n       \n       *             \x3c!--         &amp是&  args(a) 为aop通知方法中形参的名字   --\x3e\n                     <aop:before method="before2" pointcut="execution(* *..*(..)) &amp;&amp; args(a)"/>\n         \n         \n         1\n         2\n         \n\n# 通知获取返回值\n\n * 获取返回值 只有around 和 after-returning 通知\n   \n   * public void before2(Object ret) {\n             System.out.println(ret);\n         }\n     \n     \n     1\n     2\n     3\n     \n   \n   *             \x3c!--       returning为通知方法形参的名字  此变量为原始方法的返回值     --\x3e\n                 <aop:after-returning method="before2" pointcut-ref="pt" returning="ret"/>\n     \n     \n     1\n     2\n     \n   \n   * 第二种方法\n     \n     *     public Object around (ProceedingJoinPoint pjp) throws Throwable {\n               //对原始方法的调用  返回原始方法的返回值\n               Object proceed = pjp.proceed();\n               System.out.println(proceed);\n               //必须返回返回值 不然原始方法会异常\n               return proceed;\n           }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       \n     \n     * <aop:around method="around" pointcut-ref="pt"/>\n       \n       \n       \n       1\n       2\n       \n\n# 获取异常\n\n * 获取异常 around 和 after-throwing 通知获取\n   \n   *     public void afterThrowing(Throwable t){\n             System.out.println(t.getMessage());\n         }\n     \n     \n     1\n     2\n     3\n     \n   \n   *             \x3c!--      throwing为通知方法中形参的名称  用于获取异常      --\x3e\n                 <aop:after-throwing method="afterThrowing" pointcut-ref="pt" throwing="t"/>\n     \n     \n     1\n     2\n     \n   \n   * 第二种方法\n     \n     *     public Object around (ProceedingJoinPoint pjp)  {\n               //对原始方法的调用  返回原始方法的返回值\n               Object proceed = null;\n               try {\n                   proceed = pjp.proceed();\n               } catch (Throwable e) {\n                   e.printStackTrace();\n               }\n               System.out.println(proceed);\n               //必须返回返回值 不然原始方法会异常\n               return proceed;\n           }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       \n     \n     *  <aop:around method="around" pointcut-ref="pt"/>\n       \n       \n       1\n       \n\n\n# 注解\n\n * 开aop注解支持\n   \n   * <aop:aspectj-autoproxy/>\n     \n     \n     1\n     \n\n * 在AOP类上加注解\n   \n   * @Component //bean绑定\n     @Aspect  //标记AOP 注解\n     public class AOPAdvice {\n          @Pointcut("execution(* *..*(..))")  //定义一个空方法 并且绑定为aop id=pt\n         public void pt(){}\n         \n          @Before("pt()")  //标记为aop-before方法 并且绑定空间名称 pt()\n         public void before(JoinPoint jp) throws Throwable {\n             Object[] args = jp.getArgs();\n         }\n         \n             @After("pt()")\n         public void function() {\n             System.out.println("共性功能");\n         }\n         \n             @Around("pt()")\n         public Object around (ProceedingJoinPoint pjp)  {\n             //对原始方法的调用  返回原始方法的返回值\n             Object proceed = null;\n             try {\n                 proceed = pjp.proceed();\n             } catch (Throwable e) {\n                 e.printStackTrace();\n             }\n             System.out.println(proceed);\n             //必须返回返回值 不然原始方法会异常\n             return proceed;\n         }\n         \n             @AfterReturning(value = "pt()", returning = "ret")\n         public void before2(Object ret) {\n             System.out.println(ret);\n         }\n     \n         @AfterThrowing(value = "pt()", throwing = "t")\n         public void afterThrowing(Throwable t) {\n             System.out.println(t.getMessage());\n         }\n         \n     }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     25\n     26\n     27\n     28\n     29\n     30\n     31\n     32\n     33\n     34\n     35\n     36\n     37\n     38\n     39\n     40\n     41\n     \n\n * @Aspect 标记为aop 在AOP类上 记得实例化并@Component\n\n * @Pointcut 定义个一个空参无返回值的空方法 为此aop绑定空间名称 为方法名 如果在其他类中创建 则通知中调用要加上类名 如: aopconfig.pt()\n\n * @After 后置通知\n\n * @Before 前置通知\n\n# 注解通知执行顺序\n\n 1. 与方法定义位置无关\n 2. 如果通知类型同 与方法名自然排序来执行\n 3. 如果是不同AOP的通知 是与AOP类名有关\n 4. 在类上方 使用注解 @Order(n) 自定义顺序\n\n# AOP配置 (注解)\n\n前面我们用xml配置注解\n\n * @EnableAspectJAutoProxy 开启aop注解 在spring配置类中标记注解\n\n\n# 静态代理\n\n装饰者模式 (Decorator Pattern) 在不惊动原始设计的基础上,为其添加功能\n\n创建一个新的类 并继承原接口 有参构造方法 形参为原始对象 调用原始方法\n\npublic class UseServiceImplDecorator implements UserService {\n\n    private UserService userService;\n\n    public UseServiceImplDecorator(UserService userService) {\n        this.userService = userService;\n    }\n\n    @Override\n    public void save() {\n        userService.save();\n        System.out.println("刮大白");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# JDK动态代理\n\npublic class UserServiceJDKProxy {\n    public static UserService createUserServiceJDKProxy(UserService userService) {\n\n\n        ClassLoader cl = userService.getClass().getClassLoader();  //获取加载类\n        Class[] classes = userService.getClass().getInterfaces();  //获取类接口\n\n        InvocationHandler ih = new InvocationHandler() {\n            @Override\n            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n                Object invoke = method.invoke(userService, args);  //调用原始方法\n                System.out.println("刮大白");   //不影响原始类下实现增强功能\n                return invoke;\n            }\n        };\n        UserService service = (UserService) Proxy.newProxyInstance(cl, classes, ih);\n        return service;\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# CGLIB\n\nCGLIB(Code Generation Library) Code生成类库 不限定是否有具体接口 无需原始代理对象\n\npublic class UserServiceCglibProxy {\n\n    public static UserService createUserServiceCglibProxy(Class claszz) {\n        Enhancer enhancer = new Enhancer();\n        enhancer.setSuperclass(claszz);   //设置enhancer的父类\n        enhancer.setCallback(new MethodInterceptor() {\n            @Override\n            public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {\n//                method.invoke(o,objects);\n//                methodProxy.invoke(o,objects);\n                System.out.println(method.getName());  //获取方法名\n                Object ret = methodProxy.invokeSuper(o, objects);//原始方法调用 默认对所有方法做增强\n\n                if (method.getName().equals("save")) {  //需要判断方法名\n                    System.out.println("刮大白");\n                }\n                return ret;\n            }\n        });\n        return (UserService) enhancer.create();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n测试类\n\n    public static void main(String[] args) {\n        UserService userService = UserServiceCglibProxy.createUserServiceCglibProxy(UserServiceImpl.class);\n        userService.save();\n    }\n\n\n1\n2\n3\n4\n\n\n\n# AOP底层 切换动态代理方式\n\nAOP底层使用的JDK的动态代理方式\n\n我们可以配置为使用CGLIB方式 需要在aop:config中配置\n\n \x3c!--XML配置AOP 默认为flase 为jdk动态代理 true为CGLIB代理 --\x3e  \n<aop:config proxy-target-class="true">\n    \x3c!-- 注解配置AOP--\x3e\n<aop:aspectj-autoproxy proxy-target-class="false"/>\n\n\n1\n2\n3\n4\n\n\n//注解驱动\n@EnableAspectJAutoProxy(proxyTargetClass = true)\n\n\n1\n2\n\n\n\n# 织入时机\n\n\n\n\n# 事务\n\n事务指数据库中多个操作合并在一起形成的操作序列\n\n * 当操作出现失败 回滚事务 保障数据的一致性\n * 当并发访问数据库时 防止并发访问操作结果相互干扰\n * \n\n\n# 事务核心对象\n\n\n\n\n# 编程式事务\n\n    private DataSource dataSource;\n    public void setDataSource(DataSource dataSource) {\n        this.dataSource = dataSource;\n    }   \n\n//开启事务\n        PlatformTransactionManager ptm = new DataSourceTransactionManager(dataSource);\n        //事务定义\n        TransactionDefinition td = new DefaultTransactionDefinition();\n        //事务状态\n        TransactionStatus ts = ptm.getTransaction(td);\n\n        accountDao.inMoney(outName, money);\n//        int i = 1 / 0;\n        accountDao.outMoney(inName, money);\n\n        //提交事务\n        ptm.commit(ts);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n业务层要注入dataSource\n\n    <bean id="accountService" class="com.itheima.service.impl.AccountServiceImpl">\n        <property name="accountDao" ref="accountDao"/>\n        <property name="dataSource" ref="dataSource"/>\n    </bean>\n\n\n1\n2\n3\n4\n\n\n\n# AOP改造编程式事务\n\n    private DataSource dataSource;\n    public void setDataSource(DataSource dataSource) {\n        this.dataSource = dataSource;\n    }\n\n    public Object transactionManager(ProceedingJoinPoint pjp) throws Throwable {\n        //开启事务\n        PlatformTransactionManager ptm = new DataSourceTransactionManager(dataSource);\n        //事务定义\n        TransactionDefinition td = new DefaultTransactionDefinition();\n        //事务状态\n        TransactionStatus ts = ptm.getTransaction(td);\n\n        Object ret = pjp.proceed(pjp.getArgs());\n\n        //提交事务\n        ptm.commit(ts);\n\n        return ret;\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n    <bean id="txAdvice" class="com.itheima.aop.TxAdvice">\n        <property name="dataSource" ref="dataSource"/>\n    </bean>\n\n    <aop:config>\n        <aop:pointcut id="pt" expression="execution(* *..transfer(..))"/>\n        <aop:aspect ref="txAdvice">\n            <aop:around method="transactionManager" pointcut-ref="pt"/>\n        </aop:aspect>\n    </aop:config>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 声明式事务\n\n声明添加域\n\nxmlns:tx="http://www.springframework.org/schema/tx"\n\nhttp://www.springframework.org/schema/tx\n\nhttps://www.springframework.org/schema/tx/spring-tx.xsd\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xmlns:tx="http://www.springframework.org/schema/tx"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/tx\n        https://www.springframework.org/schema/tx/spring-tx.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n    <bean class="org.mybatis.spring.SqlSessionFactoryBean">\n        <property name="dataSource" ref="dataSource"/>\n        <property name="typeAliasesPackage" value="com.itheima.domain"/>\n    </bean>\n\n\n    <bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">\n        <property name="basePackage" value="com.itheima.dao"/>\n    </bean>\n\n    <bean id="accountService" class="com.itheima.service.impl.AccountServiceImpl">\n        <property name="accountDao" ref="accountDao"/>\n    </bean>\n\n\n\n\x3c!--    tx声明--\x3e\n    <bean id="txManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">\n        <property name="dataSource" ref="dataSource"/>\n    </bean>\n\n    \x3c!--    定义事务管理的通知类--\x3e\n    <tx:advice id="txAdvice" transaction-manager="txManager">\n        \x3c!--定义要控制的事务--\x3e\n        <tx:attributes>\n            \x3c!--            指定方法控制事务 read-only 是否开启只读事务--\x3e\n            \x3c!--            <tx:method name="transfer" read-only="false"/>--\x3e\n            <tx:method name="*" read-only="false"/>\n            <tx:method name="get*" read-only="true"/>\n            <tx:method name="find*" read-only="true"/>\n        </tx:attributes>\n    </tx:advice>\n\n    <aop:config>\n        <aop:pointcut id="pt" expression="execution(* com.itheima.service.*Service.*(..))"/>\n        \x3c!--      advice可以是普通类不实现接口或没有继承关系  advisor通知类必须实现通知接口  --\x3e\n        <aop:advisor advice-ref="txAdvice" pointcut-ref="pt"/>\n    </aop:config>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# 事务传播行为\n\n\n\n\n\n需要指定传播属性则在tx:method 的 propagation 属性配置\n\n <tx:method name="a父" read-only="false" propagation="REQUIRED"/>\n<tx:method name="b子" read-only="false" propagation="REQUIRED"/>\n\n\n1\n2\n\n\n\n# 注解事务\n\n在业务层 sql操作接口(全部抽象方法 推荐这个 或接口中的抽象方法)/类(全部方法)/方法上都可以 上面加 @Transactional\n\n    @Transactional(readOnly = false, timeout = -1, isolation = Isolation.DEFAULT, rollbackFor = {IOException.class},propagation = Propagation.REQUIRED)\n    public void transfer(String outName, String inName, Double money);\n\n\n1\n2\n\n 1. 开启tx注解驱动 xml版\n\n    \x3c!--事务管理为事务注解绑定的方法--\x3e\n    <tx:annotation-driven transaction-manager="txManager"/>\n\n\n1\n2\n\n 2. 注解版 注解驱动 在springconfig类上标记\n\n@EnableTransactionManagement\n\n\n1\n\n\n并且配置事务核心对象\n\n    @Bean\n    public PlatformTransactionManager getTransactionManager(DataSource dataSource){\n        return new DataSourceTransactionManager(dataSource);\n    }\n\n\n1\n2\n3\n4\n\n\n\n# Spring模板对象\n\n\n# JdbcTemplate',normalizedContent:'# spring\n\nspring是分层的javase/ee应用 full-stack轻量级 一站式 开源框架\n\n体系结构:\n\n\n\n\n# ioc 反转\n\nioc(inversion of control) 控制反转,spring反向控制应用程序所需要使用的外部资源\n\nspring控制的资源全部放置在spring容器中,该容器称为ioc容器\n\n\n# 耦合与内聚\n\n耦合(coupling): 代码书写过程中所使用技术的结合紧密度,用于衡量软件中各个模块之间的互联程度\n\n内聚(cohesion):代码书写过程中单个模块内部各组成部分间的联系,用于衡量软件中各个功能模块内部的功能联系\n\n我们追求 高内聚 低耦合\n\n\n# 工厂模式\n\n\n\n\n# 创建项目\n\n * 创建maven项目\n\n * 导入模块\n   \n   *     <dependencies>\n             <dependency>\n                 <groupid>org.springframework</groupid>\n                 <artifactid>spring-context</artifactid>\n                 <version>5.1.9.release</version>\n             </dependency>\n         </dependencies>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     \n\n * 在resources中创建applicationcontext.xml文件\n   \n   * <?xml version="1.0" encoding="utf-8"?>\n     <beans xmlns="http://www.springframework.org/schema/beans"\n            xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n            xsi:schemalocation="http://www.springframework.org/schema/beans\n             https://www.springframework.org/schema/beans/spring-beans.xsd">\n     \x3c!--   bean为映射标签 创建spring控制的资源  id为操作空间  class为实现类引用路径--\x3e\n         <bean id="userservice" class="com.itheima.service.impl.userserviceimpl"/>\n     </beans>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n * 使用方法\n   \n   * //2.加载配置文件\n             applicationcontext ctx =new classpathxmlapplicationcontext("applicationcontext.xml");\n             //3.获取资源\n             userservice userservice = (userservice) ctx.getbean("userservice");\n     \n             userservice.save();\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# bean\n\n * <bean>标签 定义spring中的资源, 此标签定义的资源将受到spring控制\n   \n   * id属性 bean的名词 通过id值获取到bean\n   \n   * class属性 bean的类型\n   \n   * name属性 bean的别名 我们可以通过id或者name来获取到bean 并且name可以起多个别名 别名之间用逗号隔开\n   \n   * scope属性 定义bean的作用范围\n     \n     * singleton值 设置创建出的对象保存在spring容器中 是一个单例对象 默认值 单例时创建spring容器时就加载bean绑定的类\n     * prototype值 设置创建出的对象保存在spring容器中 是一个**非单例对象 ** 非单例则是在getbean时创建类\n     * request、session、application、websocket值 设置创建出的对象放置在web容器对应的位置\n   \n   * init-method bean对象初始化时执行指定方法\n     \n     * 值为bean对应的类中的具体方法名\n   \n   * destroy-method bean对象销毁时执行指定方法\n     \n     * 值为bean对应的类中的具体方法名 只有classpathxmlapplicationcontext才有close方法 spring默认自动销毁但销毁方法不会执行\n     * \n   \n   * factory-method属性\n     \n     * 值为静态工厂中创建对象的方法\n   \n   * factory-bean属性\n     \n     * 值为实例工厂的id 需要先用一个bean绑定实例工厂类 然后第二个bean的factory-bean为第一个bean的id 然后用factory-method调用其实例工厂的方法\n     \n     * \x3c!--    factory-bea  实例工厂创建对象  需要bean先绑定实例工厂类 然后第二个bean的factory-bea为实例工厂的id 然后factory-method调用其创建对象方法--\x3e\n           <bean id="userservice3" class="com.itheima.service.userservicefactory2" />\n           <bean factory-bean="userservice3" factory-method="getservice"/>\n       \n       \n       1\n       2\n       3\n       \n\n\n# di 依赖注入\n\ndi(dependency injection) 依赖注入,应用程序运行依赖的资源由spring为其提供,资源进入应用程序的方式称为注入\n\n\n# set注入\n\n * <property>标签 使用set方法的形式为bean提供资源 是bean的子标签\n   * name属性\n     * 值为对应bean中的属性名 要该属性必须为私有属性同时提供set方法\n   * value属性\n     * 值为name绑定属性的 值 设定非引用类型属性的值\n   * ref属性\n     * 值为引用类型属性对象bean的id 不能与value同时使用\n\n配置\n\n    <bean id="userservice" class="com.itheima.service.impl.userserviceimpl">\n\x3c!--        将要注入的引用类型变量通过property属性进行注入 对应的name是要注入的变量名  使用ref声明要注入的bean的id  --\x3e\n        <property name="userdao" ref="userdao"></property>\n\x3c!--        如果要注入的变量为一个值 我们通过value来设置 同样需要在类中设置私有变量 和set方法--\x3e\n        <property name="num" value="13"/>\n    </bean>\n\n\x3c!--    将要注入的资源声明为bean--\x3e\n    <bean id="userdao" class="com.itheima.dao.impl.userdaoimpl"/>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n反转资源注入\n\nimport com.itheima.dao.userdao;\nimport com.itheima.service.userservice;\n\npublic class userserviceimpl implements userservice {\n    private userdao userdao;\n    private int num;\n\n    //对需要进行注入的变量添加set方法\n    public void setuserdao(userdao userdao) {\n        this.userdao = userdao;\n    }\n\n    public void setnum(int num) {\n        this.num = num;\n    }\n\n    @override\n    public void save() {\n        userdao.save();\n        system.out.println("running...");\n        system.out.println(num);\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n调用\n\npublic class userapp {\n    public static void main(string[] args) {\n        //2.加载配置文件\n        applicationcontext ctx =new classpathxmlapplicationcontext("applicationcontext.xml");\n        //3.获取资源\n        userservice userservice = (userservice) ctx.getbean("userservice");\n        userservice.save();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\n# 构造器注入\n\n * <constructor-arg> 标签 使用构造方法的形式为bean提供资源 是bean的子标签\n   * name属性\n     * 值为对应bean中的属性名 要该属性必须为私有属性同时提供set方法\n   * value属性\n     * 值为name绑定属性的 值 设定非引用类型属性的值\n   * ref属性\n     * 值为引用类型属性对象bean的id 不能与value同时使用\n   * type属性\n     * 值为设定构造方法参数的类型 用于赋值给指定的变量类型 推荐使用nanme指定变量\n   * index属性\n     * 值为设定构造方法参数的位置 用于赋值给指定 构造方法中 变量的index 从0开始 推荐使用name指定变量\n\n被注入类要声明构造方法并赋值\n\nprivate userdao userdao;\nprivate int num;\npublic userserviceimpl(userdao userdao,int num){\n        this.userdao=userdao;\n        this.num=num;\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\nbean配置\n\n\x3c!--    构造方法注入--\x3e\n\x3c!--    注入类--\x3e\n    <bean id="userdao" class="com.itheima.dao.impl.userdaoimpl"/>\n\x3c!--被注入的类--\x3e\n    <bean id="userservice" class="com.itheima.service.impl.userserviceimpl">\n        <constructor-arg ref="userdao" />\n        <constructor-arg name="num" value="456"/>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 集合类型数据注入\n\n如 <array>、<list>、<set>、<map>、<props>等集合 是归属于property或constructor-arg标签的子标签\n\n * <list>\n   \n   * <value>标签\n     * 值为元素对于的值\n\n * <props>\n   \n   * <prop>标签\n     * key属性\n       * 值为属性名\n     * 值为元素对于的值\n\n * <array>\n\n * <value>标签\n   \n   * 值为元素对于的值\n\n * <set>\n   \n   * <value>标签\n     * 值为元素对于的值\n\n * <map>\n   \n   * <enrty>标签\n     * key属性\n       * 值为key\n     * value属性\n       * 值为值\n\n    \x3c!--    集合注入--\x3e\n    <bean id="userdao" class="com.itheima.dao.impl.userdaoimpl"/>\n\n    <bean id="userservice" class="com.itheima.service.impl.userserviceimpl">\n        <property name="userdao" ref="userdao"/>\n        <property name="bookdao" ref="bookdao"/>\n    </bean>\n\n    <bean id="bookdao" class="com.itheima.dao.impl.bookdaoimpl">\n        <property name="al">\n            <list>\n                <value>helll</value>\n                <value>world</value>\n            </list>\n        </property>\n        <property name="properties">\n            <props>\n                <prop key="name">age</prop>\n                <prop key="value">19</prop>\n            </props>\n        </property>\n        <property name="arr">\n            <array>\n                <value>123</value>\n                <value>456</value>\n            </array>\n        </property>\n        <property name="hs">\n            <set>\n                <value>111</value>\n                <value>222</value>\n            </set>\n        </property>\n        <property name="hm">\n            <map>\n                <entry key="name" value="zhangsan"/>\n                <entry key="name" value="lisi"/>\n            </map>\n        </property>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# p命名空间 简化配置\n\n增加属性 xmlns:p="http://www.springframework.org/schema/p" 配置p命名空间\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xsi:schemalocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd">\n\n\n1\n2\n3\n4\n5\n\n\np命名空间为bean注入属性值 替代property\n\n\x3c!--    两个bean一样的--\x3e\n   <bean id="userservice" class="com.itheima.service.impl.userserviceimpl" p:bookdao-ref="bookdao" p:userdao-ref="userdao" />\n\n    <bean id="userservice" class="com.itheima.service.impl.userserviceimpl">\n        <property name="userdao" ref="userdao"/>\n        <property name="bookdao" ref="bookdao"/>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# spel el表达式\n\nspring通过el表达式的支持,统一属性注入格式\n\n\n\n在value中书写el表达式\n\n\n# properties文件\n\n增加属性 xmlns:context="http://www.springframework.org/schema/context"\n\n并加上约束 http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemalocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n">\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n加context命名空间的支持\n\n    \x3c!--   context命名空间 加载类路径下 所有的properties文件  加载后使用${属性名} --\x3e\n    <context:property-placeholder location="classpath:*.properties"/>\n <bean id="userservice" class="com.itheima.service.impl.userserviceimpl">\n            <constructor-arg ref="userdao" />\n            <constructor-arg name="num" value="${age}"/>\n        </bean>--\x3e\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# import 团队开发\n\nxml中通过<import>标签我们可以引入外部的ioc配置\n\n * <impoprt>标签 引用外部ioc配置\n   * resource属性 配置文件名\n\n\n# 容器运行时加载多个ioc配置\n\napplicationcontext ctx =new classpathxmlapplicationcontext("applicationcontext.xml","applicationcontext2.xml");\n\n\n1\n\n\n\n# bean注意事项\n\n * id是唯一的 同一文件中不允许存在相同id 而不同文件中后定义覆盖前面的\n * 导入配置文件可以理解为 将配置文件复制粘贴到对应位置\n * 导入配置文件顺序不同可能导致程序运行结果不同\n\n\n# applicationcontext对象\n\n\n\n\n# 第三方资源bean配置\n\n添加druid模块 druid是阿里巴巴开发的jdbc连接池组件\n\n        <dependency>\n            <groupid>com.alibaba</groupid>\n            <artifactid>druid</artifactid>\n            <version>1.1.21</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n配置文件\n\n    <bean id="datasource" class="com.alibaba.druid.pool.druiddatasource">\n        <property name="driverclassname" value="com.mysql.jdbc.driver"/>\n        <property name="url" value="jdbc:mysql://localhost:3306/heima_mm"/>\n        <property name="username" value="root"/>\n        <property name="password" value="123456"/>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n\n\n创建\n\napplicationcontext ctx =new classpathxmlapplicationcontext("applicationcontext.xml");\ndruiddatasource datasource = (druiddatasource) ctx.getbean("datasource");\nsystem.out.println(datasource);\n\n\n1\n2\n3\n\n\n看清楚 第三方类的类名 属性名 set方法\n\n\n# spring+mybatis\n\napplicationcontext.xml\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xsi:schemalocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n">\n\n    <context:property-placeholder location="classpath:*.properties"/>\n\n    <bean id="datasource" class="com.alibaba.druid.pool.druiddatasource">\n        <property name="driverclassname" value="${jdbc.driver}"/>\n        <property name="url" value="${jdbc.url}"/>\n        <property name="username" value="${jdbc.username}"/>\n        <property name="password" value="${jdbc.password}"/>\n\n    </bean>\n\n\n    \x3c!--    spring整合mybatis后控制创建连接用的对象--\x3e\n    <bean class="org.mybatis.spring.sqlsessionfactorybean">\n        <property name="datasource" ref="datasource"/>\n        <property name="typealiasespackage" value="com.itheima.domain"/>\n    </bean>\n\n    \x3c!--    加载mybatis映射配置扫描 将其作为spring的bean进行管理--\x3e\n    <bean class="org.mybatis.spring.mapper.mapperscannerconfigurer">\n        <property name="basepackage" value="com.itheima.dao"/>\n    </bean>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\npox.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n\n    <groupid>org.example</groupid>\n    <artifactid>spring+mybatis</artifactid>\n    <version>1.0-snapshot</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupid>org.mybatis</groupid>\n            <artifactid>mybatis</artifactid>\n            <version>3.5.3</version>\n        </dependency>\n        <dependency>\n            <groupid>mysql</groupid>\n            <artifactid>mysql-connector-java</artifactid>\n            <version>5.1.47</version>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-context</artifactid>\n            <version>5.1.9.release</version>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-jdbc</artifactid>\n            <version>5.1.9.release</version>\n        </dependency>\n        <dependency>\n            <groupid>com.alibaba</groupid>\n            <artifactid>druid</artifactid>\n            <version>1.1.21</version>\n        </dependency>\n        <dependency>\n            <groupid>org.mybatis</groupid>\n            <artifactid>mybatis-spring</artifactid>\n            <version>2.0.3</version>\n        </dependency>\n    </dependencies>\n\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 注解\n\n注解代替xml配置可以简化配置 提供开发效率\n\nhttps://www.cnblogs.com/alter888/p/9083963.html\n\n\n\n并且要在applicationcontext.xml中配置组件扫描\n\n    \x3c!--    配置组件扫描--\x3e\n    <context:component-scan base-package="com.itheima"/>\n\n\n1\n2\n\n\n\n# 新注解\n\n使用上面的注解还不能完全替代xml配置 如第三方的类\n\n\n\n@bean 标记第三方类\n\npublic class jdbcconfig {\n    @value("${jdbc.driver}")\n    private string driver;\n    @value("${jdbc.url}")\n    private string url;\n    @value("${jdbc.username}")\n    private string username;\n    @value("${jdbc.password}")\n    private string password;\n\n    @bean("datasource")\n    public datasource getdatasource(){\n        druiddatasource ds = new druiddatasource();\n        ds.setdriverclassname(driver);\n        ds.seturl(url);\n        ds.setusername(username);\n        ds.setpassword(password);\n        return ds;\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n# 加载控制\n\n@dependson("classname")\n\n\n\n@order(n) 控制加载顺序\n\n\n\n@lazy 延迟加载\n\n\n\n\n# 整合junit\n\n在spring中之前我们测试都需要获取容器 然后获取bean\n\n导入坐标\n\n<dependency>\n            <groupid>junit</groupid>\n            <artifactid>junit</artifactid>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-test</artifactid>\n            <version>5.1.9.release</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n测试样例\n\n//设定spring专用的类加载器\n@runwith(springjunit4classrunner.class)\n//设定加载的spring上下文对应的配置\n@contextconfiguration(classes = springconfig.class)\npublic class userservicetest {\n\n    @autowired\n    private accountservice accountservice;\n\n    @test\n    public void testfindbyid(){\n        account ac = accountservice.findbyid(2);\n//        system.out.println(ac);\n        //assert 预计值  结果值  如果不一致则测试不通过\n        assert.assertequals("jock",ac.getname());\n    }\n\n    @test\n    public void testfindall(){\n        list<account> list = accountservice.findall();\n        assert.assertequals(2,list.size());\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# ioc底层核心原理\n\n\n\n\n\n\n\n\n\n\n# 组件扫描过滤器\n\n@componentscan 组件扫描器拥有过滤指定组件功能\n\n * 按注解类型 过滤\n   * excludefilters 设置排除性过滤器\n   * includefilters 设置包含性过滤器\n\n//所有的@service注解被过滤\n@componentscan(value = "com.itheima",excludefilters = @componentscan.filter(type = filtertype.annotation,classes = service.class))\n\n\n\n1\n2\n3\n\n\n\n# 自定义组件过滤器\n\n继承typefilter 实现match方法 返回false则不过滤 返回true则过滤\n\npackage com.itheima.config.filter;\n\nimport org.springframework.core.type.classreading.metadatareader;\nimport org.springframework.core.type.classreading.metadatareaderfactory;\nimport org.springframework.core.type.filter.typefilter;\n\nimport java.io.ioexception;\n\npublic class mytypefilter implements typefilter {\n\n\n    public boolean match(metadatareader metadatareader, metadatareaderfactory metadatareaderfactory) throws ioexception {\n        classmetadata classmetadata = metadatareader.getclassmetadata();  //获取class的元数据\n        string classname = classmetadata.getclassname();  //获取类名\n        system.out.println(classname);\n        if(classname.equals("com.itheima.service.impl.accountserviceimpl")){\n            return true;\n        }\n\n        return false;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * 按自定义过滤器 过滤\n\n@componentscan(value = "com.itheima",excludefilters = @componentscan.filter(type = filtertype.custom,classes = mytypefilter.class))\n\n\n1\n\n\n\n# 自定义导入器\n\n继承 importselector 实现selectimports方法\n\npackage com.itheima.config.selector;\n\nimport org.springframework.context.annotation.importselector;\nimport org.springframework.core.type.annotationmetadata;\n\nimport java.util.resourcebundle;\n\npublic class myimportselector implements importselector {\n\n    public string[] selectimports(annotationmetadata importingclassmetadata) {\n\n        //使用properties文件读取\n        resourcebundle bundle = resourcebundle.getbundle("import");  //文件名\n        string classname = bundle.getstring("classname");  //属性名\n        return new string[]{classname};\n\n//        return new string[]{"com.itheima.service.impl.accountserviceimpl"}; //直接返回指定类不推荐使用\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n导入指定的组件 不用需要bean绑定或者注解绑定\n\n@import(myimportselector.class)\n\n\n1\n\n\n\n# 自定义注册器\n\n继承 importbeandefinitionregistrar 实现 registerbeandefinitions 方法\n\npackage com.itheima.config.registrar;\n\nimport org.springframework.beans.factory.support.beandefinitionregistry;\nimport org.springframework.context.annotation.classpathbeandefinitionscanner;\nimport org.springframework.context.annotation.importbeandefinitionregistrar;\nimport org.springframework.core.type.annotationmetadata;\nimport org.springframework.core.type.classreading.metadatareader;\nimport org.springframework.core.type.classreading.metadatareaderfactory;\nimport org.springframework.core.type.filter.typefilter;\n\nimport java.io.ioexception;\n\npublic class myimportbeandefinitionregistrar implements importbeandefinitionregistrar {\n\n    public void registerbeandefinitions(annotationmetadata importingclassmetadata, beandefinitionregistry registry) {\n        classpathbeandefinitionscanner scanner = new classpathbeandefinitionscanner(registry,false);\n        scanner.addincludefilter(new typefilter() {\n            public boolean match(metadatareader metadatareader, metadatareaderfactory metadatareaderfactory) throws ioexception {\n                return true;\n            }\n        });\n        scanner.scan("com.itheima");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n@import(myimportbeandefinitionregistrar.class)  //自定义注册器\n\n\n1\n\n\n\n# bean初始化过程\n\n\n\n\n\n\n# aop\n\naspect oriented programming 面向切门编程 通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术\n\naop 是 oop(面向对象) 的延续\n\n\n\n作用:在程序运行期间 在不修改源码的情况下对方法进行功能增强\n\n优势:减少重复代码 提高开发效率 并且便于维护\n\n\n# aop的动态代理对象\n\n * jdk代理 : 基于接口的动态代理技术\n * cglib代理 : 基于父类的动态代理技术\n\n导入坐标\n\n        <dependency>\n            <groupid>org.aspectj</groupid>\n            <artifactid>aspectjweaver</artifactid>\n            <version>1.9.4</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n把共性的功能提前出来 并提供类和方法\n\n\n# xml配置\n\n添加aop 以下声明\n\nxmlns:aop="http://www.springframework.org/schema/aop"\nhttp://www.springframework.org/schema/aop\nhttps://www.springframework.org/schema/aop/spring-aop.xsd\n\n\n1\n2\n3\n\n\n标签头\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemalocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n绑定\n\n    \x3c!--3.开启aop命名空间--\x3e\n    <bean id="userservice" class="com.itheima.service.impl.userserviceimpl"/>\n    \x3c!--2.配置共性功能成功spring控制的资源  共性功能的类--\x3e\n    <bean id="myadvice" class="com.itheima.aop.aopadvice"/>\n\n    \x3c!--4.配置aop--\x3e\n    <aop:config>\n        \x3c!--5.配置切入点--\x3e\n        <aop:pointcut id="pt" expression="execution(* *..*(..))"/>\n        \x3c!--6.配置切面（切入点与通知的关系）--\x3e\n        <aop:aspect ref="myadvice">\n            \x3c!--7.配置具体的切入点对应通知中那个操作方法  pointcut-ref为pointcut对应的id--\x3e\n            <aop:before method="function" pointcut-ref="pt"/>\n        </aop:aspect>\n    </aop:config>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n * aop:config标签 aop根标签 在beans可以拥有多个\n   * aop:aspect标签 可以在aop:config中配置多个\n     * ref属性 通知(共性类)所在的bean的id\n     * aop:before标签 切面\n       * method属性 通知中具体的方法\n       * pointcut-ref属性 与aop:pointcut中的id要一致\n       * pointcut="execution(* ..(..))" 私有切入点 不能与pointcut-ref共存\n   * aop:pointcut标签 也可以拥有多个 上级未config则为公共切入点 上级未aspect则为局部切入点\n     * id属性 名称 可以自定义\n     * expression属性 切入点模式\n\n# 切入点表达式\n\n\n\n\n\n支持逻辑运算符 和非运算\n\n# 通知类型\n\n * aop:before 前置通知 如果通知中抛出异常 则阻止原始方法运行\n * aop:after 后置通知 无论是否异常 都会执行通知\n * aop:after-running 运行通知 如果抛出异常无法通知\n * aop:after-throwing 异常通知 如果没有抛出异常无法通知\n * aop:around 环绕通知 在原始方法执行前后都有对应执行的执行,还可以阻止原始方法执行\n   * \n\n# 通知顺序\n\n当同一个切入点配置多个通知时,标签配置的顺序就是执行的顺序\n\n# 通知获取数据\n\n * 获取参数 所有的通知都可以获取参数\n   \n   * public void before(joinpoint jp) throws  throwable{\n             object[] args = jp.getargs();\n         }\n     \n     \n     1\n     2\n     3\n     \n   \n   * 第二个方法\n     \n     * 在通知方法中定义变量\n       \n       * public void before2(int a) {\n                 system.out.println(a);\n             }\n         \n         \n         1\n         2\n         3\n         \n     \n     * 在applicationcontext.xml 配置aop传参\n       \n       *             \x3c!--         &amp是&  args(a) 为aop通知方法中形参的名字   --\x3e\n                     <aop:before method="before2" pointcut="execution(* *..*(..)) &amp;&amp; args(a)"/>\n         \n         \n         1\n         2\n         \n\n# 通知获取返回值\n\n * 获取返回值 只有around 和 after-returning 通知\n   \n   * public void before2(object ret) {\n             system.out.println(ret);\n         }\n     \n     \n     1\n     2\n     3\n     \n   \n   *             \x3c!--       returning为通知方法形参的名字  此变量为原始方法的返回值     --\x3e\n                 <aop:after-returning method="before2" pointcut-ref="pt" returning="ret"/>\n     \n     \n     1\n     2\n     \n   \n   * 第二种方法\n     \n     *     public object around (proceedingjoinpoint pjp) throws throwable {\n               //对原始方法的调用  返回原始方法的返回值\n               object proceed = pjp.proceed();\n               system.out.println(proceed);\n               //必须返回返回值 不然原始方法会异常\n               return proceed;\n           }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       \n     \n     * <aop:around method="around" pointcut-ref="pt"/>\n       \n       \n       \n       1\n       2\n       \n\n# 获取异常\n\n * 获取异常 around 和 after-throwing 通知获取\n   \n   *     public void afterthrowing(throwable t){\n             system.out.println(t.getmessage());\n         }\n     \n     \n     1\n     2\n     3\n     \n   \n   *             \x3c!--      throwing为通知方法中形参的名称  用于获取异常      --\x3e\n                 <aop:after-throwing method="afterthrowing" pointcut-ref="pt" throwing="t"/>\n     \n     \n     1\n     2\n     \n   \n   * 第二种方法\n     \n     *     public object around (proceedingjoinpoint pjp)  {\n               //对原始方法的调用  返回原始方法的返回值\n               object proceed = null;\n               try {\n                   proceed = pjp.proceed();\n               } catch (throwable e) {\n                   e.printstacktrace();\n               }\n               system.out.println(proceed);\n               //必须返回返回值 不然原始方法会异常\n               return proceed;\n           }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       \n     \n     *  <aop:around method="around" pointcut-ref="pt"/>\n       \n       \n       1\n       \n\n\n# 注解\n\n * 开aop注解支持\n   \n   * <aop:aspectj-autoproxy/>\n     \n     \n     1\n     \n\n * 在aop类上加注解\n   \n   * @component //bean绑定\n     @aspect  //标记aop 注解\n     public class aopadvice {\n          @pointcut("execution(* *..*(..))")  //定义一个空方法 并且绑定为aop id=pt\n         public void pt(){}\n         \n          @before("pt()")  //标记为aop-before方法 并且绑定空间名称 pt()\n         public void before(joinpoint jp) throws throwable {\n             object[] args = jp.getargs();\n         }\n         \n             @after("pt()")\n         public void function() {\n             system.out.println("共性功能");\n         }\n         \n             @around("pt()")\n         public object around (proceedingjoinpoint pjp)  {\n             //对原始方法的调用  返回原始方法的返回值\n             object proceed = null;\n             try {\n                 proceed = pjp.proceed();\n             } catch (throwable e) {\n                 e.printstacktrace();\n             }\n             system.out.println(proceed);\n             //必须返回返回值 不然原始方法会异常\n             return proceed;\n         }\n         \n             @afterreturning(value = "pt()", returning = "ret")\n         public void before2(object ret) {\n             system.out.println(ret);\n         }\n     \n         @afterthrowing(value = "pt()", throwing = "t")\n         public void afterthrowing(throwable t) {\n             system.out.println(t.getmessage());\n         }\n         \n     }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     25\n     26\n     27\n     28\n     29\n     30\n     31\n     32\n     33\n     34\n     35\n     36\n     37\n     38\n     39\n     40\n     41\n     \n\n * @aspect 标记为aop 在aop类上 记得实例化并@component\n\n * @pointcut 定义个一个空参无返回值的空方法 为此aop绑定空间名称 为方法名 如果在其他类中创建 则通知中调用要加上类名 如: aopconfig.pt()\n\n * @after 后置通知\n\n * @before 前置通知\n\n# 注解通知执行顺序\n\n 1. 与方法定义位置无关\n 2. 如果通知类型同 与方法名自然排序来执行\n 3. 如果是不同aop的通知 是与aop类名有关\n 4. 在类上方 使用注解 @order(n) 自定义顺序\n\n# aop配置 (注解)\n\n前面我们用xml配置注解\n\n * @enableaspectjautoproxy 开启aop注解 在spring配置类中标记注解\n\n\n# 静态代理\n\n装饰者模式 (decorator pattern) 在不惊动原始设计的基础上,为其添加功能\n\n创建一个新的类 并继承原接口 有参构造方法 形参为原始对象 调用原始方法\n\npublic class useserviceimpldecorator implements userservice {\n\n    private userservice userservice;\n\n    public useserviceimpldecorator(userservice userservice) {\n        this.userservice = userservice;\n    }\n\n    @override\n    public void save() {\n        userservice.save();\n        system.out.println("刮大白");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# jdk动态代理\n\npublic class userservicejdkproxy {\n    public static userservice createuserservicejdkproxy(userservice userservice) {\n\n\n        classloader cl = userservice.getclass().getclassloader();  //获取加载类\n        class[] classes = userservice.getclass().getinterfaces();  //获取类接口\n\n        invocationhandler ih = new invocationhandler() {\n            @override\n            public object invoke(object proxy, method method, object[] args) throws throwable {\n                object invoke = method.invoke(userservice, args);  //调用原始方法\n                system.out.println("刮大白");   //不影响原始类下实现增强功能\n                return invoke;\n            }\n        };\n        userservice service = (userservice) proxy.newproxyinstance(cl, classes, ih);\n        return service;\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# cglib\n\ncglib(code generation library) code生成类库 不限定是否有具体接口 无需原始代理对象\n\npublic class userservicecglibproxy {\n\n    public static userservice createuserservicecglibproxy(class claszz) {\n        enhancer enhancer = new enhancer();\n        enhancer.setsuperclass(claszz);   //设置enhancer的父类\n        enhancer.setcallback(new methodinterceptor() {\n            @override\n            public object intercept(object o, method method, object[] objects, methodproxy methodproxy) throws throwable {\n//                method.invoke(o,objects);\n//                methodproxy.invoke(o,objects);\n                system.out.println(method.getname());  //获取方法名\n                object ret = methodproxy.invokesuper(o, objects);//原始方法调用 默认对所有方法做增强\n\n                if (method.getname().equals("save")) {  //需要判断方法名\n                    system.out.println("刮大白");\n                }\n                return ret;\n            }\n        });\n        return (userservice) enhancer.create();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n测试类\n\n    public static void main(string[] args) {\n        userservice userservice = userservicecglibproxy.createuserservicecglibproxy(userserviceimpl.class);\n        userservice.save();\n    }\n\n\n1\n2\n3\n4\n\n\n\n# aop底层 切换动态代理方式\n\naop底层使用的jdk的动态代理方式\n\n我们可以配置为使用cglib方式 需要在aop:config中配置\n\n \x3c!--xml配置aop 默认为flase 为jdk动态代理 true为cglib代理 --\x3e  \n<aop:config proxy-target-class="true">\n    \x3c!-- 注解配置aop--\x3e\n<aop:aspectj-autoproxy proxy-target-class="false"/>\n\n\n1\n2\n3\n4\n\n\n//注解驱动\n@enableaspectjautoproxy(proxytargetclass = true)\n\n\n1\n2\n\n\n\n# 织入时机\n\n\n\n\n# 事务\n\n事务指数据库中多个操作合并在一起形成的操作序列\n\n * 当操作出现失败 回滚事务 保障数据的一致性\n * 当并发访问数据库时 防止并发访问操作结果相互干扰\n * \n\n\n# 事务核心对象\n\n\n\n\n# 编程式事务\n\n    private datasource datasource;\n    public void setdatasource(datasource datasource) {\n        this.datasource = datasource;\n    }   \n\n//开启事务\n        platformtransactionmanager ptm = new datasourcetransactionmanager(datasource);\n        //事务定义\n        transactiondefinition td = new defaulttransactiondefinition();\n        //事务状态\n        transactionstatus ts = ptm.gettransaction(td);\n\n        accountdao.inmoney(outname, money);\n//        int i = 1 / 0;\n        accountdao.outmoney(inname, money);\n\n        //提交事务\n        ptm.commit(ts);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n业务层要注入datasource\n\n    <bean id="accountservice" class="com.itheima.service.impl.accountserviceimpl">\n        <property name="accountdao" ref="accountdao"/>\n        <property name="datasource" ref="datasource"/>\n    </bean>\n\n\n1\n2\n3\n4\n\n\n\n# aop改造编程式事务\n\n    private datasource datasource;\n    public void setdatasource(datasource datasource) {\n        this.datasource = datasource;\n    }\n\n    public object transactionmanager(proceedingjoinpoint pjp) throws throwable {\n        //开启事务\n        platformtransactionmanager ptm = new datasourcetransactionmanager(datasource);\n        //事务定义\n        transactiondefinition td = new defaulttransactiondefinition();\n        //事务状态\n        transactionstatus ts = ptm.gettransaction(td);\n\n        object ret = pjp.proceed(pjp.getargs());\n\n        //提交事务\n        ptm.commit(ts);\n\n        return ret;\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n    <bean id="txadvice" class="com.itheima.aop.txadvice">\n        <property name="datasource" ref="datasource"/>\n    </bean>\n\n    <aop:config>\n        <aop:pointcut id="pt" expression="execution(* *..transfer(..))"/>\n        <aop:aspect ref="txadvice">\n            <aop:around method="transactionmanager" pointcut-ref="pt"/>\n        </aop:aspect>\n    </aop:config>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 声明式事务\n\n声明添加域\n\nxmlns:tx="http://www.springframework.org/schema/tx"\n\nhttp://www.springframework.org/schema/tx\n\nhttps://www.springframework.org/schema/tx/spring-tx.xsd\n\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xmlns:tx="http://www.springframework.org/schema/tx"\n       xsi:schemalocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/tx\n        https://www.springframework.org/schema/tx/spring-tx.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n    <bean class="org.mybatis.spring.sqlsessionfactorybean">\n        <property name="datasource" ref="datasource"/>\n        <property name="typealiasespackage" value="com.itheima.domain"/>\n    </bean>\n\n\n    <bean class="org.mybatis.spring.mapper.mapperscannerconfigurer">\n        <property name="basepackage" value="com.itheima.dao"/>\n    </bean>\n\n    <bean id="accountservice" class="com.itheima.service.impl.accountserviceimpl">\n        <property name="accountdao" ref="accountdao"/>\n    </bean>\n\n\n\n\x3c!--    tx声明--\x3e\n    <bean id="txmanager" class="org.springframework.jdbc.datasource.datasourcetransactionmanager">\n        <property name="datasource" ref="datasource"/>\n    </bean>\n\n    \x3c!--    定义事务管理的通知类--\x3e\n    <tx:advice id="txadvice" transaction-manager="txmanager">\n        \x3c!--定义要控制的事务--\x3e\n        <tx:attributes>\n            \x3c!--            指定方法控制事务 read-only 是否开启只读事务--\x3e\n            \x3c!--            <tx:method name="transfer" read-only="false"/>--\x3e\n            <tx:method name="*" read-only="false"/>\n            <tx:method name="get*" read-only="true"/>\n            <tx:method name="find*" read-only="true"/>\n        </tx:attributes>\n    </tx:advice>\n\n    <aop:config>\n        <aop:pointcut id="pt" expression="execution(* com.itheima.service.*service.*(..))"/>\n        \x3c!--      advice可以是普通类不实现接口或没有继承关系  advisor通知类必须实现通知接口  --\x3e\n        <aop:advisor advice-ref="txadvice" pointcut-ref="pt"/>\n    </aop:config>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# 事务传播行为\n\n\n\n\n\n需要指定传播属性则在tx:method 的 propagation 属性配置\n\n <tx:method name="a父" read-only="false" propagation="required"/>\n<tx:method name="b子" read-only="false" propagation="required"/>\n\n\n1\n2\n\n\n\n# 注解事务\n\n在业务层 sql操作接口(全部抽象方法 推荐这个 或接口中的抽象方法)/类(全部方法)/方法上都可以 上面加 @transactional\n\n    @transactional(readonly = false, timeout = -1, isolation = isolation.default, rollbackfor = {ioexception.class},propagation = propagation.required)\n    public void transfer(string outname, string inname, double money);\n\n\n1\n2\n\n 1. 开启tx注解驱动 xml版\n\n    \x3c!--事务管理为事务注解绑定的方法--\x3e\n    <tx:annotation-driven transaction-manager="txmanager"/>\n\n\n1\n2\n\n 2. 注解版 注解驱动 在springconfig类上标记\n\n@enabletransactionmanagement\n\n\n1\n\n\n并且配置事务核心对象\n\n    @bean\n    public platformtransactionmanager gettransactionmanager(datasource datasource){\n        return new datasourcetransactionmanager(datasource);\n    }\n\n\n1\n2\n3\n4\n\n\n\n# spring模板对象\n\n\n# jdbctemplate',charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"Dubbo",frontmatter:{title:"Dubbo",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/dc966e/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/10.Dubbo.html",relativePath:"后端/02.JavaEE/10.Dubbo.md",key:"v-7d022df1",path:"/pages/dc966e/",headers:[{level:2,title:"集群和分布式",slug:"集群和分布式",normalizedTitle:"集群和分布式",charIndex:133},{level:2,title:"分布式的演进过程",slug:"分布式的演进过程",normalizedTitle:"分布式的演进过程",charIndex:206},{level:2,title:"IDEA项目",slug:"idea项目",normalizedTitle:"idea项目",charIndex:460},{level:2,title:"Dubbo-admin",slug:"dubbo-admin",normalizedTitle:"dubbo-admin",charIndex:2941},{level:2,title:"序列化",slug:"序列化",normalizedTitle:"序列化",charIndex:3189},{level:2,title:"地址缓存",slug:"地址缓存",normalizedTitle:"地址缓存",charIndex:3301},{level:2,title:"超时",slug:"超时",normalizedTitle:"超时",charIndex:3358},{level:2,title:"重试",slug:"重试",normalizedTitle:"重试",charIndex:3461},{level:2,title:"多版本",slug:"多版本",normalizedTitle:"多版本",charIndex:3584},{level:2,title:"负载均衡",slug:"负载均衡",normalizedTitle:"负载均衡",charIndex:3770},{level:2,title:"集群容错",slug:"集群容错",normalizedTitle:"集群容错",charIndex:4001},{level:2,title:"服务降级",slug:"服务降级",normalizedTitle:"服务降级",charIndex:4379},{level:2,title:"添加事务",slug:"添加事务",normalizedTitle:"添加事务",charIndex:4546}],headersStr:"集群和分布式 分布式的演进过程 IDEA项目 Dubbo-admin 序列化 地址缓存 超时 重试 多版本 负载均衡 集群容错 服务降级 添加事务",content:'# Dubbo\n\nDubbo 是阿里巴巴开源的一个高性能 轻量级的Java RPC 框架\n\nhttps://dubbo.apache.org/zh/ 下载\n\nhttps://dubbo.apache.org/zh/docs/introduction/ 文档\n\n\n# 集群和分布式\n\n * 集群 一个业务模块,部署在多台服务器上\n * 分布式 一个大的业务系统,拆分为小的业务模块,分别部署在不同的机器上\n\n\n# 分布式的演进过程\n\n * 垂直架构 将多个模块拆分为多个独立项目 形成多个独立的单体架构 存在重复性功能太多\n * 分布式架构 在垂直架构基础上,将公共业务模块抽取出来,作为独立的服务 以实现服务的共享和重用 存在一旦产生变更 所有消费方都需要变更\n * SOA架构 面向服务架构 是一个组件模型 它将应用程序的不同服务进行拆分 通过ESB(企业服务总线) 服务中介 来进行服务之间的交互\n * 微服务架构 在SOA架构上做的升华 将原有的单个业务拆分为多个可以独立开发 设计 运行的小应用\n\n\n\n\n\n\n# IDEA项目\n\n坐标\n\n \x3c!--Dubbo的起步依赖，版本2.7之后统一为rg.apache.dubb --\x3e\n        <dependency>\n            <groupId>org.apache.dubbo</groupId>\n            <artifactId>dubbo</artifactId>\n            <version>2.7.4.1</version>\n        </dependency>\n        \x3c!--ZooKeeper客户端实现 --\x3e\n        <dependency>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-framework</artifactId>\n            <version>4.0.0</version>\n        </dependency>\n        \x3c!--ZooKeeper客户端实现 --\x3e\n        <dependency>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-recipes</artifactId>\n            <version>4.0.0</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\napplicationContext.xml 约束定义\n\nxmlns:dubbo="http://dubbo.apache.org/schema/dubbo"\n\nhttp://dubbo.apache.org/schema/dubbo\n\nhttp://dubbo.apache.org/schema/dubbo/dubbo.xsd\n\n<?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n\t   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n\t   xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xmlns:context="http://www.springframework.org/schema/context"\n\t   xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd">\n\n\n\t\x3c!--<context:component-scan base-package="com.itheima.service" />--\x3e\n\n\t\x3c!--dubbo的配置--\x3e\n\t\x3c!--1.配置项目的名称,唯一--\x3e\n\t<dubbo:application name="dubbo-service"/>\n\t\x3c!--2.配置注册中心的地址--\x3e\n\t<dubbo:registry address="zookeeper://192.168.149.135:2181"/>\n\t\x3c!--3.配置dubbo包扫描--\x3e\n\t<dubbo:annotation package="com.itheima.service.impl" />\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n将bean绑定换成dubbo包下的\n\n//@Service//将该类的对象创建出来，放到Spring的IOC容器中  bean定义\n\n@Service//将这个类提供的方法（服务）对外发布。将访问的地址 ip，端口，路径注册到注册中心中\npublic class UserServiceImpl implements UserService {\n\n    public String sayHello() {\n        return "hello dubbo hello!~";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n@Autowired注入换成远程注入@Reference\n\n //注入Service\n    //@Autowired//本地注入\n\n    /*\n        1. 从zookeeper注册中心获取userService的访问url\n        2. 进行远程调用RPC\n        3. 将结果封装为一个代理对象。给变量赋值\n\n     */\n\n    @Reference//远程注入\n    private UserService userService;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n远程注入需要提供一个接口\n\n 1. 要么在当前工程下 创建此接口\n 2. 要么创建一个公共工程 专门存放远程注入接口的 让所有的工程都继承于公共工程\n\n\n# Dubbo-admin\n\nhttps://github.com/apache/dubbo-admin\n\n下载发布版\n\n * 项目目录下 命令行 mvn clean package 打包完成后在dubbo-admin-server的target有jar包\n * 在jar包目录下 java -jar .\\dubbo-admin-server-0.1.jar\n * 在 dubbo-admin-ui 先 npm install 或者 cnpm install 然后再 npm run dev\n\n\n# 序列化\n\n消费者和生产者 之间 传输对象 需要用到序列化和反序列化通过流来传输 实现序列化接口 Serializable\n\n但Dubbo 帮我们整合了 我们只需将全部pojp 都实现接口Serializable既可\n\n\n# 地址缓存\n\ndubbo服务消费者在第一次调用时,会将服务提供方地址缓存到本地,以后调用则不会访问注册中心\n\n\n# 超时\n\n在@Service或者@Reference中 当两个注解同时拥有timeout属性 则@Reference权重较高\n\n设置timeout属性 值为毫秒数 默认为1000毫秒 超时则释放线程\n\n\n# 重试\n\n设置了超时时间 在指定时间内 无法完成服务访问 则自动断开连接\n\n如果出现网络抖动 则这一次请求就会失败\n\n与超时设置一致\n\n通过设置 retries 属性来设置重试次数 默认为2 即当第一次超时后 重试2次如还是失败则断开连接\n\n\n# 多版本\n\n灰度发布: 当出现新的功能时,会让一部分用户先使用新功能,用户反馈没问题时,再将所有用户迁移到新功能\n\ndubbo 使用 version 属性 设置版本\n\n在@Service 中设置 version属性 @Service(version="v2.0")\n\n然后在@Reference中设置指定版本号 @Reference(version="v1.0")\n\n\n# 负载均衡\n\n通过@Reference中设置属性 loadbalance属性 @Reference(loadbalance ="random")\n\n * Random 按权重随机,默认值 按权重设置随机概率 权重通过@Service(weight = 100)设置\n * RoundRobin 按权重轮询\n * LeastActiove 最少活跃调用数,相同活跃的随机\n * ConsistentHash 一致性Hash 相同参数的请求总是发到同一提供者\n\n\n# 集群容错\n\n通过@Reference中设置属性 culster属性 @Reference(cluster = "Failover Cluster" )\n\n * Failover Cluster: 失败重试. ,默认值 当出现失败 重试其他服务器 默认重试2此 使用retries配置 一般用于读操作\n\n * Failfast Cluster 快速失败 只发起一次调用 失败立即报错 通常用于写操作\n\n * Falisafe Cluster 失败安全 出现异常时 直接忽略 返回一个空结果\n\n * Failback Cluster 失败自动恢复 后台记录失败请求 定时重复\n\n * Foriking Cluster 并行调用多个服务器 只要一个成功即返回\n\n * Broadcast Cluster 广播调用所有提供者,逐个调用,任何一台报错则报错\n\n\n# 服务降级\n\n通过@Reference中设置属性 mock属性 @Reference(mock = "" )\n\n * force:return null 表示消费者对该服务的方法调用都直接返回 null 不发起远程调用\n * fail:return null 表示消费者对该服务的方法调用在失败后 再返回null 值 不抛异常\n\n\n# 添加事务\n\n如果类添加了事务注解 则@Service需要指定实现的是哪个接口\n\n@Service(interfaceClass = CheckItemService.class)\n@Transactional\npublic class CheckItemServiceImpl implements CheckItemService {\n}\n\n\n1\n2\n3\n4\n',normalizedContent:'# dubbo\n\ndubbo 是阿里巴巴开源的一个高性能 轻量级的java rpc 框架\n\nhttps://dubbo.apache.org/zh/ 下载\n\nhttps://dubbo.apache.org/zh/docs/introduction/ 文档\n\n\n# 集群和分布式\n\n * 集群 一个业务模块,部署在多台服务器上\n * 分布式 一个大的业务系统,拆分为小的业务模块,分别部署在不同的机器上\n\n\n# 分布式的演进过程\n\n * 垂直架构 将多个模块拆分为多个独立项目 形成多个独立的单体架构 存在重复性功能太多\n * 分布式架构 在垂直架构基础上,将公共业务模块抽取出来,作为独立的服务 以实现服务的共享和重用 存在一旦产生变更 所有消费方都需要变更\n * soa架构 面向服务架构 是一个组件模型 它将应用程序的不同服务进行拆分 通过esb(企业服务总线) 服务中介 来进行服务之间的交互\n * 微服务架构 在soa架构上做的升华 将原有的单个业务拆分为多个可以独立开发 设计 运行的小应用\n\n\n\n\n\n\n# idea项目\n\n坐标\n\n \x3c!--dubbo的起步依赖，版本2.7之后统一为rg.apache.dubb --\x3e\n        <dependency>\n            <groupid>org.apache.dubbo</groupid>\n            <artifactid>dubbo</artifactid>\n            <version>2.7.4.1</version>\n        </dependency>\n        \x3c!--zookeeper客户端实现 --\x3e\n        <dependency>\n            <groupid>org.apache.curator</groupid>\n            <artifactid>curator-framework</artifactid>\n            <version>4.0.0</version>\n        </dependency>\n        \x3c!--zookeeper客户端实现 --\x3e\n        <dependency>\n            <groupid>org.apache.curator</groupid>\n            <artifactid>curator-recipes</artifactid>\n            <version>4.0.0</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\napplicationcontext.xml 约束定义\n\nxmlns:dubbo="http://dubbo.apache.org/schema/dubbo"\n\nhttp://dubbo.apache.org/schema/dubbo\n\nhttp://dubbo.apache.org/schema/dubbo/dubbo.xsd\n\n<?xml version="1.0" encoding="utf-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n\t   xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n\t   xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xmlns:context="http://www.springframework.org/schema/context"\n\t   xsi:schemalocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd">\n\n\n\t\x3c!--<context:component-scan base-package="com.itheima.service" />--\x3e\n\n\t\x3c!--dubbo的配置--\x3e\n\t\x3c!--1.配置项目的名称,唯一--\x3e\n\t<dubbo:application name="dubbo-service"/>\n\t\x3c!--2.配置注册中心的地址--\x3e\n\t<dubbo:registry address="zookeeper://192.168.149.135:2181"/>\n\t\x3c!--3.配置dubbo包扫描--\x3e\n\t<dubbo:annotation package="com.itheima.service.impl" />\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n将bean绑定换成dubbo包下的\n\n//@service//将该类的对象创建出来，放到spring的ioc容器中  bean定义\n\n@service//将这个类提供的方法（服务）对外发布。将访问的地址 ip，端口，路径注册到注册中心中\npublic class userserviceimpl implements userservice {\n\n    public string sayhello() {\n        return "hello dubbo hello!~";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n@autowired注入换成远程注入@reference\n\n //注入service\n    //@autowired//本地注入\n\n    /*\n        1. 从zookeeper注册中心获取userservice的访问url\n        2. 进行远程调用rpc\n        3. 将结果封装为一个代理对象。给变量赋值\n\n     */\n\n    @reference//远程注入\n    private userservice userservice;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n远程注入需要提供一个接口\n\n 1. 要么在当前工程下 创建此接口\n 2. 要么创建一个公共工程 专门存放远程注入接口的 让所有的工程都继承于公共工程\n\n\n# dubbo-admin\n\nhttps://github.com/apache/dubbo-admin\n\n下载发布版\n\n * 项目目录下 命令行 mvn clean package 打包完成后在dubbo-admin-server的target有jar包\n * 在jar包目录下 java -jar .\\dubbo-admin-server-0.1.jar\n * 在 dubbo-admin-ui 先 npm install 或者 cnpm install 然后再 npm run dev\n\n\n# 序列化\n\n消费者和生产者 之间 传输对象 需要用到序列化和反序列化通过流来传输 实现序列化接口 serializable\n\n但dubbo 帮我们整合了 我们只需将全部pojp 都实现接口serializable既可\n\n\n# 地址缓存\n\ndubbo服务消费者在第一次调用时,会将服务提供方地址缓存到本地,以后调用则不会访问注册中心\n\n\n# 超时\n\n在@service或者@reference中 当两个注解同时拥有timeout属性 则@reference权重较高\n\n设置timeout属性 值为毫秒数 默认为1000毫秒 超时则释放线程\n\n\n# 重试\n\n设置了超时时间 在指定时间内 无法完成服务访问 则自动断开连接\n\n如果出现网络抖动 则这一次请求就会失败\n\n与超时设置一致\n\n通过设置 retries 属性来设置重试次数 默认为2 即当第一次超时后 重试2次如还是失败则断开连接\n\n\n# 多版本\n\n灰度发布: 当出现新的功能时,会让一部分用户先使用新功能,用户反馈没问题时,再将所有用户迁移到新功能\n\ndubbo 使用 version 属性 设置版本\n\n在@service 中设置 version属性 @service(version="v2.0")\n\n然后在@reference中设置指定版本号 @reference(version="v1.0")\n\n\n# 负载均衡\n\n通过@reference中设置属性 loadbalance属性 @reference(loadbalance ="random")\n\n * random 按权重随机,默认值 按权重设置随机概率 权重通过@service(weight = 100)设置\n * roundrobin 按权重轮询\n * leastactiove 最少活跃调用数,相同活跃的随机\n * consistenthash 一致性hash 相同参数的请求总是发到同一提供者\n\n\n# 集群容错\n\n通过@reference中设置属性 culster属性 @reference(cluster = "failover cluster" )\n\n * failover cluster: 失败重试. ,默认值 当出现失败 重试其他服务器 默认重试2此 使用retries配置 一般用于读操作\n\n * failfast cluster 快速失败 只发起一次调用 失败立即报错 通常用于写操作\n\n * falisafe cluster 失败安全 出现异常时 直接忽略 返回一个空结果\n\n * failback cluster 失败自动恢复 后台记录失败请求 定时重复\n\n * foriking cluster 并行调用多个服务器 只要一个成功即返回\n\n * broadcast cluster 广播调用所有提供者,逐个调用,任何一台报错则报错\n\n\n# 服务降级\n\n通过@reference中设置属性 mock属性 @reference(mock = "" )\n\n * force:return null 表示消费者对该服务的方法调用都直接返回 null 不发起远程调用\n * fail:return null 表示消费者对该服务的方法调用在失败后 再返回null 值 不抛异常\n\n\n# 添加事务\n\n如果类添加了事务注解 则@service需要指定实现的是哪个接口\n\n@service(interfaceclass = checkitemservice.class)\n@transactional\npublic class checkitemserviceimpl implements checkitemservice {\n}\n\n\n1\n2\n3\n4\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Spring MVC",frontmatter:{title:"Spring MVC",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/8cd1ce/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/08.Spring%20MVC.html",relativePath:"后端/02.JavaEE/08.Spring MVC.md",key:"v-14c62766",path:"/pages/8cd1ce/",headers:[{level:2,title:"三层架构",slug:"三层架构",normalizedTitle:"三层架构",charIndex:17},{level:2,title:"MVC",slug:"mvc",normalizedTitle:"mvc",charIndex:9},{level:2,title:"项目构建 xml",slug:"项目构建-xml",normalizedTitle:"项目构建 xml",charIndex:210},{level:2,title:"技术架构图",slug:"技术架构图",normalizedTitle:"技术架构图",charIndex:3842},{level:2,title:"注解驱动",slug:"注解驱动",normalizedTitle:"注解驱动",charIndex:3854},{level:3,title:"扫描包含Controller注解的类",slug:"扫描包含controller注解的类",normalizedTitle:"扫描包含controller注解的类",charIndex:3863},{level:3,title:"注解配置指定放行的资源",slug:"注解配置指定放行的资源",normalizedTitle:"注解配置指定放行的资源",charIndex:4056},{level:3,title:"注解配置放行所有静态资源",slug:"注解配置放行所有静态资源",normalizedTitle:"注解配置放行所有静态资源",charIndex:4555},{level:3,title:"servlet扫描mvc配置文件",slug:"servlet扫描mvc配置文件",normalizedTitle:"servlet扫描mvc配置文件",charIndex:4854},{level:2,title:"请求",slug:"请求",normalizedTitle:"请求",charIndex:6924},{level:3,title:"请求参数",slug:"请求参数",normalizedTitle:"请求参数",charIndex:6931},{level:3,title:"类型转换器",slug:"类型转换器",normalizedTitle:"类型转换器",charIndex:10328},{level:3,title:"请求映射",slug:"请求映射",normalizedTitle:"请求映射",charIndex:12471},{level:2,title:"响应",slug:"响应",normalizedTitle:"响应",charIndex:12717},{level:3,title:"页面跳转方式",slug:"页面跳转方式",normalizedTitle:"页面跳转方式",charIndex:12735},{level:3,title:"request传递数据",slug:"request传递数据",normalizedTitle:"request传递数据",charIndex:13494},{level:3,title:"Model 类型形参进行数据传递",slug:"model-类型形参进行数据传递",normalizedTitle:"model 类型形参进行数据传递",charIndex:13691},{level:3,title:"ModelAndView 类型传递",slug:"modelandview-类型传递",normalizedTitle:"modelandview 类型传递",charIndex:14021},{level:3,title:"返回JSON数据",slug:"返回json数据",normalizedTitle:"返回json数据",charIndex:14915},{level:2,title:"Servlet相关接口",slug:"servlet相关接口",normalizedTitle:"servlet相关接口",charIndex:15785},{level:3,title:"Head数据获取",slug:"head数据获取",normalizedTitle:"head数据获取",charIndex:15856},{level:3,title:"Cookie 数据获取",slug:"cookie-数据获取",normalizedTitle:"cookie 数据获取",charIndex:16127},{level:3,title:"Session 数据获取和设置",slug:"session-数据获取和设置",normalizedTitle:"session 数据获取和设置",charIndex:16402},{level:2,title:"发送异步请求",slug:"发送异步请求",normalizedTitle:"发送异步请求",charIndex:16735},{level:2,title:"异步请求响应",slug:"异步请求响应",normalizedTitle:"异步请求响应",charIndex:17463},{level:2,title:"跨域访问",slug:"跨域访问",normalizedTitle:"跨域访问",charIndex:18586},{level:2,title:"拦截器",slug:"拦截器",normalizedTitle:"拦截器",charIndex:18815},{level:3,title:"拦截器参数",slug:"拦截器参数",normalizedTitle:"拦截器参数",charIndex:19946},{level:3,title:"多个拦截器",slug:"多个拦截器",normalizedTitle:"多个拦截器",charIndex:18910},{level:3,title:"责任链模式",slug:"责任链模式",normalizedTitle:"责任链模式",charIndex:20164},{level:2,title:"异常处理",slug:"异常处理",normalizedTitle:"异常处理",charIndex:20176},{level:3,title:"注解版异常处理",slug:"注解版异常处理",normalizedTitle:"注解版异常处理",charIndex:21110},{level:2,title:"文件上传下载",slug:"文件上传下载",normalizedTitle:"文件上传下载",charIndex:21975},{level:2,title:"Restful",slug:"restful",normalizedTitle:"restful",charIndex:22206},{level:3,title:"PathVariable",slug:"pathvariable",normalizedTitle:"pathvariable",charIndex:22222},{level:3,title:"RestController",slug:"restcontroller",normalizedTitle:"restcontroller",charIndex:22521},{level:3,title:"XXXMapping",slug:"xxxmapping",normalizedTitle:"xxxmapping",charIndex:22624},{level:2,title:"表单验证框架",slug:"表单验证框架",normalizedTitle:"表单验证框架",charIndex:22898},{level:3,title:"分组校验",slug:"分组校验",normalizedTitle:"分组校验",charIndex:24755},{level:2,title:"SSM",slug:"ssm",normalizedTitle:"ssm",charIndex:25680}],headersStr:"三层架构 MVC 项目构建 xml 技术架构图 注解驱动 扫描包含Controller注解的类 注解配置指定放行的资源 注解配置放行所有静态资源 servlet扫描mvc配置文件 请求 请求参数 类型转换器 请求映射 响应 页面跳转方式 request传递数据 Model 类型形参进行数据传递 ModelAndView 类型传递 返回JSON数据 Servlet相关接口 Head数据获取 Cookie 数据获取 Session 数据获取和设置 发送异步请求 异步请求响应 跨域访问 拦截器 拦截器参数 多个拦截器 责任链模式 异常处理 注解版异常处理 文件上传下载 Restful PathVariable RestController XXXMapping 表单验证框架 分组校验 SSM",content:'# Spring MVC\n\n\n# 三层架构\n\n\n\n\n# MVC\n\nMVC(Model View Controller) 设计创建Web应用程序表现层的模式\n\n * Model 模型 : 数据模型,用于封装数据\n * View 视图 : 页面视图,用于展示数据\n   * jsp\n   * html\n * Contoller 控制器 :处理用户交互的调度器,用于根据用户需求处理程序逻辑\n   * Servlet\n\n\n# 项目构建 xml\n\n导入坐标\n\n         \x3c!-- servlet3.1规范的坐标 --\x3e\n      <dependency>\n        <groupId>javax.servlet</groupId>\n        <artifactId>javax.servlet-api</artifactId>\n        <version>3.1.0</version>\n        <scope>provided</scope>\n      </dependency>\n      \x3c!--jsp坐标--\x3e\n      <dependency>\n        <groupId>javax.servlet.jsp</groupId>\n        <artifactId>jsp-api</artifactId>\n        <version>2.1</version>\n        <scope>provided</scope>\n      </dependency> \n\t\t\x3c!--spring web的坐标--\x3e\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-web</artifactId>\n            <version>5.1.9.RELEASE</version>\n        </dependency>\n        \x3c!--springmvc的坐标--\x3e\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-webmvc</artifactId>\n            <version>5.1.9.RELEASE</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\nspring-mvc.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:mvc="http://www.springframework.org/schema/mvc"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\n        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd">\n\n    <context:component-scan base-package="com.itheima">\n        \x3c!-- 包含以下注解则加载 --\x3e\n        <context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/>\n    </context:component-scan>\n\n\n    \x3c!--放行指定类型静态资源配置方式--\x3e\n\x3c!--    <mvc:resources mapping="/img/**" location="/img/"/>--\x3e\n\x3c!--    <mvc:resources mapping="/js/**" location="/js/"/>--\x3e\n\x3c!--    <mvc:resources mapping="/css/**" location="/css/"/>--\x3e\n\n    \x3c!--SpringMVC提供的通用资源放行方式 释放所有静态资源--\x3e\n    <mvc:default-servlet-handler/>\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nweb.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd"\n         version="3.1">\n\n    \x3c!--乱码处理过滤器，与Servlet中使用的完全相同，差异之处在于处理器的类由Spring提供--\x3e\n    <filter>\n        <filter-name>CharacterEncodingFilter</filter-name>\n        <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>\n        <init-param>\n            <param-name>encoding</param-name>\n            <param-value>UTF-8</param-value>\n        </init-param>\n    </filter>\n    <filter-mapping>\n        <filter-name>CharacterEncodingFilter</filter-name>\n        <url-pattern>/*</url-pattern>\n    </filter-mapping>\n\t\x3c!-- 扫描mvc配置文件 --\x3e\n    <servlet>\n        <servlet-name>DispatcherServlet</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            <param-value>classpath*:spring-mvc.xml</param-value>\n        </init-param>\n    </servlet>\n    <servlet-mapping>\n        <servlet-name>DispatcherServlet</servlet-name>\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 技术架构图\n\n\n\n\n# 注解驱动\n\n\n# 扫描包含Controller注解的类\n\n在springmvc 配置类中\n\n@ComponentScan(value = "com.itheima",includeFilters =\n    @ComponentScan.Filter(type=FilterType.ANNOTATION,classes = {Controller.class})\n    )\n\n\n1\n2\n3\n\n\n\n# 注解配置指定放行的资源\n\n实现WebMvcConfigurer接口 重写addResourceHandlers方法\n\npublic class SpringMVCConfiguration implements WebMvcConfigurer{\n    //注解配置放行指定资源格式\n    @Override\n    public void addResourceHandlers(ResourceHandlerRegistry registry) {\n        \t\t\t  registry.addResourceHandler("/img/**").addResourceLocations("/img/");\n        registry.addResourceHandler("/js/**").addResourceLocations("/js/");\n        registry.addResourceHandler("/css/**").addResourceLocations("/css/");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 注解配置放行所有静态资源\n\n实现WebMvcConfigurer接口 重写configureDefaultServletHandling方法\n\npublic class SpringMVCConfiguration implements WebMvcConfigurer{\n\t@Override\n    public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) {\n        configurer.enable();;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# servlet扫描mvc配置文件\n\n继承 AbstractDispatcherServletInitializer 类 重写里面的 createServletApplicationContext createRootApplicationContext 和 getServletMappings 方法\n\nimport org.springframework.web.context.WebApplicationContext;\nimport org.springframework.web.context.support.AnnotationConfigWebApplicationContext;\nimport org.springframework.web.filter.CharacterEncodingFilter;\nimport org.springframework.web.servlet.support.AbstractDispatcherServletInitializer;\n\nimport javax.servlet.DispatcherType;\nimport javax.servlet.FilterRegistration;\nimport javax.servlet.ServletContext;\nimport javax.servlet.ServletException;\nimport java.util.EnumSet;\n\npublic class ServletContainersInitConfig extends AbstractDispatcherServletInitializer {\n    //创建Servlet容器时，使用注解的方式加载SPRINGMVC配置类中的信息，并加载成WEB专用的ApplicationContext对象\n    //该对象放入了ServletContext范围，后期在整个WEB容器中可以随时获取调用\n    @Override\n    protected WebApplicationContext createServletApplicationContext() {\n        AnnotationConfigWebApplicationContext ctx = new AnnotationConfigWebApplicationContext();\n        //需要一个MVC配置类\n        ctx.register(SpringMVCConfiguration.class);\n        return ctx;\n    }\n\n    //注解配置映射地址方式，服务于SpringMVC的核心控制器DispatcherServlet\n    @Override\n    protected String[] getServletMappings() {\n        return new String[]{"/"};\n    }\n\n    @Override\n    protected WebApplicationContext createRootApplicationContext() {\n        return null;\n    }\n\n    //乱码处理作为过滤器，在servlet容器启动时进行配置，相关内容参看Servlet零配置相关课程\n    @Override\n    public void onStartup(ServletContext servletContext) throws ServletException {\n        super.onStartup(servletContext);\n        CharacterEncodingFilter cef = new CharacterEncodingFilter();\n        cef.setEncoding("UTF-8");\n        FilterRegistration.Dynamic registration = servletContext.addFilter("characterEncodingFilter", cef);\n        registration.addMappingForUrlPatterns(EnumSet.of(DispatcherType.REQUEST,DispatcherType.FORWARD,DispatcherType.INCLUDE),false,"/*");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 请求\n\n\n# 请求参数\n\nSpringMVC将传递的参数封装到处理器方法的形参中,达到快速访问参数的目的\n\n只需在形参上与请求地址上的参数名字一致 即可获取到请求参数的值\n\n    //请求参数  http://localhost/requestParam1?name=hello\n    @RequestMapping("/requestParam1")\n    public String requestParam1(String name){\n        System.out.println(name);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 普通类型\n\n参数名与处理器方法形参名保持一致 否则无法获取对应的值\n\n如果想要绑定具体的请求名 则需要使用 @RequestParam\n\n * @RequestParam\n   * value 请求属性名\n   * required 不允许为空 默认为true\n   * defaultValue 为空时默认值\n\n    //请求参数形参绑定名  http://localhost/requestParam2?username=hello&age=10\n    @RequestMapping("/requestParam2")\n    public String requestParam2(@RequestParam(value = "username",required = true,defaultValue = "zhangsan") String name, int age){\n        System.out.println(name);\n        System.out.println(age);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# POJO类型\n\n如果形参为一个对象 则请求参数会将值 一一对应为对象中属性值\n\n    //POJO类型参数  http://localhost/requestParam3?username=zhangsan&age=10\n    @RequestMapping("/requestParam3")\n    public String requestParam3(User user){\n        System.out.println(user.toString());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# POJO类型与普通类型同时存在\n\n将会被同时赋值 如果想进行区分 建议使用@RequestParam进行绑定\n\n    //POJO类型参数与普通类型同时存在  http://localhost/requestParam4?name=zhangsan&age=10\n    @RequestMapping("/requestParam4")\n    public String requestParam4(User user,String age){\n        System.out.println(user.toString());\n        System.out.println(age);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 复杂POJO类型 嵌套\n\n如果POJO中嵌套POJO则 需要请求地址 需要按照层次结构要书写\n\n    //复杂POJO类型  http://localhost/requestParam5?address.city=shanghai\n    @RequestMapping("/requestParam5")\n    public String requestParam5(User user){\n        System.out.println(user.getAddress().getCity());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 复杂POJO类型 集合\n\n如果POJO中出现集合 保存简单数据 使用多个相同名称的参数为其进行赋值\n\n\n    //复杂POJO类型 集合  http://localhost/requestParam6?nick=zhangsan&nick=lisi&nick=wangwu\n    @RequestMapping("/requestParam6")\n    public String requestParam6(User user){\n        System.out.println(user);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 复杂POJO类型 集合对象\n\n集合对象 以索引形式的请求地址\n\n    //复杂POJO类型 集合对象  http://localhost/requestParam7?addresses[0].province=bj&addresses[1].province=gd\n    @RequestMapping("/requestParam7")\n    public String requestParam7(User user){\n        System.out.println(user.getAddresses());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 复杂POJO类型 Map集合\n\nMap集合 在请求地址中以 key的形式赋值\n\n    //复杂POJO类型 集合对象  http://localhost/requestParam8?addressMap[\'home\'].province=bj&addressMap[\'job\'].province=gd\n    @RequestMapping("/requestParam8")\n    public String requestParam8(User user){\n        System.out.println(user.getAddressMap());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 数组类型\n\n请求参数名与数组名一致 并请求参数数量大于1个\n\n    //数组类型  http://localhost/requestParam9?nick=abc&nick=def\n    @RequestMapping("/requestParam9")\n    public String requestParam9(String[] nick) {\n        System.out.println(nick[0] + " " + nick[1]);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 集合类型\n\nMVC默认将list作为对象处理 赋值前先创建对象 然后将nick作为对象的属性赋值 但list是接口 无法创建对象 和 有此属性值 所以报错\n\n我们通过@RequestParam 将请求参数打包成数组\n\n    //集合类型  http://localhost/requestParam10?nick=abc&nick=def\n    @RequestMapping("/requestParam10")\n    public String requestParam10(@RequestParam("nick") List<String> nick) {\n        System.out.println(nick);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 类型转换器\n\nMVC对接受的数据进行自动类型转换 通过Converter接口实现\n\n# xml 日期类型格式转换\n\nspring默认的日期格式为 2021/09/11 如果我们传递为2021-09-11则会报错 我们需要自定义日期格式转换\n\n    \x3c!--    自定义转换格式--\x3e\n    <mvc:annotation-driven conversion-service="conversionService"/>\n    \x3c!--    注册bean 让spring管理--\x3e\n    <bean id="conversionService" class="org.springframework.format.support.FormattingConversionServiceFactoryBean">\n        <property name="formatters">\n            <set>\n                <bean class="org.springframework.format.datetime.DateFormatter">\n                    <property name="pattern" value="yyyy-MM-dd"/>\n                </bean>\n            </set>\n        </property>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 注解版 日期类型格式转换\n\n    //日期类型 自定义格式转换  http://localhost/requestParam11?date=2021-09-11\n    @RequestMapping("/requestParam11")\n    public String requestParam11(@DateTimeFormat(pattern = "yyyy-MM-dd") Date date) {\n        System.out.println(date);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n并开启mvc注解驱动\n\n<mvc:annotation-driven />\n\n\n1\n\n\n# 自定义类型转换器\n\n实现 Converter 接口 泛型1为原始数据类型 泛型2为返回的数据类型 并实现 convert 方法\n\n    \x3c!--    自定义类型转换器--\x3e\n    <mvc:annotation-driven conversion-service="conversionService"/>\n    \x3c!--    注册bean 让spring管理--\x3e\n    <bean id="conversionService" class="org.springframework.context.support.ConversionServiceFactoryBean">\n        <property name="converters">\n            <set>\n                <bean class="com.itheima.converter.MyDateConverter">\n                </bean>\n            </set>\n        </property>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n处理类\n\n\nimport org.springframework.core.convert.converter.Converter;\n\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\npublic class MyDateConverter implements Converter<String, Date> {\n    @Override\n    public Date convert(String s) {\n        SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd");\n        Date date = null;\n        try {\n            date = simpleDateFormat.parse(s);\n        } catch (ParseException e) {\n            e.printStackTrace();\n        }\n\n        return date;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 请求映射\n\n * @RequestMapping 设置在方法上则是方法体的请求映射 定义在类上则是整个类的访问前缀\n * value 请求路径\n * method = RequestMethod.GET 请求方式\n * params = "name" 请求地址必须传递此属性才能访问\n * headers = "content-type=text/*" 请求头条件\n * consumes = "text/*" 可以接受的请求正文类型\n * produces = "text/*" 可以生成的响应正文类型\n\n\n# 响应\n\n\n# 页面跳转方式\n\n * 转发(默认)\n   \n   @RequestMapping("/showPageAndData1")\n       public String showPageAndData1(HttpServletRequest request){\n           return "forward:success.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 重定向\n   \n       @RequestMapping("/showPageAndData7")\n       public String showPageAndData7(){\n           return "redirect:index.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 页面访问快捷设定 设置了快捷访问 后默认访问是设定的路径下制定的后缀文件 默认为转发跳转 无法重定向\n   \n       <bean class="org.springframework.web.servlet.view.InternalResourceViewResolver">\n           <property name="prefix" value="/WEB-INF/page/"/>\n           <property name="suffix" value=".jsp"/>\n       </bean>\n   \n   \n   1\n   2\n   3\n   4\n   \n   \n   如果方法体没有返回值 也配置了快捷方式 则自动跳转到指定的路径下 访问路径 + 后缀 的文件\n   \n   \n\n\n# request传递数据\n\n@RequestMapping("/showPageAndData1")\npublic String showPageAndData1(HttpServletRequest request){\n    request.setAttribute("name","hello");\n    return "success.jsp";\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# Model 类型形参进行数据传递\n\n    @RequestMapping("/showPageAndData2")\n    public String showPageAndData2(Model model){\n        model.addAttribute("name","hello");\n        User user =new User();\n        user.setAge(156);\n        //可以传递对象\n        model.addAttribute("user",user);  \n        return "success.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# ModelAndView 类型传递\n\n    @RequestMapping("/showPageAndData3")\n    public ModelAndView showPageAndData3(ModelAndView modelAndView){\n        modelAndView.addObject("name","hello");\n        User user =new User();\n        user.setAge(156);\n        //添加属性\n        modelAndView.addObject("user",user);\n        //跳转页面\n        modelAndView.setViewName("success.jsp");\n        return modelAndView;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 重定向\n\n    @RequestMapping("/showPageAndData5")\n    public ModelAndView showPageAndData5(ModelAndView modelAndView){\n        //重定向\n        modelAndView.setViewName("redirect:index.jsp");\n        return modelAndView;\n    }\n\n    @RequestMapping("/showPageAndData6")\n    public ModelAndView showPageAndData6(ModelAndView modelAndView){\n        //跳转\n        modelAndView.setViewName("forward:index.jsp");\n        return modelAndView;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 返回JSON数据\n\n响应方法体 返回的数据默认会直接跳转到 返回值对应的页面文件\n\n我们可以在方法体上 加上 @ResponseBody 注解 来声明为一个响应体\n\n    @RequestMapping("showData1")\n    @ResponseBody\n    public String showData1() throws JsonProcessingException {\n        User user = new User();\n        user.setAge(15);\n        user.setName("zhangsan");\n        ObjectMapper om = new ObjectMapper();\n        return om.writeValueAsString(user);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n * 返回一个对象或者集合 Jackson 已经帮我们做好了 自定义类型转换了 我们只需要在spring配置中开启注解驱动 即可\n   \n    <mvc:annotation-driven />\n   \n   \n   1\n   \n   \n   注解版 开启注解驱动 为 @EnableWebMvc\n   \n       @RequestMapping("showData2")\n       @ResponseBody\n       public User showData2() throws JsonProcessingException {\n           User user = new User();\n           user.setAge(15);\n           user.setName("zhangsan");\n           return user;\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   \n\n\n# Servlet相关接口\n\n原始的 request response 和 session 提供给我们使用但是不太推荐使用原生的功能\n\n\n\n\n# Head数据获取\n\n * @RequestHeader 获取头中指定的值\n   \n       @RequestMapping("/headApi")\n       public String headApi(@RequestHeader("Accept-Language") String head){\n           System.out.println(head);\n           return "index.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n\n# Cookie 数据获取\n\n * @CookieValue 获取cookie指定key的值\n   \n       @RequestMapping("/cookieApi")\n       public String cookieApi(@CookieValue("_xsrf") String cookie){\n           System.out.println(cookie);\n           return "index.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n\n# Session 数据获取和设置\n\n * 设置 只有在类注解上方标记的变量名称才会被放到session中\n   \n   * \n\n * 获取\n   \n   *     @RequestMapping("/sessionApi")\n         public String sessionApi(@SessionAttribute("name") String session){\n             System.out.println(session);\n             return "index.jsp";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# 发送异步请求\n\n通过ajax发送的异步请求 是无法直接被赋值给形参的 需要在形参前面 加上 @RequestBody\n\n    @RequestMapping("/ajax1")\n    public String ajax1(@RequestBody String data) {\n        System.out.println(data);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n\n\n * JSON转POJO 会自动赋值给POJO中属性\n   \n   *     @RequestMapping("/ajax2")\n         public String ajax2(@RequestBody User user) {\n             System.out.println(user);\n             return "index.jsp";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 集合\n   \n   *     @RequestMapping("/ajax3")\n         public String ajax3(@RequestBody List<User> user) {\n             System.out.println(user);\n             return "index.jsp";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# 异步请求响应\n\n * 返回字符串\n   \n   *     @RequestMapping("/ajax4")\n         @ResponseBody\n         public String ajax4() {\n             return "hello";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 返回对象\n   \n   *     @RequestMapping("/ajax5")\n         @ResponseBody\n         public User ajax5() {\n             User user = new User();\n             user.setName("hhh");\n             user.setAge(13);\n             return user;\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n * 返回集合\n   \n   *     @RequestMapping("/ajax6")\n         @ResponseBody\n         public List ajax6() {\n             User user = new User();\n             User user2 = new User();\n             user.setName("hhh");\n             user.setAge(13);\n             user2.setName("qqqq");\n             user2.setAge(17);\n             ArrayList<User> arrayList = new ArrayList<>();\n             arrayList.add(user);\n             arrayList.add(user2);\n             return arrayList;\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n\n# 跨域访问\n\n当通过A域名下的操作访问 域名B下的资源时 称为跨域访问\n\n我们可以通过设置头信息 开启跨域访问限制\n\n但spring 帮我完成这个操作 我们只需要添加@CrossOrigin 就可以解决 定义在方法体或类上\n\n    @RequestMapping("/cross")\n    @ResponseBody\n    @CrossOrigin\n    public void cross() {\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 拦截器\n\n拦截器(Interceptor) 是一种动态拦截方法调用的机制\n\n 1. 在指定的方法调用前后执行预先设定后的代码\n 2. 阻止原始方法的执行\n\n核心原理:AOP思想\n\n拦截器链:多个拦截器按照一定的顺序 对原始被调用功能进行增强\n\n实现 HandlerInterceptor 接口 重写需要的方法\n\npublic class MyInterceptor implements HandlerInterceptor {\n\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n        System.out.println("前置运行");\n        //如果为false 则直接拦截业务处理器不放行\n        return true;\n    }\n\n    @Override\n    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {\n        System.out.println("后置运行");\n    }\n\n    @Override\n    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {\n        System.out.println("完成运行");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n <mvc:interceptors>\n        <mvc:interceptor>\n            <mvc:mapping path="/handleRun"/>\n            <bean class="com.itheima.interceptor.MyInterceptor"/>\n        </mvc:interceptor>\n    </mvc:interceptors>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n# 拦截器参数\n\n * request 请求对象\n * response 响应对象\n * handler 被调用的处理器对象,对反射中的method对象进行了包装\n * ModelAndView 可以读取或者修改 页面信息和对应数据\n * Exception 如果处理器执行中出现异常 如何处理\n\n\n# 多个拦截器\n\n多个拦截器执行顺序与xml配置有关\n\n但多个拦截器中的前置运行 后置运行 都会同时启用\n\n按照链式 执行顺序\n\n\n\n\n# 责任链模式\n\n\n\n\n# 异常处理\n\n类注解 @Component 绑定Bean为 并实现 HandlerExceptionResolver 接口 实现方法\n\n此bean会拦截mvc上请求的异常\n\n@Component\npublic class ExceptionResolver implements HandlerExceptionResolver {\n    @Override\n    public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) {\n\n        System.out.println("发生异常了");\n\n        ModelAndView modelAndView =new ModelAndView();\n\n        if(e instanceof NullPointerException){\n            //添加错误信息\n            modelAndView.addObject("msg","空指针异常");\n        }else if (e instanceof ArithmeticException){\n            //添加错误信息\n            modelAndView.addObject("msg","算术异常");\n        }else {\n            //添加错误信息\n            modelAndView.addObject("msg","未知异常");\n        }\n\n        //转发页面\n        modelAndView.setViewName("error.jsp");\n        return modelAndView;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 注解版异常处理\n\n * @ExceptionHandler 注解 标记要捕抓的异常\n\n@Component\n@ControllerAdvice\npublic class ExceptionAdivce {\n\n    //异常 的类\n    @ExceptionHandler(NullPointerException.class)\n    @ResponseBody\n    public String doNullException(Exception e){\n        System.out.println("空指针异常");\n        return "空指针异常";\n    }\n\n    //异常 的类\n    @ExceptionHandler(ArithmeticException.class)\n    @ResponseBody\n    public String doArithmeticException(Exception e){\n        System.out.println("算术异常");\n        return "算术异常";\n    }\n\n    //异常 的类\n    @ExceptionHandler(Exception.class)\n    @ResponseBody\n    public String doException(Exception e){\n        System.out.println("all");\n        return "all";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n注意事项:\n\n@ExceptionHandler 注解 会比 HandlerExceptionResolver 运行早\n\nHandlerExceptionResolver 无法捕抓到参数异常 而注解可以捕抓到\n\n\n# 文件上传下载\n\n坐标\n\n  \x3c!--文件上传下载--\x3e\n    <dependency>\n      <groupId>commons-fileupload</groupId>\n      <artifactId>commons-fileupload</artifactId>\n      <version>1.4</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n上传文件传递的name 要与 形参的名称一致\n\n\n# Restful\n\n\n\n\n\n\n# PathVariable\n\n@PathVariable 为Restful 规范的请求路径 赋值给指定形参\n\n并且在@RequestMapping 的 method 指定请求方式\n\n    @RequestMapping(value = "/user/{id}",method = RequestMethod.GET)\n    public String restLocation(@PathVariable Integer id){\n        System.out.println(id);\n        return "page.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n\n\n\n# RestController\n\n@RestController 结合了 @Controller和@ResponseBody 两个注解的功能\n\n绑定为bean 并且 所有返回内容都不会解析为网页结构\n\n\n# XXXMapping\n\n如 @GetMapping @PostMapping 请求等等\n\n我们不用在@RequestMapping 中定义指定的请求方式 和 请求路径\n\n只需在方法体加上指定请求方式的注解即\n\n    @PostMapping("{id}")\n    public String postrestLocation(@PathVariable Integer id){\n        System.out.println(id);\n        return "page.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n\n\n\n# 表单验证框架\n\nHibernate框架\n\n    <dependency>\n      <groupId>org.hibernate</groupId>\n      <artifactId>hibernate-validator</artifactId>\n      <version>6.1.0.Final</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n\n * @Valid 形参注解 开启此形参的验证\n * @Valid 属性注解 开启此对象或属性验证 对象校验在对象内部属性定义规则\n * @NotBlank(message = "提醒信息") 属性验证规则 不能为null 并且长度必须大于0和不全为空格 则不能为空\n * @NotNull (message = "提醒信息") 不能为null 可以为空\n * @NotEmpty(message = "") 不能为空和null\n * @Max(value = "" , message =" ") 限定最大值\n * @Min(value = "" , message = "") 最小值\n * @Range(max = ,min = ,message = ) 最大最小值\n\n @RequestMapping(value = "/addemployee2")\n    //使用@Valid开启校验，使用@Validated也可以开启校验\n    //Errors对象用于封装校验结果，如果不满足校验规则，对应的校验结果封装到该对象中，包含校验的属性名和校验不通过返回的消息\n    public String addEmployee2(@Valid Employee employee, Errors errors, Model m){\n        //判定Errors对象中是否存在未通过校验的字段\n        if(errors.hasErrors()){\n            //获取所有未通过校验规则的信息\n            List<FieldError> fieldErrors = errors.getFieldErrors();\n            System.out.println(fieldErrors.size());\n            for(FieldError error : fieldErrors){\n                System.out.println(error.getField());\n                System.out.println(error.getDefaultMessage());\n                //将校验结果信息添加到Model对象中，用于页面显示，后期实际开发中无需这样设定，返回json数据即可\n                m.addAttribute(error.getField(),error.getDefaultMessage());\n            }\n            //当出现未通过校验的字段时，跳转页面到原始页面，进行数据回显\n            return "addemployee.jsp";\n        }\n        return "success.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n    @NotBlank(message = "姓名不能为空")\n    private String name;//员工姓名\n\n    //一个属性可以添加多个校验器\n    @NotNull(message = "请输入您的年龄")\n    @Max(value = 60,message = "年龄最大值不允许超过60岁")\n    @Min(value = 18,message = "年龄最小值不允许低于18岁")\n    private Integer age;//员工年龄\n\n    //实体类中的引用类型通过标注@Valid注解，设定开启当前引用类型字段中的属性参与校验\n    @Valid\n    private Address address;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 分组校验\n\n如果不开启分组校验 则会校验当前全部开启校验的属性/对象\n\n在属性/对象校验注解中 加上 groups 属性 并给予一个 用于标识的字节码文件\n\n//设定校验器，设置校验不通过对应的消息，设定所参与的校验组\n    @NotBlank(message = "姓名不能为空",groups = {GroupA.class})\n    private String name;//员工姓名\n\n\n1\n2\n3\n\n\n拦截校验器 开启校验注解要改为 @Validated 并加上标识字节码文件\n\n    @RequestMapping(value = "/addemployee")\n    public String addEmployee(@Validated({GroupA.class}) Employee employee, Errors errors, Model m){\n        if(errors.hasErrors()){\n            List<FieldError> fieldErrors = errors.getFieldErrors();\n            System.out.println(fieldErrors.size());\n            for(FieldError error : fieldErrors){\n                System.out.println(error.getField());\n                System.out.println(error.getDefaultMessage());\n                m.addAttribute(error.getField(),error.getDefaultMessage());\n            }\n            return "addemployee.jsp";\n        }\n        return "success.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# SSM\n\nSpring + SpringMVC + MyBatis',normalizedContent:'# spring mvc\n\n\n# 三层架构\n\n\n\n\n# mvc\n\nmvc(model view controller) 设计创建web应用程序表现层的模式\n\n * model 模型 : 数据模型,用于封装数据\n * view 视图 : 页面视图,用于展示数据\n   * jsp\n   * html\n * contoller 控制器 :处理用户交互的调度器,用于根据用户需求处理程序逻辑\n   * servlet\n\n\n# 项目构建 xml\n\n导入坐标\n\n         \x3c!-- servlet3.1规范的坐标 --\x3e\n      <dependency>\n        <groupid>javax.servlet</groupid>\n        <artifactid>javax.servlet-api</artifactid>\n        <version>3.1.0</version>\n        <scope>provided</scope>\n      </dependency>\n      \x3c!--jsp坐标--\x3e\n      <dependency>\n        <groupid>javax.servlet.jsp</groupid>\n        <artifactid>jsp-api</artifactid>\n        <version>2.1</version>\n        <scope>provided</scope>\n      </dependency> \n\t\t\x3c!--spring web的坐标--\x3e\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-web</artifactid>\n            <version>5.1.9.release</version>\n        </dependency>\n        \x3c!--springmvc的坐标--\x3e\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-webmvc</artifactid>\n            <version>5.1.9.release</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\nspring-mvc.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:mvc="http://www.springframework.org/schema/mvc"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xsi:schemalocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\n        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd">\n\n    <context:component-scan base-package="com.itheima">\n        \x3c!-- 包含以下注解则加载 --\x3e\n        <context:include-filter type="annotation" expression="org.springframework.stereotype.controller"/>\n    </context:component-scan>\n\n\n    \x3c!--放行指定类型静态资源配置方式--\x3e\n\x3c!--    <mvc:resources mapping="/img/**" location="/img/"/>--\x3e\n\x3c!--    <mvc:resources mapping="/js/**" location="/js/"/>--\x3e\n\x3c!--    <mvc:resources mapping="/css/**" location="/css/"/>--\x3e\n\n    \x3c!--springmvc提供的通用资源放行方式 释放所有静态资源--\x3e\n    <mvc:default-servlet-handler/>\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nweb.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd"\n         version="3.1">\n\n    \x3c!--乱码处理过滤器，与servlet中使用的完全相同，差异之处在于处理器的类由spring提供--\x3e\n    <filter>\n        <filter-name>characterencodingfilter</filter-name>\n        <filter-class>org.springframework.web.filter.characterencodingfilter</filter-class>\n        <init-param>\n            <param-name>encoding</param-name>\n            <param-value>utf-8</param-value>\n        </init-param>\n    </filter>\n    <filter-mapping>\n        <filter-name>characterencodingfilter</filter-name>\n        <url-pattern>/*</url-pattern>\n    </filter-mapping>\n\t\x3c!-- 扫描mvc配置文件 --\x3e\n    <servlet>\n        <servlet-name>dispatcherservlet</servlet-name>\n        <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n        <init-param>\n            <param-name>contextconfiglocation</param-name>\n            <param-value>classpath*:spring-mvc.xml</param-value>\n        </init-param>\n    </servlet>\n    <servlet-mapping>\n        <servlet-name>dispatcherservlet</servlet-name>\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 技术架构图\n\n\n\n\n# 注解驱动\n\n\n# 扫描包含controller注解的类\n\n在springmvc 配置类中\n\n@componentscan(value = "com.itheima",includefilters =\n    @componentscan.filter(type=filtertype.annotation,classes = {controller.class})\n    )\n\n\n1\n2\n3\n\n\n\n# 注解配置指定放行的资源\n\n实现webmvcconfigurer接口 重写addresourcehandlers方法\n\npublic class springmvcconfiguration implements webmvcconfigurer{\n    //注解配置放行指定资源格式\n    @override\n    public void addresourcehandlers(resourcehandlerregistry registry) {\n        \t\t\t  registry.addresourcehandler("/img/**").addresourcelocations("/img/");\n        registry.addresourcehandler("/js/**").addresourcelocations("/js/");\n        registry.addresourcehandler("/css/**").addresourcelocations("/css/");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 注解配置放行所有静态资源\n\n实现webmvcconfigurer接口 重写configuredefaultservlethandling方法\n\npublic class springmvcconfiguration implements webmvcconfigurer{\n\t@override\n    public void configuredefaultservlethandling(defaultservlethandlerconfigurer configurer) {\n        configurer.enable();;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# servlet扫描mvc配置文件\n\n继承 abstractdispatcherservletinitializer 类 重写里面的 createservletapplicationcontext createrootapplicationcontext 和 getservletmappings 方法\n\nimport org.springframework.web.context.webapplicationcontext;\nimport org.springframework.web.context.support.annotationconfigwebapplicationcontext;\nimport org.springframework.web.filter.characterencodingfilter;\nimport org.springframework.web.servlet.support.abstractdispatcherservletinitializer;\n\nimport javax.servlet.dispatchertype;\nimport javax.servlet.filterregistration;\nimport javax.servlet.servletcontext;\nimport javax.servlet.servletexception;\nimport java.util.enumset;\n\npublic class servletcontainersinitconfig extends abstractdispatcherservletinitializer {\n    //创建servlet容器时，使用注解的方式加载springmvc配置类中的信息，并加载成web专用的applicationcontext对象\n    //该对象放入了servletcontext范围，后期在整个web容器中可以随时获取调用\n    @override\n    protected webapplicationcontext createservletapplicationcontext() {\n        annotationconfigwebapplicationcontext ctx = new annotationconfigwebapplicationcontext();\n        //需要一个mvc配置类\n        ctx.register(springmvcconfiguration.class);\n        return ctx;\n    }\n\n    //注解配置映射地址方式，服务于springmvc的核心控制器dispatcherservlet\n    @override\n    protected string[] getservletmappings() {\n        return new string[]{"/"};\n    }\n\n    @override\n    protected webapplicationcontext createrootapplicationcontext() {\n        return null;\n    }\n\n    //乱码处理作为过滤器，在servlet容器启动时进行配置，相关内容参看servlet零配置相关课程\n    @override\n    public void onstartup(servletcontext servletcontext) throws servletexception {\n        super.onstartup(servletcontext);\n        characterencodingfilter cef = new characterencodingfilter();\n        cef.setencoding("utf-8");\n        filterregistration.dynamic registration = servletcontext.addfilter("characterencodingfilter", cef);\n        registration.addmappingforurlpatterns(enumset.of(dispatchertype.request,dispatchertype.forward,dispatchertype.include),false,"/*");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 请求\n\n\n# 请求参数\n\nspringmvc将传递的参数封装到处理器方法的形参中,达到快速访问参数的目的\n\n只需在形参上与请求地址上的参数名字一致 即可获取到请求参数的值\n\n    //请求参数  http://localhost/requestparam1?name=hello\n    @requestmapping("/requestparam1")\n    public string requestparam1(string name){\n        system.out.println(name);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 普通类型\n\n参数名与处理器方法形参名保持一致 否则无法获取对应的值\n\n如果想要绑定具体的请求名 则需要使用 @requestparam\n\n * @requestparam\n   * value 请求属性名\n   * required 不允许为空 默认为true\n   * defaultvalue 为空时默认值\n\n    //请求参数形参绑定名  http://localhost/requestparam2?username=hello&age=10\n    @requestmapping("/requestparam2")\n    public string requestparam2(@requestparam(value = "username",required = true,defaultvalue = "zhangsan") string name, int age){\n        system.out.println(name);\n        system.out.println(age);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# pojo类型\n\n如果形参为一个对象 则请求参数会将值 一一对应为对象中属性值\n\n    //pojo类型参数  http://localhost/requestparam3?username=zhangsan&age=10\n    @requestmapping("/requestparam3")\n    public string requestparam3(user user){\n        system.out.println(user.tostring());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# pojo类型与普通类型同时存在\n\n将会被同时赋值 如果想进行区分 建议使用@requestparam进行绑定\n\n    //pojo类型参数与普通类型同时存在  http://localhost/requestparam4?name=zhangsan&age=10\n    @requestmapping("/requestparam4")\n    public string requestparam4(user user,string age){\n        system.out.println(user.tostring());\n        system.out.println(age);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 复杂pojo类型 嵌套\n\n如果pojo中嵌套pojo则 需要请求地址 需要按照层次结构要书写\n\n    //复杂pojo类型  http://localhost/requestparam5?address.city=shanghai\n    @requestmapping("/requestparam5")\n    public string requestparam5(user user){\n        system.out.println(user.getaddress().getcity());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 复杂pojo类型 集合\n\n如果pojo中出现集合 保存简单数据 使用多个相同名称的参数为其进行赋值\n\n\n    //复杂pojo类型 集合  http://localhost/requestparam6?nick=zhangsan&nick=lisi&nick=wangwu\n    @requestmapping("/requestparam6")\n    public string requestparam6(user user){\n        system.out.println(user);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 复杂pojo类型 集合对象\n\n集合对象 以索引形式的请求地址\n\n    //复杂pojo类型 集合对象  http://localhost/requestparam7?addresses[0].province=bj&addresses[1].province=gd\n    @requestmapping("/requestparam7")\n    public string requestparam7(user user){\n        system.out.println(user.getaddresses());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 复杂pojo类型 map集合\n\nmap集合 在请求地址中以 key的形式赋值\n\n    //复杂pojo类型 集合对象  http://localhost/requestparam8?addressmap[\'home\'].province=bj&addressmap[\'job\'].province=gd\n    @requestmapping("/requestparam8")\n    public string requestparam8(user user){\n        system.out.println(user.getaddressmap());\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 数组类型\n\n请求参数名与数组名一致 并请求参数数量大于1个\n\n    //数组类型  http://localhost/requestparam9?nick=abc&nick=def\n    @requestmapping("/requestparam9")\n    public string requestparam9(string[] nick) {\n        system.out.println(nick[0] + " " + nick[1]);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 集合类型\n\nmvc默认将list作为对象处理 赋值前先创建对象 然后将nick作为对象的属性赋值 但list是接口 无法创建对象 和 有此属性值 所以报错\n\n我们通过@requestparam 将请求参数打包成数组\n\n    //集合类型  http://localhost/requestparam10?nick=abc&nick=def\n    @requestmapping("/requestparam10")\n    public string requestparam10(@requestparam("nick") list<string> nick) {\n        system.out.println(nick);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 类型转换器\n\nmvc对接受的数据进行自动类型转换 通过converter接口实现\n\n# xml 日期类型格式转换\n\nspring默认的日期格式为 2021/09/11 如果我们传递为2021-09-11则会报错 我们需要自定义日期格式转换\n\n    \x3c!--    自定义转换格式--\x3e\n    <mvc:annotation-driven conversion-service="conversionservice"/>\n    \x3c!--    注册bean 让spring管理--\x3e\n    <bean id="conversionservice" class="org.springframework.format.support.formattingconversionservicefactorybean">\n        <property name="formatters">\n            <set>\n                <bean class="org.springframework.format.datetime.dateformatter">\n                    <property name="pattern" value="yyyy-mm-dd"/>\n                </bean>\n            </set>\n        </property>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 注解版 日期类型格式转换\n\n    //日期类型 自定义格式转换  http://localhost/requestparam11?date=2021-09-11\n    @requestmapping("/requestparam11")\n    public string requestparam11(@datetimeformat(pattern = "yyyy-mm-dd") date date) {\n        system.out.println(date);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n并开启mvc注解驱动\n\n<mvc:annotation-driven />\n\n\n1\n\n\n# 自定义类型转换器\n\n实现 converter 接口 泛型1为原始数据类型 泛型2为返回的数据类型 并实现 convert 方法\n\n    \x3c!--    自定义类型转换器--\x3e\n    <mvc:annotation-driven conversion-service="conversionservice"/>\n    \x3c!--    注册bean 让spring管理--\x3e\n    <bean id="conversionservice" class="org.springframework.context.support.conversionservicefactorybean">\n        <property name="converters">\n            <set>\n                <bean class="com.itheima.converter.mydateconverter">\n                </bean>\n            </set>\n        </property>\n    </bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n处理类\n\n\nimport org.springframework.core.convert.converter.converter;\n\nimport java.text.parseexception;\nimport java.text.simpledateformat;\nimport java.util.date;\n\npublic class mydateconverter implements converter<string, date> {\n    @override\n    public date convert(string s) {\n        simpledateformat simpledateformat = new simpledateformat("yyyy-mm-dd");\n        date date = null;\n        try {\n            date = simpledateformat.parse(s);\n        } catch (parseexception e) {\n            e.printstacktrace();\n        }\n\n        return date;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 请求映射\n\n * @requestmapping 设置在方法上则是方法体的请求映射 定义在类上则是整个类的访问前缀\n * value 请求路径\n * method = requestmethod.get 请求方式\n * params = "name" 请求地址必须传递此属性才能访问\n * headers = "content-type=text/*" 请求头条件\n * consumes = "text/*" 可以接受的请求正文类型\n * produces = "text/*" 可以生成的响应正文类型\n\n\n# 响应\n\n\n# 页面跳转方式\n\n * 转发(默认)\n   \n   @requestmapping("/showpageanddata1")\n       public string showpageanddata1(httpservletrequest request){\n           return "forward:success.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 重定向\n   \n       @requestmapping("/showpageanddata7")\n       public string showpageanddata7(){\n           return "redirect:index.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 页面访问快捷设定 设置了快捷访问 后默认访问是设定的路径下制定的后缀文件 默认为转发跳转 无法重定向\n   \n       <bean class="org.springframework.web.servlet.view.internalresourceviewresolver">\n           <property name="prefix" value="/web-inf/page/"/>\n           <property name="suffix" value=".jsp"/>\n       </bean>\n   \n   \n   1\n   2\n   3\n   4\n   \n   \n   如果方法体没有返回值 也配置了快捷方式 则自动跳转到指定的路径下 访问路径 + 后缀 的文件\n   \n   \n\n\n# request传递数据\n\n@requestmapping("/showpageanddata1")\npublic string showpageanddata1(httpservletrequest request){\n    request.setattribute("name","hello");\n    return "success.jsp";\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# model 类型形参进行数据传递\n\n    @requestmapping("/showpageanddata2")\n    public string showpageanddata2(model model){\n        model.addattribute("name","hello");\n        user user =new user();\n        user.setage(156);\n        //可以传递对象\n        model.addattribute("user",user);  \n        return "success.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# modelandview 类型传递\n\n    @requestmapping("/showpageanddata3")\n    public modelandview showpageanddata3(modelandview modelandview){\n        modelandview.addobject("name","hello");\n        user user =new user();\n        user.setage(156);\n        //添加属性\n        modelandview.addobject("user",user);\n        //跳转页面\n        modelandview.setviewname("success.jsp");\n        return modelandview;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 重定向\n\n    @requestmapping("/showpageanddata5")\n    public modelandview showpageanddata5(modelandview modelandview){\n        //重定向\n        modelandview.setviewname("redirect:index.jsp");\n        return modelandview;\n    }\n\n    @requestmapping("/showpageanddata6")\n    public modelandview showpageanddata6(modelandview modelandview){\n        //跳转\n        modelandview.setviewname("forward:index.jsp");\n        return modelandview;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 返回json数据\n\n响应方法体 返回的数据默认会直接跳转到 返回值对应的页面文件\n\n我们可以在方法体上 加上 @responsebody 注解 来声明为一个响应体\n\n    @requestmapping("showdata1")\n    @responsebody\n    public string showdata1() throws jsonprocessingexception {\n        user user = new user();\n        user.setage(15);\n        user.setname("zhangsan");\n        objectmapper om = new objectmapper();\n        return om.writevalueasstring(user);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n * 返回一个对象或者集合 jackson 已经帮我们做好了 自定义类型转换了 我们只需要在spring配置中开启注解驱动 即可\n   \n    <mvc:annotation-driven />\n   \n   \n   1\n   \n   \n   注解版 开启注解驱动 为 @enablewebmvc\n   \n       @requestmapping("showdata2")\n       @responsebody\n       public user showdata2() throws jsonprocessingexception {\n           user user = new user();\n           user.setage(15);\n           user.setname("zhangsan");\n           return user;\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   \n\n\n# servlet相关接口\n\n原始的 request response 和 session 提供给我们使用但是不太推荐使用原生的功能\n\n\n\n\n# head数据获取\n\n * @requestheader 获取头中指定的值\n   \n       @requestmapping("/headapi")\n       public string headapi(@requestheader("accept-language") string head){\n           system.out.println(head);\n           return "index.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n\n# cookie 数据获取\n\n * @cookievalue 获取cookie指定key的值\n   \n       @requestmapping("/cookieapi")\n       public string cookieapi(@cookievalue("_xsrf") string cookie){\n           system.out.println(cookie);\n           return "index.jsp";\n       }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n\n# session 数据获取和设置\n\n * 设置 只有在类注解上方标记的变量名称才会被放到session中\n   \n   * \n\n * 获取\n   \n   *     @requestmapping("/sessionapi")\n         public string sessionapi(@sessionattribute("name") string session){\n             system.out.println(session);\n             return "index.jsp";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# 发送异步请求\n\n通过ajax发送的异步请求 是无法直接被赋值给形参的 需要在形参前面 加上 @requestbody\n\n    @requestmapping("/ajax1")\n    public string ajax1(@requestbody string data) {\n        system.out.println(data);\n        return "index.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n\n\n * json转pojo 会自动赋值给pojo中属性\n   \n   *     @requestmapping("/ajax2")\n         public string ajax2(@requestbody user user) {\n             system.out.println(user);\n             return "index.jsp";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 集合\n   \n   *     @requestmapping("/ajax3")\n         public string ajax3(@requestbody list<user> user) {\n             system.out.println(user);\n             return "index.jsp";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# 异步请求响应\n\n * 返回字符串\n   \n   *     @requestmapping("/ajax4")\n         @responsebody\n         public string ajax4() {\n             return "hello";\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 返回对象\n   \n   *     @requestmapping("/ajax5")\n         @responsebody\n         public user ajax5() {\n             user user = new user();\n             user.setname("hhh");\n             user.setage(13);\n             return user;\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n * 返回集合\n   \n   *     @requestmapping("/ajax6")\n         @responsebody\n         public list ajax6() {\n             user user = new user();\n             user user2 = new user();\n             user.setname("hhh");\n             user.setage(13);\n             user2.setname("qqqq");\n             user2.setage(17);\n             arraylist<user> arraylist = new arraylist<>();\n             arraylist.add(user);\n             arraylist.add(user2);\n             return arraylist;\n         }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n\n# 跨域访问\n\n当通过a域名下的操作访问 域名b下的资源时 称为跨域访问\n\n我们可以通过设置头信息 开启跨域访问限制\n\n但spring 帮我完成这个操作 我们只需要添加@crossorigin 就可以解决 定义在方法体或类上\n\n    @requestmapping("/cross")\n    @responsebody\n    @crossorigin\n    public void cross() {\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 拦截器\n\n拦截器(interceptor) 是一种动态拦截方法调用的机制\n\n 1. 在指定的方法调用前后执行预先设定后的代码\n 2. 阻止原始方法的执行\n\n核心原理:aop思想\n\n拦截器链:多个拦截器按照一定的顺序 对原始被调用功能进行增强\n\n实现 handlerinterceptor 接口 重写需要的方法\n\npublic class myinterceptor implements handlerinterceptor {\n\n    @override\n    public boolean prehandle(httpservletrequest request, httpservletresponse response, object handler) throws exception {\n        system.out.println("前置运行");\n        //如果为false 则直接拦截业务处理器不放行\n        return true;\n    }\n\n    @override\n    public void posthandle(httpservletrequest request, httpservletresponse response, object handler, modelandview modelandview) throws exception {\n        system.out.println("后置运行");\n    }\n\n    @override\n    public void aftercompletion(httpservletrequest request, httpservletresponse response, object handler, exception ex) throws exception {\n        system.out.println("完成运行");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n <mvc:interceptors>\n        <mvc:interceptor>\n            <mvc:mapping path="/handlerun"/>\n            <bean class="com.itheima.interceptor.myinterceptor"/>\n        </mvc:interceptor>\n    </mvc:interceptors>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n# 拦截器参数\n\n * request 请求对象\n * response 响应对象\n * handler 被调用的处理器对象,对反射中的method对象进行了包装\n * modelandview 可以读取或者修改 页面信息和对应数据\n * exception 如果处理器执行中出现异常 如何处理\n\n\n# 多个拦截器\n\n多个拦截器执行顺序与xml配置有关\n\n但多个拦截器中的前置运行 后置运行 都会同时启用\n\n按照链式 执行顺序\n\n\n\n\n# 责任链模式\n\n\n\n\n# 异常处理\n\n类注解 @component 绑定bean为 并实现 handlerexceptionresolver 接口 实现方法\n\n此bean会拦截mvc上请求的异常\n\n@component\npublic class exceptionresolver implements handlerexceptionresolver {\n    @override\n    public modelandview resolveexception(httpservletrequest httpservletrequest, httpservletresponse httpservletresponse, object o, exception e) {\n\n        system.out.println("发生异常了");\n\n        modelandview modelandview =new modelandview();\n\n        if(e instanceof nullpointerexception){\n            //添加错误信息\n            modelandview.addobject("msg","空指针异常");\n        }else if (e instanceof arithmeticexception){\n            //添加错误信息\n            modelandview.addobject("msg","算术异常");\n        }else {\n            //添加错误信息\n            modelandview.addobject("msg","未知异常");\n        }\n\n        //转发页面\n        modelandview.setviewname("error.jsp");\n        return modelandview;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 注解版异常处理\n\n * @exceptionhandler 注解 标记要捕抓的异常\n\n@component\n@controlleradvice\npublic class exceptionadivce {\n\n    //异常 的类\n    @exceptionhandler(nullpointerexception.class)\n    @responsebody\n    public string donullexception(exception e){\n        system.out.println("空指针异常");\n        return "空指针异常";\n    }\n\n    //异常 的类\n    @exceptionhandler(arithmeticexception.class)\n    @responsebody\n    public string doarithmeticexception(exception e){\n        system.out.println("算术异常");\n        return "算术异常";\n    }\n\n    //异常 的类\n    @exceptionhandler(exception.class)\n    @responsebody\n    public string doexception(exception e){\n        system.out.println("all");\n        return "all";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n注意事项:\n\n@exceptionhandler 注解 会比 handlerexceptionresolver 运行早\n\nhandlerexceptionresolver 无法捕抓到参数异常 而注解可以捕抓到\n\n\n# 文件上传下载\n\n坐标\n\n  \x3c!--文件上传下载--\x3e\n    <dependency>\n      <groupid>commons-fileupload</groupid>\n      <artifactid>commons-fileupload</artifactid>\n      <version>1.4</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n上传文件传递的name 要与 形参的名称一致\n\n\n# restful\n\n\n\n\n\n\n# pathvariable\n\n@pathvariable 为restful 规范的请求路径 赋值给指定形参\n\n并且在@requestmapping 的 method 指定请求方式\n\n    @requestmapping(value = "/user/{id}",method = requestmethod.get)\n    public string restlocation(@pathvariable integer id){\n        system.out.println(id);\n        return "page.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n\n\n\n# restcontroller\n\n@restcontroller 结合了 @controller和@responsebody 两个注解的功能\n\n绑定为bean 并且 所有返回内容都不会解析为网页结构\n\n\n# xxxmapping\n\n如 @getmapping @postmapping 请求等等\n\n我们不用在@requestmapping 中定义指定的请求方式 和 请求路径\n\n只需在方法体加上指定请求方式的注解即\n\n    @postmapping("{id}")\n    public string postrestlocation(@pathvariable integer id){\n        system.out.println(id);\n        return "page.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n\n\n\n# 表单验证框架\n\nhibernate框架\n\n    <dependency>\n      <groupid>org.hibernate</groupid>\n      <artifactid>hibernate-validator</artifactid>\n      <version>6.1.0.final</version>\n    </dependency>\n\n\n1\n2\n3\n4\n5\n\n * @valid 形参注解 开启此形参的验证\n * @valid 属性注解 开启此对象或属性验证 对象校验在对象内部属性定义规则\n * @notblank(message = "提醒信息") 属性验证规则 不能为null 并且长度必须大于0和不全为空格 则不能为空\n * @notnull (message = "提醒信息") 不能为null 可以为空\n * @notempty(message = "") 不能为空和null\n * @max(value = "" , message =" ") 限定最大值\n * @min(value = "" , message = "") 最小值\n * @range(max = ,min = ,message = ) 最大最小值\n\n @requestmapping(value = "/addemployee2")\n    //使用@valid开启校验，使用@validated也可以开启校验\n    //errors对象用于封装校验结果，如果不满足校验规则，对应的校验结果封装到该对象中，包含校验的属性名和校验不通过返回的消息\n    public string addemployee2(@valid employee employee, errors errors, model m){\n        //判定errors对象中是否存在未通过校验的字段\n        if(errors.haserrors()){\n            //获取所有未通过校验规则的信息\n            list<fielderror> fielderrors = errors.getfielderrors();\n            system.out.println(fielderrors.size());\n            for(fielderror error : fielderrors){\n                system.out.println(error.getfield());\n                system.out.println(error.getdefaultmessage());\n                //将校验结果信息添加到model对象中，用于页面显示，后期实际开发中无需这样设定，返回json数据即可\n                m.addattribute(error.getfield(),error.getdefaultmessage());\n            }\n            //当出现未通过校验的字段时，跳转页面到原始页面，进行数据回显\n            return "addemployee.jsp";\n        }\n        return "success.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n    @notblank(message = "姓名不能为空")\n    private string name;//员工姓名\n\n    //一个属性可以添加多个校验器\n    @notnull(message = "请输入您的年龄")\n    @max(value = 60,message = "年龄最大值不允许超过60岁")\n    @min(value = 18,message = "年龄最小值不允许低于18岁")\n    private integer age;//员工年龄\n\n    //实体类中的引用类型通过标注@valid注解，设定开启当前引用类型字段中的属性参与校验\n    @valid\n    private address address;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 分组校验\n\n如果不开启分组校验 则会校验当前全部开启校验的属性/对象\n\n在属性/对象校验注解中 加上 groups 属性 并给予一个 用于标识的字节码文件\n\n//设定校验器，设置校验不通过对应的消息，设定所参与的校验组\n    @notblank(message = "姓名不能为空",groups = {groupa.class})\n    private string name;//员工姓名\n\n\n1\n2\n3\n\n\n拦截校验器 开启校验注解要改为 @validated 并加上标识字节码文件\n\n    @requestmapping(value = "/addemployee")\n    public string addemployee(@validated({groupa.class}) employee employee, errors errors, model m){\n        if(errors.haserrors()){\n            list<fielderror> fielderrors = errors.getfielderrors();\n            system.out.println(fielderrors.size());\n            for(fielderror error : fielderrors){\n                system.out.println(error.getfield());\n                system.out.println(error.getdefaultmessage());\n                m.addattribute(error.getfield(),error.getdefaultmessage());\n            }\n            return "addemployee.jsp";\n        }\n        return "success.jsp";\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# ssm\n\nspring + springmvc + mybatis',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Spring Security",frontmatter:{title:"Spring Security",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/ca6c88/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/12.Spring%20Security.html",relativePath:"后端/02.JavaEE/12.Spring Security.md",key:"v-758de2ee",path:"/pages/ca6c88/",headers:[{level:2,title:"入门案例",slug:"入门案例",normalizedTitle:"入门案例",charIndex:185},{level:2,title:"配置可匿名访问的资源",slug:"配置可匿名访问的资源",normalizedTitle:"配置可匿名访问的资源",charIndex:4688},{level:2,title:"指定登陆页面",slug:"指定登陆页面",normalizedTitle:"指定登陆页面",charIndex:5057},{level:2,title:"从数据库查询用户信息",slug:"从数据库查询用户信息",normalizedTitle:"从数据库查询用户信息",charIndex:4228},{level:2,title:"密码加密",slug:"密码加密",normalizedTitle:"密码加密",charIndex:8944},{level:2,title:"配置多种校验规则",slug:"配置多种校验规则",normalizedTitle:"配置多种校验规则",charIndex:12999},{level:2,title:"注解权限控制",slug:"注解权限控制",normalizedTitle:"注解权限控制",charIndex:13603},{level:2,title:"退出登陆",slug:"退出登陆",normalizedTitle:"退出登陆",charIndex:15020}],headersStr:"入门案例 配置可匿名访问的资源 指定登陆页面 从数据库查询用户信息 密码加密 配置多种校验规则 注解权限控制 退出登陆",content:'# Spring Security\n\nSpring Security是 Spring提供的安全认证服务的框架。 使用Spring Security可以帮助我们来简化认证和授权的过程。官网：https://spring.io/projects/spring-security\n\n常用的权限框架除了Spring Security，还有Apache的shiro框架。\n\n\n# 入门案例\n\n坐标\n\n<dependency>\n  <groupId>org.springframework.security</groupId>\n  <artifactId>spring-security-web</artifactId>\n  <version>5.0.5.RELEASE</version>\n</dependency>\n<dependency>\n  <groupId>org.springframework.security</groupId>\n  <artifactId>spring-security-config</artifactId>\n  <version>5.0.5.RELEASE</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n配置web.xml\n\n<!DOCTYPE web-app PUBLIC\n        "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"\n        "://java.sun.com/dtd/web-app_2_3.dtd" >\n<web-app>\n  <display-name>Archetype Created Web Application</display-name>\n  <filter>\n    \x3c!--\n      DelegatingFilterProxy用于整合第三方框架\n      整合Spring Security时过滤器的名称必须为springSecurityFilterChain，\n      否则会抛出NoSuchBeanDefinitionException异常\n    --\x3e\n    <filter-name>springSecurityFilterChain</filter-name>\n    <filter-class>org.springframework.web.filter.DelegatingFilterProxy</filter-class>\n  </filter>\n  <filter-mapping>\n    <filter-name>springSecurityFilterChain</filter-name>\n    <url-pattern>/*</url-pattern>\n  </filter-mapping>\n  <servlet>\n    <servlet-name>springmvc</servlet-name>\n    <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n    \x3c!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载 --\x3e\n    <init-param>\n      <param-name>contextConfigLocation</param-name>\n      <param-value>classpath:spring-security.xml</param-value>\n    </init-param>\n    <load-on-startup>1</load-on-startup>\n  </servlet>\n  <servlet-mapping>\n    <servlet-name>springmvc</servlet-name>\n    <url-pattern>*.do</url-pattern>\n  </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n配置spring-security.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:dubbo="http://code.alibabatech.com/schema/dubbo"\n       xmlns:mvc="http://www.springframework.org/schema/mvc"\n       xmlns:security="http://www.springframework.org/schema/security"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n                        http://www.springframework.org/schema/beans/spring-beans.xsd\n                        http://www.springframework.org/schema/mvc\n                        http://www.springframework.org/schema/mvc/spring-mvc.xsd\n                        http://code.alibabatech.com/schema/dubbo\n                        http://code.alibabatech.com/schema/dubbo/dubbo.xsd\n                        http://www.springframework.org/schema/context\n                        http://www.springframework.org/schema/context/spring-context.xsd\n                          http://www.springframework.org/schema/security\n                          http://www.springframework.org/schema/security/spring-security.xsd">\n\n    \x3c!--\n        http：用于定义相关权限控制\n        auto-config：是否自动配置\n                        设置为true时框架会提供默认的一些配置，例如提供默认的登录页面、登出处理等\n                        设置为false时需要显示提供登录表单配置，否则会报错\n        use-expressions：用于指定intercept-url中的access属性是否使用表达式来描述权限\n    --\x3e\n    <security:http auto-config="true" use-expressions="true">\n        \x3c!--\n            intercept-url：定义一个拦截规则\n            pattern：对哪些url进行权限控制   /**表示拦截所有请求  /*只能 拦截/a.html /b.html 无法逻辑 /a/b/c.html\n            access：在请求对应的URL时需要什么权限，默认配置时它应该是一个以逗号分隔的角色列表，\n                  请求的用户只需拥有其中的一个角色就能成功访问对应的URL\n                  指定所需的访问角色或者访问权限   hasRole()为表达式  如果表达式没有开启则值直接为角色名称\n        --\x3e\n        <security:intercept-url pattern="/**"  access="hasRole(\'ROLE_ADMIN\')" />\n    </security:http>\n\n    \x3c!--\n        authentication-manager：认证管理器，用于处理认证操作\n    --\x3e\n    <security:authentication-manager>\n        \x3c!--\n            authentication-provider：认证提供者，执行具体的认证逻辑\n        --\x3e\n        <security:authentication-provider>\n            \x3c!--\n                user-service：用于获取用户信息，提供给authentication-provider进行认证\n            --\x3e\n            <security:user-service>\n                \x3c!--\n                    user：定义用户信息，可以指定用户名、密码、角色，后期可以改为从数据库查询用户信息\n                  {noop}：表示当前使用的密码为明文\n                --\x3e\n                <security:user name="admin" password="{noop}1234" authorities="ROLE_ADMIN"/>\n            </security:user-service>\n        </security:authentication-provider>\n    </security:authentication-manager>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n\n# 配置可匿名访问的资源\n\n第一步：在项目中创建pages目录，在pages目录中创建a.html和b.html\n\n第二步：在spring-security.xml文件中配置，指定哪些资源可以匿名访问\n\n    \x3c!--    配置资源匿名访问\n\t http：用于定义相关权限控制\n  指定哪些资源不需要进行权限校验，可以使用通配符\n--\x3e\n    <security:http security="none" pattern="/pages/a.html"/>\n    <security:http security="none" pattern="/pages/b.html"/>\n    <security:http security="none" pattern="/pages/**"/>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 指定登陆页面\n\nSpring Security 框架默认提供了登陆页面给我们,我们需要自定义为自己的登陆页面\n\n 1. 配置匿名访问\n    \n    <security:http security="none" pattern="/login.html" />\n    \n    \n    1\n    \n\n 2. 修改spring-security.xml文件，加入表单登录信息的配置 为security:http子标签\n    \n      \x3c!--\n      form-login：定义表单登录信息\n      login-page:登陆页面\n      username-parameter:用户输入框对应的name\n      password-parameter:密码输入框对应的name\n      login-processing-url:提交地址\n      default-target-url:成功后默认跳转地址\n      authentication-failure-url:失败后跳转页面\n    --\x3e\n            <security:form-login login-page="/login.html"\n                                 username-parameter="username"\n                                 password-parameter="password"\n                                 login-processing-url="/login.do"\n                                 default-target-url="/index.html"\n                                 authentication-failure-url="/login.html"/>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n 3. 修改spring-security.xml文件，关闭CsrfFilter过滤器 为security:http子标签\n    \n    \x3c!--\n      csrf：对应CsrfFilter过滤器\n      disabled：是否启用CsrfFilter过滤器，如果使用自定义登录页面需要关闭此项，否则登录操作会被禁用（403）\n    --\x3e\n    <security:csrf disabled="true"></security:csrf>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n\n# 从数据库查询用户信息\n\n实现UserDetailsService接口 实现loadUserByUsername方法 获取到用户名\n\npackage com.itheima.service;\n\nimport com.itheima.pojo.User;\nimport org.springframework.security.core.GrantedAuthority;\nimport org.springframework.security.core.authority.SimpleGrantedAuthority;\nimport org.springframework.security.core.userdetails.UserDetails;\nimport org.springframework.security.core.userdetails.UserDetailsService;\nimport org.springframework.security.core.userdetails.UsernameNotFoundException;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class SpringSecurityUserService implements UserDetailsService {\n    //模拟数据库中的用户数据\n    public static Map<String, User> map = new HashMap<>();\n\n    static {\n        com.itheima.pojo.User user1 = new com.itheima.pojo.User();\n        user1.setUsername("admin");\n        user1.setPassword("admin");\n\n        com.itheima.pojo.User user2 = new com.itheima.pojo.User();\n        user2.setUsername("xiaoming");\n        user2.setPassword("1234");\n\n        map.put(user1.getUsername(), user1);\n        map.put(user2.getUsername(), user2);\n    }\n\n\n    //根据用户名查询用户信息\n    @Override\n    public UserDetails loadUserByUsername(String s) throws UsernameNotFoundException {\n        System.out.println(s);\n        //根据用户名查询数据库获取用户信息(包含数据库中的信息)\n        User user = map.get(s);\n        if (user == null) {\n            //用户名不存在\n            return null;\n        }\n\n        //将用户信息返回给框架\n        //框架会进行密码比对(页面与数据库中查询的密码对比)\n        ArrayList<GrantedAuthority> list = new ArrayList<>();\n        //为当前登录用户授权 后期改为数据库中对应的权限\n        list.add(new SimpleGrantedAuthority("permission_A"));//授权\n        list.add(new SimpleGrantedAuthority("permission_B"));\n        list.add(new SimpleGrantedAuthority("ROLE_ADMIN"));//授予角色\n        org.springframework.security.core.userdetails.User security = new org.springframework.security.core.userdetails.User(s, "{noop}" + user.getPassword(), list);\n\n        //返回\n        return security;\n\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\nbean 绑定\n\n \x3c!--\n        authentication-manager：认证管理器，用于处理认证操作\n    --\x3e\n    <security:authentication-manager>\n        \x3c!--\n            authentication-provider：认证提供者，执行具体的认证逻辑\n\t\t\tuser-service-ref 为user实现类的beanid\n        --\x3e\n        <security:authentication-provider user-service-ref="userService">\n        </security:authentication-provider>\n    </security:authentication-manager>\n\n    <bean id="userService" class="com.itheima.service.SpringSecurityUserService"></bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 密码加密\n\n前面我们使用的密码都是明文的，这是非常不安全的。一般情况下用户的密码需要进行加密后再保存到数据库中。\n\n常见的密码加密方式有：\n\n3DES、AES、DES：使用对称加密算法，可以通过解密来还原出原始密码\n\nMD5、SHA1：使用单向HASH算法，无法通过计算还原出原始密码，但是可以建立彩虹表进行查表破解\n\nbcrypt：将salt随机并混入最终加密后的密码，验证时也无需单独提供之前的salt，从而无需单独处理salt问题\n\n 1. 在spring-security.xml文件中指定密码加密对象\n    \n        <security:authentication-manager>\n    \t\t\t\x3c!--追加 --\x3e\n                <security:password-encoder ref="passwordEncoder"/>\n            </security:authentication-provider>\n        </security:authentication-manager>\n    \n    \n    \x3c!--配置密码加密对象--\x3e\n    <bean id="passwordEncoder" \n          class="org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder" />\n    \n    \x3c!--认证管理器，用于处理认证操作--\x3e\n    <security:authentication-manager>\n      \x3c!--认证提供者，执行具体的认证逻辑--\x3e\n      <security:authentication-provider user-service-ref="userService">\n        \x3c!--指定密码加密策略--\x3e\n        <security:password-encoder ref="passwordEncoder" />\n      </security:authentication-provider>\n    </security:authentication-manager>\n    \x3c!--开启spring注解使用--\x3e\n    <context:annotation-config></context:annotation-config>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n 2. 修改UserService实现类\n    \n    package com.itheima.service;\n    \n    import com.itheima.pojo.User;\n    import org.springframework.beans.factory.annotation.Autowired;\n    import org.springframework.security.core.GrantedAuthority;\n    import org.springframework.security.core.authority.SimpleGrantedAuthority;\n    import org.springframework.security.core.userdetails.UserDetails;\n    import org.springframework.security.core.userdetails.UserDetailsService;\n    import org.springframework.security.core.userdetails.UsernameNotFoundException;\n    import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;\n    \n    import java.util.ArrayList;\n    import java.util.HashMap;\n    import java.util.Map;\n    \n    public class SpringSecurityUserService2 implements UserDetailsService {\n        @Autowired\n        private BCryptPasswordEncoder passwordEncoder;\n    \n        //模拟数据库中的用户数据\n        public Map<String, User> map = new HashMap<>();\n    \n        public void initUserDate() {\n            User user1 = new User();\n            user1.setUsername("admin");\n            user1.setPassword(passwordEncoder.encode("admin"));  //使用bcrypt进行加密\n    \n            User user2 = new User();\n            user2.setUsername("xiaoming");\n            user2.setPassword(passwordEncoder.encode("1234"));\n    \n            map.put(user1.getUsername(), user1);\n            map.put(user2.getUsername(), user2);\n        }\n    \n    \n        //根据用户名查询用户信息\n        @Override\n        public UserDetails loadUserByUsername(String s) throws UsernameNotFoundException {\n            initUserDate();\n            System.out.println(s);\n            //根据用户名查询数据库获取用户信息(包含数据库中的信息)\n            User user = map.get(s);\n            if (user == null) {\n                //用户名不存在\n                return null;\n            }\n    \n            //将用户信息返回给框架\n            //框架会进行密码比对(页面与数据库中查询的密码对比)\n            ArrayList<GrantedAuthority> list = new ArrayList<>();\n            //为当前登录用户授权 后期改为数据库中对应的权限\n            list.add(new SimpleGrantedAuthority("permission_A"));//授权\n            list.add(new SimpleGrantedAuthority("permission_B"));\n            list.add(new SimpleGrantedAuthority("ROLE_ADMIN"));//授予角色\n            org.springframework.security.core.userdetails.User security = new org.springframework.security.core.userdetails.User(s, user.getPassword(), list);\n    \n            //返回\n            return security;\n    \n        }\n    }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    \n\n\n# 配置多种校验规则\n\n修改spring-security.xml文件：\n\n\x3c!--只要认证通过就可以访问--\x3e\n<security:intercept-url pattern="/index.jsp"  access="isAuthenticated()" />\n<security:intercept-url pattern="/a.html"  access="isAuthenticated()" />\n\n\x3c!--拥有add权限就可以访问b.html页面--\x3e\n<security:intercept-url pattern="/b.html"  access="hasAuthority(\'add\')" />\n\n\x3c!--拥有ROLE_ADMIN角色就可以访问c.html页面--\x3e\n<security:intercept-url pattern="/c.html"  access="hasRole(\'ROLE_ADMIN\')" />\n\n\x3c!--拥有ROLE_ADMIN角色就可以访问d.html页面，\n    注意：此处虽然写的是ADMIN角色，框架会自动加上前缀ROLE_--\x3e\n<security:intercept-url pattern="/d.html"  access="hasRole(\'ADMIN\')" />\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 注解权限控制\n\n 1. 在spring-security.xml文件中配置组件扫描，用于扫描Controller\n    \n    <mvc:annotation-driven></mvc:annotation-driven>\n    <context:component-scan base-package="com.itheima.controller"></context:component-scan>\n    \n    \n    1\n    2\n    \n\n 2. 在spring-security.xml文件中开启权限注解支持\n    \n    \x3c!--开启注解方式权限控制--\x3e\n    <security:global-method-security pre-post-annotations="enabled" />\n    \n    \n    1\n    2\n    \n\n 3. 创建Controller类并在Controller的方法上加入注解进行权限控制 @PreAuthorize()\n    \n    package com.itheima.controller;\n    \n    import org.springframework.security.access.prepost.PreAuthorize;\n    import org.springframework.web.bind.annotation.RestController;\n    import org.springframework.web.bind.annotation.RequestMapping;\n    \n    @RestController\n    @RequestMapping("/hello")\n    public class HelloController {\n        @RequestMapping("/add")\n        @PreAuthorize("hasAuthority(\'add\')")//表示用户必须拥有add权限才能调用当前方法\n        public String add(){\n            System.out.println("add...");\n            return "success";\n        }\n    \n        @RequestMapping("/delete")\n        @PreAuthorize("hasRole(\'ROLE_ADMIN\')")//表示用户必须拥有ROLE_ADMIN角色才能调用当前方法\n        public String delete(){\n            System.out.println("delete...");\n            return "success";\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n\n\n# 退出登陆\n\n在spring-security.xml文件 配置 为security:http子标签\n\n\x3c!--\n  logout：退出登录\n  logout-url：退出登录操作对应的请求路径\n  logout-success-url：退出登录后的跳转页面\n--\x3e\n<security:logout logout-url="/logout.do" \n                 logout-success-url="/login.html" invalidate-session="true"/>\n\n\n1\n2\n3\n4\n5\n6\n7\n',normalizedContent:'# spring security\n\nspring security是 spring提供的安全认证服务的框架。 使用spring security可以帮助我们来简化认证和授权的过程。官网：https://spring.io/projects/spring-security\n\n常用的权限框架除了spring security，还有apache的shiro框架。\n\n\n# 入门案例\n\n坐标\n\n<dependency>\n  <groupid>org.springframework.security</groupid>\n  <artifactid>spring-security-web</artifactid>\n  <version>5.0.5.release</version>\n</dependency>\n<dependency>\n  <groupid>org.springframework.security</groupid>\n  <artifactid>spring-security-config</artifactid>\n  <version>5.0.5.release</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n配置web.xml\n\n<!doctype web-app public\n        "-//sun microsystems, inc.//dtd web application 2.3//en"\n        "://java.sun.com/dtd/web-app_2_3.dtd" >\n<web-app>\n  <display-name>archetype created web application</display-name>\n  <filter>\n    \x3c!--\n      delegatingfilterproxy用于整合第三方框架\n      整合spring security时过滤器的名称必须为springsecurityfilterchain，\n      否则会抛出nosuchbeandefinitionexception异常\n    --\x3e\n    <filter-name>springsecurityfilterchain</filter-name>\n    <filter-class>org.springframework.web.filter.delegatingfilterproxy</filter-class>\n  </filter>\n  <filter-mapping>\n    <filter-name>springsecurityfilterchain</filter-name>\n    <url-pattern>/*</url-pattern>\n  </filter-mapping>\n  <servlet>\n    <servlet-name>springmvc</servlet-name>\n    <servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class>\n    \x3c!-- 指定加载的配置文件 ，通过参数contextconfiglocation加载 --\x3e\n    <init-param>\n      <param-name>contextconfiglocation</param-name>\n      <param-value>classpath:spring-security.xml</param-value>\n    </init-param>\n    <load-on-startup>1</load-on-startup>\n  </servlet>\n  <servlet-mapping>\n    <servlet-name>springmvc</servlet-name>\n    <url-pattern>*.do</url-pattern>\n  </servlet-mapping>\n\n</web-app>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n配置spring-security.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:dubbo="http://code.alibabatech.com/schema/dubbo"\n       xmlns:mvc="http://www.springframework.org/schema/mvc"\n       xmlns:security="http://www.springframework.org/schema/security"\n       xsi:schemalocation="http://www.springframework.org/schema/beans\n                        http://www.springframework.org/schema/beans/spring-beans.xsd\n                        http://www.springframework.org/schema/mvc\n                        http://www.springframework.org/schema/mvc/spring-mvc.xsd\n                        http://code.alibabatech.com/schema/dubbo\n                        http://code.alibabatech.com/schema/dubbo/dubbo.xsd\n                        http://www.springframework.org/schema/context\n                        http://www.springframework.org/schema/context/spring-context.xsd\n                          http://www.springframework.org/schema/security\n                          http://www.springframework.org/schema/security/spring-security.xsd">\n\n    \x3c!--\n        http：用于定义相关权限控制\n        auto-config：是否自动配置\n                        设置为true时框架会提供默认的一些配置，例如提供默认的登录页面、登出处理等\n                        设置为false时需要显示提供登录表单配置，否则会报错\n        use-expressions：用于指定intercept-url中的access属性是否使用表达式来描述权限\n    --\x3e\n    <security:http auto-config="true" use-expressions="true">\n        \x3c!--\n            intercept-url：定义一个拦截规则\n            pattern：对哪些url进行权限控制   /**表示拦截所有请求  /*只能 拦截/a.html /b.html 无法逻辑 /a/b/c.html\n            access：在请求对应的url时需要什么权限，默认配置时它应该是一个以逗号分隔的角色列表，\n                  请求的用户只需拥有其中的一个角色就能成功访问对应的url\n                  指定所需的访问角色或者访问权限   hasrole()为表达式  如果表达式没有开启则值直接为角色名称\n        --\x3e\n        <security:intercept-url pattern="/**"  access="hasrole(\'role_admin\')" />\n    </security:http>\n\n    \x3c!--\n        authentication-manager：认证管理器，用于处理认证操作\n    --\x3e\n    <security:authentication-manager>\n        \x3c!--\n            authentication-provider：认证提供者，执行具体的认证逻辑\n        --\x3e\n        <security:authentication-provider>\n            \x3c!--\n                user-service：用于获取用户信息，提供给authentication-provider进行认证\n            --\x3e\n            <security:user-service>\n                \x3c!--\n                    user：定义用户信息，可以指定用户名、密码、角色，后期可以改为从数据库查询用户信息\n                  {noop}：表示当前使用的密码为明文\n                --\x3e\n                <security:user name="admin" password="{noop}1234" authorities="role_admin"/>\n            </security:user-service>\n        </security:authentication-provider>\n    </security:authentication-manager>\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n\n# 配置可匿名访问的资源\n\n第一步：在项目中创建pages目录，在pages目录中创建a.html和b.html\n\n第二步：在spring-security.xml文件中配置，指定哪些资源可以匿名访问\n\n    \x3c!--    配置资源匿名访问\n\t http：用于定义相关权限控制\n  指定哪些资源不需要进行权限校验，可以使用通配符\n--\x3e\n    <security:http security="none" pattern="/pages/a.html"/>\n    <security:http security="none" pattern="/pages/b.html"/>\n    <security:http security="none" pattern="/pages/**"/>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 指定登陆页面\n\nspring security 框架默认提供了登陆页面给我们,我们需要自定义为自己的登陆页面\n\n 1. 配置匿名访问\n    \n    <security:http security="none" pattern="/login.html" />\n    \n    \n    1\n    \n\n 2. 修改spring-security.xml文件，加入表单登录信息的配置 为security:http子标签\n    \n      \x3c!--\n      form-login：定义表单登录信息\n      login-page:登陆页面\n      username-parameter:用户输入框对应的name\n      password-parameter:密码输入框对应的name\n      login-processing-url:提交地址\n      default-target-url:成功后默认跳转地址\n      authentication-failure-url:失败后跳转页面\n    --\x3e\n            <security:form-login login-page="/login.html"\n                                 username-parameter="username"\n                                 password-parameter="password"\n                                 login-processing-url="/login.do"\n                                 default-target-url="/index.html"\n                                 authentication-failure-url="/login.html"/>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n 3. 修改spring-security.xml文件，关闭csrffilter过滤器 为security:http子标签\n    \n    \x3c!--\n      csrf：对应csrffilter过滤器\n      disabled：是否启用csrffilter过滤器，如果使用自定义登录页面需要关闭此项，否则登录操作会被禁用（403）\n    --\x3e\n    <security:csrf disabled="true"></security:csrf>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n\n# 从数据库查询用户信息\n\n实现userdetailsservice接口 实现loaduserbyusername方法 获取到用户名\n\npackage com.itheima.service;\n\nimport com.itheima.pojo.user;\nimport org.springframework.security.core.grantedauthority;\nimport org.springframework.security.core.authority.simplegrantedauthority;\nimport org.springframework.security.core.userdetails.userdetails;\nimport org.springframework.security.core.userdetails.userdetailsservice;\nimport org.springframework.security.core.userdetails.usernamenotfoundexception;\n\nimport java.util.arraylist;\nimport java.util.hashmap;\nimport java.util.map;\n\npublic class springsecurityuserservice implements userdetailsservice {\n    //模拟数据库中的用户数据\n    public static map<string, user> map = new hashmap<>();\n\n    static {\n        com.itheima.pojo.user user1 = new com.itheima.pojo.user();\n        user1.setusername("admin");\n        user1.setpassword("admin");\n\n        com.itheima.pojo.user user2 = new com.itheima.pojo.user();\n        user2.setusername("xiaoming");\n        user2.setpassword("1234");\n\n        map.put(user1.getusername(), user1);\n        map.put(user2.getusername(), user2);\n    }\n\n\n    //根据用户名查询用户信息\n    @override\n    public userdetails loaduserbyusername(string s) throws usernamenotfoundexception {\n        system.out.println(s);\n        //根据用户名查询数据库获取用户信息(包含数据库中的信息)\n        user user = map.get(s);\n        if (user == null) {\n            //用户名不存在\n            return null;\n        }\n\n        //将用户信息返回给框架\n        //框架会进行密码比对(页面与数据库中查询的密码对比)\n        arraylist<grantedauthority> list = new arraylist<>();\n        //为当前登录用户授权 后期改为数据库中对应的权限\n        list.add(new simplegrantedauthority("permission_a"));//授权\n        list.add(new simplegrantedauthority("permission_b"));\n        list.add(new simplegrantedauthority("role_admin"));//授予角色\n        org.springframework.security.core.userdetails.user security = new org.springframework.security.core.userdetails.user(s, "{noop}" + user.getpassword(), list);\n\n        //返回\n        return security;\n\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\nbean 绑定\n\n \x3c!--\n        authentication-manager：认证管理器，用于处理认证操作\n    --\x3e\n    <security:authentication-manager>\n        \x3c!--\n            authentication-provider：认证提供者，执行具体的认证逻辑\n\t\t\tuser-service-ref 为user实现类的beanid\n        --\x3e\n        <security:authentication-provider user-service-ref="userservice">\n        </security:authentication-provider>\n    </security:authentication-manager>\n\n    <bean id="userservice" class="com.itheima.service.springsecurityuserservice"></bean>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 密码加密\n\n前面我们使用的密码都是明文的，这是非常不安全的。一般情况下用户的密码需要进行加密后再保存到数据库中。\n\n常见的密码加密方式有：\n\n3des、aes、des：使用对称加密算法，可以通过解密来还原出原始密码\n\nmd5、sha1：使用单向hash算法，无法通过计算还原出原始密码，但是可以建立彩虹表进行查表破解\n\nbcrypt：将salt随机并混入最终加密后的密码，验证时也无需单独提供之前的salt，从而无需单独处理salt问题\n\n 1. 在spring-security.xml文件中指定密码加密对象\n    \n        <security:authentication-manager>\n    \t\t\t\x3c!--追加 --\x3e\n                <security:password-encoder ref="passwordencoder"/>\n            </security:authentication-provider>\n        </security:authentication-manager>\n    \n    \n    \x3c!--配置密码加密对象--\x3e\n    <bean id="passwordencoder" \n          class="org.springframework.security.crypto.bcrypt.bcryptpasswordencoder" />\n    \n    \x3c!--认证管理器，用于处理认证操作--\x3e\n    <security:authentication-manager>\n      \x3c!--认证提供者，执行具体的认证逻辑--\x3e\n      <security:authentication-provider user-service-ref="userservice">\n        \x3c!--指定密码加密策略--\x3e\n        <security:password-encoder ref="passwordencoder" />\n      </security:authentication-provider>\n    </security:authentication-manager>\n    \x3c!--开启spring注解使用--\x3e\n    <context:annotation-config></context:annotation-config>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n 2. 修改userservice实现类\n    \n    package com.itheima.service;\n    \n    import com.itheima.pojo.user;\n    import org.springframework.beans.factory.annotation.autowired;\n    import org.springframework.security.core.grantedauthority;\n    import org.springframework.security.core.authority.simplegrantedauthority;\n    import org.springframework.security.core.userdetails.userdetails;\n    import org.springframework.security.core.userdetails.userdetailsservice;\n    import org.springframework.security.core.userdetails.usernamenotfoundexception;\n    import org.springframework.security.crypto.bcrypt.bcryptpasswordencoder;\n    \n    import java.util.arraylist;\n    import java.util.hashmap;\n    import java.util.map;\n    \n    public class springsecurityuserservice2 implements userdetailsservice {\n        @autowired\n        private bcryptpasswordencoder passwordencoder;\n    \n        //模拟数据库中的用户数据\n        public map<string, user> map = new hashmap<>();\n    \n        public void inituserdate() {\n            user user1 = new user();\n            user1.setusername("admin");\n            user1.setpassword(passwordencoder.encode("admin"));  //使用bcrypt进行加密\n    \n            user user2 = new user();\n            user2.setusername("xiaoming");\n            user2.setpassword(passwordencoder.encode("1234"));\n    \n            map.put(user1.getusername(), user1);\n            map.put(user2.getusername(), user2);\n        }\n    \n    \n        //根据用户名查询用户信息\n        @override\n        public userdetails loaduserbyusername(string s) throws usernamenotfoundexception {\n            inituserdate();\n            system.out.println(s);\n            //根据用户名查询数据库获取用户信息(包含数据库中的信息)\n            user user = map.get(s);\n            if (user == null) {\n                //用户名不存在\n                return null;\n            }\n    \n            //将用户信息返回给框架\n            //框架会进行密码比对(页面与数据库中查询的密码对比)\n            arraylist<grantedauthority> list = new arraylist<>();\n            //为当前登录用户授权 后期改为数据库中对应的权限\n            list.add(new simplegrantedauthority("permission_a"));//授权\n            list.add(new simplegrantedauthority("permission_b"));\n            list.add(new simplegrantedauthority("role_admin"));//授予角色\n            org.springframework.security.core.userdetails.user security = new org.springframework.security.core.userdetails.user(s, user.getpassword(), list);\n    \n            //返回\n            return security;\n    \n        }\n    }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    \n\n\n# 配置多种校验规则\n\n修改spring-security.xml文件：\n\n\x3c!--只要认证通过就可以访问--\x3e\n<security:intercept-url pattern="/index.jsp"  access="isauthenticated()" />\n<security:intercept-url pattern="/a.html"  access="isauthenticated()" />\n\n\x3c!--拥有add权限就可以访问b.html页面--\x3e\n<security:intercept-url pattern="/b.html"  access="hasauthority(\'add\')" />\n\n\x3c!--拥有role_admin角色就可以访问c.html页面--\x3e\n<security:intercept-url pattern="/c.html"  access="hasrole(\'role_admin\')" />\n\n\x3c!--拥有role_admin角色就可以访问d.html页面，\n    注意：此处虽然写的是admin角色，框架会自动加上前缀role_--\x3e\n<security:intercept-url pattern="/d.html"  access="hasrole(\'admin\')" />\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 注解权限控制\n\n 1. 在spring-security.xml文件中配置组件扫描，用于扫描controller\n    \n    <mvc:annotation-driven></mvc:annotation-driven>\n    <context:component-scan base-package="com.itheima.controller"></context:component-scan>\n    \n    \n    1\n    2\n    \n\n 2. 在spring-security.xml文件中开启权限注解支持\n    \n    \x3c!--开启注解方式权限控制--\x3e\n    <security:global-method-security pre-post-annotations="enabled" />\n    \n    \n    1\n    2\n    \n\n 3. 创建controller类并在controller的方法上加入注解进行权限控制 @preauthorize()\n    \n    package com.itheima.controller;\n    \n    import org.springframework.security.access.prepost.preauthorize;\n    import org.springframework.web.bind.annotation.restcontroller;\n    import org.springframework.web.bind.annotation.requestmapping;\n    \n    @restcontroller\n    @requestmapping("/hello")\n    public class hellocontroller {\n        @requestmapping("/add")\n        @preauthorize("hasauthority(\'add\')")//表示用户必须拥有add权限才能调用当前方法\n        public string add(){\n            system.out.println("add...");\n            return "success";\n        }\n    \n        @requestmapping("/delete")\n        @preauthorize("hasrole(\'role_admin\')")//表示用户必须拥有role_admin角色才能调用当前方法\n        public string delete(){\n            system.out.println("delete...");\n            return "success";\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n\n\n# 退出登陆\n\n在spring-security.xml文件 配置 为security:http子标签\n\n\x3c!--\n  logout：退出登录\n  logout-url：退出登录操作对应的请求路径\n  logout-success-url：退出登录后的跳转页面\n--\x3e\n<security:logout logout-url="/logout.do" \n                 logout-success-url="/login.html" invalidate-session="true"/>\n\n\n1\n2\n3\n4\n5\n6\n7\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Zookeeper",frontmatter:{title:"Zookeeper",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/7bc195/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/11.Zookeeper.html",relativePath:"后端/02.JavaEE/11.Zookeeper.md",key:"v-43b009de",path:"/pages/7bc195/",headers:[{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:113},{level:2,title:"数据模型",slug:"数据模型",normalizedTitle:"数据模型",charIndex:429},{level:2,title:"服务端常用命令",slug:"服务端常用命令",normalizedTitle:"服务端常用命令",charIndex:440},{level:2,title:"客户端常用命令",slug:"客户端常用命令",normalizedTitle:"客户端常用命令",charIndex:570},{level:2,title:"Curator",slug:"curator",normalizedTitle:"curator",charIndex:846},{level:3,title:"获取客户端",slug:"获取客户端",normalizedTitle:"获取客户端",charIndex:2488},{level:3,title:"关闭客户端",slug:"关闭客户端",normalizedTitle:"关闭客户端",charIndex:3149},{level:3,title:"创建节点",slug:"创建节点",normalizedTitle:"创建节点",charIndex:3182},{level:3,title:"查询节点",slug:"查询节点",normalizedTitle:"查询节点",charIndex:3635},{level:3,title:"修改节点",slug:"修改节点",normalizedTitle:"修改节点",charIndex:4002},{level:3,title:"删除节点",slug:"删除节点",normalizedTitle:"删除节点",charIndex:4411},{level:3,title:"Watch事件监听",slug:"watch事件监听",normalizedTitle:"watch事件监听",charIndex:4924},{level:3,title:"NodeCache",slug:"nodecache",normalizedTitle:"nodecache",charIndex:5141},{level:3,title:"PathChildrenCache",slug:"pathchildrencache",normalizedTitle:"pathchildrencache",charIndex:5167},{level:3,title:"TreeCache",slug:"treecache",normalizedTitle:"treecache",charIndex:5202},{level:2,title:"分布式锁",slug:"分布式锁",normalizedTitle:"分布式锁",charIndex:99},{level:3,title:"Zookeeper分布式锁原理",slug:"zookeeper分布式锁原理",normalizedTitle:"zookeeper分布式锁原理",charIndex:7330},{level:2,title:"集群搭建",slug:"集群搭建",normalizedTitle:"集群搭建",charIndex:8430},{level:3,title:"Leader选举",slug:"leader选举",normalizedTitle:"leader选举",charIndex:9046},{level:2,title:"模拟集群异常",slug:"模拟集群异常",normalizedTitle:"模拟集群异常",charIndex:9172},{level:2,title:"集群角色",slug:"集群角色",normalizedTitle:"集群角色",charIndex:9438}],headersStr:"安装 数据模型 服务端常用命令 客户端常用命令 Curator 获取客户端 关闭客户端 创建节点 查询节点 修改节点 删除节点 Watch事件监听 NodeCache PathChildrenCache TreeCache 分布式锁 Zookeeper分布式锁原理 集群搭建 Leader选举 模拟集群异常 集群角色",content:'# Zookeeper\n\nZookeeper 是 Apache Hadoop 项目下的一个子项目 , 是一个树形目录服务\n\n是一个分布式 开源的分布式应用程序的协调服务\n\n主要功能包括: 配置管理 分布式锁 集群管理\n\n\n# 安装\n\ntar -zxvf /opt/software/apache-zookeeper-3.5.6-bin.tar.gz -C /opt/\nmv /opt/apache-zookeeper-3.5.6-bin /opt/zookeeper\nmkdir zkdata\ncd /opt/zookeeper/conf\ncp zoo_sample.cfg zoo.cfg\n\nvim zoo.cfg\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n修改存储路径\n\ndataDir=/opt/zookeeper/zkdata\n\n\n1\n\n\n启动\n\ncd /opt/zookeeper/bin/\n./zkServer.sh start\n\n\n1\n2\n\n\n\n# 数据模型\n\n\n\n\n# 服务端常用命令\n\n#启动\n./zkServer.sh start\n#状态\n./zkServer.sh status\n#停止\n./zkServer.sh stop\n#重启\n./zkServer.sh restart\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 客户端常用命令\n\n#连接  如果是本机直接忽略后面的参数 忽略连接必须为默认端口2181\n./zkCli.sh -server localhost:2181\n\n\n1\n2\n\n * create [节点 -e -s -es] 路径 [内容] 创建文件\n * get [节点] 路径 获取文件内容\n * ls 路径 查看路径下的所有文件\n * set [节点] 路径 内容 修改文件内容\n * delete [节点] 路径 删除文件\n * deleteall [节点] 路径 删除该文件夹下的所有文件\n * quit 退出\n * help 帮助\n\n\n\n\n# Curator\n\nZooKeeper 客户端库\n\n * 原生Java Api\n * ZkClient\n * Curator\n\nhttps://curator.apache.org/\n\n坐标\n\n<dependencies>\n\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.10</version>\n            <scope>test</scope>\n        </dependency>\n\n        \x3c!--curator--\x3e\n        <dependency>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-framework</artifactId>\n            <version>4.0.0</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.apache.curator</groupId>\n            <artifactId>curator-recipes</artifactId>\n            <version>4.0.0</version>\n        </dependency>\n        \x3c!--日志--\x3e\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n            <version>1.7.21</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n            <version>1.7.21</version>\n        </dependency>\n\n    </dependencies>\n\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.1</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 获取客户端\n\n        //第一种方式\n        ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(3000,10);\n        //多个服务端 connectString参数 地址用逗号隔开  连接超时时间  会话超时时间  重试策略\n        CuratorFramework client = CuratorFrameworkFactory.newClient("192.168.130.124:2181",\n                60 * 1000, 15 * 1000, retryPolicy);\n        client.start();  //开启连接\n        client.close();\n        //第二种方式  namespace为设置根节点路径\n        client2 = CuratorFrameworkFactory.builder().connectString("192.168.130.124:2181").sessionTimeoutMs(60 * 1000).connectionTimeoutMs(15 * 1000).retryPolicy(retryPolicy).namespace("itheima").build();\n        client2.start();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 关闭客户端\n\nclient2.close();\n\n\n1\n\n\n\n# 创建节点\n\n//创建节点  节点默持久化的节点   内容默认将当前客户端ip作为内容\nString path = client2.create().forPath("/app1");\nSystem.out.println(path);\n//带内容创建\nclient2.create().forPath("/app2","hello".getBytes(StandardCharsets.UTF_8));\n//设置节点类型  使用withMode方法 值为一个枚举值\nclient2.create().withMode(CreateMode.EPHEMERAL).forPath("/app3");\n//创建多级节点  creatingParentContainersIfNeeded如果父节点不存在则自动创建\nclient2.create().creatingParentContainersIfNeeded().forPath("/app4/p1");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 查询节点\n\n//查询节点\nbyte[] bytes = client2.getData().forPath("/app1");\nSystem.out.println(new String(bytes));\n\n//查询子节点\nList<String> path = client2.getChildren().forPath("/app4");\nSystem.out.println(path);\n\n//查询节点状态信息  ls -s  需要一个Stat类\nStat stat =new Stat();\nclient2.getData().storingStatIn(stat).forPath("/app1");\nSystem.out.println(stat);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 修改节点\n\n//修改内容\nclient2.setData().forPath("/app1","hello".getBytes(StandardCharsets.UTF_8));\n//根据版本修改\nStat stat =new Stat();\nclient2.getData().storingStatIn(stat).forPath("/app1");\nSystem.out.println(stat);\nint version = stat.getVersion();\n//withVersion  查询版本与之前版本是否相同 相同则修改 不相同则不执行\nclient2.setData().withVersion(version).forPath("/app1","hello world".getBytes(StandardCharsets.UTF_8));\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 删除节点\n\n//删除节点\nclient2.delete().forPath("/app1");\n//删除带有子节点的节点\nclient2.delete().deletingChildrenIfNeeded().forPath("/app4");\n//必须删除成功\nclient2.delete().guaranteed().forPath("/app2");\n//回调\nclient2.delete().guaranteed().inBackground(new BackgroundCallback() {\n    @Override\n    public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception {\n        //curatorEvent 存储删除后的信息 如路径 是否成功\n        System.out.println(curatorEvent);\n    }\n});\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# Watch事件监听\n\nZooKeeper 允许用户在指定节点上注册一些Watcher,并且在一些特定事情触发的时候,ZooKeeper服务端会将事件通知到感兴趣的客户端上去,该机制是ZooKeeper实现分布式协调服务的重要特征\n\nWatch机制来实现了发布/订阅功能,能够让多个订阅者同时监听某一个对象,当一个对象自身状态变化时,会通知所有订阅者\n\nCurator引入了Cache来实现对ZooKeeper服务端事件的监听\n\n * NodeCache 只是监听某一个特点的节点\n * PathChildrenCache 监控一个ZNdoe的子节点\n * TreeCache 可以监控整个树上的所有节点 类似于NodeCache 和 PathChildrenCache 结合\n\n\n# NodeCache\n\n//创建NodeCache对象\nfinal NodeCache nodeCache = new NodeCache(client2, "/app1");\n//注册监听\nnodeCache.getListenable().addListener(new NodeCacheListener() {\n    @Override\n    public void nodeChanged() throws Exception {\n        System.out.println("节点发生变化");\n        //获取修改节点后的数据\n        byte[] data = nodeCache.getCurrentData().getData();\n        System.out.println(new String(data));\n    }\n});\n\n//开启监听  如果为true 则开启监听时,加载缓冲数据\nnodeCache.start(true);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n发生增删改操作都会触发监听 并且当此方法(线程)结束后不再监听 一般我们持续让此线程存活\n\n\n# PathChildrenCache\n\n//创建监听器\nPathChildrenCache pathChildrenCache = new PathChildrenCache(client2,"/app2",true);\npathChildrenCache.getListenable().addListener(new PathChildrenCacheListener() {\n    @Override\n    public void childEvent(CuratorFramework curatorFramework, PathChildrenCacheEvent pathChildrenCacheEvent) throws Exception {\n        System.out.println("子节点发生变化");\n        System.out.println(pathChildrenCacheEvent);\n        //监听子节点的数据变更,并获取到对应的数据\n        //1.获取类型\n        PathChildrenCacheEvent.Type type = pathChildrenCacheEvent.getType();\n        //判断类型\n        if (type.equals(PathChildrenCacheEvent.Type.CHILD_UPDATED)){\n            byte[] data = pathChildrenCacheEvent.getData().getData();\n            System.out.println(new String(data));\n        }\n\n    }\n});\npathChildrenCache.start();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n当发生连接时 childEvent会执行一次 我们可以通过type来判断\n\n\n# TreeCache\n\n//创建监听器\nTreeCache treeCache = new TreeCache(client2, "/app2");\ntreeCache.getListenable().addListener(new TreeCacheListener() {\n    @Override\n    public void childEvent(CuratorFramework curatorFramework, TreeCacheEvent treeCacheEvent) throws Exception {\n        System.out.println("节点发生变化");\n        byte[] data = treeCacheEvent.getData().getData();\n        System.out.println(new String(data));\n    }\n});\ntreeCache.start();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 分布式锁\n\n在以前我们涉及并发同步的时候,我们往往采用synchronized或者Lock的方式来解决多线程间的代码同步问题,这时多线程的运行都是在同一个JVM之下\n\n我们分布式集群工作时,属于多JVM下的工作环境,跨JVM之间已经无法通过多线程的锁解决同步问题\n\n分布式锁------处理跨机器的进程之间的数据同步问题\n\n\n# Zookeeper分布式锁原理\n\n核心思想: 当客户端要获取锁,则创建节点,使用完锁,则删除该节点\n\n 1. 客户端获取锁时,在lock节点下创建临时顺序节点\n 2. 然后获取lock下面的所有子节点,客户端获取到所有的子节点之后,如果发现自己创建的子节点序号最小,那么就任务该客户端获取到了锁.使用完锁后,将该节点删除\n 3. 如果发现自己创建的节点并非lock所有子节点中最小的,说明自己还没有获取到锁,此时客户端需要找到比自己小的那个节点,同时对其注册事件监听器,监听删除事件\n 4. 如果发现比自己小的那个节点被删除,则客户端的Watcher会收到相应的通知,此时再次判断自己创建的节点是否是lock子节点中序号最小的,如果是则获取到了锁,如果不是则重复以上步骤继续获取到比自己小的一个节点并注册监听\n\n# Curator 实现分布式锁\n\n在Curator中有五种锁方案\n\n * InterProcessSemaphoreMutex 分布式排它锁(非可重入锁)\n * InterProcessMutex 分布式可重入排它锁\n * InterProcessReadWriteLock 分布式读写锁\n * InterProcessMultiLock 将多个锁作为单个实体管理的容器\n * InterProcessSemaphoreV2 共享信号量\n\n获取锁对象\n\nprivate InterProcessMutex lock;\n\npublic Ticket() {\n    ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(3000, 10);\n    CuratorFramework client = CuratorFrameworkFactory.builder().connectString("192.168.130.124:2181").sessionTimeoutMs(60 * 1000).connectionTimeoutMs(15 * 1000).retryPolicy(retryPolicy).namespace("itheima").build();\n    client.start();\n    lock = new InterProcessMutex(client, "lock");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n获取锁\n\nlock.acquire(3, TimeUnit.SECONDS);\n\n\n1\n\n\n释放锁\n\nlock.release();\n\n\n1\n\n\n\n# 集群搭建\n\n先安装ZooKeeper\n\n在每个ZooKeeper的data目录下创建一个myid文件,内容分别是1 2 3 这个文件就是记录每个服务器的ID\n\n#1 \necho "1" > /opt/zookeeper/zkdata/myid\n\n#2\necho "2" > /opt/zookeeper/zkdata/myid\n\n#3\necho "2" > /opt/zookeeper/zkdata/myid\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n配置每一台的ZooKeeper 的 zoo.cfg\n\nvim /opt/zookeeper/conf/zoo.cfg\n\n\n1\n\n\n添加集群地址 ip和端口根据配置修改 .后面为 设置的myid\n\nserver.1=realtime-1:2888:3888\nserver.2=realtime-2:2888:3888\nserver.3=realtime-3:2888:3888\n\n\n1\n2\n3\n\n\nserver.服务器ID=服务器IP地址:服务器之间通讯端口:服务器之间投票选举端口\n\n启动集群\n\n#1\n./opt/zookeeper/bin/zkServer.sh start\n#2\n./opt/zookeeper/bin/zkServer.sh start\n#3\n./opt/zookeeper/bin/zkServer.sh start\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Leader选举\n\nServerid:服务器id 编号越大在选择算法的权重越大\n\nZxid: 数据id 值越大数据越新 在选举中数据越新 权重越大\n\n如果某台ZooKeeper获得超过半数的选票 则此ZooKeeper就可以成为Leader了\n\n\n# 模拟集群异常\n\n关闭3号集群\n\n./opt/zookeeper/bin/zkServer.sh stop\n\n\n1\n\n\n1号和2号状态没有变化\n\n再关闭1号机器\n\n./opt/zookeeper/bin/zkServer.sh stop\n\n\n1\n\n\n此时只有2号机器 并没有关闭 但处于休眠状态\n\n再把1号机器重新启动\n\n./opt/zookeeper/bin/zkServer.sh start\n\n\n1\n\n\n2号机器重新运行,\n\n再把3号重新开启, 然后停掉2号\n\n此时重新选举leader 3号超过半数票成为leader\n\n\n# 集群角色\n\n * Leader 领导者\n   1. 处理事务请求\n   2. 集群内部各服务器的调度者\n * Follower 跟随者\n   1. 处理客户端非事务请求,转发事务请求给Leader服务器\n   2. 参与Leader选举投票\n * Observer 观察者\n   1. 处理客户端非事务请求,转发事务请求给Leader服务器\n\n',normalizedContent:'# zookeeper\n\nzookeeper 是 apache hadoop 项目下的一个子项目 , 是一个树形目录服务\n\n是一个分布式 开源的分布式应用程序的协调服务\n\n主要功能包括: 配置管理 分布式锁 集群管理\n\n\n# 安装\n\ntar -zxvf /opt/software/apache-zookeeper-3.5.6-bin.tar.gz -c /opt/\nmv /opt/apache-zookeeper-3.5.6-bin /opt/zookeeper\nmkdir zkdata\ncd /opt/zookeeper/conf\ncp zoo_sample.cfg zoo.cfg\n\nvim zoo.cfg\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n修改存储路径\n\ndatadir=/opt/zookeeper/zkdata\n\n\n1\n\n\n启动\n\ncd /opt/zookeeper/bin/\n./zkserver.sh start\n\n\n1\n2\n\n\n\n# 数据模型\n\n\n\n\n# 服务端常用命令\n\n#启动\n./zkserver.sh start\n#状态\n./zkserver.sh status\n#停止\n./zkserver.sh stop\n#重启\n./zkserver.sh restart\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 客户端常用命令\n\n#连接  如果是本机直接忽略后面的参数 忽略连接必须为默认端口2181\n./zkcli.sh -server localhost:2181\n\n\n1\n2\n\n * create [节点 -e -s -es] 路径 [内容] 创建文件\n * get [节点] 路径 获取文件内容\n * ls 路径 查看路径下的所有文件\n * set [节点] 路径 内容 修改文件内容\n * delete [节点] 路径 删除文件\n * deleteall [节点] 路径 删除该文件夹下的所有文件\n * quit 退出\n * help 帮助\n\n\n\n\n# curator\n\nzookeeper 客户端库\n\n * 原生java api\n * zkclient\n * curator\n\nhttps://curator.apache.org/\n\n坐标\n\n<dependencies>\n\n        <dependency>\n            <groupid>junit</groupid>\n            <artifactid>junit</artifactid>\n            <version>4.10</version>\n            <scope>test</scope>\n        </dependency>\n\n        \x3c!--curator--\x3e\n        <dependency>\n            <groupid>org.apache.curator</groupid>\n            <artifactid>curator-framework</artifactid>\n            <version>4.0.0</version>\n        </dependency>\n\n        <dependency>\n            <groupid>org.apache.curator</groupid>\n            <artifactid>curator-recipes</artifactid>\n            <version>4.0.0</version>\n        </dependency>\n        \x3c!--日志--\x3e\n        <dependency>\n            <groupid>org.slf4j</groupid>\n            <artifactid>slf4j-api</artifactid>\n            <version>1.7.21</version>\n        </dependency>\n\n        <dependency>\n            <groupid>org.slf4j</groupid>\n            <artifactid>slf4j-log4j12</artifactid>\n            <version>1.7.21</version>\n        </dependency>\n\n    </dependencies>\n\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.apache.maven.plugins</groupid>\n                <artifactid>maven-compiler-plugin</artifactid>\n                <version>3.1</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 获取客户端\n\n        //第一种方式\n        exponentialbackoffretry retrypolicy = new exponentialbackoffretry(3000,10);\n        //多个服务端 connectstring参数 地址用逗号隔开  连接超时时间  会话超时时间  重试策略\n        curatorframework client = curatorframeworkfactory.newclient("192.168.130.124:2181",\n                60 * 1000, 15 * 1000, retrypolicy);\n        client.start();  //开启连接\n        client.close();\n        //第二种方式  namespace为设置根节点路径\n        client2 = curatorframeworkfactory.builder().connectstring("192.168.130.124:2181").sessiontimeoutms(60 * 1000).connectiontimeoutms(15 * 1000).retrypolicy(retrypolicy).namespace("itheima").build();\n        client2.start();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 关闭客户端\n\nclient2.close();\n\n\n1\n\n\n\n# 创建节点\n\n//创建节点  节点默持久化的节点   内容默认将当前客户端ip作为内容\nstring path = client2.create().forpath("/app1");\nsystem.out.println(path);\n//带内容创建\nclient2.create().forpath("/app2","hello".getbytes(standardcharsets.utf_8));\n//设置节点类型  使用withmode方法 值为一个枚举值\nclient2.create().withmode(createmode.ephemeral).forpath("/app3");\n//创建多级节点  creatingparentcontainersifneeded如果父节点不存在则自动创建\nclient2.create().creatingparentcontainersifneeded().forpath("/app4/p1");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 查询节点\n\n//查询节点\nbyte[] bytes = client2.getdata().forpath("/app1");\nsystem.out.println(new string(bytes));\n\n//查询子节点\nlist<string> path = client2.getchildren().forpath("/app4");\nsystem.out.println(path);\n\n//查询节点状态信息  ls -s  需要一个stat类\nstat stat =new stat();\nclient2.getdata().storingstatin(stat).forpath("/app1");\nsystem.out.println(stat);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 修改节点\n\n//修改内容\nclient2.setdata().forpath("/app1","hello".getbytes(standardcharsets.utf_8));\n//根据版本修改\nstat stat =new stat();\nclient2.getdata().storingstatin(stat).forpath("/app1");\nsystem.out.println(stat);\nint version = stat.getversion();\n//withversion  查询版本与之前版本是否相同 相同则修改 不相同则不执行\nclient2.setdata().withversion(version).forpath("/app1","hello world".getbytes(standardcharsets.utf_8));\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 删除节点\n\n//删除节点\nclient2.delete().forpath("/app1");\n//删除带有子节点的节点\nclient2.delete().deletingchildrenifneeded().forpath("/app4");\n//必须删除成功\nclient2.delete().guaranteed().forpath("/app2");\n//回调\nclient2.delete().guaranteed().inbackground(new backgroundcallback() {\n    @override\n    public void processresult(curatorframework curatorframework, curatorevent curatorevent) throws exception {\n        //curatorevent 存储删除后的信息 如路径 是否成功\n        system.out.println(curatorevent);\n    }\n});\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# watch事件监听\n\nzookeeper 允许用户在指定节点上注册一些watcher,并且在一些特定事情触发的时候,zookeeper服务端会将事件通知到感兴趣的客户端上去,该机制是zookeeper实现分布式协调服务的重要特征\n\nwatch机制来实现了发布/订阅功能,能够让多个订阅者同时监听某一个对象,当一个对象自身状态变化时,会通知所有订阅者\n\ncurator引入了cache来实现对zookeeper服务端事件的监听\n\n * nodecache 只是监听某一个特点的节点\n * pathchildrencache 监控一个zndoe的子节点\n * treecache 可以监控整个树上的所有节点 类似于nodecache 和 pathchildrencache 结合\n\n\n# nodecache\n\n//创建nodecache对象\nfinal nodecache nodecache = new nodecache(client2, "/app1");\n//注册监听\nnodecache.getlistenable().addlistener(new nodecachelistener() {\n    @override\n    public void nodechanged() throws exception {\n        system.out.println("节点发生变化");\n        //获取修改节点后的数据\n        byte[] data = nodecache.getcurrentdata().getdata();\n        system.out.println(new string(data));\n    }\n});\n\n//开启监听  如果为true 则开启监听时,加载缓冲数据\nnodecache.start(true);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n发生增删改操作都会触发监听 并且当此方法(线程)结束后不再监听 一般我们持续让此线程存活\n\n\n# pathchildrencache\n\n//创建监听器\npathchildrencache pathchildrencache = new pathchildrencache(client2,"/app2",true);\npathchildrencache.getlistenable().addlistener(new pathchildrencachelistener() {\n    @override\n    public void childevent(curatorframework curatorframework, pathchildrencacheevent pathchildrencacheevent) throws exception {\n        system.out.println("子节点发生变化");\n        system.out.println(pathchildrencacheevent);\n        //监听子节点的数据变更,并获取到对应的数据\n        //1.获取类型\n        pathchildrencacheevent.type type = pathchildrencacheevent.gettype();\n        //判断类型\n        if (type.equals(pathchildrencacheevent.type.child_updated)){\n            byte[] data = pathchildrencacheevent.getdata().getdata();\n            system.out.println(new string(data));\n        }\n\n    }\n});\npathchildrencache.start();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n当发生连接时 childevent会执行一次 我们可以通过type来判断\n\n\n# treecache\n\n//创建监听器\ntreecache treecache = new treecache(client2, "/app2");\ntreecache.getlistenable().addlistener(new treecachelistener() {\n    @override\n    public void childevent(curatorframework curatorframework, treecacheevent treecacheevent) throws exception {\n        system.out.println("节点发生变化");\n        byte[] data = treecacheevent.getdata().getdata();\n        system.out.println(new string(data));\n    }\n});\ntreecache.start();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 分布式锁\n\n在以前我们涉及并发同步的时候,我们往往采用synchronized或者lock的方式来解决多线程间的代码同步问题,这时多线程的运行都是在同一个jvm之下\n\n我们分布式集群工作时,属于多jvm下的工作环境,跨jvm之间已经无法通过多线程的锁解决同步问题\n\n分布式锁------处理跨机器的进程之间的数据同步问题\n\n\n# zookeeper分布式锁原理\n\n核心思想: 当客户端要获取锁,则创建节点,使用完锁,则删除该节点\n\n 1. 客户端获取锁时,在lock节点下创建临时顺序节点\n 2. 然后获取lock下面的所有子节点,客户端获取到所有的子节点之后,如果发现自己创建的子节点序号最小,那么就任务该客户端获取到了锁.使用完锁后,将该节点删除\n 3. 如果发现自己创建的节点并非lock所有子节点中最小的,说明自己还没有获取到锁,此时客户端需要找到比自己小的那个节点,同时对其注册事件监听器,监听删除事件\n 4. 如果发现比自己小的那个节点被删除,则客户端的watcher会收到相应的通知,此时再次判断自己创建的节点是否是lock子节点中序号最小的,如果是则获取到了锁,如果不是则重复以上步骤继续获取到比自己小的一个节点并注册监听\n\n# curator 实现分布式锁\n\n在curator中有五种锁方案\n\n * interprocesssemaphoremutex 分布式排它锁(非可重入锁)\n * interprocessmutex 分布式可重入排它锁\n * interprocessreadwritelock 分布式读写锁\n * interprocessmultilock 将多个锁作为单个实体管理的容器\n * interprocesssemaphorev2 共享信号量\n\n获取锁对象\n\nprivate interprocessmutex lock;\n\npublic ticket() {\n    exponentialbackoffretry retrypolicy = new exponentialbackoffretry(3000, 10);\n    curatorframework client = curatorframeworkfactory.builder().connectstring("192.168.130.124:2181").sessiontimeoutms(60 * 1000).connectiontimeoutms(15 * 1000).retrypolicy(retrypolicy).namespace("itheima").build();\n    client.start();\n    lock = new interprocessmutex(client, "lock");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n获取锁\n\nlock.acquire(3, timeunit.seconds);\n\n\n1\n\n\n释放锁\n\nlock.release();\n\n\n1\n\n\n\n# 集群搭建\n\n先安装zookeeper\n\n在每个zookeeper的data目录下创建一个myid文件,内容分别是1 2 3 这个文件就是记录每个服务器的id\n\n#1 \necho "1" > /opt/zookeeper/zkdata/myid\n\n#2\necho "2" > /opt/zookeeper/zkdata/myid\n\n#3\necho "2" > /opt/zookeeper/zkdata/myid\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n配置每一台的zookeeper 的 zoo.cfg\n\nvim /opt/zookeeper/conf/zoo.cfg\n\n\n1\n\n\n添加集群地址 ip和端口根据配置修改 .后面为 设置的myid\n\nserver.1=realtime-1:2888:3888\nserver.2=realtime-2:2888:3888\nserver.3=realtime-3:2888:3888\n\n\n1\n2\n3\n\n\nserver.服务器id=服务器ip地址:服务器之间通讯端口:服务器之间投票选举端口\n\n启动集群\n\n#1\n./opt/zookeeper/bin/zkserver.sh start\n#2\n./opt/zookeeper/bin/zkserver.sh start\n#3\n./opt/zookeeper/bin/zkserver.sh start\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# leader选举\n\nserverid:服务器id 编号越大在选择算法的权重越大\n\nzxid: 数据id 值越大数据越新 在选举中数据越新 权重越大\n\n如果某台zookeeper获得超过半数的选票 则此zookeeper就可以成为leader了\n\n\n# 模拟集群异常\n\n关闭3号集群\n\n./opt/zookeeper/bin/zkserver.sh stop\n\n\n1\n\n\n1号和2号状态没有变化\n\n再关闭1号机器\n\n./opt/zookeeper/bin/zkserver.sh stop\n\n\n1\n\n\n此时只有2号机器 并没有关闭 但处于休眠状态\n\n再把1号机器重新启动\n\n./opt/zookeeper/bin/zkserver.sh start\n\n\n1\n\n\n2号机器重新运行,\n\n再把3号重新开启, 然后停掉2号\n\n此时重新选举leader 3号超过半数票成为leader\n\n\n# 集群角色\n\n * leader 领导者\n   1. 处理事务请求\n   2. 集群内部各服务器的调度者\n * follower 跟随者\n   1. 处理客户端非事务请求,转发事务请求给leader服务器\n   2. 参与leader选举投票\n * observer 观察者\n   1. 处理客户端非事务请求,转发事务请求给leader服务器\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Spring Boot",frontmatter:{title:"Spring Boot",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/4f5fb3/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/13.Spring%20Boot.html",relativePath:"后端/02.JavaEE/13.Spring Boot.md",key:"v-16cba74e",path:"/pages/4f5fb3/",headers:[{level:2,title:"起步依赖原来分析",slug:"起步依赖原来分析",normalizedTitle:"起步依赖原来分析",charIndex:286},{level:2,title:"SpringBoot配置",slug:"springboot配置",normalizedTitle:"springboot配置",charIndex:464},{level:3,title:"YAML",slug:"yaml",normalizedTitle:"yaml",charIndex:884},{level:3,title:"数据格式",slug:"数据格式",normalizedTitle:"数据格式",charIndex:1112},{level:3,title:"读取配置文件内容",slug:"读取配置文件内容",normalizedTitle:"读取配置文件内容",charIndex:1630},{level:3,title:"profile",slug:"profile",normalizedTitle:"profile",charIndex:2936},{level:3,title:"内部配置加载顺序",slug:"内部配置加载顺序",normalizedTitle:"内部配置加载顺序",charIndex:3760},{level:3,title:"外部配置加载顺序",slug:"外部配置加载顺序",normalizedTitle:"外部配置加载顺序",charIndex:3933},{level:2,title:"SpringBoot整合其他框架",slug:"springboot整合其他框架",normalizedTitle:"springboot整合其他框架",charIndex:4194},{level:3,title:"Junit",slug:"junit",normalizedTitle:"junit",charIndex:4215},{level:3,title:"Redis",slug:"redis",normalizedTitle:"redis",charIndex:4535},{level:3,title:"MyBatis",slug:"mybatis",normalizedTitle:"mybatis",charIndex:5209}],headersStr:"起步依赖原来分析 SpringBoot配置 YAML 数据格式 读取配置文件内容 profile 内部配置加载顺序 外部配置加载顺序 SpringBoot整合其他框架 Junit Redis MyBatis",content:'# Spring Boot\n\n 1. 自动配置 Spring Boot的自动配置是一个运行时的过程,考虑了众多因素,Spring配置应该用哪个,不该用哪个.该过程是SpringBoot自动完成的\n 2. 起步依赖 起步依赖本质上是一个Maven项目对象模型,定义了对其他库的传递依赖,简单来说,就是将具备某种功能的坐标打包到一起,并提供一些默认的功能\n 3. 辅助功能 提供了一些大型项目中常见的非功能性特征,如嵌入式服务器 安全 指标 健康检测 外部配置等\n\nSpring Boot 并不是对Spring 功能上的增强 而是提供了一种快速使用Spring的方式\n\n\n# 起步依赖原来分析\n\n在Spring-boot-starter-parent中定义了各种技术的版本信息,组合了一套最优搭配的技术版本\n\n在各种starter中,定义了完成该功能需要的坐标合集,其中大部分版本信息来自于父工程\n\n我们的工程继承parent,引入starter后,通过依赖传递,就可以简单方便获取需要的jar包,并不会存在版本冲突等问题\n\n\n# SpringBoot配置\n\nSpringBoot是基于约定的,所有很多配置都有默认值,但如果想替换默认值,必须使用application.properties或者application.yml 或 application.yaml/yam 进行配置\n\n * properties 以键值对方式\n   \n   server.port=8080\n   \n   \n   1\n   \n\n * xml\n   \n   <server>\n       <port>8080</port>\n   </server>\n   \n   \n   1\n   2\n   3\n   \n\n * yml/yaml\n   \n   server:\n   \tport: 8080\n   \n   \n   1\n   2\n   \n\n如果项目中存在多个application配置文件 会根据文件类型按顺序加载 先加载的无法被覆盖\n\nproperties > yml > yaml\n\n\n# YAML\n\nYAML文件是以数据为核心,比传统的xml方式更加简洁\n\nhttps://toyaml.com/index.html 在线转换\n\n * 大小写敏感\n * 数据值前边必须有空格,作为分隔符\n * 使用缩进表示层级关系\n * 缩进时不允许使用Tab键,只允许使用空格(各个系统Tab对应的 空格数目可能不同,导致层次混乱)\n * 缩进的空格数目不重要,只要相同层级的元素左侧对齐即可\n * #表示注释,从这个字符一直到行尾,都会被解析忽略\n\n\n# 数据格式\n\n * 对象(map):键值对的集合\n   \n   person:\n   \tname: zhangsan\n   #行内写法\n   person: {name: zhangsan}\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 数组:一组按次序排列的值\n   \n   address:\n   \t- beijing\n   \t- shanghai\n   #行内写法\n   address: [beijing,shanghai]\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n * 纯量: 单个的 不可再分的值\n   \n   msg1: \'hello \\n world\' # 单引忽略转义字符\n   msg2: "hello \\n world" # 双引识别转义字符\n   \n   \n   1\n   2\n   \n   \n   字符串不用加单引号或者双引号,双引号是用来转义\n\n * 参数引用\n   \n   name: lisi\n   person:\n   \tname: ${name}  #引用上边定义的name值\n   \n   \n   1\n   2\n   3\n   \n\n\n# 读取配置文件内容\n\n 1. @Vlaue\n    \n    @Value("${name}")\n        private String name;\n    @Value("${test.hello:test}")   //防止忘记配置 可以提供默认值 在变量名后加上:\n    private String testHello;  \n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. Environment 是一类 可以注入 使用内置的getProperty获取指定键的值\n    \n     @Autowired\n     private Environment environment;\n    \n    @RequestMapping("/hello2")\n        public void hello2() {\n            //通过getProperty 方法获取指定键的值\n             System.out.println(environment.getProperty("address[0]"));\n        }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    \n\n 3. @ConfigurationProperties 在自定义类映射为指定键的成员属性 需要gei和set方法\n    \n    @Component\n    @ConfigurationProperties(prefix = "person")  //prefix为键\n    public class Person {\n        //需要提供get和set方法\n        private String name;\n        private int age;\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    @Autowired\n    private Person person; //注入ConfigurationProperties的类才可以使用\n    \n    \n    1\n    2\n    \n    \n    配置文件中根据@ConfigurationProperties 注解 标识的类 提示对应的成员属性 坐标\n    \n     <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-configuration-processor</artifactId>\n            </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    \n\n\n# profile\n\nprofile功能是来进行动态配置切换 可以帮助我们快速的切换 开发 测试 生产 环境配置\n\n# 配置文件切换\n\n# properties多文件配置\n\n通过application配置\n\nspring.profiles.active=dev  #dev为application-dev  -后面的名称\n\n\n1\n\n\n不同的application以-进行区分 如:application-dev application-test\n\n在spring.profiles.active= 横杠后的名称 调用指定的环境配置\n\n\n\n# yml单文件配置\n\n以三个横杠区分不同的环境 ---\n\n---\nserver:\n  port: 8081\nspring:\n  config:\n    activate:\n      on-profile: dev\n\n---\nserver:\n  port: 8082\nspring:\n  config:\n    activate:\n      on-profile: pro\n\n---\n\nserver:\n  port: 8083\nspring:\n  config:\n    activate:\n      on-profile: test  #配置名称为test环境\n---\n\nspring:\n  profiles:\n    active: pro  #使用pro环境\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# profile激活方式\n\n * 在虚拟机中配置 以-Dsrping.profiles:active=对应的环境名称\n\n\n\n * 通过jar包运行\n\njava -jar springboot.jar --spring.profiles.active=pro\n\n\n1\n\n\n\n# 内部配置加载顺序\n\n 1. file:./config/ 当前项目下的/config目录下\n 2. file:./ 当前项目的根目录\n 3. classpath:/config/ classpath的/config目录\n 4. classpath:/ : classpath的根目录 resource为此处\n\n优先使用先加载配置中的属性\n\n\n# 外部配置加载顺序\n\n 1. 通过--spring.config.location= 来指定外部配置文件的路径\n\njava -jar springboot.jar --sporing.config.location=配置路径\n\n\n1\n\n\n 2. 在jar包的同级文件下放置配置文件 会优先于jar中的配置文件\n    \n    java -jar springboot.jar\n    \n    \n    1\n    \n\n 3. 在jar包的同级文件下创建config文件夹放置配置文件 会优先于jar中的配置文件\n\n\n# SpringBoot整合其他框架\n\n\n# Junit\n\n//如果test类跟springboot启动类的包路径一致 则不需要指定classes\n@SpringBootTest(classes = SpringbootProfilesApplication.class)\nclass SpringbootProfilesApplicationTests {\n\n    @Autowired\n    private UserService userService;\n\n    @Test\n    void contextLoads() {\n        userService.add();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# Redis\n\n创建maven时选择redis\n\n\n\n       <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-redis</artifactId>\n        </dependency>\n\n\n1\n2\n3\n4\n\n\ntest类\n\n    @Autowired\n    private RedisTemplate redisTemplate;\n\n\n    @Test\n    public void testSet(){\n        redisTemplate.boundValueOps("name").set("zhangsang");\n    }\n\n    @Test\n    public void testGet(){\n        Object name = redisTemplate.boundValueOps("name").get();\n        System.out.println(name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\napplication配置默认是为本机地址和6379端口\n\nspring:\n    redis:\n       host: 127.0.0.1  #redisip\n       port: 6379  #端口\n\n\n1\n2\n3\n4\n\n\n\n# MyBatis\n\n\n\n创建项目时勾选\n\n   <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n            <version>2.2.0</version>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n配置类\n\nspring:\n\tdatasource:\n    \tdriver-class-name: com.mysql.cj.jdbc.Driver  #注意驱动地址\n    \tusername: root\n    \tpassword: 123456\n    \turl: jdbc:mysql:///springboot?serverTimezone=UTC  #如果是本地可以忽略ip和端口  必须设置时区否则会报错\n\n\n1\n2\n3\n4\n5\n6\n\n 1. 注解版mapper\n\n@Repository\n@Mapper\npublic interface UserMapper {\n    @Select("select * from t_user")\n    List<User>  findAll();\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\ntest\n\n  @Autowired\n    private UserMapper userMapper;\n\n    @Test\n    public void testFindAll(){\n        List<User> all = userMapper.findAll();\n        System.out.println(all);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n 2. xml版mapper\n    \n    @Repository\n    @Mapper\n    public interface UserMapper {\n        List<User>  findAll();\n    }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n    \n    xml\n    \n    <?xml version="1.0" encoding="UTF-8"?>\n    <!DOCTYPE mapper\n            PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"\n            "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n    \n    <mapper namespace="com.itheima.springbootprofiles.mapper.UserMapper">\n        <select id="findAll" resultType="user">\n            select * from t_user\n        </select>\n    </mapper>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n    \n    配置文件\n    \n    spring:\n    \tdatasource:\n        \tdriver-class-name: com.mysql.cj.jdbc.Driver\n        \tusername: root\n        \tpassword: 123456\n        \turl: jdbc:mysql:///springboot?serverTimezone=UTC\n        \t\n    mybatis:\n      mapper-locations: classpath:mapper/*Mapper.xml  #mapper的映射文件路径\n      type-aliases-package: com.itheima.springbootprofiles.domain  #配置别名\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n    \n    test\n    \n    @Autowired\n        private UserMapper userMapper;\n    \n        @Test\n        public void testFindAll(){\n            List<User> all = userMapper.findAll();\n            System.out.println(all);\n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    ',normalizedContent:'# spring boot\n\n 1. 自动配置 spring boot的自动配置是一个运行时的过程,考虑了众多因素,spring配置应该用哪个,不该用哪个.该过程是springboot自动完成的\n 2. 起步依赖 起步依赖本质上是一个maven项目对象模型,定义了对其他库的传递依赖,简单来说,就是将具备某种功能的坐标打包到一起,并提供一些默认的功能\n 3. 辅助功能 提供了一些大型项目中常见的非功能性特征,如嵌入式服务器 安全 指标 健康检测 外部配置等\n\nspring boot 并不是对spring 功能上的增强 而是提供了一种快速使用spring的方式\n\n\n# 起步依赖原来分析\n\n在spring-boot-starter-parent中定义了各种技术的版本信息,组合了一套最优搭配的技术版本\n\n在各种starter中,定义了完成该功能需要的坐标合集,其中大部分版本信息来自于父工程\n\n我们的工程继承parent,引入starter后,通过依赖传递,就可以简单方便获取需要的jar包,并不会存在版本冲突等问题\n\n\n# springboot配置\n\nspringboot是基于约定的,所有很多配置都有默认值,但如果想替换默认值,必须使用application.properties或者application.yml 或 application.yaml/yam 进行配置\n\n * properties 以键值对方式\n   \n   server.port=8080\n   \n   \n   1\n   \n\n * xml\n   \n   <server>\n       <port>8080</port>\n   </server>\n   \n   \n   1\n   2\n   3\n   \n\n * yml/yaml\n   \n   server:\n   \tport: 8080\n   \n   \n   1\n   2\n   \n\n如果项目中存在多个application配置文件 会根据文件类型按顺序加载 先加载的无法被覆盖\n\nproperties > yml > yaml\n\n\n# yaml\n\nyaml文件是以数据为核心,比传统的xml方式更加简洁\n\nhttps://toyaml.com/index.html 在线转换\n\n * 大小写敏感\n * 数据值前边必须有空格,作为分隔符\n * 使用缩进表示层级关系\n * 缩进时不允许使用tab键,只允许使用空格(各个系统tab对应的 空格数目可能不同,导致层次混乱)\n * 缩进的空格数目不重要,只要相同层级的元素左侧对齐即可\n * #表示注释,从这个字符一直到行尾,都会被解析忽略\n\n\n# 数据格式\n\n * 对象(map):键值对的集合\n   \n   person:\n   \tname: zhangsan\n   #行内写法\n   person: {name: zhangsan}\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 数组:一组按次序排列的值\n   \n   address:\n   \t- beijing\n   \t- shanghai\n   #行内写法\n   address: [beijing,shanghai]\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n * 纯量: 单个的 不可再分的值\n   \n   msg1: \'hello \\n world\' # 单引忽略转义字符\n   msg2: "hello \\n world" # 双引识别转义字符\n   \n   \n   1\n   2\n   \n   \n   字符串不用加单引号或者双引号,双引号是用来转义\n\n * 参数引用\n   \n   name: lisi\n   person:\n   \tname: ${name}  #引用上边定义的name值\n   \n   \n   1\n   2\n   3\n   \n\n\n# 读取配置文件内容\n\n 1. @vlaue\n    \n    @value("${name}")\n        private string name;\n    @value("${test.hello:test}")   //防止忘记配置 可以提供默认值 在变量名后加上:\n    private string testhello;  \n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. environment 是一类 可以注入 使用内置的getproperty获取指定键的值\n    \n     @autowired\n     private environment environment;\n    \n    @requestmapping("/hello2")\n        public void hello2() {\n            //通过getproperty 方法获取指定键的值\n             system.out.println(environment.getproperty("address[0]"));\n        }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    \n\n 3. @configurationproperties 在自定义类映射为指定键的成员属性 需要gei和set方法\n    \n    @component\n    @configurationproperties(prefix = "person")  //prefix为键\n    public class person {\n        //需要提供get和set方法\n        private string name;\n        private int age;\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    @autowired\n    private person person; //注入configurationproperties的类才可以使用\n    \n    \n    1\n    2\n    \n    \n    配置文件中根据@configurationproperties 注解 标识的类 提示对应的成员属性 坐标\n    \n     <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-configuration-processor</artifactid>\n            </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    \n\n\n# profile\n\nprofile功能是来进行动态配置切换 可以帮助我们快速的切换 开发 测试 生产 环境配置\n\n# 配置文件切换\n\n# properties多文件配置\n\n通过application配置\n\nspring.profiles.active=dev  #dev为application-dev  -后面的名称\n\n\n1\n\n\n不同的application以-进行区分 如:application-dev application-test\n\n在spring.profiles.active= 横杠后的名称 调用指定的环境配置\n\n\n\n# yml单文件配置\n\n以三个横杠区分不同的环境 ---\n\n---\nserver:\n  port: 8081\nspring:\n  config:\n    activate:\n      on-profile: dev\n\n---\nserver:\n  port: 8082\nspring:\n  config:\n    activate:\n      on-profile: pro\n\n---\n\nserver:\n  port: 8083\nspring:\n  config:\n    activate:\n      on-profile: test  #配置名称为test环境\n---\n\nspring:\n  profiles:\n    active: pro  #使用pro环境\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# profile激活方式\n\n * 在虚拟机中配置 以-dsrping.profiles:active=对应的环境名称\n\n\n\n * 通过jar包运行\n\njava -jar springboot.jar --spring.profiles.active=pro\n\n\n1\n\n\n\n# 内部配置加载顺序\n\n 1. file:./config/ 当前项目下的/config目录下\n 2. file:./ 当前项目的根目录\n 3. classpath:/config/ classpath的/config目录\n 4. classpath:/ : classpath的根目录 resource为此处\n\n优先使用先加载配置中的属性\n\n\n# 外部配置加载顺序\n\n 1. 通过--spring.config.location= 来指定外部配置文件的路径\n\njava -jar springboot.jar --sporing.config.location=配置路径\n\n\n1\n\n\n 2. 在jar包的同级文件下放置配置文件 会优先于jar中的配置文件\n    \n    java -jar springboot.jar\n    \n    \n    1\n    \n\n 3. 在jar包的同级文件下创建config文件夹放置配置文件 会优先于jar中的配置文件\n\n\n# springboot整合其他框架\n\n\n# junit\n\n//如果test类跟springboot启动类的包路径一致 则不需要指定classes\n@springboottest(classes = springbootprofilesapplication.class)\nclass springbootprofilesapplicationtests {\n\n    @autowired\n    private userservice userservice;\n\n    @test\n    void contextloads() {\n        userservice.add();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# redis\n\n创建maven时选择redis\n\n\n\n       <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-data-redis</artifactid>\n        </dependency>\n\n\n1\n2\n3\n4\n\n\ntest类\n\n    @autowired\n    private redistemplate redistemplate;\n\n\n    @test\n    public void testset(){\n        redistemplate.boundvalueops("name").set("zhangsang");\n    }\n\n    @test\n    public void testget(){\n        object name = redistemplate.boundvalueops("name").get();\n        system.out.println(name);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\napplication配置默认是为本机地址和6379端口\n\nspring:\n    redis:\n       host: 127.0.0.1  #redisip\n       port: 6379  #端口\n\n\n1\n2\n3\n4\n\n\n\n# mybatis\n\n\n\n创建项目时勾选\n\n   <dependency>\n            <groupid>org.mybatis.spring.boot</groupid>\n            <artifactid>mybatis-spring-boot-starter</artifactid>\n            <version>2.2.0</version>\n        </dependency>\n        <dependency>\n            <groupid>mysql</groupid>\n            <artifactid>mysql-connector-java</artifactid>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n配置类\n\nspring:\n\tdatasource:\n    \tdriver-class-name: com.mysql.cj.jdbc.driver  #注意驱动地址\n    \tusername: root\n    \tpassword: 123456\n    \turl: jdbc:mysql:///springboot?servertimezone=utc  #如果是本地可以忽略ip和端口  必须设置时区否则会报错\n\n\n1\n2\n3\n4\n5\n6\n\n 1. 注解版mapper\n\n@repository\n@mapper\npublic interface usermapper {\n    @select("select * from t_user")\n    list<user>  findall();\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\ntest\n\n  @autowired\n    private usermapper usermapper;\n\n    @test\n    public void testfindall(){\n        list<user> all = usermapper.findall();\n        system.out.println(all);\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n 2. xml版mapper\n    \n    @repository\n    @mapper\n    public interface usermapper {\n        list<user>  findall();\n    }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n    \n    xml\n    \n    <?xml version="1.0" encoding="utf-8"?>\n    <!doctype mapper\n            public "-//mybatis.org//dtd mapper 3.0//en"\n            "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n    \n    <mapper namespace="com.itheima.springbootprofiles.mapper.usermapper">\n        <select id="findall" resulttype="user">\n            select * from t_user\n        </select>\n    </mapper>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n    \n    配置文件\n    \n    spring:\n    \tdatasource:\n        \tdriver-class-name: com.mysql.cj.jdbc.driver\n        \tusername: root\n        \tpassword: 123456\n        \turl: jdbc:mysql:///springboot?servertimezone=utc\n        \t\n    mybatis:\n      mapper-locations: classpath:mapper/*mapper.xml  #mapper的映射文件路径\n      type-aliases-package: com.itheima.springbootprofiles.domain  #配置别名\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n    \n    test\n    \n    @autowired\n        private usermapper usermapper;\n    \n        @test\n        public void testfindall(){\n            list<user> all = usermapper.findall();\n            system.out.println(all);\n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"RabbitMQ",frontmatter:{title:"RabbitMQ",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/3a5e24/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/15.RabbitMQ.html",relativePath:"后端/02.JavaEE/15.RabbitMQ.md",key:"v-37078826",path:"/pages/3a5e24/",headers:[{level:2,title:"常用的MQ产品",slug:"常用的mq产品",normalizedTitle:"常用的mq产品",charIndex:102},{level:2,title:"RabbitMQ简介",slug:"rabbitmq简介",normalizedTitle:"rabbitmq简介",charIndex:116},{level:3,title:"JMS",slug:"jms",normalizedTitle:"jms",charIndex:222},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:275},{level:2,title:"入门案例",slug:"入门案例",normalizedTitle:"入门案例",charIndex:1559},{level:3,title:"生产者",slug:"生产者",normalizedTitle:"生产者",charIndex:1568},{level:3,title:"消费者",slug:"消费者",normalizedTitle:"消费者",charIndex:2843},{level:2,title:"Work queues 工作队列模式",slug:"work-queues-工作队列模式",normalizedTitle:"work queues 工作队列模式",charIndex:5627},{level:2,title:"Pub/Sub 订阅模式",slug:"pub-sub-订阅模式",normalizedTitle:"pub/sub 订阅模式",charIndex:5782},{level:2,title:"Routing 路由模式",slug:"routing-路由模式",normalizedTitle:"routing 路由模式",charIndex:9748},{level:2,title:"Topics 通配符模式",slug:"topics-通配符模式",normalizedTitle:"topics 通配符模式",charIndex:13825},{level:2,title:"Spring 整合 RabbitMQ",slug:"spring-整合-rabbitmq",normalizedTitle:"spring 整合 rabbitmq",charIndex:17498},{level:3,title:"生产者",slug:"生产者-2",normalizedTitle:"生产者",charIndex:1568},{level:3,title:"消费者",slug:"消费者-2",normalizedTitle:"消费者",charIndex:2843},{level:2,title:"Spring Boot 整合 RabbitMQ",slug:"spring-boot-整合-rabbitmq",normalizedTitle:"spring boot 整合 rabbitmq",charIndex:26390},{level:3,title:"生产者",slug:"生产者-3",normalizedTitle:"生产者",charIndex:1568},{level:3,title:"消费者",slug:"消费者-3",normalizedTitle:"消费者",charIndex:2843}],headersStr:"常用的MQ产品 RabbitMQ简介 JMS 安装 入门案例 生产者 消费者 Work queues 工作队列模式 Pub/Sub 订阅模式 Routing 路由模式 Topics 通配符模式 Spring 整合 RabbitMQ 生产者 消费者 Spring Boot 整合 RabbitMQ 生产者 消费者",content:'# RabbitMQ\n\nMQ全称 Message Queue(消息队列) 是在消息的传输过程中保存消息的容器 多用于分布式系统之间进行通信\n\n\n\n * 优势\n\n\n\n\n\n\n\n\n\n * 劣势\n\n\n\n\n\n\n# 常用的MQ产品\n\n\n\n\n# RabbitMQ简介\n\nAMQP 即 Advanced Message Queuing Protocol (高级消息队列协议) 是一个网络协议 是应用层协议的一个开放标准 为面向消息的中间件设计\n\n\n\n\n\n\n# JMS\n\nJMS 即 Java 消息服务 应用程序接口 一个Java平台中关于面向中间件的API\n\n\n# 安装\n\n 1. 安装erlang\n\nyum -y install gcc glibc-devel make ncurses-devel openssl-devel xmlto perl wget gtk2-devel binutils-devel\nwget http://erlang.org/download/otp_src_22.0.tar.gz\ntar -zxvf otp_src_22.0.tar.gz\nmv otp_src_22.0 /usr/local/\ncd /usr/local/otp_src_22.0/\nmkdir ../erlang\n./configure --prefix=/usr/local/erlang\nmake install\n\nll /usr/local/erlang/bin\necho \'export PATH=$PATH:/usr/local/erlang/bin\' >> /etc/profile\nsource /etc/profile\nerl\n\nhalt().\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n 2. 安装RabbitMQ\n\ncd /root\nwget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.15/rabbitmq-server-generic-unix-3.7.15.tar.xz\nyum install -y xz\n/bin/xz -d rabbitmq-server-generic-unix-3.7.15.tar.xz\ntar -xvf rabbitmq-server-generic-unix-3.7.15.tar\nmv rabbitmq_server-3.7.15/ /usr/local/\nmv /usr/local/rabbitmq_server-3.7.15  rabbitmq\necho \'export PATH=$PATH:/usr/local/rabbitmq/sbin\' >> /etc/profile\nsource /etc/profile\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 3. 启动\n\nrabbitmq-server -detached\nrabbitmq-plugins enable rabbitmq_management   #开启web插件\nrabbitmqctl stop #停止\nrabbitmqctl status #状态\n\n\n\n1\n2\n3\n4\n5\n\n\n默认账号密码：guest guest（这个账号只允许本机访问）\n\nfirewall-cmd --zone=public --add-port=15672/tcp --permanent\nvim /usr/local/rabbitmq/ebin/rabbit.app  #配置\n\n\n1\n2\n\n\n\n\n访问http://192.168.130.124:15672/\n\n\n# 入门案例\n\n\n# 生产者\n\n <dependencies>\n        <dependency>\n            <groupId>com.rabbitmq</groupId>\n            <artifactId>amqp-client</artifactId>\n            <version>5.6.0</version>\n\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.8.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n//1.创建连接工厂\nConnectionFactory factory = new ConnectionFactory();\n//2.设置参数\nfactory.setHost("192.168.130.124"); //ip\nfactory.setPort(5672);  //端口\nfactory.setVirtualHost("/itcast");  //虚拟机 默认值\nfactory.setUsername("iekr");  //用户名\nfactory.setPassword("iekr");  //密码 默认值为guest\n//3.创建连接 connection\nConnection connection = factory.newConnection();\n//4.创建channel\nChannel channel = connection.createChannel();\n//5.创建队列queue\n/**\n * (String queue, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments)\n * queue 队列名称\n * durable 是否持久化 当mq重启之后还在\n * exclusive 是否独占,只能有一个消费者监听这个队列  当Connection关闭时是否删除队列\n * autoDelete 是否自动删除 当没有Consumer时 自动删除\n * arguments 参数\n *\n */\n//如果没有一个叫hello_world的队列 则自动创建\nchannel.queueDeclare("hello_world",true,false,false,null);\n//6.发送消息\n/**\n * String var1, String var2, BasicProperties var3, byte[] var4\n * var1 交换机名称 简单模式下会使用默认的""\n * var2  路由名称\n * var3  配置信息\n * var4  发送消息数据\n */\nString body = "hello world";\nchannel.basicPublish("","hello_world",null,body.getBytes(StandardCharsets.UTF_8));\n\n//7.释放资源\nchannel.close();\nconnection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# 消费者\n\n与生产者坐标一致\n\n        //1.创建连接工厂\n        ConnectionFactory factory = new ConnectionFactory();\n        //2.设置参数\n        factory.setHost("192.168.130.124"); //ip\n        factory.setPort(5672);  //端口\n        factory.setVirtualHost("/itcast");  //虚拟机 默认值\n        factory.setUsername("iekr");  //用户名\n        factory.setPassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        Connection connection = factory.newConnection();\n        //4.创建channel\n        Channel channel = connection.createChannel();\n        //5.创建队列queue\n        /**\n         * (String queue, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments)\n         * queue 队列名称\n         * durable 是否持久化 当mq重启之后还在\n         * exclusive 是否独占,只能有一个消费者监听这个队列  当Connection关闭时是否删除队列\n         * autoDelete 是否自动删除 当没有Consumer时 自动删除\n         * arguments 参数\n         *\n         */\n        //如果没有一个叫hello_world的队列 则自动创建\n        channel.queueDeclare("hello_world",true,false,false,null);\n        //6.接受消息\n     \n        Consumer consumer = new DefaultConsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumerTag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws IOException\n             */\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n                System.out.println(consumerTag);\n                System.out.println(envelope.getExchange());\n                System.out.println(envelope.getRoutingKey());\n                System.out.println(properties);\n                System.out.println(new String(body));\n            }\n        };\n        /**\n         * String var1, DeliverCallback var2, CancelCallback var3\n         * queue 队列名称\n         * autoAck 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicConsume("hello_world",true,consumer);\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# Work queues 工作队列模式\n\n\n\n多个消费者共同消费一个队列中的消息\n\n对于任务过重或者任务较多情况使用工作队列可以提高任务处理的速度\n\n生产者生成多条消息 而消费者轮流切换接受\n\nWork queues 代码与生产者 消费者没有太大区别 只是生产者在频道中发送多条 多个消费者轮流接受消息\n\n\n# Pub/Sub 订阅模式\n\n\n\nX为交换机 生产者发送消息给交换机 而交换机转发消息 有三种模式\n\n * Fanout 广播模式 将消息交给所有绑定到交换机的队列\n * Direct 定向 把消息交给符合指定 routing key 的队列\n * Topic 通配符 把消息交给符合 routing pattern (路由模式)\n\n生产者\n\n        //1.创建连接工厂\n        ConnectionFactory factory = new ConnectionFactory();\n        //2.设置参数\n        factory.setHost("192.168.130.124"); //ip\n        factory.setPort(5672);  //端口\n        factory.setVirtualHost("/itcast");  //虚拟机 默认值\n        factory.setUsername("iekr");  //用户名\n        factory.setPassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        Connection connection = factory.newConnection();\n        //4.创建channel\n        Channel channel = connection.createChannel();\n        //5.创建交换机\n        /**\n         * String var1, BuiltinExchangeType var2, boolean var3, boolean var4, boolean var5, Map<String, Object> var6\n         * exchange 交换机名称\n         * type 交换机类型 枚举  DIRECT("direct")定向   FANOUT("fanout")扇形(广播)   TOPIC("topic")通配符 HEADERS("headers") 参数匹配\n         * durable 是否持久化\n         * autoDelete 自动删除\n         * internal 内部使用 一般为false\n         * arguments 参数\n         */\n        String exchangeName = "test_fanout";\n        channel.exchangeDeclare(exchangeName, BuiltinExchangeType.FANOUT, true, false, false, null);\n        //6.创建队列\n        String queue1Name = "test_fanout_queue1";\n        String queue2Name = "test_fanout_queue2";\n        channel.queueDeclare(queue1Name, true, false, false, null);\n        channel.queueDeclare(queue2Name, true, false, false, null);\n        //7.绑定队列和交换机\n        /**  String queue, String exchange, String routingKey\n         *  queue  队列名称\n         *  exchange 交换机名称\n         *  routingKey 路由键绑定规则   如果交换机类型为FANOUT 则routingKey为""\n         */\n        channel.queueBind(queue1Name, exchangeName, "");\n        channel.queueBind(queue2Name, exchangeName, "");\n\n        //8.发送消息\n        String body = "日志信息:";\n        channel.basicPublish(exchangeName, "", null, body.getBytes(StandardCharsets.UTF_8));\n\n        //9.释放资源\n        channel.close();\n        connection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n多个消费者绑定不同的队列\n\n        //1.创建连接工厂\n        ConnectionFactory factory = new ConnectionFactory();\n        //2.设置参数\n        factory.setHost("192.168.130.124"); //ip\n        factory.setPort(5672);  //端口\n        factory.setVirtualHost("/itcast");  //虚拟机 默认值\n        factory.setUsername("iekr");  //用户名\n        factory.setPassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        Connection connection = factory.newConnection();\n        //4.创建channel\n        Channel channel = connection.createChannel();\n\n        //6.接受消息\n        String queue1Name = "test_fanout_queue1";\n        String queue2Name = "test_fanout_queue2";\n        Consumer consumer = new DefaultConsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumerTag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws IOException\n             */\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n//                System.out.println(consumerTag);\n//                System.out.println(envelope.getExchange());\n//                System.out.println(envelope.getRoutingKey());\n//                System.out.println(properties);\n                System.out.println(new String(body));\n                System.out.println("第一个消费者");\n            }\n        };\n        /**\n         * String var1, DeliverCallback var2, CancelCallback var3\n         * queue 队列名称\n         * autoAck 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicConsume(queue1Name,true,consumer);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# Routing 路由模式\n\n\n\n生产者发送不同key的消息给交换机 而交换机根据队列的key转发消息给有标识的队列\n\n        //1.创建连接工厂\n        ConnectionFactory factory = new ConnectionFactory();\n        //2.设置参数\n        factory.setHost("192.168.130.124"); //ip\n        factory.setPort(5672);  //端口\n        factory.setVirtualHost("/itcast");  //虚拟机 默认值\n        factory.setUsername("iekr");  //用户名\n        factory.setPassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        Connection connection = factory.newConnection();\n        //4.创建channel\n        Channel channel = connection.createChannel();\n        //5.创建交换机\n        /**\n         * String var1, BuiltinExchangeType var2, boolean var3, boolean var4, boolean var5, Map<String, Object> var6\n         * exchange 交换机名称\n         * type 交换机类型 枚举  DIRECT("direct")定向   FANOUT("fanout")扇形(广播)   TOPIC("topic")通配符 HEADERS("headers") 参数匹配\n         * durable 是否持久化\n         * autoDelete 自动删除\n         * internal 内部使用 一般为false\n         * arguments 参数\n         */\n        String exchangeName = "test_direct";\n        channel.exchangeDeclare(exchangeName, BuiltinExchangeType.DIRECT, true, false, false, null);\n        //6.创建队列\n        String queue1Name = "test_direct_queue1";\n        String queue2Name = "test_direct_queue2";\n\n        channel.queueDeclare(queue1Name, true, false, false, null);\n        channel.queueDeclare(queue2Name, true, false, false, null);\n        //7.绑定队列和交换机\n        /**  String queue, String exchange, String routingKey\n         *  queue  队列名称\n         *  exchange 交换机名称\n         *  routingKey 路由键绑定规则   如果交换机类型为FANOUT 则routingKey为""\n         */\n        //队列1的绑定\n        channel.queueBind(queue1Name, exchangeName, "error");\n        //队列2的绑定\n        channel.queueBind(queue2Name, exchangeName, "info");\n        channel.queueBind(queue2Name, exchangeName, "error");\n        channel.queueBind(queue2Name, exchangeName, "warning");\n\n        //8.发送消息\n        String body = "日志信息:";\n        //队列1只接受error消息 而队列2所有类型都接受\n        channel.basicPublish(exchangeName, "info", null, body.getBytes(StandardCharsets.UTF_8));\n\n        //9.释放资源\n        channel.close();\n        connection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n消费者\n\n        //1.创建连接工厂\n        ConnectionFactory factory = new ConnectionFactory();\n        //2.设置参数\n        factory.setHost("192.168.130.124"); //ip\n        factory.setPort(5672);  //端口\n        factory.setVirtualHost("/itcast");  //虚拟机 默认值\n        factory.setUsername("iekr");  //用户名\n        factory.setPassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        Connection connection = factory.newConnection();\n        //4.创建channel\n        Channel channel = connection.createChannel();\n\n        //6.接受消息\n        String queue1Name = "test_direct_queue1";\n        String queue2Name = "test_direct_queue2";\n        Consumer consumer = new DefaultConsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumerTag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws IOException\n             */\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n//                System.out.println(consumerTag);\n//                System.out.println(envelope.getExchange());\n//                System.out.println(envelope.getRoutingKey());\n//                System.out.println(properties);\n                System.out.println(new String(body));\n                System.out.println("队列1 存储到数据库");\n            }\n        };\n        /**\n         * String var1, DeliverCallback var2, CancelCallback var3\n         * queue 队列名称\n         * autoAck 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicConsume(queue1Name,true,consumer);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# Topics 通配符模式\n\n\n\n使用通配符和路由器转发 让队列更加灵活的接受对应的消息\n\n*星号代表0个或多个单词\n\n#井号代表1个单词\n\n生产者\n\n//1.创建连接工厂\nConnectionFactory factory = new ConnectionFactory();\n//2.设置参数\nfactory.setHost("192.168.130.124"); //ip\nfactory.setPort(5672);  //端口\nfactory.setVirtualHost("/itcast");  //虚拟机 默认值\nfactory.setUsername("iekr");  //用户名\nfactory.setPassword("iekr");  //密码 默认值为guest\n//3.创建连接 connection\nConnection connection = factory.newConnection();\n//4.创建channel\nChannel channel = connection.createChannel();\n//5.创建交换机\n/**\n * String var1, BuiltinExchangeType var2, boolean var3, boolean var4, boolean var5, Map<String, Object> var6\n * exchange 交换机名称\n * type 交换机类型 枚举  DIRECT("direct")定向   FANOUT("fanout")扇形(广播)   TOPIC("topic")通配符 HEADERS("headers") 参数匹配\n * durable 是否持久化\n * autoDelete 自动删除\n * internal 内部使用 一般为false\n * arguments 参数\n */\nString exchangeName = "test_topics";\nchannel.exchangeDeclare(exchangeName, BuiltinExchangeType.TOPIC, true, false, false, null);\n//6.创建队列\nString queue1Name = "test_topics_queue1";\nString queue2Name = "test_topics_queue2";\nchannel.queueDeclare(queue1Name, true, false, false, null);\nchannel.queueDeclare(queue2Name, true, false, false, null);\n//7.绑定队列和交换机\n/**  String queue, String exchange, String routingKey\n *  queue  队列名称\n *  exchange 交换机名称\n *  routingKey 路由键绑定规则   如果交换机类型为FANOUT 则routingKey为""\n */\n//routing key  系统的名称.日志的级别\nchannel.queueBind(queue1Name, exchangeName, "#.error"); //以.error结尾\nchannel.queueBind(queue1Name, exchangeName, "order.*");  //以order.开头\nchannel.queueBind(queue2Name, exchangeName, "*.*");    //队列2所有消息都可以接受到\n\n//8.发送消息\nString body = "日志信息:";\nchannel.basicPublish(exchangeName, "goods.info", null, body.getBytes(StandardCharsets.UTF_8));\n\n//9.释放资源\nchannel.close();\nconnection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n消费者\n\n        //1.创建连接工厂\n        ConnectionFactory factory = new ConnectionFactory();\n        //2.设置参数\n        factory.setHost("192.168.130.124"); //ip\n        factory.setPort(5672);  //端口\n        factory.setVirtualHost("/itcast");  //虚拟机 默认值\n        factory.setUsername("iekr");  //用户名\n        factory.setPassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        Connection connection = factory.newConnection();\n        //4.创建channel\n        Channel channel = connection.createChannel();\n\n        //6.接受消息\n        String queue1Name = "test_topics_queue1";\n        String queue2Name = "test_topics_queue2";\n        Consumer consumer = new DefaultConsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumerTag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws IOException\n             */\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\n//                System.out.println(consumerTag);\n//                System.out.println(envelope.getExchange());\n//                System.out.println(envelope.getRoutingKey());\n//                System.out.println(properties);\n                System.out.println(new String(body));\n                System.out.println("队列1 存储到数据库");\n            }\n        };\n        /**\n         * String var1, DeliverCallback var2, CancelCallback var3\n         * queue 队列名称\n         * autoAck 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicConsume(queue1Name,true,consumer);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# Spring 整合 RabbitMQ\n\n坐标\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context</artifactId>\n            <version>5.3.10</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.amqp</groupId>\n            <artifactId>spring-rabbit</artifactId>\n            <version>2.3.9</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.13</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-test</artifactId>\n            <version>5.3.10</version>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.8.1</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\nrabbitmq.properties\n\nrabbitmq.host=192.168.130.124\nrabbitmq.port=5672\nrabbitmq.username=iekr\nrabbitmq.password=iekr\nrabbitmq.virtual-host=/itcast\n\n\n1\n2\n3\n4\n5\n\n\n\n# 生产者\n\nspring-rabbitmq-producer.xml\n\n<?xml version="1.0" encoding="UTF-8" ?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:rabbit="http://www.springframework.org/schema/rabbit"\n       xsi:schemaLocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/rabbit\n        https://www.springframework.org/schema/rabbit/spring-rabbit.xsd\n">\n    \x3c!--    加载配置文件--\x3e\n    <context:property-placeholder location="classpath:rabbitmq.properties"/>\n\n    \x3c!--    定义rabbitmq connectionFactory--\x3e\n    <rabbit:connection-factory id="connectionFactory" host="${rabbitmq.host}"\n                               port="${rabbitmq.port}"\n                               username="${rabbitmq.username}"\n                               password="${rabbitmq.password}"\n                               virtual-host="${rabbitmq.virtual-host}"/>\n    \x3c!--    定义管理交换机 队列--\x3e\n    <rabbit:admin connection-factory="connectionFactory"/>\n\n    \x3c!--    定义持久化队列 不存在则自动创建 不绑定到交换机则绑定到默认交换机   默认交换机为direct  名字为""  路由键位队列名称--\x3e\n    <rabbit:queue id="spring_queue" name="spring_queue" auto-declare="true"/>\n\n    \x3c!--    定义广播交换机中的持久化队列 不存在则自动创建--\x3e\n    <rabbit:queue id="spring_fanout_queue_1" name="spring_fanout_queue_1" auto-declare="true"/>\n    <rabbit:queue id="spring_fanout_queue_2" name="spring_fanout_queue_2" auto-declare="true"/>\n\n    \x3c!--    定义广播类型交换机 绑定上面两个队列--\x3e\n    <rabbit:fanout-exchange name="spring_fanout_exchange"\n                            id="spring_fanout_exchange"\n                            auto-declare="true">\n        <rabbit:bindings>\n            <rabbit:binding queue="spring_fanout_queue_1"/>\n            <rabbit:binding queue="spring_fanout_queue_2"/>\n        </rabbit:bindings>\n    </rabbit:fanout-exchange>\n\n    \x3c!--    通配符队列--\x3e\n    <rabbit:queue id="spring_topic_queue_star" name="spring_topic_queue_star" auto-declare="true"/>\n    <rabbit:queue id="spring_topic_queue_well" name="spring_topic_queue_well" auto-declare="true"/>\n    <rabbit:queue id="spring_topic_queue_well2" name="spring_topic_queue_well2" auto-declare="true"/>\n\n    \x3c!--    通配符定义--\x3e\n    <rabbit:topic-exchange name="spring_topic_exchange" id="spring_topic_exchange" auto-declare="true">\n        <rabbit:bindings>\n            <rabbit:binding pattern="heima.*" queue="spring_topic_queue_star"/>\n            <rabbit:binding pattern="heima.#" queue="spring_topic_queue_well"/>\n            <rabbit:binding pattern="itcast.#" queue="spring_topic_queue_well2"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n\n    \x3c!--    定义rabbitTemplate对象操作可以在代码中方便发送消息--\x3e\n    <rabbit:template id="rabbitTemplate" connection-factory="connectionFactory"/>\n\n\n</beans>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\ntest\n\npackage com.itheima;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.test.context.ContextConfiguration;\nimport org.springframework.test.context.junit4.SpringJUnit4ClassRunner;\n\n@RunWith(SpringJUnit4ClassRunner.class)\n@ContextConfiguration(locations = "classpath:spring-rabbitmq-producer.xml")\npublic class ProducerTest {\n\n    //注入\n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n\n    //1对1\n    @Test\n    public void testHelloWorld() {\n        //发送消息\n        rabbitTemplate.convertAndSend("spring_queue", "hello world spring ...");\n    }\n\n    //广播\n    @Test\n    public void testFanout() {\n        //发送消息\n        rabbitTemplate.convertAndSend("spring_fanout_exchange", "", "spring fanout...");\n    }\n\n    //topic\n    @Test\n    public void testTopic() {\n        //发送消息\n        rabbitTemplate.convertAndSend("spring_topic_exchange", "heima.123.456", "spring topic...");\n    }\n\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n# 消费者\n\nspring-rabbitmq-consumer.xml\n\n<?xml version="1.0" encoding="UTF-8" ?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:rabbit="http://www.springframework.org/schema/rabbit"\n       xsi:schemaLocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/rabbit\n        https://www.springframework.org/schema/rabbit/spring-rabbit.xsd\n">\n    \x3c!--    加载配置文件--\x3e\n    <context:property-placeholder location="classpath:rabbitmq.properties"/>\n\n    \x3c!--    定义rabbitmq connectionFactory--\x3e\n    <rabbit:connection-factory id="connectionFactory" host="${rabbitmq.host}"\n                               port="${rabbitmq.port}"\n                               username="${rabbitmq.username}"\n                               password="${rabbitmq.password}"\n                               virtual-host="${rabbitmq.virtual-host}"/>\n\n    <bean id="springQueueListener" class="com.itheima.rabbitmq.SpringQueueListener"/>\n\x3c!--    <bean id="fanoutListener1" class="com.itheima.rabbitmq.FanoutListener1"/>--\x3e\n\x3c!--    <bean id="fanoutListener2" class="com.itheima.rabbitmq.FanoutListener2"/>--\x3e\n\x3c!--    <bean id="topicListenerStar" class="com.itheima.rabbitmq.TopicListenerStar"/>--\x3e\n\x3c!--    <bean id="topicListenerWell" class="com.itheima.rabbitmq.TopicListenerWell"/>--\x3e\n\x3c!--    <bean id="topicListenerWell2" class="com.itheima.rabbitmq.TopicListenerWell2"/>--\x3e\n\n    <rabbit:listener-container connection-factory="connectionFactory" auto-declare="true">\n        <rabbit:listener ref="springQueueListener" queue-names="spring_queue"/>\n\x3c!--        <rabbit:listener ref="fanoutListener1" queue-names="spring_fanout_queue1"/>--\x3e\n\x3c!--        <rabbit:listener ref="fanoutListener2" queue-names="spring_fanout_queue2"/>--\x3e\n\x3c!--        <rabbit:listener ref="topicListenerStar" queue-names="spring_topic_queue_star"/>--\x3e\n\x3c!--        <rabbit:listener ref="topicListenerWell" queue-names="spring_topic_queue_well"/>--\x3e\n\x3c!--        <rabbit:listener ref="topicListenerWell2" queue-names="spring_topic_queue_well2"/>--\x3e\n    </rabbit:listener-container>\n\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n消费者类 根据bean id 编写对应的类名 并 实现 MessageListener 重写 onMessage 方法\n\npackage com.itheima.rabbitmq;\n\nimport org.springframework.amqp.core.Message;\nimport org.springframework.amqp.core.MessageListener;\n\npublic class SpringQueueListener implements MessageListener {\n    @Override\n    public void onMessage(Message message) {\n        System.out.println(new String(message.getBody()));\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# Spring Boot 整合 RabbitMQ\n\n\n\n\n# 生产者\n\n绑定交换机和队列\n\npackage com.example.springrabbitmqproducer.rabbitmq.config;\n\nimport org.springframework.amqp.core.*;\nimport org.springframework.beans.factory.annotation.Qualifier;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class RabbitMQConfig {\n    public static final String EXCHANGE_NAME = "boot_topic_exchange";\n    public static final String QUEUE_NAME = "boot_queue";\n\n    //1.交换机\n    @Bean("bootExchange")\n    public Exchange bootExchange() {\n        //获取4种类型的交换机\n        return ExchangeBuilder.topicExchange(EXCHANGE_NAME).durable(true).build();\n    }\n\n    //2.队列\n    @Bean("bootQueue")\n    public Queue bootQueue() {\n        return QueueBuilder.durable(QUEUE_NAME).build();\n    }\n\n    //3.队列和交换机的绑定\n    @Bean\n    public Binding bindQueueExchange(@Qualifier("bootQueue") Queue queue, @Qualifier("bootExchange") Exchange exchange) {\n        return BindingBuilder.bind(queue).to(exchange).with("boot.#").noargs();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\ntest\n\npackage com.example.springrabbitmqproducer;\n\nimport com.example.springrabbitmqproducer.rabbitmq.config.RabbitMQConfig;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\n\n@SpringBootTest\nclass SpringRabbitmqProducerApplicationTests {\n\n    //注入RabbitTemplate\n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n\n    @Test\n    public void testSend(){\n        rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,"boot.haha","boot mq hello");\n    }\n\n\n\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 消费者\n\n@Component\npublic class RabbitMQListener {\n\n    //监听指定队列\n    @RabbitListener(queues = "boot_queue")\n    public void ListenerQueue(Message message) {      \n        System.out.println(new String(message.getBody()));\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',normalizedContent:'# rabbitmq\n\nmq全称 message queue(消息队列) 是在消息的传输过程中保存消息的容器 多用于分布式系统之间进行通信\n\n\n\n * 优势\n\n\n\n\n\n\n\n\n\n * 劣势\n\n\n\n\n\n\n# 常用的mq产品\n\n\n\n\n# rabbitmq简介\n\namqp 即 advanced message queuing protocol (高级消息队列协议) 是一个网络协议 是应用层协议的一个开放标准 为面向消息的中间件设计\n\n\n\n\n\n\n# jms\n\njms 即 java 消息服务 应用程序接口 一个java平台中关于面向中间件的api\n\n\n# 安装\n\n 1. 安装erlang\n\nyum -y install gcc glibc-devel make ncurses-devel openssl-devel xmlto perl wget gtk2-devel binutils-devel\nwget http://erlang.org/download/otp_src_22.0.tar.gz\ntar -zxvf otp_src_22.0.tar.gz\nmv otp_src_22.0 /usr/local/\ncd /usr/local/otp_src_22.0/\nmkdir ../erlang\n./configure --prefix=/usr/local/erlang\nmake install\n\nll /usr/local/erlang/bin\necho \'export path=$path:/usr/local/erlang/bin\' >> /etc/profile\nsource /etc/profile\nerl\n\nhalt().\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n 2. 安装rabbitmq\n\ncd /root\nwget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.15/rabbitmq-server-generic-unix-3.7.15.tar.xz\nyum install -y xz\n/bin/xz -d rabbitmq-server-generic-unix-3.7.15.tar.xz\ntar -xvf rabbitmq-server-generic-unix-3.7.15.tar\nmv rabbitmq_server-3.7.15/ /usr/local/\nmv /usr/local/rabbitmq_server-3.7.15  rabbitmq\necho \'export path=$path:/usr/local/rabbitmq/sbin\' >> /etc/profile\nsource /etc/profile\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 3. 启动\n\nrabbitmq-server -detached\nrabbitmq-plugins enable rabbitmq_management   #开启web插件\nrabbitmqctl stop #停止\nrabbitmqctl status #状态\n\n\n\n1\n2\n3\n4\n5\n\n\n默认账号密码：guest guest（这个账号只允许本机访问）\n\nfirewall-cmd --zone=public --add-port=15672/tcp --permanent\nvim /usr/local/rabbitmq/ebin/rabbit.app  #配置\n\n\n1\n2\n\n\n\n\n访问http://192.168.130.124:15672/\n\n\n# 入门案例\n\n\n# 生产者\n\n <dependencies>\n        <dependency>\n            <groupid>com.rabbitmq</groupid>\n            <artifactid>amqp-client</artifactid>\n            <version>5.6.0</version>\n\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.apache.maven.plugins</groupid>\n                <artifactid>maven-compiler-plugin</artifactid>\n                <version>3.8.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n//1.创建连接工厂\nconnectionfactory factory = new connectionfactory();\n//2.设置参数\nfactory.sethost("192.168.130.124"); //ip\nfactory.setport(5672);  //端口\nfactory.setvirtualhost("/itcast");  //虚拟机 默认值\nfactory.setusername("iekr");  //用户名\nfactory.setpassword("iekr");  //密码 默认值为guest\n//3.创建连接 connection\nconnection connection = factory.newconnection();\n//4.创建channel\nchannel channel = connection.createchannel();\n//5.创建队列queue\n/**\n * (string queue, boolean durable, boolean exclusive, boolean autodelete, map<string, object> arguments)\n * queue 队列名称\n * durable 是否持久化 当mq重启之后还在\n * exclusive 是否独占,只能有一个消费者监听这个队列  当connection关闭时是否删除队列\n * autodelete 是否自动删除 当没有consumer时 自动删除\n * arguments 参数\n *\n */\n//如果没有一个叫hello_world的队列 则自动创建\nchannel.queuedeclare("hello_world",true,false,false,null);\n//6.发送消息\n/**\n * string var1, string var2, basicproperties var3, byte[] var4\n * var1 交换机名称 简单模式下会使用默认的""\n * var2  路由名称\n * var3  配置信息\n * var4  发送消息数据\n */\nstring body = "hello world";\nchannel.basicpublish("","hello_world",null,body.getbytes(standardcharsets.utf_8));\n\n//7.释放资源\nchannel.close();\nconnection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# 消费者\n\n与生产者坐标一致\n\n        //1.创建连接工厂\n        connectionfactory factory = new connectionfactory();\n        //2.设置参数\n        factory.sethost("192.168.130.124"); //ip\n        factory.setport(5672);  //端口\n        factory.setvirtualhost("/itcast");  //虚拟机 默认值\n        factory.setusername("iekr");  //用户名\n        factory.setpassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        connection connection = factory.newconnection();\n        //4.创建channel\n        channel channel = connection.createchannel();\n        //5.创建队列queue\n        /**\n         * (string queue, boolean durable, boolean exclusive, boolean autodelete, map<string, object> arguments)\n         * queue 队列名称\n         * durable 是否持久化 当mq重启之后还在\n         * exclusive 是否独占,只能有一个消费者监听这个队列  当connection关闭时是否删除队列\n         * autodelete 是否自动删除 当没有consumer时 自动删除\n         * arguments 参数\n         *\n         */\n        //如果没有一个叫hello_world的队列 则自动创建\n        channel.queuedeclare("hello_world",true,false,false,null);\n        //6.接受消息\n     \n        consumer consumer = new defaultconsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumertag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws ioexception\n             */\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n                system.out.println(consumertag);\n                system.out.println(envelope.getexchange());\n                system.out.println(envelope.getroutingkey());\n                system.out.println(properties);\n                system.out.println(new string(body));\n            }\n        };\n        /**\n         * string var1, delivercallback var2, cancelcallback var3\n         * queue 队列名称\n         * autoack 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicconsume("hello_world",true,consumer);\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# work queues 工作队列模式\n\n\n\n多个消费者共同消费一个队列中的消息\n\n对于任务过重或者任务较多情况使用工作队列可以提高任务处理的速度\n\n生产者生成多条消息 而消费者轮流切换接受\n\nwork queues 代码与生产者 消费者没有太大区别 只是生产者在频道中发送多条 多个消费者轮流接受消息\n\n\n# pub/sub 订阅模式\n\n\n\nx为交换机 生产者发送消息给交换机 而交换机转发消息 有三种模式\n\n * fanout 广播模式 将消息交给所有绑定到交换机的队列\n * direct 定向 把消息交给符合指定 routing key 的队列\n * topic 通配符 把消息交给符合 routing pattern (路由模式)\n\n生产者\n\n        //1.创建连接工厂\n        connectionfactory factory = new connectionfactory();\n        //2.设置参数\n        factory.sethost("192.168.130.124"); //ip\n        factory.setport(5672);  //端口\n        factory.setvirtualhost("/itcast");  //虚拟机 默认值\n        factory.setusername("iekr");  //用户名\n        factory.setpassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        connection connection = factory.newconnection();\n        //4.创建channel\n        channel channel = connection.createchannel();\n        //5.创建交换机\n        /**\n         * string var1, builtinexchangetype var2, boolean var3, boolean var4, boolean var5, map<string, object> var6\n         * exchange 交换机名称\n         * type 交换机类型 枚举  direct("direct")定向   fanout("fanout")扇形(广播)   topic("topic")通配符 headers("headers") 参数匹配\n         * durable 是否持久化\n         * autodelete 自动删除\n         * internal 内部使用 一般为false\n         * arguments 参数\n         */\n        string exchangename = "test_fanout";\n        channel.exchangedeclare(exchangename, builtinexchangetype.fanout, true, false, false, null);\n        //6.创建队列\n        string queue1name = "test_fanout_queue1";\n        string queue2name = "test_fanout_queue2";\n        channel.queuedeclare(queue1name, true, false, false, null);\n        channel.queuedeclare(queue2name, true, false, false, null);\n        //7.绑定队列和交换机\n        /**  string queue, string exchange, string routingkey\n         *  queue  队列名称\n         *  exchange 交换机名称\n         *  routingkey 路由键绑定规则   如果交换机类型为fanout 则routingkey为""\n         */\n        channel.queuebind(queue1name, exchangename, "");\n        channel.queuebind(queue2name, exchangename, "");\n\n        //8.发送消息\n        string body = "日志信息:";\n        channel.basicpublish(exchangename, "", null, body.getbytes(standardcharsets.utf_8));\n\n        //9.释放资源\n        channel.close();\n        connection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n多个消费者绑定不同的队列\n\n        //1.创建连接工厂\n        connectionfactory factory = new connectionfactory();\n        //2.设置参数\n        factory.sethost("192.168.130.124"); //ip\n        factory.setport(5672);  //端口\n        factory.setvirtualhost("/itcast");  //虚拟机 默认值\n        factory.setusername("iekr");  //用户名\n        factory.setpassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        connection connection = factory.newconnection();\n        //4.创建channel\n        channel channel = connection.createchannel();\n\n        //6.接受消息\n        string queue1name = "test_fanout_queue1";\n        string queue2name = "test_fanout_queue2";\n        consumer consumer = new defaultconsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumertag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws ioexception\n             */\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n//                system.out.println(consumertag);\n//                system.out.println(envelope.getexchange());\n//                system.out.println(envelope.getroutingkey());\n//                system.out.println(properties);\n                system.out.println(new string(body));\n                system.out.println("第一个消费者");\n            }\n        };\n        /**\n         * string var1, delivercallback var2, cancelcallback var3\n         * queue 队列名称\n         * autoack 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicconsume(queue1name,true,consumer);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# routing 路由模式\n\n\n\n生产者发送不同key的消息给交换机 而交换机根据队列的key转发消息给有标识的队列\n\n        //1.创建连接工厂\n        connectionfactory factory = new connectionfactory();\n        //2.设置参数\n        factory.sethost("192.168.130.124"); //ip\n        factory.setport(5672);  //端口\n        factory.setvirtualhost("/itcast");  //虚拟机 默认值\n        factory.setusername("iekr");  //用户名\n        factory.setpassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        connection connection = factory.newconnection();\n        //4.创建channel\n        channel channel = connection.createchannel();\n        //5.创建交换机\n        /**\n         * string var1, builtinexchangetype var2, boolean var3, boolean var4, boolean var5, map<string, object> var6\n         * exchange 交换机名称\n         * type 交换机类型 枚举  direct("direct")定向   fanout("fanout")扇形(广播)   topic("topic")通配符 headers("headers") 参数匹配\n         * durable 是否持久化\n         * autodelete 自动删除\n         * internal 内部使用 一般为false\n         * arguments 参数\n         */\n        string exchangename = "test_direct";\n        channel.exchangedeclare(exchangename, builtinexchangetype.direct, true, false, false, null);\n        //6.创建队列\n        string queue1name = "test_direct_queue1";\n        string queue2name = "test_direct_queue2";\n\n        channel.queuedeclare(queue1name, true, false, false, null);\n        channel.queuedeclare(queue2name, true, false, false, null);\n        //7.绑定队列和交换机\n        /**  string queue, string exchange, string routingkey\n         *  queue  队列名称\n         *  exchange 交换机名称\n         *  routingkey 路由键绑定规则   如果交换机类型为fanout 则routingkey为""\n         */\n        //队列1的绑定\n        channel.queuebind(queue1name, exchangename, "error");\n        //队列2的绑定\n        channel.queuebind(queue2name, exchangename, "info");\n        channel.queuebind(queue2name, exchangename, "error");\n        channel.queuebind(queue2name, exchangename, "warning");\n\n        //8.发送消息\n        string body = "日志信息:";\n        //队列1只接受error消息 而队列2所有类型都接受\n        channel.basicpublish(exchangename, "info", null, body.getbytes(standardcharsets.utf_8));\n\n        //9.释放资源\n        channel.close();\n        connection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n消费者\n\n        //1.创建连接工厂\n        connectionfactory factory = new connectionfactory();\n        //2.设置参数\n        factory.sethost("192.168.130.124"); //ip\n        factory.setport(5672);  //端口\n        factory.setvirtualhost("/itcast");  //虚拟机 默认值\n        factory.setusername("iekr");  //用户名\n        factory.setpassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        connection connection = factory.newconnection();\n        //4.创建channel\n        channel channel = connection.createchannel();\n\n        //6.接受消息\n        string queue1name = "test_direct_queue1";\n        string queue2name = "test_direct_queue2";\n        consumer consumer = new defaultconsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumertag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws ioexception\n             */\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n//                system.out.println(consumertag);\n//                system.out.println(envelope.getexchange());\n//                system.out.println(envelope.getroutingkey());\n//                system.out.println(properties);\n                system.out.println(new string(body));\n                system.out.println("队列1 存储到数据库");\n            }\n        };\n        /**\n         * string var1, delivercallback var2, cancelcallback var3\n         * queue 队列名称\n         * autoack 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicconsume(queue1name,true,consumer);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# topics 通配符模式\n\n\n\n使用通配符和路由器转发 让队列更加灵活的接受对应的消息\n\n*星号代表0个或多个单词\n\n#井号代表1个单词\n\n生产者\n\n//1.创建连接工厂\nconnectionfactory factory = new connectionfactory();\n//2.设置参数\nfactory.sethost("192.168.130.124"); //ip\nfactory.setport(5672);  //端口\nfactory.setvirtualhost("/itcast");  //虚拟机 默认值\nfactory.setusername("iekr");  //用户名\nfactory.setpassword("iekr");  //密码 默认值为guest\n//3.创建连接 connection\nconnection connection = factory.newconnection();\n//4.创建channel\nchannel channel = connection.createchannel();\n//5.创建交换机\n/**\n * string var1, builtinexchangetype var2, boolean var3, boolean var4, boolean var5, map<string, object> var6\n * exchange 交换机名称\n * type 交换机类型 枚举  direct("direct")定向   fanout("fanout")扇形(广播)   topic("topic")通配符 headers("headers") 参数匹配\n * durable 是否持久化\n * autodelete 自动删除\n * internal 内部使用 一般为false\n * arguments 参数\n */\nstring exchangename = "test_topics";\nchannel.exchangedeclare(exchangename, builtinexchangetype.topic, true, false, false, null);\n//6.创建队列\nstring queue1name = "test_topics_queue1";\nstring queue2name = "test_topics_queue2";\nchannel.queuedeclare(queue1name, true, false, false, null);\nchannel.queuedeclare(queue2name, true, false, false, null);\n//7.绑定队列和交换机\n/**  string queue, string exchange, string routingkey\n *  queue  队列名称\n *  exchange 交换机名称\n *  routingkey 路由键绑定规则   如果交换机类型为fanout 则routingkey为""\n */\n//routing key  系统的名称.日志的级别\nchannel.queuebind(queue1name, exchangename, "#.error"); //以.error结尾\nchannel.queuebind(queue1name, exchangename, "order.*");  //以order.开头\nchannel.queuebind(queue2name, exchangename, "*.*");    //队列2所有消息都可以接受到\n\n//8.发送消息\nstring body = "日志信息:";\nchannel.basicpublish(exchangename, "goods.info", null, body.getbytes(standardcharsets.utf_8));\n\n//9.释放资源\nchannel.close();\nconnection.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n消费者\n\n        //1.创建连接工厂\n        connectionfactory factory = new connectionfactory();\n        //2.设置参数\n        factory.sethost("192.168.130.124"); //ip\n        factory.setport(5672);  //端口\n        factory.setvirtualhost("/itcast");  //虚拟机 默认值\n        factory.setusername("iekr");  //用户名\n        factory.setpassword("iekr");  //密码 默认值为guest\n        //3.创建连接 connection\n        connection connection = factory.newconnection();\n        //4.创建channel\n        channel channel = connection.createchannel();\n\n        //6.接受消息\n        string queue1name = "test_topics_queue1";\n        string queue2name = "test_topics_queue2";\n        consumer consumer = new defaultconsumer(channel){\n            //回调方法 当收到消息后 会执行该方法\n\n            /**\n             *\n             * @param consumertag  标识\n             * @param envelope  获取一些信息 交换机 路由key\n             * @param properties  配置信息\n             * @param body  数据\n             * @throws ioexception\n             */\n            @override\n            public void handledelivery(string consumertag, envelope envelope, amqp.basicproperties properties, byte[] body) throws ioexception {\n//                system.out.println(consumertag);\n//                system.out.println(envelope.getexchange());\n//                system.out.println(envelope.getroutingkey());\n//                system.out.println(properties);\n                system.out.println(new string(body));\n                system.out.println("队列1 存储到数据库");\n            }\n        };\n        /**\n         * string var1, delivercallback var2, cancelcallback var3\n         * queue 队列名称\n         * autoack 是否自动确认\n         * callback 回调对象\n         */\n        channel.basicconsume(queue1name,true,consumer);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# spring 整合 rabbitmq\n\n坐标\n\n    <dependencies>\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-context</artifactid>\n            <version>5.3.10</version>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework.amqp</groupid>\n            <artifactid>spring-rabbit</artifactid>\n            <version>2.3.9</version>\n        </dependency>\n        <dependency>\n            <groupid>junit</groupid>\n            <artifactid>junit</artifactid>\n            <version>4.13</version>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework</groupid>\n            <artifactid>spring-test</artifactid>\n            <version>5.3.10</version>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.apache.maven.plugins</groupid>\n                <artifactid>maven-compiler-plugin</artifactid>\n                <version>3.8.1</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\nrabbitmq.properties\n\nrabbitmq.host=192.168.130.124\nrabbitmq.port=5672\nrabbitmq.username=iekr\nrabbitmq.password=iekr\nrabbitmq.virtual-host=/itcast\n\n\n1\n2\n3\n4\n5\n\n\n\n# 生产者\n\nspring-rabbitmq-producer.xml\n\n<?xml version="1.0" encoding="utf-8" ?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:rabbit="http://www.springframework.org/schema/rabbit"\n       xsi:schemalocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/rabbit\n        https://www.springframework.org/schema/rabbit/spring-rabbit.xsd\n">\n    \x3c!--    加载配置文件--\x3e\n    <context:property-placeholder location="classpath:rabbitmq.properties"/>\n\n    \x3c!--    定义rabbitmq connectionfactory--\x3e\n    <rabbit:connection-factory id="connectionfactory" host="${rabbitmq.host}"\n                               port="${rabbitmq.port}"\n                               username="${rabbitmq.username}"\n                               password="${rabbitmq.password}"\n                               virtual-host="${rabbitmq.virtual-host}"/>\n    \x3c!--    定义管理交换机 队列--\x3e\n    <rabbit:admin connection-factory="connectionfactory"/>\n\n    \x3c!--    定义持久化队列 不存在则自动创建 不绑定到交换机则绑定到默认交换机   默认交换机为direct  名字为""  路由键位队列名称--\x3e\n    <rabbit:queue id="spring_queue" name="spring_queue" auto-declare="true"/>\n\n    \x3c!--    定义广播交换机中的持久化队列 不存在则自动创建--\x3e\n    <rabbit:queue id="spring_fanout_queue_1" name="spring_fanout_queue_1" auto-declare="true"/>\n    <rabbit:queue id="spring_fanout_queue_2" name="spring_fanout_queue_2" auto-declare="true"/>\n\n    \x3c!--    定义广播类型交换机 绑定上面两个队列--\x3e\n    <rabbit:fanout-exchange name="spring_fanout_exchange"\n                            id="spring_fanout_exchange"\n                            auto-declare="true">\n        <rabbit:bindings>\n            <rabbit:binding queue="spring_fanout_queue_1"/>\n            <rabbit:binding queue="spring_fanout_queue_2"/>\n        </rabbit:bindings>\n    </rabbit:fanout-exchange>\n\n    \x3c!--    通配符队列--\x3e\n    <rabbit:queue id="spring_topic_queue_star" name="spring_topic_queue_star" auto-declare="true"/>\n    <rabbit:queue id="spring_topic_queue_well" name="spring_topic_queue_well" auto-declare="true"/>\n    <rabbit:queue id="spring_topic_queue_well2" name="spring_topic_queue_well2" auto-declare="true"/>\n\n    \x3c!--    通配符定义--\x3e\n    <rabbit:topic-exchange name="spring_topic_exchange" id="spring_topic_exchange" auto-declare="true">\n        <rabbit:bindings>\n            <rabbit:binding pattern="heima.*" queue="spring_topic_queue_star"/>\n            <rabbit:binding pattern="heima.#" queue="spring_topic_queue_well"/>\n            <rabbit:binding pattern="itcast.#" queue="spring_topic_queue_well2"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n\n    \x3c!--    定义rabbittemplate对象操作可以在代码中方便发送消息--\x3e\n    <rabbit:template id="rabbittemplate" connection-factory="connectionfactory"/>\n\n\n</beans>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\ntest\n\npackage com.itheima;\n\nimport org.junit.test;\nimport org.junit.runner.runwith;\nimport org.springframework.amqp.rabbit.core.rabbittemplate;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.test.context.contextconfiguration;\nimport org.springframework.test.context.junit4.springjunit4classrunner;\n\n@runwith(springjunit4classrunner.class)\n@contextconfiguration(locations = "classpath:spring-rabbitmq-producer.xml")\npublic class producertest {\n\n    //注入\n    @autowired\n    private rabbittemplate rabbittemplate;\n\n    //1对1\n    @test\n    public void testhelloworld() {\n        //发送消息\n        rabbittemplate.convertandsend("spring_queue", "hello world spring ...");\n    }\n\n    //广播\n    @test\n    public void testfanout() {\n        //发送消息\n        rabbittemplate.convertandsend("spring_fanout_exchange", "", "spring fanout...");\n    }\n\n    //topic\n    @test\n    public void testtopic() {\n        //发送消息\n        rabbittemplate.convertandsend("spring_topic_exchange", "heima.123.456", "spring topic...");\n    }\n\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n# 消费者\n\nspring-rabbitmq-consumer.xml\n\n<?xml version="1.0" encoding="utf-8" ?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:p="http://www.springframework.org/schema/p"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:rabbit="http://www.springframework.org/schema/rabbit"\n       xsi:schemalocation="\n       http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/rabbit\n        https://www.springframework.org/schema/rabbit/spring-rabbit.xsd\n">\n    \x3c!--    加载配置文件--\x3e\n    <context:property-placeholder location="classpath:rabbitmq.properties"/>\n\n    \x3c!--    定义rabbitmq connectionfactory--\x3e\n    <rabbit:connection-factory id="connectionfactory" host="${rabbitmq.host}"\n                               port="${rabbitmq.port}"\n                               username="${rabbitmq.username}"\n                               password="${rabbitmq.password}"\n                               virtual-host="${rabbitmq.virtual-host}"/>\n\n    <bean id="springqueuelistener" class="com.itheima.rabbitmq.springqueuelistener"/>\n\x3c!--    <bean id="fanoutlistener1" class="com.itheima.rabbitmq.fanoutlistener1"/>--\x3e\n\x3c!--    <bean id="fanoutlistener2" class="com.itheima.rabbitmq.fanoutlistener2"/>--\x3e\n\x3c!--    <bean id="topiclistenerstar" class="com.itheima.rabbitmq.topiclistenerstar"/>--\x3e\n\x3c!--    <bean id="topiclistenerwell" class="com.itheima.rabbitmq.topiclistenerwell"/>--\x3e\n\x3c!--    <bean id="topiclistenerwell2" class="com.itheima.rabbitmq.topiclistenerwell2"/>--\x3e\n\n    <rabbit:listener-container connection-factory="connectionfactory" auto-declare="true">\n        <rabbit:listener ref="springqueuelistener" queue-names="spring_queue"/>\n\x3c!--        <rabbit:listener ref="fanoutlistener1" queue-names="spring_fanout_queue1"/>--\x3e\n\x3c!--        <rabbit:listener ref="fanoutlistener2" queue-names="spring_fanout_queue2"/>--\x3e\n\x3c!--        <rabbit:listener ref="topiclistenerstar" queue-names="spring_topic_queue_star"/>--\x3e\n\x3c!--        <rabbit:listener ref="topiclistenerwell" queue-names="spring_topic_queue_well"/>--\x3e\n\x3c!--        <rabbit:listener ref="topiclistenerwell2" queue-names="spring_topic_queue_well2"/>--\x3e\n    </rabbit:listener-container>\n\n\n</beans>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n消费者类 根据bean id 编写对应的类名 并 实现 messagelistener 重写 onmessage 方法\n\npackage com.itheima.rabbitmq;\n\nimport org.springframework.amqp.core.message;\nimport org.springframework.amqp.core.messagelistener;\n\npublic class springqueuelistener implements messagelistener {\n    @override\n    public void onmessage(message message) {\n        system.out.println(new string(message.getbody()));\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# spring boot 整合 rabbitmq\n\n\n\n\n# 生产者\n\n绑定交换机和队列\n\npackage com.example.springrabbitmqproducer.rabbitmq.config;\n\nimport org.springframework.amqp.core.*;\nimport org.springframework.beans.factory.annotation.qualifier;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\n\n@configuration\npublic class rabbitmqconfig {\n    public static final string exchange_name = "boot_topic_exchange";\n    public static final string queue_name = "boot_queue";\n\n    //1.交换机\n    @bean("bootexchange")\n    public exchange bootexchange() {\n        //获取4种类型的交换机\n        return exchangebuilder.topicexchange(exchange_name).durable(true).build();\n    }\n\n    //2.队列\n    @bean("bootqueue")\n    public queue bootqueue() {\n        return queuebuilder.durable(queue_name).build();\n    }\n\n    //3.队列和交换机的绑定\n    @bean\n    public binding bindqueueexchange(@qualifier("bootqueue") queue queue, @qualifier("bootexchange") exchange exchange) {\n        return bindingbuilder.bind(queue).to(exchange).with("boot.#").noargs();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\ntest\n\npackage com.example.springrabbitmqproducer;\n\nimport com.example.springrabbitmqproducer.rabbitmq.config.rabbitmqconfig;\nimport org.junit.jupiter.api.test;\nimport org.springframework.amqp.rabbit.core.rabbittemplate;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.boot.test.context.springboottest;\n\n@springboottest\nclass springrabbitmqproducerapplicationtests {\n\n    //注入rabbittemplate\n    @autowired\n    private rabbittemplate rabbittemplate;\n\n    @test\n    public void testsend(){\n        rabbittemplate.convertandsend(rabbitmqconfig.exchange_name,"boot.haha","boot mq hello");\n    }\n\n\n\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 消费者\n\n@component\npublic class rabbitmqlistener {\n\n    //监听指定队列\n    @rabbitlistener(queues = "boot_queue")\n    public void listenerqueue(message message) {      \n        system.out.println(new string(message.getbody()));\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"RabbitMQ 高级",frontmatter:{title:"RabbitMQ 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/da3871/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/16.RabbitMQ%20%E9%AB%98%E7%BA%A7.html",relativePath:"后端/02.JavaEE/16.RabbitMQ 高级.md",key:"v-0f5ad0f4",path:"/pages/da3871/",headers:[{level:2,title:"消息的可靠投递(生产端)",slug:"消息的可靠投递-生产端",normalizedTitle:"消息的可靠投递(生产端)",charIndex:18},{level:2,title:"Consumer Ack(消费端)",slug:"consumer-ack-消费端",normalizedTitle:"consumer ack(消费端)",charIndex:2510},{level:2,title:"消费端限流(消费端)",slug:"消费端限流-消费端",normalizedTitle:"消费端限流(消费端)",charIndex:4273},{level:2,title:"TTL(生产端)",slug:"ttl-生产端",normalizedTitle:"ttl(生产端)",charIndex:5616},{level:2,title:"死信队列",slug:"死信队列",normalizedTitle:"死信队列",charIndex:6806},{level:2,title:"延迟队列",slug:"延迟队列",normalizedTitle:"延迟队列",charIndex:10661},{level:2,title:"日志监控",slug:"日志监控",normalizedTitle:"日志监控",charIndex:13699},{level:3,title:"消息追踪",slug:"消息追踪",normalizedTitle:"消息追踪",charIndex:13761},{level:2,title:"消息补偿",slug:"消息补偿",normalizedTitle:"消息补偿",charIndex:13833},{level:2,title:"消息幂等性保障",slug:"消息幂等性保障",normalizedTitle:"消息幂等性保障",charIndex:13844},{level:2,title:"集群搭建",slug:"集群搭建",normalizedTitle:"集群搭建",charIndex:13955},{level:3,title:"镜像队列",slug:"镜像队列",normalizedTitle:"镜像队列",charIndex:14616},{level:3,title:"负载均衡-HAProxy",slug:"负载均衡-haproxy",normalizedTitle:"负载均衡-haproxy",charIndex:14739}],headersStr:"消息的可靠投递(生产端) Consumer Ack(消费端) 消费端限流(消费端) TTL(生产端) 死信队列 延迟队列 日志监控 消息追踪 消息补偿 消息幂等性保障 集群搭建 镜像队列 负载均衡-HAProxy",content:'# RabbitMQ 高级\n\n\n# 消息的可靠投递(生产端)\n\nRabbitMQ为我们提供了两种方式用来控制消息的投递可靠性模式\n\n * confirm 确认模式 已经过时\n * return 退回模式\n\nrabbitmq 整个消息投递路径为\n\nproducer ---\x3e rabbitmq broker --\x3e exchange ---\x3e queue --\x3e consumer\n\n * 消息从 producer 到 exchange 则会返回一个 confirmCallback\n * 消息从 exchange --\x3e queue 投递失败则会返回一个 returnCallback\n\n利用这两个callback控制消息的可靠性投递\n\nrabbitmq配置文件\n\n    \x3c!--    加载配置文件--\x3e\n    <context:property-placeholder location="classpath:rabbitmq.properties"/>\n\n    \x3c!--    定义rabbitmq connectionFactory--\x3e\n    \x3c!--  returns开启回退模式设置为 publisher-returns="true" --\x3e\n    <rabbit:connection-factory id="connectionFactory" host="${rabbitmq.host}"\n                               port="${rabbitmq.port}"\n                               username="${rabbitmq.username}"\n                               password="${rabbitmq.password}"\n                               virtual-host="${rabbitmq.virtual-host}"\n                               publisher-returns="true"\n\n    />\n\x3c!-- 消息可靠性投递(生产端) --\x3e\n    <rabbit:queue id="test_queue_confirm" name="test_queue_confirm"/>\n    <rabbit:direct-exchange name="test_exchange_confirm">\n        <rabbit:bindings>\n            <rabbit:binding queue="test_queue_confirm" key="confirm"/>\n        </rabbit:bindings>\n    </rabbit:direct-exchange>\n\n\n    \x3c!--    定义rabbitTemplate对象操作可以在代码中方便发送消息--\x3e\n    <rabbit:template id="rabbitTemplate" connection-factory="connectionFactory"/>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\ntest\n\n    /*\n    回退模式 :当消息发送给Exchange后,Exchange路由到Queue失败是 才会执行 ReturnCallBack\n     */\n    @Test\n    public void testReturn(){\n\n        //设置交换机处理失败消息的模式 必须开启之后 失败发送失败才会回调\n        rabbitTemplate.setMandatory(true);\n\n        //设置returncallback\n        rabbitTemplate.setReturnsCallback(new RabbitTemplate.ReturnsCallback() {\n            @Override\n            public void returnedMessage(ReturnedMessage returnedMessage) {\n                System.out.println("return 执行了");\n                System.out.println(returnedMessage.getMessage());  //消息对象\n                System.out.println(returnedMessage.getReplyCode());  //错误码\n                System.out.println(returnedMessage.getReplyText());  //错误信息\n                System.out.println(returnedMessage.getExchange());  //交换机\n                System.out.println(returnedMessage.getRoutingKey());  //路由键\n            }\n        });\n\n        //给不存在频道中的key发送消息 \n        rabbitTemplate.convertAndSend("test_exchange_confirm","confirm111","hello callback");\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# Consumer Ack(消费端)\n\nack指 Acknowledge 确认 表示消费端收到消息后的确认方式\n\n有三种确认方式\n\n * 默认为自动确认 acknowledge="none"\n * 手动确认 acknowledge="manual"\n * 根据异常情况确认 acknowledge="auto"\n\nrabbitmq配置文件 消费端\n\n    \x3c!--spring扫描监听器类    --\x3e\n    <context:component-scan base-package="com.itheima.listener"/>\n\n    \x3c!--    定义监听器容器--\x3e\n    <rabbit:listener-container connection-factory="connectionFactory" acknowledge="manual">\n        <rabbit:listener ref="ackListener" queue-names="test_queue_confirm" />\n    </rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.Channel;\nimport org.springframework.amqp.core.Message;\nimport org.springframework.amqp.rabbit.listener.api.ChannelAwareMessageListener;\nimport org.springframework.stereotype.Component;\n\nimport java.io.IOException;\n\n/**\n * Consumer ACK机制\n * 设置手动签收 acknowledge="manual"\n * 让监听器类实现 ChannelAwareMessageListener 接口 实现 onMessage方法\n */\n\n@Component\npublic class AckListener implements ChannelAwareMessageListener {\n    @Override\n    public void onMessage(Message message, Channel channel) throws Exception {\n        long deliveryTag = message.getMessageProperties().getDeliveryTag();\n\n        try {\n            //接受信息\n            System.out.println(new String(message.getBody()));\n\n            //业务逻辑\n            System.out.println("处理");\n\n            //手动签收\n            channel.basicAck(deliveryTag, true);\n        } catch (IOException e) {\n//            e.printStackTrace();\n            //拒绝签收  basicNack允许多条消息 第三个参数requeue     设置为true则重新回到队列中\n            channel.basicNack(deliveryTag, true, true);\n//            channel.basicReject(deliveryTag, true);\n        }\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# 消费端限流(消费端)\n\nrabbitmq配置文件\n\n    \x3c!--spring扫描监听器类    --\x3e\n    <context:component-scan base-package="com.itheima.listener"/>\n\n    \x3c!--    定义监听器容器--\x3e\n    <rabbit:listener-container connection-factory="connectionFactory" acknowledge="manual" prefetch="1">\n\x3c!--        <rabbit:listener ref="ackListener" queue-names="test_queue_confirm" />--\x3e\n        <rabbit:listener ref="qosListener" queue-names="test_queue_confirm" />\n    </rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n限流监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.Channel;\nimport org.springframework.amqp.core.Message;\nimport org.springframework.amqp.rabbit.listener.api.ChannelAwareMessageListener;\nimport org.springframework.stereotype.Component;\n\n/**\n * 限流机制\n * acr机制设置为手动确认\n * listener-container配置属性 perfetch = 1  表示消费端每次从mq拉取一条消息  直到手动确认消费完毕后才去拉取下一条消息\n */\n@Component\npublic class QosListener implements ChannelAwareMessageListener {\n    @Override\n    public void onMessage(Message message, Channel channel) throws Exception {\n        //获取消息\n        System.out.println(new String(message.getBody()));\n\n        //处理业务逻辑\n        System.out.println("业务逻辑");\n\n\n        //签收  直到手动确认消费完毕后才去拉取下一条消息\n        channel.basicAck(message.getMessageProperties().getDeliveryTag(),true);\n\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# TTL(生产端)\n\nTTL 为存活时间 / 过期时间\n\n当消息到达存活时间后 还没有被消费 会被自动清除\n\nRabbitMQ可以对消息设置过期时间 也可以对整个队列设置过期时间\n\n生产端配置文件\n\n    \x3c!--    ttl--\x3e\n    <rabbit:queue name="test_spring_queue_ttl" auto-declare="true">\n        <rabbit:queue-arguments>\n            <entry key="x-message-ttl" value-type="long" value="5000"/>\n        </rabbit:queue-arguments>\n    </rabbit:queue>\n\n    <rabbit:topic-exchange name="test_exchange_ttl">\n        <rabbit:bindings>\n            <rabbit:binding pattern="ttl.#" queue="test_spring_queue_ttl"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\ntest发送消息\n\n@Test\npublic void testTtl() {\n    //ttl发送消息\n    rabbitTemplate.convertAndSend("test_exchange_ttl", "ttl.eee", "hello ttl", new MessagePostProcessor() {\n        //消息后处理对象 设置一些消息的参数信息\n        @Override\n        public Message postProcessMessage(Message message) throws AmqpException {\n            //1.设置message的消息\n            message.getMessageProperties().setExpiration("5000");//消息的过期时间 毫秒\n\n            return message;  //如果设置了队列过期时间和消息的过期时间 则以时间短的为准\n            //队列过期后会将该队列的所有消息清空\n            //消息过期后.只有消息在队列顶端,才会判断其是否过期\n        }\n    });\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 死信队列\n\nDLX(Dead Letter Exchange) 死信交换机 当消息成为 Dead message后可以被重新发送到另外一台交换机 那么这个交换机就是DLX\n\n\n\n消息成为死信的情况\n\n 1. 队列消息长度到达限制\n 2. 消费者拒接消费消息,basicNack/basicReject 并且不把消息重新放入原目标队列.requeue=false\n 3. 原队列存在消费过期设置 消息到达超时时间未被消费\n\n定义正常队列和死信队列\n\n    \x3c!--    死信队列 --\x3e\n    \x3c!--     1.声明正常的队列(test_queue_dlx)和交换机(test_exchange_dlx)--\x3e\n    <rabbit:queue id="test_queue_dlx" name="test_queue_dlx">\n        \x3c!--            3.正常队列绑定死信交换机--\x3e\n        <rabbit:queue-arguments>\n            \x3c!--         - x-dead-letter-exchange:死信交换机名称--\x3e\n            <entry key="x-dead-letter-exchange" value="exchange_dlx"/>\n            \x3c!--                    - x-dead-letter-routing-key 死信交换机的routingkey--\x3e\n            <entry key="x-dead-letter-routing-key" value="dlx.hehe"/>\n            \x3c!--        设置队列过期时间--\x3e\n            <entry key="x-message-ttl" value="5000" value-type="java.lang.Integer"/>\n            \x3c!--        设置队列长度--\x3e\n            <entry key="x-max-length" value="10" value-type="java.lang.Integer"/>\n        </rabbit:queue-arguments>\n    </rabbit:queue>\n    <rabbit:topic-exchange name="test_exchange_dlx">\n        <rabbit:bindings>\n            <rabbit:binding pattern="test.dlx.#" queue="test_queue_dlx"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n    \x3c!--    2.声明死信的队列(queue.dlx)和交换机(exchange_dxl)--\x3e\n    <rabbit:queue id="queue_dlx" name="queue_dlx"/>\n    <rabbit:topic-exchange name="exchange_dlx">\n        <rabbit:bindings>\n            <rabbit:binding pattern="dlx.#" queue="queue_dlx"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\ntest\n\n    @Test\n    public void testDlx() {\n        //1.测试过期时间\n//        rabbitTemplate.convertAndSend("test_exchange_dlx", "test.dlx.haha", "dlx");\n\n        //2.超过队列长度  消息死信\n//        for (int i = 0; i < 20; i++) {\n//            rabbitTemplate.convertAndSend("test_exchange_dlx", "test.dlx.haha", "dlx" + i);\n//        }\n\n        //3.消费者拒接签收消息 并设置不重回队列中\n        rabbitTemplate.convertAndSend("test_exchange_dlx", "test.dlx.haha", "dlx");\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n消费者拒接签收消息 监听配置\n\n    <context:component-scan base-package="com.itheima.listener"/>\n\x3c!--    定义监听器容器--\x3e\n    <rabbit:listener-container connection-factory="connectionFactory" acknowledge="manual" prefetch="1">\n        \x3c!--        <rabbit:listener ref="ackListener" queue-names="test_queue_confirm" />--\x3e\n        \x3c!--        <rabbit:listener ref="qosListener" queue-names="test_queue_confirm" />--\x3e\n        \x3c!--        监听正常的队列--\x3e\n        <rabbit:listener ref="dlxListener" queue-names="test_queue_dlx"/>\n    </rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.Channel;\nimport org.springframework.amqp.core.Message;\nimport org.springframework.amqp.rabbit.listener.api.ChannelAwareMessageListener;\nimport org.springframework.stereotype.Component;\n\nimport java.io.IOException;\n\n\n@Component\npublic class DlxListener implements ChannelAwareMessageListener {\n    @Override\n    public void onMessage(Message message, Channel channel) throws Exception {\n        long deliveryTag = message.getMessageProperties().getDeliveryTag();\n\n        try {\n            //接受信息\n            System.out.println(new String(message.getBody()));\n\n            //业务逻辑\n            System.out.println("处理");\n            int i = 3 / 0;// 出现异常\n            //手动签收\n            channel.basicAck(deliveryTag, true);\n        } catch (Exception e) {\n//            e.printStackTrace();\n            System.out.println("拒绝签收");\n            //拒绝签收  basicNack允许多条消息 第三个参数requeue     设置为true则重新回到队列中 此处是死信队列所以我们不重回队列中\n            channel.basicNack(deliveryTag, true, false);\n//            channel.basicReject(deliveryTag, true);\n        }\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# 延迟队列\n\n延迟队列 即消息进入队列后不会立即被消费 只有到达指定时间后 才会被消费\n\n\n\nrabbitmq中并没有提供延迟队列功能 但是我们可以通过使用 TTL+死信队列 组合实现延迟队列的效果\n\n\n\n 1. 定义正常队列和死信队列 并且设置TTL过去时间\n\n\x3c!--    延迟队列 通过TTL+死信队列实现--\x3e\n\x3c!--    定义正常队列 和交换机--\x3e\n<rabbit:queue id="order_queue" name="order_queue">\n    \x3c!--        绑定死信队列 和设置TTL过期时间--\x3e\n     <rabbit:queue-arguments>\n         <entry key="x-dead-letter-exchange" value="order_exchange_dlx"/>\n         <entry key="x-dead-letter-routing-key" value="dlx.order.cancel"/>\n         <entry key="x-message-ttl" value="5000" value-type="java.lang.Integer"/>\n     </rabbit:queue-arguments>\n</rabbit:queue>\n<rabbit:topic-exchange name="order_exchange">\n    <rabbit:bindings>\n        <rabbit:binding pattern="order.#" queue="order_queue"/>\n    </rabbit:bindings>\n</rabbit:topic-exchange>\n\x3c!--    死信队列--\x3e\n<rabbit:queue id="order_queue_dlx" name="order_queue_dlx">\n</rabbit:queue>\n<rabbit:topic-exchange name="order_exchange_dlx">\n    <rabbit:bindings>\n        <rabbit:binding pattern="dlx.order.#" queue="order_queue_dlx"/>\n    </rabbit:bindings>\n</rabbit:topic-exchange>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n 2. 发送消息\n\n@Test\npublic void  testDelay(){\n    rabbitTemplate.convertAndSend("order_exchange","order.msg","延迟队列");\n}\n\n\n1\n2\n3\n4\n\n 3. 监听配置\n\n\x3c!--spring扫描监听器类    --\x3e\n<context:component-scan base-package="com.itheima.listener"/>\n\n\x3c!--    定义监听器容器--\x3e\n<rabbit:listener-container connection-factory="connectionFactory" acknowledge="manual" prefetch="1">\n    \x3c!--        <rabbit:listener ref="ackListener" queue-names="test_queue_confirm" />--\x3e\n    \x3c!--        <rabbit:listener ref="qosListener" queue-names="test_queue_confirm" />--\x3e\n    \x3c!--        监听正常的队列--\x3e\n    \x3c!--        <rabbit:listener ref="dlxListener" queue-names="test_queue_dlx"/>--\x3e\n    \x3c!--        延迟队列  监听的是TTL过期后的死信队列--\x3e\n    <rabbit:listener ref="orderListener" queue-names="order_queue_dlx"/>\n</rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n 4. 监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.Channel;\nimport org.springframework.amqp.core.Message;\nimport org.springframework.amqp.rabbit.listener.api.ChannelAwareMessageListener;\nimport org.springframework.stereotype.Component;\n\nimport java.io.IOException;\n\n@Component\npublic class OrderListener implements ChannelAwareMessageListener {\n    @Override\n    public void onMessage(Message message, Channel channel) throws Exception {\n        long deliveryTag = message.getMessageProperties().getDeliveryTag();\n\n        try {\n            //接受信息\n            System.out.println(new String(message.getBody()));\n\n            //业务逻辑\n            System.out.println("业务 处理");\n\n            //手动签收\n            channel.basicAck(deliveryTag, true);\n        } catch (IOException e) {\n//            e.printStackTrace();\n            //拒绝签收  basicNack允许多条消息 第三个参数requeue     设置为true则重新回到队列中\n            channel.basicNack(deliveryTag, true, true);\n//            channel.basicReject(deliveryTag, true);\n        }\n    }\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 日志监控\n\nRabbitMQ默认日志存放路径 /var/log/rabbitmq/rabbit@xxx.log\n\n\n\n\n# 消息追踪\n\n开启firehose后 默认路由会将消息重新发送一遍 并且包含消息从哪里传递等具体信息打包过去队列中\n\n\n\n网页插件版\n\n\n\n\n# 消息补偿\n\n\n\n\n# 消息幂等性保障\n\n幂等性指一次和多次请求某一个资源,对于资源本身应该具有同样的结果 也就是说其 任意多次执行对资源所产生的影响与一次执行的影响相同\n\n在MQ中指 消费多条相同的消息 与消费一条消息得到结果一致\n\n\n\n\n# 集群搭建\n\n停止rabbitmq服务\n\nrabbitmqctl stop\n\n\n1\n\n\n启动第一个节点 此处是单机器 多端口搭建伪集群 集群用ip区分即可\n\nRABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit1 rabbitmq-server start\n\n\n1\n\n\n启动第二个节点 因为默认web插件端口被占用所以也要设置web插口端口\n\nRABBITMQ_NODE_PORT=5674 RABBITMQ_SERVER_START_ARGS="-rabbitmq_management listener [{port,15674}]" RABBITMQ_NODENAME=rabbit2 rabbitmq-server start\n\n\n1\n\n\n设置rabbit1为主节点\n\nrabbitmqctl -n rabbit1 stop_app\nrabbitmqctl -n rabbit1 reset\nrabbitmqctl -n rabbit1 start_app\n\n\n1\n2\n3\n\n\n设置rabbit2为从节点\n\nrabbitmqctl -n rabbit2 stop_app\nrabbitmqctl -n rabbit2 reset\nrabbitmqctl -n rabbit2 join_cluster rabbit1@\'dubbo\'  #@\'\'后面为系统用户名 需要自己更改\nrabbitmqctl -n rabbit2 start_app\n\n\n1\n2\n3\n4\n\n\n\n# 镜像队列\n\n从节点默认是从主节点中获取数据 我们可以通过镜像队列来将每个节点同存放想相同数据\n\n命令方式\n\nrabbitmqctl set_policy my_ha "^"\'{"ha-mode":"all"}\'\n\n\n1\n\n\n网页方式\n\n\n\n\n# 负载均衡-HAProxy\n\nhttps://blog.csdn.net/William0318/article/details/99677701\n\n安装\n\nyum install haproxy -y\nhaproxy -v\n\n\n1\n2\n\n\n配置文件\n\nvim /etc/haproxy/haproxy.cfg\n\n\n1\n\n\n编辑内容\n\n\n#全局配置\nglobal\n    #设置日志\n    log 127.0.0.1 local0 info\n    #当前工作目录\n    chroot /usr/local/haproxy\n    #用户与用户组\n    user haproxy\n    group haproxy\n    #运行进程ID\n    uid 99\n    gid 99\n    #守护进程启动\n    daemon\n    #最大连接数\n    maxconn 4096\n#默认配置\ndefaults\n    #应用全局的日志配置\n    log global\n    #默认的模式mode {tcp|http|health}\n    #TCP是4层，HTTP是7层，health只返回OK\n    mode tcp\n    #日志类别tcplog\n    option tcplog\n    #不记录健康检查日志信息\n    option dontlognull\n    #3次失败则认为服务不可用\n    retries 3\n    #每个进程可用的最大连接数\n    maxconn 2000\n    #连接超时\n    timeout connect 5s\n    #客户端超时\n    timeout client 120s\n    #服务端超时\n    timeout server 120s\n#绑定配置\nlisten rabbitmq_cluster \n        bind 0.0.0.0:5672\n        #配置TCP模式\n        mode tcp\n        #简单的轮询\n        balance roundrobin\n        #RabbitMQ集群节点配置\n        server rmq_node1 127.0.0.1:5673 check inter 5000 rise 2 fall 3 weight 1\n        server rmq_node2 127.0.0.1:5674 check inter 5000 rise 2 fall 3 weight 1\n#haproxy监控页面地址\nlisten monitor \n        bind 0.0.0.0:8100\n        mode http\n        option httplog\n        stats enable\n        stats uri /stats\n        stats refresh 5s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n\n\n检查配置文件是否错误\n\nhaproxy -f /etc/haproxy/haproxy.cfg -c\n\n\n1\n\n\n启动\n\nhaproxy -f /etc/haproxy/haproxy.cfg -d\n\n\n1\n\n\n访问HAProxy后台\n\nhttp://192.168.130.124:8100/stats\n\n后续将消息队列的地址和ip设置为HAProxy监听的地址即可\n\n\nspring:\n  #消息队列配置\n  rabbitmq:\n    host: 192.168.0.104 #HAProxy的地址\n    port: 5672\n    username: guest\n    password: guest\n    virtual-host: /\n    publisher-returns: true\n    publisher-confirms: true\n    connection-timeout: 5000ms\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n',normalizedContent:'# rabbitmq 高级\n\n\n# 消息的可靠投递(生产端)\n\nrabbitmq为我们提供了两种方式用来控制消息的投递可靠性模式\n\n * confirm 确认模式 已经过时\n * return 退回模式\n\nrabbitmq 整个消息投递路径为\n\nproducer ---\x3e rabbitmq broker --\x3e exchange ---\x3e queue --\x3e consumer\n\n * 消息从 producer 到 exchange 则会返回一个 confirmcallback\n * 消息从 exchange --\x3e queue 投递失败则会返回一个 returncallback\n\n利用这两个callback控制消息的可靠性投递\n\nrabbitmq配置文件\n\n    \x3c!--    加载配置文件--\x3e\n    <context:property-placeholder location="classpath:rabbitmq.properties"/>\n\n    \x3c!--    定义rabbitmq connectionfactory--\x3e\n    \x3c!--  returns开启回退模式设置为 publisher-returns="true" --\x3e\n    <rabbit:connection-factory id="connectionfactory" host="${rabbitmq.host}"\n                               port="${rabbitmq.port}"\n                               username="${rabbitmq.username}"\n                               password="${rabbitmq.password}"\n                               virtual-host="${rabbitmq.virtual-host}"\n                               publisher-returns="true"\n\n    />\n\x3c!-- 消息可靠性投递(生产端) --\x3e\n    <rabbit:queue id="test_queue_confirm" name="test_queue_confirm"/>\n    <rabbit:direct-exchange name="test_exchange_confirm">\n        <rabbit:bindings>\n            <rabbit:binding queue="test_queue_confirm" key="confirm"/>\n        </rabbit:bindings>\n    </rabbit:direct-exchange>\n\n\n    \x3c!--    定义rabbittemplate对象操作可以在代码中方便发送消息--\x3e\n    <rabbit:template id="rabbittemplate" connection-factory="connectionfactory"/>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\ntest\n\n    /*\n    回退模式 :当消息发送给exchange后,exchange路由到queue失败是 才会执行 returncallback\n     */\n    @test\n    public void testreturn(){\n\n        //设置交换机处理失败消息的模式 必须开启之后 失败发送失败才会回调\n        rabbittemplate.setmandatory(true);\n\n        //设置returncallback\n        rabbittemplate.setreturnscallback(new rabbittemplate.returnscallback() {\n            @override\n            public void returnedmessage(returnedmessage returnedmessage) {\n                system.out.println("return 执行了");\n                system.out.println(returnedmessage.getmessage());  //消息对象\n                system.out.println(returnedmessage.getreplycode());  //错误码\n                system.out.println(returnedmessage.getreplytext());  //错误信息\n                system.out.println(returnedmessage.getexchange());  //交换机\n                system.out.println(returnedmessage.getroutingkey());  //路由键\n            }\n        });\n\n        //给不存在频道中的key发送消息 \n        rabbittemplate.convertandsend("test_exchange_confirm","confirm111","hello callback");\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# consumer ack(消费端)\n\nack指 acknowledge 确认 表示消费端收到消息后的确认方式\n\n有三种确认方式\n\n * 默认为自动确认 acknowledge="none"\n * 手动确认 acknowledge="manual"\n * 根据异常情况确认 acknowledge="auto"\n\nrabbitmq配置文件 消费端\n\n    \x3c!--spring扫描监听器类    --\x3e\n    <context:component-scan base-package="com.itheima.listener"/>\n\n    \x3c!--    定义监听器容器--\x3e\n    <rabbit:listener-container connection-factory="connectionfactory" acknowledge="manual">\n        <rabbit:listener ref="acklistener" queue-names="test_queue_confirm" />\n    </rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.channel;\nimport org.springframework.amqp.core.message;\nimport org.springframework.amqp.rabbit.listener.api.channelawaremessagelistener;\nimport org.springframework.stereotype.component;\n\nimport java.io.ioexception;\n\n/**\n * consumer ack机制\n * 设置手动签收 acknowledge="manual"\n * 让监听器类实现 channelawaremessagelistener 接口 实现 onmessage方法\n */\n\n@component\npublic class acklistener implements channelawaremessagelistener {\n    @override\n    public void onmessage(message message, channel channel) throws exception {\n        long deliverytag = message.getmessageproperties().getdeliverytag();\n\n        try {\n            //接受信息\n            system.out.println(new string(message.getbody()));\n\n            //业务逻辑\n            system.out.println("处理");\n\n            //手动签收\n            channel.basicack(deliverytag, true);\n        } catch (ioexception e) {\n//            e.printstacktrace();\n            //拒绝签收  basicnack允许多条消息 第三个参数requeue     设置为true则重新回到队列中\n            channel.basicnack(deliverytag, true, true);\n//            channel.basicreject(deliverytag, true);\n        }\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n# 消费端限流(消费端)\n\nrabbitmq配置文件\n\n    \x3c!--spring扫描监听器类    --\x3e\n    <context:component-scan base-package="com.itheima.listener"/>\n\n    \x3c!--    定义监听器容器--\x3e\n    <rabbit:listener-container connection-factory="connectionfactory" acknowledge="manual" prefetch="1">\n\x3c!--        <rabbit:listener ref="acklistener" queue-names="test_queue_confirm" />--\x3e\n        <rabbit:listener ref="qoslistener" queue-names="test_queue_confirm" />\n    </rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n限流监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.channel;\nimport org.springframework.amqp.core.message;\nimport org.springframework.amqp.rabbit.listener.api.channelawaremessagelistener;\nimport org.springframework.stereotype.component;\n\n/**\n * 限流机制\n * acr机制设置为手动确认\n * listener-container配置属性 perfetch = 1  表示消费端每次从mq拉取一条消息  直到手动确认消费完毕后才去拉取下一条消息\n */\n@component\npublic class qoslistener implements channelawaremessagelistener {\n    @override\n    public void onmessage(message message, channel channel) throws exception {\n        //获取消息\n        system.out.println(new string(message.getbody()));\n\n        //处理业务逻辑\n        system.out.println("业务逻辑");\n\n\n        //签收  直到手动确认消费完毕后才去拉取下一条消息\n        channel.basicack(message.getmessageproperties().getdeliverytag(),true);\n\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# ttl(生产端)\n\nttl 为存活时间 / 过期时间\n\n当消息到达存活时间后 还没有被消费 会被自动清除\n\nrabbitmq可以对消息设置过期时间 也可以对整个队列设置过期时间\n\n生产端配置文件\n\n    \x3c!--    ttl--\x3e\n    <rabbit:queue name="test_spring_queue_ttl" auto-declare="true">\n        <rabbit:queue-arguments>\n            <entry key="x-message-ttl" value-type="long" value="5000"/>\n        </rabbit:queue-arguments>\n    </rabbit:queue>\n\n    <rabbit:topic-exchange name="test_exchange_ttl">\n        <rabbit:bindings>\n            <rabbit:binding pattern="ttl.#" queue="test_spring_queue_ttl"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\ntest发送消息\n\n@test\npublic void testttl() {\n    //ttl发送消息\n    rabbittemplate.convertandsend("test_exchange_ttl", "ttl.eee", "hello ttl", new messagepostprocessor() {\n        //消息后处理对象 设置一些消息的参数信息\n        @override\n        public message postprocessmessage(message message) throws amqpexception {\n            //1.设置message的消息\n            message.getmessageproperties().setexpiration("5000");//消息的过期时间 毫秒\n\n            return message;  //如果设置了队列过期时间和消息的过期时间 则以时间短的为准\n            //队列过期后会将该队列的所有消息清空\n            //消息过期后.只有消息在队列顶端,才会判断其是否过期\n        }\n    });\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 死信队列\n\ndlx(dead letter exchange) 死信交换机 当消息成为 dead message后可以被重新发送到另外一台交换机 那么这个交换机就是dlx\n\n\n\n消息成为死信的情况\n\n 1. 队列消息长度到达限制\n 2. 消费者拒接消费消息,basicnack/basicreject 并且不把消息重新放入原目标队列.requeue=false\n 3. 原队列存在消费过期设置 消息到达超时时间未被消费\n\n定义正常队列和死信队列\n\n    \x3c!--    死信队列 --\x3e\n    \x3c!--     1.声明正常的队列(test_queue_dlx)和交换机(test_exchange_dlx)--\x3e\n    <rabbit:queue id="test_queue_dlx" name="test_queue_dlx">\n        \x3c!--            3.正常队列绑定死信交换机--\x3e\n        <rabbit:queue-arguments>\n            \x3c!--         - x-dead-letter-exchange:死信交换机名称--\x3e\n            <entry key="x-dead-letter-exchange" value="exchange_dlx"/>\n            \x3c!--                    - x-dead-letter-routing-key 死信交换机的routingkey--\x3e\n            <entry key="x-dead-letter-routing-key" value="dlx.hehe"/>\n            \x3c!--        设置队列过期时间--\x3e\n            <entry key="x-message-ttl" value="5000" value-type="java.lang.integer"/>\n            \x3c!--        设置队列长度--\x3e\n            <entry key="x-max-length" value="10" value-type="java.lang.integer"/>\n        </rabbit:queue-arguments>\n    </rabbit:queue>\n    <rabbit:topic-exchange name="test_exchange_dlx">\n        <rabbit:bindings>\n            <rabbit:binding pattern="test.dlx.#" queue="test_queue_dlx"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n    \x3c!--    2.声明死信的队列(queue.dlx)和交换机(exchange_dxl)--\x3e\n    <rabbit:queue id="queue_dlx" name="queue_dlx"/>\n    <rabbit:topic-exchange name="exchange_dlx">\n        <rabbit:bindings>\n            <rabbit:binding pattern="dlx.#" queue="queue_dlx"/>\n        </rabbit:bindings>\n    </rabbit:topic-exchange>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\ntest\n\n    @test\n    public void testdlx() {\n        //1.测试过期时间\n//        rabbittemplate.convertandsend("test_exchange_dlx", "test.dlx.haha", "dlx");\n\n        //2.超过队列长度  消息死信\n//        for (int i = 0; i < 20; i++) {\n//            rabbittemplate.convertandsend("test_exchange_dlx", "test.dlx.haha", "dlx" + i);\n//        }\n\n        //3.消费者拒接签收消息 并设置不重回队列中\n        rabbittemplate.convertandsend("test_exchange_dlx", "test.dlx.haha", "dlx");\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n消费者拒接签收消息 监听配置\n\n    <context:component-scan base-package="com.itheima.listener"/>\n\x3c!--    定义监听器容器--\x3e\n    <rabbit:listener-container connection-factory="connectionfactory" acknowledge="manual" prefetch="1">\n        \x3c!--        <rabbit:listener ref="acklistener" queue-names="test_queue_confirm" />--\x3e\n        \x3c!--        <rabbit:listener ref="qoslistener" queue-names="test_queue_confirm" />--\x3e\n        \x3c!--        监听正常的队列--\x3e\n        <rabbit:listener ref="dlxlistener" queue-names="test_queue_dlx"/>\n    </rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.channel;\nimport org.springframework.amqp.core.message;\nimport org.springframework.amqp.rabbit.listener.api.channelawaremessagelistener;\nimport org.springframework.stereotype.component;\n\nimport java.io.ioexception;\n\n\n@component\npublic class dlxlistener implements channelawaremessagelistener {\n    @override\n    public void onmessage(message message, channel channel) throws exception {\n        long deliverytag = message.getmessageproperties().getdeliverytag();\n\n        try {\n            //接受信息\n            system.out.println(new string(message.getbody()));\n\n            //业务逻辑\n            system.out.println("处理");\n            int i = 3 / 0;// 出现异常\n            //手动签收\n            channel.basicack(deliverytag, true);\n        } catch (exception e) {\n//            e.printstacktrace();\n            system.out.println("拒绝签收");\n            //拒绝签收  basicnack允许多条消息 第三个参数requeue     设置为true则重新回到队列中 此处是死信队列所以我们不重回队列中\n            channel.basicnack(deliverytag, true, false);\n//            channel.basicreject(deliverytag, true);\n        }\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# 延迟队列\n\n延迟队列 即消息进入队列后不会立即被消费 只有到达指定时间后 才会被消费\n\n\n\nrabbitmq中并没有提供延迟队列功能 但是我们可以通过使用 ttl+死信队列 组合实现延迟队列的效果\n\n\n\n 1. 定义正常队列和死信队列 并且设置ttl过去时间\n\n\x3c!--    延迟队列 通过ttl+死信队列实现--\x3e\n\x3c!--    定义正常队列 和交换机--\x3e\n<rabbit:queue id="order_queue" name="order_queue">\n    \x3c!--        绑定死信队列 和设置ttl过期时间--\x3e\n     <rabbit:queue-arguments>\n         <entry key="x-dead-letter-exchange" value="order_exchange_dlx"/>\n         <entry key="x-dead-letter-routing-key" value="dlx.order.cancel"/>\n         <entry key="x-message-ttl" value="5000" value-type="java.lang.integer"/>\n     </rabbit:queue-arguments>\n</rabbit:queue>\n<rabbit:topic-exchange name="order_exchange">\n    <rabbit:bindings>\n        <rabbit:binding pattern="order.#" queue="order_queue"/>\n    </rabbit:bindings>\n</rabbit:topic-exchange>\n\x3c!--    死信队列--\x3e\n<rabbit:queue id="order_queue_dlx" name="order_queue_dlx">\n</rabbit:queue>\n<rabbit:topic-exchange name="order_exchange_dlx">\n    <rabbit:bindings>\n        <rabbit:binding pattern="dlx.order.#" queue="order_queue_dlx"/>\n    </rabbit:bindings>\n</rabbit:topic-exchange>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n 2. 发送消息\n\n@test\npublic void  testdelay(){\n    rabbittemplate.convertandsend("order_exchange","order.msg","延迟队列");\n}\n\n\n1\n2\n3\n4\n\n 3. 监听配置\n\n\x3c!--spring扫描监听器类    --\x3e\n<context:component-scan base-package="com.itheima.listener"/>\n\n\x3c!--    定义监听器容器--\x3e\n<rabbit:listener-container connection-factory="connectionfactory" acknowledge="manual" prefetch="1">\n    \x3c!--        <rabbit:listener ref="acklistener" queue-names="test_queue_confirm" />--\x3e\n    \x3c!--        <rabbit:listener ref="qoslistener" queue-names="test_queue_confirm" />--\x3e\n    \x3c!--        监听正常的队列--\x3e\n    \x3c!--        <rabbit:listener ref="dlxlistener" queue-names="test_queue_dlx"/>--\x3e\n    \x3c!--        延迟队列  监听的是ttl过期后的死信队列--\x3e\n    <rabbit:listener ref="orderlistener" queue-names="order_queue_dlx"/>\n</rabbit:listener-container>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n 4. 监听类\n\npackage com.itheima.listener;\n\nimport com.rabbitmq.client.channel;\nimport org.springframework.amqp.core.message;\nimport org.springframework.amqp.rabbit.listener.api.channelawaremessagelistener;\nimport org.springframework.stereotype.component;\n\nimport java.io.ioexception;\n\n@component\npublic class orderlistener implements channelawaremessagelistener {\n    @override\n    public void onmessage(message message, channel channel) throws exception {\n        long deliverytag = message.getmessageproperties().getdeliverytag();\n\n        try {\n            //接受信息\n            system.out.println(new string(message.getbody()));\n\n            //业务逻辑\n            system.out.println("业务 处理");\n\n            //手动签收\n            channel.basicack(deliverytag, true);\n        } catch (ioexception e) {\n//            e.printstacktrace();\n            //拒绝签收  basicnack允许多条消息 第三个参数requeue     设置为true则重新回到队列中\n            channel.basicnack(deliverytag, true, true);\n//            channel.basicreject(deliverytag, true);\n        }\n    }\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 日志监控\n\nrabbitmq默认日志存放路径 /var/log/rabbitmq/rabbit@xxx.log\n\n\n\n\n# 消息追踪\n\n开启firehose后 默认路由会将消息重新发送一遍 并且包含消息从哪里传递等具体信息打包过去队列中\n\n\n\n网页插件版\n\n\n\n\n# 消息补偿\n\n\n\n\n# 消息幂等性保障\n\n幂等性指一次和多次请求某一个资源,对于资源本身应该具有同样的结果 也就是说其 任意多次执行对资源所产生的影响与一次执行的影响相同\n\n在mq中指 消费多条相同的消息 与消费一条消息得到结果一致\n\n\n\n\n# 集群搭建\n\n停止rabbitmq服务\n\nrabbitmqctl stop\n\n\n1\n\n\n启动第一个节点 此处是单机器 多端口搭建伪集群 集群用ip区分即可\n\nrabbitmq_node_port=5673 rabbitmq_nodename=rabbit1 rabbitmq-server start\n\n\n1\n\n\n启动第二个节点 因为默认web插件端口被占用所以也要设置web插口端口\n\nrabbitmq_node_port=5674 rabbitmq_server_start_args="-rabbitmq_management listener [{port,15674}]" rabbitmq_nodename=rabbit2 rabbitmq-server start\n\n\n1\n\n\n设置rabbit1为主节点\n\nrabbitmqctl -n rabbit1 stop_app\nrabbitmqctl -n rabbit1 reset\nrabbitmqctl -n rabbit1 start_app\n\n\n1\n2\n3\n\n\n设置rabbit2为从节点\n\nrabbitmqctl -n rabbit2 stop_app\nrabbitmqctl -n rabbit2 reset\nrabbitmqctl -n rabbit2 join_cluster rabbit1@\'dubbo\'  #@\'\'后面为系统用户名 需要自己更改\nrabbitmqctl -n rabbit2 start_app\n\n\n1\n2\n3\n4\n\n\n\n# 镜像队列\n\n从节点默认是从主节点中获取数据 我们可以通过镜像队列来将每个节点同存放想相同数据\n\n命令方式\n\nrabbitmqctl set_policy my_ha "^"\'{"ha-mode":"all"}\'\n\n\n1\n\n\n网页方式\n\n\n\n\n# 负载均衡-haproxy\n\nhttps://blog.csdn.net/william0318/article/details/99677701\n\n安装\n\nyum install haproxy -y\nhaproxy -v\n\n\n1\n2\n\n\n配置文件\n\nvim /etc/haproxy/haproxy.cfg\n\n\n1\n\n\n编辑内容\n\n\n#全局配置\nglobal\n    #设置日志\n    log 127.0.0.1 local0 info\n    #当前工作目录\n    chroot /usr/local/haproxy\n    #用户与用户组\n    user haproxy\n    group haproxy\n    #运行进程id\n    uid 99\n    gid 99\n    #守护进程启动\n    daemon\n    #最大连接数\n    maxconn 4096\n#默认配置\ndefaults\n    #应用全局的日志配置\n    log global\n    #默认的模式mode {tcp|http|health}\n    #tcp是4层，http是7层，health只返回ok\n    mode tcp\n    #日志类别tcplog\n    option tcplog\n    #不记录健康检查日志信息\n    option dontlognull\n    #3次失败则认为服务不可用\n    retries 3\n    #每个进程可用的最大连接数\n    maxconn 2000\n    #连接超时\n    timeout connect 5s\n    #客户端超时\n    timeout client 120s\n    #服务端超时\n    timeout server 120s\n#绑定配置\nlisten rabbitmq_cluster \n        bind 0.0.0.0:5672\n        #配置tcp模式\n        mode tcp\n        #简单的轮询\n        balance roundrobin\n        #rabbitmq集群节点配置\n        server rmq_node1 127.0.0.1:5673 check inter 5000 rise 2 fall 3 weight 1\n        server rmq_node2 127.0.0.1:5674 check inter 5000 rise 2 fall 3 weight 1\n#haproxy监控页面地址\nlisten monitor \n        bind 0.0.0.0:8100\n        mode http\n        option httplog\n        stats enable\n        stats uri /stats\n        stats refresh 5s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n\n\n检查配置文件是否错误\n\nhaproxy -f /etc/haproxy/haproxy.cfg -c\n\n\n1\n\n\n启动\n\nhaproxy -f /etc/haproxy/haproxy.cfg -d\n\n\n1\n\n\n访问haproxy后台\n\nhttp://192.168.130.124:8100/stats\n\n后续将消息队列的地址和ip设置为haproxy监听的地址即可\n\n\nspring:\n  #消息队列配置\n  rabbitmq:\n    host: 192.168.0.104 #haproxy的地址\n    port: 5672\n    username: guest\n    password: guest\n    virtual-host: /\n    publisher-returns: true\n    publisher-confirms: true\n    connection-timeout: 5000ms\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Spring Boot 高级",frontmatter:{title:"Spring Boot 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/207023/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/14.Spring%20Boot%20%E9%AB%98%E7%BA%A7.html",relativePath:"后端/02.JavaEE/14.Spring Boot 高级.md",key:"v-3859f86a",path:"/pages/207023/",headers:[{level:2,title:"自动配置",slug:"自动配置",normalizedTitle:"自动配置",charIndex:21},{level:3,title:"Condition",slug:"condition",normalizedTitle:"condition",charIndex:30},{level:3,title:"自定义Conditional注解",slug:"自定义conditional注解",normalizedTitle:"自定义conditional注解",charIndex:1339},{level:3,title:"常用条件注解",slug:"常用条件注解",normalizedTitle:"常用条件注解",charIndex:3161},{level:3,title:"切换内置web服务器",slug:"切换内置web服务器",normalizedTitle:"切换内置web服务器",charIndex:3780},{level:3,title:"@Enable* 注解",slug:"enable-注解",normalizedTitle:"@enable* 注解",charIndex:4669},{level:3,title:"自定义自动配置",slug:"自定义自动配置",normalizedTitle:"自定义自动配置",charIndex:6506},{level:2,title:"监听机制",slug:"监听机制",normalizedTitle:"监听机制",charIndex:8072},{level:3,title:"启动流程",slug:"启动流程",normalizedTitle:"启动流程",charIndex:12936},{level:2,title:"监控",slug:"监控",normalizedTitle:"监控",charIndex:12945},{level:2,title:"Spring Boot Admin",slug:"spring-boot-admin",normalizedTitle:"spring boot admin",charIndex:13374},{level:2,title:"部署",slug:"部署",normalizedTitle:"部署",charIndex:14169}],headersStr:"自动配置 Condition 自定义Conditional注解 常用条件注解 切换内置web服务器 @Enable* 注解 自定义自动配置 监听机制 启动流程 监控 Spring Boot Admin 部署",content:'# Spring Boot 高级\n\n\n# 自动配置\n\n\n# Condition\n\nCondition是spring 4.0 增加的条件判断功能 通个这个功能可以实现选择性的创建Bean操作\n\n启动类返回IOC容器 获取bean对象\n\n        //启动springboot的应用 返回spring的IOC容器\n        ConfigurableApplicationContext context = SpringApplication.run(SpringbootProfilesApplication.class, args);\n        \n        //获取bean\n        Object redisTemplate = context.getBean("redisTemplate");\n        System.out.println(redisTemplate);\n\n\n1\n2\n3\n4\n5\n6\n\n\n创建一个类实现 Condition接口 实现matches方法 返回一个布尔值 true为允许创建bean false为不允许\n\nimport org.springframework.context.annotation.Condition;\nimport org.springframework.context.annotation.ConditionContext;\nimport org.springframework.core.type.AnnotatedTypeMetadata;\n\npublic class ClassCondition implements Condition {\n    @Override\n    public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) {\n\n\n        try {\n            //获取指定的class 是否加载 没有则不允许创建新的bean\n            Class<?> cls = Class.forName("redis.clients.jedis.Jedis");\n        } catch (ClassNotFoundException e) {\n            return false;\n        }\n\n\n        return true;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n使用注解@Conditional\n\n@Configuration\npublic class UserConfig {\n\n    @Bean\n    //如果为true则创建该bean\n    @Conditional(ClassCondition.class)\n    public User user(){\n        return new User();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 自定义Conditional注解\n\n创建注解\n\npackage com.itheima.springbootprofiles.condtion;\n\n\nimport org.springframework.context.annotation.Conditional;\n\nimport java.lang.annotation.*;\n\n@Target({ElementType.TYPE, ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Conditional(ClassCondition.class)\npublic @interface ConditionOnClass {\n    String[] value();\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建一个类实现 Condition接口 实现matches方法 返回一个布尔值 true为允许创建bean false为不允许\n\npackage com.itheima.springbootprofiles.condtion;\n\nimport org.springframework.context.annotation.Condition;\nimport org.springframework.context.annotation.ConditionContext;\nimport org.springframework.core.type.AnnotatedTypeMetadata;\n\nimport java.util.Map;\n\npublic class ClassCondition implements Condition {\n    /**\n     *\n     * @param context 上下文对象 用于获取环境 ioc容器 classloader对象\n     * @param metadata 注解元对象 可以用于获取注解定义的属性值\n     * @return\n     */\n    @Override\n    public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) {\n\n\n        Map<String, Object> map = metadata.getAnnotationAttributes(ConditionOnClass.class.getName());  //获取注解中的元数据\n        String[] value = (String[]) map.get("value");  //获取value中值\n\n        try {\n            //获取指定的class 是否加载 没有则不允许创建新的bean\n            for (String classNmae : value) {\n                Class<?> cls = Class.forName(classNmae);\n            }\n        } catch (ClassNotFoundException e) {\n            return false;\n        }\n\n\n        return true;\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n使用注解\n\n@Configuration\npublic class UserConfig {\n\n    @Bean\n//    @Conditional(ClassCondition.class)\n    @ConditionOnClass("com.alibaba.fastjson.Json")\n    public User user(){\n        return new User();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 常用条件注解\n\n * @ConditionalOnProperty 判断配置文件中是否有对应属性和值才初始化Bean\n * @ConditionalOnClass 判断环境中是否有对应的字节码文件才初始化Bean\n * @ConditionalOnMissingBean 判断环境中没有对应的Bean时才初始化Bean\n * @ConditionalOnBean 判断容器中有指定组件时才注册该被标注的组件\n\n@Bean\n//当application配置文件中有此键值对时才创建此bean\n@ConditionalOnProperty(name = "name",havingValue = "zhangsan")\npublic User user2(){\n    return new User();\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n@ConditionalOnBean(name = "tomcat")\n@Bean  // 给容器注册添加组件 用bean声明   返回的值,就是组件在容器中的实例\n    public User user01() {\n        User zhangsan = new User("zhangsan",18);\n        zhangsan.setPet(tomcatpet());\n        return zhangsan;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 切换内置web服务器\n\nSpringBoot的web环境默认为tomcat作为内置服务器,Springboot提供了4种内置服务器让我们选择\n\n * 排除tomcat 引入jetty依赖\n   \n           <dependency>\n               <groupId>org.springframework.boot</groupId>\n               <artifactId>spring-boot-starter-web</artifactId>\n               \x3c!--            排除tomcat--\x3e\n               <exclusions>\n                   <exclusion>\n                       <groupId>org.springframework.boot</groupId>\n                       <artifactId>spring-boot-starter-tomcat</artifactId>\n                   </exclusion>\n               </exclusions>\n           </dependency>\n           \x3c!--        引入jetty的依赖--\x3e\n           <dependency>\n               <groupId>org.springframework.boot</groupId>\n               <artifactId>spring-boot-starter-jetty</artifactId>\n           </dependency>\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n\n# @Enable* 注解\n\nSpringBoot中提供了很多Enable开头的注解,这些注解都是用于动态启用某些功能的,而底层原理是使用了@Import注解导入一些配置类,实现Bean的动态加载\n\n# 第三方包导入\n\n在项目或者POM中添加坐标\n\nSpringboot并不会加载到我们自己手动导入的第三方包\n\n 1. 使用@ComponentScan("引用路径") 重新定义扫描目录 SpringBoot默认为启动类的根路径下Bean\n 2. 使用@Import(字节码) 导入第三方jar包\n 3. 自定义注解 实现@Import 并继承其之前的注解 简化我们书写\n\n# @Import注解\n\n@Enable*底层依赖于@Import注解导入一些类,使用@Import导入的类会被Spring加载到IOC容器中,而@Import提供4种用法\n\n 1. 导入Bean\n\n 2. 导入配置类\n\n 3. 导入ImportSelector实现类 一般用于加载配置文件中的类\n    \n    public class MyImportSelector implements ImportSelector {\n        //实现selectImports方法 需要一个元数据\n        @Override\n        public String[] selectImports(AnnotationMetadata importingClassMetadata) {\n            return new String[]{"com.itheima.domain.user"};\n        }\n    }\n    \n    //使用@Import导入实现类\n    @Import(MyImportSelector.class)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n\n 4. 导入 ImportBeanDefinitionRegistrar 实现类\n    \n    public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar {\n        @Override\n        public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {\n            //Bean对象\n            AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(User.class).getBeanDefinition();\n            //创建bean的名称 以及需要一个bean对象\n            registry.registerBeanDefinition("user",beanDefinition);\n        }\n    }\n    \n    \n    //使用@Import导入实现类\n    @Import(MyImportBeanDefinitionRegistrar.class)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n# @EnableAutoConfiguration\n\n * @EnableAutoConfiguration 注解内部使用@Import(AutoConfigurationImporttSelector.class) 来加载配置类\n * 配置文件位置: META-INF/spring.factories 该配置文件中定义了大量的配置类 当SpringBoot应用启动时,会自动加载这些配置类 初始化Bean\n * 并不是所有Bean都会被初始化 在配置类中使用Condition来加载满足条件的Bean\n\n\n# 自定义自动配置\n\n配置redis 配置类\n\npackage com.itheima.redisspringbootautoconfigure.redis.config;\n\nimport org.springframework.boot.context.properties.ConfigurationProperties;\n\n@ConfigurationProperties(prefix = "redis")\npublic class RedisProperties {\n    private String host="localhost";\n    private int port =6379;\n\n    public String getHost() {\n        return host;\n    }\n\n    public void setHost(String host) {\n        this.host = host;\n    }\n\n    public int getPort() {\n        return port;\n    }\n\n    public void setPort(int port) {\n        this.port = port;\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n自动配置类\n\npackage com.itheima.redisspringbootautoconfigure.redis.config;\n\nimport org.springframework.boot.context.properties.EnableConfigurationProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport redis.clients.jedis.Jedis;\n\n@Configuration\n@EnableConfigurationProperties(RedisProperties.class)\n@ConditionalOnClass(Jedis.class)\npublic class RedisAutoConfiguration {\n\n    @Bean\n    @ConditionalOnMissingBean(name = "jedis")\n    public Jedis jedis(RedisProperties redisProperties) {\n        return new Jedis(redisProperties.getHost(), redisProperties.getPort());\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n在resource 下创建META-INF目录 创建spring.factories文件\n\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\n com.itheima.redisspringbootautoconfigure.redis.config.RedisAutoConfiguration\n\n\n1\n2\n\n * 使用 在项目pom中引入自定义自动配置的项目坐标\n\n启动时自动配置\n\n\n# 监听机制\n\nSpringBoot的监听机制,其实是对java提供的事件监听机制的封装\n\nJava中的事件监听机制定义了以下几个角色:\n\n 1. 事件: Event, 继承java.util.EventObject类的对象\n 2. 事件源: Source , 任意对象Object\n 3. 监听器: Listener 实现java.util.EventListener接口的对象\n\nSpringBoot在项目启动时,会对几个监听器进行回调,我们可以实现这些监听器接口,在项目启动时完成一些操作\n\n * ApplicationContextInitializer 项目图标加载后监听\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.context.ApplicationContextInitializer;\n   import org.springframework.context.ConfigurableApplicationContext;\n   import org.springframework.stereotype.Component;\n   \n   @Component\n   public class MyApplicationContextInitializer implements ApplicationContextInitializer {\n       @Override\n       public void initialize(ConfigurableApplicationContext applicationContext) {\n           System.out.println("ApplicationContextInitializer...initialize");\n       }\n   }\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   \n   \n   并且配置resource下的 META-INF 的spring.factories配置\n   \n   org.springframework.context.ApplicationContextInitializer=com.itheima.springbootinit.listener.MyApplicationContextInitializer\n   \n   \n   1\n   \n\n * SpringApplicationRunListener 生命周期监听\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.boot.ConfigurableBootstrapContext;\n   import org.springframework.boot.SpringApplication;\n   import org.springframework.boot.SpringApplicationRunListener;\n   import org.springframework.context.ConfigurableApplicationContext;\n   import org.springframework.core.env.ConfigurableEnvironment;\n   import org.springframework.stereotype.Component;\n   \n   \n   public class MySpringApplicationRunListener implements SpringApplicationRunListener {\n       public MySpringApplicationRunListener(SpringApplication application, String[] args) {\n       }\n   \n       @Override\n       public void starting(ConfigurableBootstrapContext bootstrapContext) {\n           System.out.println("starting...项目启动中");\n       }\n   \n       @Override\n       public void environmentPrepared(ConfigurableBootstrapContext bootstrapContext, ConfigurableEnvironment environment) {\n           System.out.println("environmentPrepared...环境对象开始准备");\n   \n       }\n   \n       @Override\n       public void contextPrepared(ConfigurableApplicationContext context) {\n           System.out.println("contextPrepared...上下文对象开始准备");\n   \n       }\n   \n       @Override\n       public void contextLoaded(ConfigurableApplicationContext context) {\n           System.out.println("contextLoaded...上下文对象开始加载");\n       }\n   \n       @Override\n       public void started(ConfigurableApplicationContext context) {\n           System.out.println("started...上下文对象加载完成");\n   \n       }\n   \n       @Override\n       public void running(ConfigurableApplicationContext context) {\n           System.out.println("running...项目启动完成,开始运行");\n   \n       }\n   \n       @Override\n       public void failed(ConfigurableApplicationContext context, Throwable exception) {\n           System.out.println("failed...项目启动失败");\n   \n       }\n   }\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   17\n   18\n   19\n   20\n   21\n   22\n   23\n   24\n   25\n   26\n   27\n   28\n   29\n   30\n   31\n   32\n   33\n   34\n   35\n   36\n   37\n   38\n   39\n   40\n   41\n   42\n   43\n   44\n   45\n   46\n   47\n   48\n   49\n   50\n   51\n   52\n   53\n   54\n   55\n   \n   \n   并且配置resource下的 META-INF 的spring.factories配置\n   \n   org.springframework.boot.SpringApplicationRunListener=com.itheima.springbootinit.listener.MySpringApplicationRunListener\n   \n   \n   1\n   \n\n * CommandLineRunner\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.boot.CommandLineRunner;\n   import org.springframework.stereotype.Component;\n   \n   import java.util.Arrays;\n   \n   @Component\n   public class MyCommandLineRunner implements CommandLineRunner {\n       @Override\n       public void run(String... args) throws Exception {\n           System.out.println("CommandLineRunner...run");\n           System.out.println(Arrays.toString(args));  //java运行传递的参数\n       }\n   }\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n * ApplicationRunner\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.boot.ApplicationArguments;\n   import org.springframework.boot.ApplicationRunner;\n   import org.springframework.stereotype.Component;\n   \n   import java.util.Arrays;\n   \n   @Component\n   public class MyApplicationRunner implements ApplicationRunner {\n       @Override\n       public void run(ApplicationArguments args) throws Exception {\n           System.out.println("ApplicationRunner...run");\n           System.out.println(Arrays.toString(args.getSourceArgs())); //java运行传递的参数\n       }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n\n# 启动流程\n\n\n# 监控\n\nSpringBoot自带监控功能Actuator,可以帮助实现对程序内部运行情况监控,比如监控状况 Bean加载情况 配置属性 日志信息\n\n\n\n      <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n\n\n1\n2\n3\n4\n\n\n访问localhost:8080/acruator\n\n * 开启(health)健康检查完整信息\n\nmanagement.endpoint.health.show-details=always\n\n\n1\n\n * 将所有的监控endpoint暴露出来\n\nmanagement.endpoints.web.exposure.include=*\n\n\n1\n\n\n\n# Spring Boot Admin\n\nSpring Boot Admin是一个开源社区项目 用于管理和监控SpringBoot应用程序\n\n分服务端和客户端 客户端用于查看监控 服务端用于监控Spring\n\n服务端\n\n\n\n    <dependency>\n            <groupId>de.codecentric</groupId>\n            <artifactId>spring-boot-admin-starter-server</artifactId>\n            <version>2.5.0</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n在启动类中加上 注解 @EnableAdminServer 用于监控springboot\n\n客户端\n\n创建另外一个项目\n\n\n\n        <dependency>\n            <groupId>de.codecentric</groupId>\n            <artifactId>spring-boot-admin-starter-client</artifactId>\n            <version>2.5.0</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n在application配置admin.server地址\n\nspring.boot.admin.client.url=http://localhost:8080  #对应的是server的ip和端口\nmanagement.endpoint.health.show-details=always\nmanagement.endpoints.web.exposure.include=*\n\n\n1\n2\n3\n\n\n\n\n\n# 部署\n\nSpringBoot项目开发完毕后,支持两种方式部署到服务器上\n\n 1. jar包(官方推荐)\n    \n    直接在maven中打包即可\n\n 2. war包 在pom文件中packageing定义为war包 启动类继承 SpringBootServletInitializer 类 重写configure方法\n    \n    @Override\n    protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) {\n        return builder.sources(SpringbootProfilesApplication.class); //将启动类字节码文件传递过去\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    放置war包到tomcat的webapps的目录下启动tomcat即可',normalizedContent:'# spring boot 高级\n\n\n# 自动配置\n\n\n# condition\n\ncondition是spring 4.0 增加的条件判断功能 通个这个功能可以实现选择性的创建bean操作\n\n启动类返回ioc容器 获取bean对象\n\n        //启动springboot的应用 返回spring的ioc容器\n        configurableapplicationcontext context = springapplication.run(springbootprofilesapplication.class, args);\n        \n        //获取bean\n        object redistemplate = context.getbean("redistemplate");\n        system.out.println(redistemplate);\n\n\n1\n2\n3\n4\n5\n6\n\n\n创建一个类实现 condition接口 实现matches方法 返回一个布尔值 true为允许创建bean false为不允许\n\nimport org.springframework.context.annotation.condition;\nimport org.springframework.context.annotation.conditioncontext;\nimport org.springframework.core.type.annotatedtypemetadata;\n\npublic class classcondition implements condition {\n    @override\n    public boolean matches(conditioncontext context, annotatedtypemetadata metadata) {\n\n\n        try {\n            //获取指定的class 是否加载 没有则不允许创建新的bean\n            class<?> cls = class.forname("redis.clients.jedis.jedis");\n        } catch (classnotfoundexception e) {\n            return false;\n        }\n\n\n        return true;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n使用注解@conditional\n\n@configuration\npublic class userconfig {\n\n    @bean\n    //如果为true则创建该bean\n    @conditional(classcondition.class)\n    public user user(){\n        return new user();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 自定义conditional注解\n\n创建注解\n\npackage com.itheima.springbootprofiles.condtion;\n\n\nimport org.springframework.context.annotation.conditional;\n\nimport java.lang.annotation.*;\n\n@target({elementtype.type, elementtype.method})\n@retention(retentionpolicy.runtime)\n@documented\n@conditional(classcondition.class)\npublic @interface conditiononclass {\n    string[] value();\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n创建一个类实现 condition接口 实现matches方法 返回一个布尔值 true为允许创建bean false为不允许\n\npackage com.itheima.springbootprofiles.condtion;\n\nimport org.springframework.context.annotation.condition;\nimport org.springframework.context.annotation.conditioncontext;\nimport org.springframework.core.type.annotatedtypemetadata;\n\nimport java.util.map;\n\npublic class classcondition implements condition {\n    /**\n     *\n     * @param context 上下文对象 用于获取环境 ioc容器 classloader对象\n     * @param metadata 注解元对象 可以用于获取注解定义的属性值\n     * @return\n     */\n    @override\n    public boolean matches(conditioncontext context, annotatedtypemetadata metadata) {\n\n\n        map<string, object> map = metadata.getannotationattributes(conditiononclass.class.getname());  //获取注解中的元数据\n        string[] value = (string[]) map.get("value");  //获取value中值\n\n        try {\n            //获取指定的class 是否加载 没有则不允许创建新的bean\n            for (string classnmae : value) {\n                class<?> cls = class.forname(classnmae);\n            }\n        } catch (classnotfoundexception e) {\n            return false;\n        }\n\n\n        return true;\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n使用注解\n\n@configuration\npublic class userconfig {\n\n    @bean\n//    @conditional(classcondition.class)\n    @conditiononclass("com.alibaba.fastjson.json")\n    public user user(){\n        return new user();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 常用条件注解\n\n * @conditionalonproperty 判断配置文件中是否有对应属性和值才初始化bean\n * @conditionalonclass 判断环境中是否有对应的字节码文件才初始化bean\n * @conditionalonmissingbean 判断环境中没有对应的bean时才初始化bean\n * @conditionalonbean 判断容器中有指定组件时才注册该被标注的组件\n\n@bean\n//当application配置文件中有此键值对时才创建此bean\n@conditionalonproperty(name = "name",havingvalue = "zhangsan")\npublic user user2(){\n    return new user();\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n@conditionalonbean(name = "tomcat")\n@bean  // 给容器注册添加组件 用bean声明   返回的值,就是组件在容器中的实例\n    public user user01() {\n        user zhangsan = new user("zhangsan",18);\n        zhangsan.setpet(tomcatpet());\n        return zhangsan;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 切换内置web服务器\n\nspringboot的web环境默认为tomcat作为内置服务器,springboot提供了4种内置服务器让我们选择\n\n * 排除tomcat 引入jetty依赖\n   \n           <dependency>\n               <groupid>org.springframework.boot</groupid>\n               <artifactid>spring-boot-starter-web</artifactid>\n               \x3c!--            排除tomcat--\x3e\n               <exclusions>\n                   <exclusion>\n                       <groupid>org.springframework.boot</groupid>\n                       <artifactid>spring-boot-starter-tomcat</artifactid>\n                   </exclusion>\n               </exclusions>\n           </dependency>\n           \x3c!--        引入jetty的依赖--\x3e\n           <dependency>\n               <groupid>org.springframework.boot</groupid>\n               <artifactid>spring-boot-starter-jetty</artifactid>\n           </dependency>\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n\n# @enable* 注解\n\nspringboot中提供了很多enable开头的注解,这些注解都是用于动态启用某些功能的,而底层原理是使用了@import注解导入一些配置类,实现bean的动态加载\n\n# 第三方包导入\n\n在项目或者pom中添加坐标\n\nspringboot并不会加载到我们自己手动导入的第三方包\n\n 1. 使用@componentscan("引用路径") 重新定义扫描目录 springboot默认为启动类的根路径下bean\n 2. 使用@import(字节码) 导入第三方jar包\n 3. 自定义注解 实现@import 并继承其之前的注解 简化我们书写\n\n# @import注解\n\n@enable*底层依赖于@import注解导入一些类,使用@import导入的类会被spring加载到ioc容器中,而@import提供4种用法\n\n 1. 导入bean\n\n 2. 导入配置类\n\n 3. 导入importselector实现类 一般用于加载配置文件中的类\n    \n    public class myimportselector implements importselector {\n        //实现selectimports方法 需要一个元数据\n        @override\n        public string[] selectimports(annotationmetadata importingclassmetadata) {\n            return new string[]{"com.itheima.domain.user"};\n        }\n    }\n    \n    //使用@import导入实现类\n    @import(myimportselector.class)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n\n 4. 导入 importbeandefinitionregistrar 实现类\n    \n    public class myimportbeandefinitionregistrar implements importbeandefinitionregistrar {\n        @override\n        public void registerbeandefinitions(annotationmetadata importingclassmetadata, beandefinitionregistry registry) {\n            //bean对象\n            abstractbeandefinition beandefinition = beandefinitionbuilder.rootbeandefinition(user.class).getbeandefinition();\n            //创建bean的名称 以及需要一个bean对象\n            registry.registerbeandefinition("user",beandefinition);\n        }\n    }\n    \n    \n    //使用@import导入实现类\n    @import(myimportbeandefinitionregistrar.class)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n# @enableautoconfiguration\n\n * @enableautoconfiguration 注解内部使用@import(autoconfigurationimporttselector.class) 来加载配置类\n * 配置文件位置: meta-inf/spring.factories 该配置文件中定义了大量的配置类 当springboot应用启动时,会自动加载这些配置类 初始化bean\n * 并不是所有bean都会被初始化 在配置类中使用condition来加载满足条件的bean\n\n\n# 自定义自动配置\n\n配置redis 配置类\n\npackage com.itheima.redisspringbootautoconfigure.redis.config;\n\nimport org.springframework.boot.context.properties.configurationproperties;\n\n@configurationproperties(prefix = "redis")\npublic class redisproperties {\n    private string host="localhost";\n    private int port =6379;\n\n    public string gethost() {\n        return host;\n    }\n\n    public void sethost(string host) {\n        this.host = host;\n    }\n\n    public int getport() {\n        return port;\n    }\n\n    public void setport(int port) {\n        this.port = port;\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n自动配置类\n\npackage com.itheima.redisspringbootautoconfigure.redis.config;\n\nimport org.springframework.boot.context.properties.enableconfigurationproperties;\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport redis.clients.jedis.jedis;\n\n@configuration\n@enableconfigurationproperties(redisproperties.class)\n@conditionalonclass(jedis.class)\npublic class redisautoconfiguration {\n\n    @bean\n    @conditionalonmissingbean(name = "jedis")\n    public jedis jedis(redisproperties redisproperties) {\n        return new jedis(redisproperties.gethost(), redisproperties.getport());\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n在resource 下创建meta-inf目录 创建spring.factories文件\n\norg.springframework.boot.autoconfigure.enableautoconfiguration=\\\n com.itheima.redisspringbootautoconfigure.redis.config.redisautoconfiguration\n\n\n1\n2\n\n * 使用 在项目pom中引入自定义自动配置的项目坐标\n\n启动时自动配置\n\n\n# 监听机制\n\nspringboot的监听机制,其实是对java提供的事件监听机制的封装\n\njava中的事件监听机制定义了以下几个角色:\n\n 1. 事件: event, 继承java.util.eventobject类的对象\n 2. 事件源: source , 任意对象object\n 3. 监听器: listener 实现java.util.eventlistener接口的对象\n\nspringboot在项目启动时,会对几个监听器进行回调,我们可以实现这些监听器接口,在项目启动时完成一些操作\n\n * applicationcontextinitializer 项目图标加载后监听\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.context.applicationcontextinitializer;\n   import org.springframework.context.configurableapplicationcontext;\n   import org.springframework.stereotype.component;\n   \n   @component\n   public class myapplicationcontextinitializer implements applicationcontextinitializer {\n       @override\n       public void initialize(configurableapplicationcontext applicationcontext) {\n           system.out.println("applicationcontextinitializer...initialize");\n       }\n   }\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   \n   \n   并且配置resource下的 meta-inf 的spring.factories配置\n   \n   org.springframework.context.applicationcontextinitializer=com.itheima.springbootinit.listener.myapplicationcontextinitializer\n   \n   \n   1\n   \n\n * springapplicationrunlistener 生命周期监听\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.boot.configurablebootstrapcontext;\n   import org.springframework.boot.springapplication;\n   import org.springframework.boot.springapplicationrunlistener;\n   import org.springframework.context.configurableapplicationcontext;\n   import org.springframework.core.env.configurableenvironment;\n   import org.springframework.stereotype.component;\n   \n   \n   public class myspringapplicationrunlistener implements springapplicationrunlistener {\n       public myspringapplicationrunlistener(springapplication application, string[] args) {\n       }\n   \n       @override\n       public void starting(configurablebootstrapcontext bootstrapcontext) {\n           system.out.println("starting...项目启动中");\n       }\n   \n       @override\n       public void environmentprepared(configurablebootstrapcontext bootstrapcontext, configurableenvironment environment) {\n           system.out.println("environmentprepared...环境对象开始准备");\n   \n       }\n   \n       @override\n       public void contextprepared(configurableapplicationcontext context) {\n           system.out.println("contextprepared...上下文对象开始准备");\n   \n       }\n   \n       @override\n       public void contextloaded(configurableapplicationcontext context) {\n           system.out.println("contextloaded...上下文对象开始加载");\n       }\n   \n       @override\n       public void started(configurableapplicationcontext context) {\n           system.out.println("started...上下文对象加载完成");\n   \n       }\n   \n       @override\n       public void running(configurableapplicationcontext context) {\n           system.out.println("running...项目启动完成,开始运行");\n   \n       }\n   \n       @override\n       public void failed(configurableapplicationcontext context, throwable exception) {\n           system.out.println("failed...项目启动失败");\n   \n       }\n   }\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   17\n   18\n   19\n   20\n   21\n   22\n   23\n   24\n   25\n   26\n   27\n   28\n   29\n   30\n   31\n   32\n   33\n   34\n   35\n   36\n   37\n   38\n   39\n   40\n   41\n   42\n   43\n   44\n   45\n   46\n   47\n   48\n   49\n   50\n   51\n   52\n   53\n   54\n   55\n   \n   \n   并且配置resource下的 meta-inf 的spring.factories配置\n   \n   org.springframework.boot.springapplicationrunlistener=com.itheima.springbootinit.listener.myspringapplicationrunlistener\n   \n   \n   1\n   \n\n * commandlinerunner\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.boot.commandlinerunner;\n   import org.springframework.stereotype.component;\n   \n   import java.util.arrays;\n   \n   @component\n   public class mycommandlinerunner implements commandlinerunner {\n       @override\n       public void run(string... args) throws exception {\n           system.out.println("commandlinerunner...run");\n           system.out.println(arrays.tostring(args));  //java运行传递的参数\n       }\n   }\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n * applicationrunner\n   \n   package com.itheima.springbootinit.listener;\n   \n   import org.springframework.boot.applicationarguments;\n   import org.springframework.boot.applicationrunner;\n   import org.springframework.stereotype.component;\n   \n   import java.util.arrays;\n   \n   @component\n   public class myapplicationrunner implements applicationrunner {\n       @override\n       public void run(applicationarguments args) throws exception {\n           system.out.println("applicationrunner...run");\n           system.out.println(arrays.tostring(args.getsourceargs())); //java运行传递的参数\n       }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n\n# 启动流程\n\n\n# 监控\n\nspringboot自带监控功能actuator,可以帮助实现对程序内部运行情况监控,比如监控状况 bean加载情况 配置属性 日志信息\n\n\n\n      <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-actuator</artifactid>\n        </dependency>\n\n\n1\n2\n3\n4\n\n\n访问localhost:8080/acruator\n\n * 开启(health)健康检查完整信息\n\nmanagement.endpoint.health.show-details=always\n\n\n1\n\n * 将所有的监控endpoint暴露出来\n\nmanagement.endpoints.web.exposure.include=*\n\n\n1\n\n\n\n# spring boot admin\n\nspring boot admin是一个开源社区项目 用于管理和监控springboot应用程序\n\n分服务端和客户端 客户端用于查看监控 服务端用于监控spring\n\n服务端\n\n\n\n    <dependency>\n            <groupid>de.codecentric</groupid>\n            <artifactid>spring-boot-admin-starter-server</artifactid>\n            <version>2.5.0</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n在启动类中加上 注解 @enableadminserver 用于监控springboot\n\n客户端\n\n创建另外一个项目\n\n\n\n        <dependency>\n            <groupid>de.codecentric</groupid>\n            <artifactid>spring-boot-admin-starter-client</artifactid>\n            <version>2.5.0</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n\n\n在application配置admin.server地址\n\nspring.boot.admin.client.url=http://localhost:8080  #对应的是server的ip和端口\nmanagement.endpoint.health.show-details=always\nmanagement.endpoints.web.exposure.include=*\n\n\n1\n2\n3\n\n\n\n\n\n# 部署\n\nspringboot项目开发完毕后,支持两种方式部署到服务器上\n\n 1. jar包(官方推荐)\n    \n    直接在maven中打包即可\n\n 2. war包 在pom文件中packageing定义为war包 启动类继承 springbootservletinitializer 类 重写configure方法\n    \n    @override\n    protected springapplicationbuilder configure(springapplicationbuilder builder) {\n        return builder.sources(springbootprofilesapplication.class); //将启动类字节码文件传递过去\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    放置war包到tomcat的webapps的目录下启动tomcat即可',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Docker",frontmatter:{title:"Docker",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/c588b5/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/18.Docker.html",relativePath:"后端/02.JavaEE/18.Docker.md",key:"v-01a7b822",path:"/pages/c588b5/",headers:[{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:34},{level:2,title:"架构",slug:"架构",normalizedTitle:"架构",charIndex:69},{level:2,title:"Docker 镜像加速器",slug:"docker-镜像加速器",normalizedTitle:"docker 镜像加速器",charIndex:78},{level:2,title:"Docker 命令",slug:"docker-命令",normalizedTitle:"docker 命令",charIndex:374},{level:3,title:"服务",slug:"服务",normalizedTitle:"服务",charIndex:388},{level:3,title:"镜像",slug:"镜像",normalizedTitle:"镜像",charIndex:85},{level:3,title:"容器",slug:"容器",normalizedTitle:"容器",charIndex:25},{level:2,title:"容器数据卷",slug:"容器数据卷",normalizedTitle:"容器数据卷",charIndex:1865},{level:2,title:"数据卷容器",slug:"数据卷容器",normalizedTitle:"数据卷容器",charIndex:2026},{level:2,title:"应用部署",slug:"应用部署",normalizedTitle:"应用部署",charIndex:2391},{level:3,title:"MySQL",slug:"mysql",normalizedTitle:"mysql",charIndex:2400},{level:3,title:"Tomcat",slug:"tomcat",normalizedTitle:"tomcat",charIndex:2677},{level:3,title:"Nginx",slug:"nginx",normalizedTitle:"nginx",charIndex:2801},{level:3,title:"Redis",slug:"redis",normalizedTitle:"redis",charIndex:2914},{level:2,title:"Docker 镜像",slug:"docker-镜像",normalizedTitle:"docker 镜像",charIndex:78},{level:3,title:"镜像制作",slug:"镜像制作",normalizedTitle:"镜像制作",charIndex:3084},{level:3,title:"Dockerfile",slug:"dockerfile",normalizedTitle:"dockerfile",charIndex:3341},{level:2,title:"Docker Compose服务编排",slug:"docker-compose服务编排",normalizedTitle:"docker compose服务编排",charIndex:6285},{level:2,title:"私有仓库",slug:"私有仓库",normalizedTitle:"私有仓库",charIndex:7345},{level:2,title:"Docker Hub",slug:"docker-hub",normalizedTitle:"docker hub",charIndex:7880},{level:2,title:"docker容器与虚拟机比较",slug:"docker容器与虚拟机比较",normalizedTitle:"docker容器与虚拟机比较",charIndex:8015},{level:2,title:"本地镜像导出和导入",slug:"本地镜像导出和导入",normalizedTitle:"本地镜像导出和导入",charIndex:8038},{level:3,title:"导出",slug:"导出",normalizedTitle:"导出",charIndex:8042},{level:3,title:"导入",slug:"导入",normalizedTitle:"导入",charIndex:8045}],headersStr:"安装 架构 Docker 镜像加速器 Docker 命令 服务 镜像 容器 容器数据卷 数据卷容器 应用部署 MySQL Tomcat Nginx Redis Docker 镜像 镜像制作 Dockerfile Docker Compose服务编排 私有仓库 Docker Hub docker容器与虚拟机比较 本地镜像导出和导入 导出 导入",content:'# Docker\n\nDocker 是一个开源的应用容器引擎\n\n\n# 安装\n\nyum install -y docker\n\n\n1\n\n\n\n# 架构\n\n\n\n\n# Docker 镜像加速器\n\nhttps://cr.console.aliyun.com/cn-hangzhou/instances/mirrors\n\nsudo mkdir -p /etc/docker\nsudo tee /etc/docker/daemon.json <<-\'EOF\'\n{\n  "registry-mirrors": ["https://o7uzc3zp.mirror.aliyuncs.com"]\n}\nEOF\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# Docker 命令\n\n\n# 服务\n\nsystemctl start docker  # 启动\nsystemctl stop docker  # 停止\nsystemctl status docker  #状态\nsystemctl restart docker #重启\nsystemctl enabled docker #开机自启\n\n\n1\n2\n3\n4\n5\n\n\n\n# 镜像\n\n * 查看本地镜像\n   \n   docker images\n   \n   \n   1\n   \n\n * 搜索镜像\n   \n   docker search 镜像名\n   \n   \n   1\n   \n\n * 拉取镜像\n   \n   docker pull 镜像名:版本号 # 版本号默认为latest\n   \n   \n   1\n   \n\n * 删除镜像\n   \n   docker rmi 镜像id    # 也可以通过名称:版本号删除\n   \n   \n   1\n   \n\n * 删除所有镜像\n   \n   docker rmi `docker images -q`\n   \n   \n   1\n   \n\n\n# 容器\n\n * 创建容器并 运行参数run\n   \n   * -i 保持运行\n   * -t 以终端模式运行并进入\n   * -d 后台常驻模式\n   * --name=xxx 此容器名称\n   * 镜像名/id\n   * /bin/bash 初始化命令\n   \n   docker run -it --name=redis redis /bin/bash  #进入容器内部\n   docker run -d --name=redis2 redis # 后台运行容器\n   \n   \n   1\n   2\n   \n\n * 查询容器\n   \n   docker ps  #查询正在运行的容器\n   docker ps -a # 查看所有容器包括已停止的\n   \n   \n   1\n   2\n   \n\n * 进入已经运行的容器内部\n   \n   docker exec -it 名称/id /bin/bash \n   \n   \n   1\n   \n\n * 运行 停止容器\n   \n   docker stop 名称/id  #停止容器\n   docker start 名称/id  #启动容器\n   \n   \n   1\n   2\n   \n\n * 删除00容器\n   \n   docker rm 名称/id # 删除容器 先docker ps -a 查询\n   docker rm `docker ps -aq`   # 删除所有容器 慎用 正在运行的容器无法删除\n   \n   \n   1\n   2\n   \n\n * 查看容器 配置信息\n   \n   docker inspect 名称/id\n   \n   \n   1\n   \n\n * 修改配置信息\n\nsystemctl stop docker\ncd /var/lib/docker/containers/docekr inspect查出的id\nvim config.v2.json\nsystemctl restart docker\n\n\n1\n2\n3\n4\n\n\n * 用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及即无tag的镜像 谨慎使用\n   \n   docker system prune\n   \n   \n   1\n   \n\n * 查看docker自身磁盘使用情况\n   \n   docker system df\n   \n   \n   1\n   \n\n\n# 容器数据卷\n\n数据卷是宿主机中的一个目录或文件 称为挂载目录 一个数据卷可以被多个容器同时挂载\n\n * 创建启动容器时 使用 -v参数 设置数据集\n   \n   docker run -d -v 宿主机目录:容器内目录\n   \n   \n   1\n   \n   \n   目录必须为绝对路径 如果目录不存在则自动创建\n\n\n# 数据卷容器\n\n多容器进行数据交换 之前我们通过多容器绑定宿主机的同目录\n\n 1. 创建一个数据卷容器 单创建容器目录 作为数据卷容器\n    \n    docker run -it --name=c3 -v /volume redis /bin/bash\n    \n    \n    1\n    \n\n 2. 创建 c1 c2 容器 使用 --volume-from 参数 设置数据卷from c3数据卷容器中\n    \n    docker run -it --name=c1 --volume-from c3 redis /bin/bash\n    docker run -it --name=c2 --volume-from c3 redis /bin/bash\n    \n    \n    1\n    2\n    \n\n\n# 应用部署\n\n\n# MySQL\n\n创建容器\n\ndocker run -id \\\n-p 3307:3306 \\\n--name=my_mysql \\\n-v $PWD/conf:/etc/mysql \\\n-v $PWD/logs:/logs \\\n-v $PWD/data:/var/lib/mysql \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\nmysql:5.6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n进入容器中\n\ndocker exec -it my_mysql /bin/bash\nmysql -uroot -p123456\n\n\n1\n2\n\n\n\n# Tomcat\n\n创建容器\n\ndocker run -d \\\n--name=my_tomcat \\\n-p 8080:8080 \\\n-v $PWD:/usr/local/tomcat/webapps \\\ntomcat\n\n\n1\n2\n3\n4\n5\n\n\n\n# Nginx\n\ndocker run -id \\\n--name=my_nginx \\\n-p 80:80 \\\n-v $PWD/html:/usr/share/nginx/html \\\nnginx\n\n\n1\n2\n3\n4\n5\n\n\n\n# Redis\n\ndocker run -id --name=my_redis -p 6379:6379 redis\n\n\n1\n\n\n\n# Docker 镜像\n\n\n\nDocker镜像是由特殊的文件系统叠加而成\n\n最底端是bootfs 并使用宿主机的bootfs\n\n第二层是 root 文件系统 rootfs 称为 base image\n\n\n\n\n# 镜像制作\n\n 1. 容器转为镜像 注意通过映射方式挂载的目录或文件 打包后并不会保存在镜像中\n    \n    docker commit 容器id 镜像名称:版本号\n    \n    \n    1\n    \n\n 2. 镜像转为压缩文件\n    \n    docker save -o 压缩名称.tar 镜像名称:版本号\n    \n    \n    1\n    \n\n 3. 从压缩文件中 加载镜像\n    \n    docker load -i 压缩名称.tar\n    \n    \n    1\n    \n\n\n# Dockerfile\n\nDockerfile是一个文本文件 包含许多指令 每一条指令构建一层 基于基础镜像 最终构建出一个新的镜像\n\n命令            说明                 示例                                                       \nFROM          基于这个Image开始        FROM nginx:latest                                        \nENV           环境变量               ENV localfile /usr/local/nginx                           \nRUN           新层中执行命令            RUN /bin/bash -c \'source $HOME/.bashrc; echo $HOME\'      \nLABEL         设置metadata         LABEL version="1.0"                                      \nMAINTAINER    维护者 (deprecated)   MAINTAINER ducafecat                                     \nEXPOSE        声明容器监听端口           EXPOSE 80 443                                            \nADD           复制文件               ADD ./dist ${foo}/html                                   \nCOPY          复制文件               COPY ./dist ${foo}/html                                  \nENTRYPOINT    容器启动时执行指令          CMD ["ls"]                                               \nCMD           容器启动时执行指令默认值       CMD ["-la"]                                              \nVOLUME        挂载点                VOLUME ["/data"]                                         \nUSER          指定操作用户             USER www                                                 \nWORKDIR       设置工作目录             WORKDIR /path/to/workdir                                 \nARG           设置参数               ARG user=www                                             \nONBUILD       镜像被From时触发         ONBUILD RUN /bin/bash -c \'echo ONBUILD ...\'              \nSTOPSIGNAL    停止信号退出             STOPSIGNAL SIGTERM                                       \nHEALTHCHECK   检查容器的健康状况          HEALTHCHECK --interval=5m --timeout=3s CMD curl -f       exit 1\n                                 http://localhost/\nSHELL         覆盖默认shell          SHELL ["powershell", "-command"]                         \n\n# 通过官方centos 自定义\n\n 1. 创建dockerfile文件\n\nFROM centos:7\nMAINTAINER Iekr <Iekr_wh@qq.com>\n\nRUN yum install -y vim\nWORKDIR /usr\nCMD /bin/bash\n\n\n1\n2\n3\n4\n5\n6\n\n\n 2. 通过file文件build镜像\n    \n    #  docker buid -f  dockerfile文件 -t 镜像名称:版本号 .\n    # 注意最后有一个 点\n    docker build -f ./my_dockerfile -t my_centos:1 .\n    \n    \n    1\n    2\n    3\n    \n\n 3. 运行容器\n    \n    #docker run --name <容器名称> -d -p 8080:8080 <上一步构建的镜像名称>\n    docker run --name test  -d -p 8080:8080 my_centos:1\n    \n    \n    1\n    2\n    \n\n# Spring Boot dockerfile\n\nFROM java:8\nMAINTAINER iekr iekr_wh@qq.com\nVOLUME /home/iekr\nADD Schoolelectricity-1.0-SNAPSHOT.jar app.jar\nRUN bash -c \'touch /app.jar\'\nENTRYPOINT ["java","-Duser.timezone=GMT+8","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]\n\n\n1\n2\n3\n4\n5\n6\n\n\ndocker build -t mystory .\ndocker run -d -p 8080:8080 mystory\n\n\n1\n2\n\n\n第二个方案\n\ndocker run -id -p 8080:8080 openjdk:8-jre-alpine\n#添加时区\napk add tzdata\n\n#下载好复制时区过去 就可以了\ncp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\n\n\n1\n2\n3\n4\n5\n6\n\n\nnohup java -jar Schoolelectricity-1.0-SNAPSHOT.jar > /logs.log 2>&1 & #后台启动java\n\n\n1\n\n\n\n# Docker Compose服务编排\n\n微服务架构的应用系统一般包含若干个微服务,每个微服务一般都会部署多个实例,如果每个微服务都要手动启停,工作量会非常的大\n\nDocker Compose是一个编排多容器分布式部署的工具 提供命令集管理容器化应用的完整开发周期,包括服务构建 启动 停止\n\n * 安装 Docker Compose\n\ncurl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\nln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\ndocker-compose --version\n\n\n1\n2\n3\n4\n\n\n 1. 创建 docker-compose.yml\n    \n    # yaml 配置实例\n    version: \'3\'\n    services:\n      web:  #自定义名称\n        build: .  # builddockerfile\n        ports:    #映射端口\n       - "5000:5000"\n        volumes: #映射目录\n       - .:/code\n        - logvolume01:/var/log\n        links:  #当前容器可以访问下面的容器\n       - redis\n      redis:  # 第二个容器\n        image: redis  #容器名称\n    volumes:  \n      logvolume01: {}\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n\n 2. 在当前目录 构建编写配置\n    \n    docker-compose up  #启动配置\n    docker-compose up -d # 后台启动\n    \n    \n    1\n    2\n    \n\n\n# 私有仓库\n\n服务端\n\ndocker pull registry\ndocker run -id --name=registry -p 5000:5000 registry\n#访问 http://192.168.130.124:5000/v2/_catalog 如果出现{"repositories":[]}则安装成功\n\n\n1\n2\n3\n\n\n客户端\n\nvim /etc/docker/daemon.json  \n#添加为私有仓库服务端地址\n{"insecure-registries":["192.168.130.124:5000"]}\n#重启docker服务\nsystemctl restart docker\n\n\n1\n2\n3\n4\n5\n\n\n上传前先给镜像打上标记\n\n#docker tag 镜像名:版本号 私有仓库地址/镜像名:版本号\ndocker tag centos:7 192.168.130.124:5000/centos:7\n\n\n1\n2\n\n\n上传\n\ndocker push 192.168.130.124:5000/centos:7\n\n\n1\n\n\n从仓库拉取镜像\n\ndocker pull 192.168.130.124:5000/centos:7\n\n\n1\n\n\n\n# Docker Hub\n\n登陆 输入账号和密码\n\ndocker login \n\n\n1\n\n\n镜像改名 以自己 用户名/镜像名 改\n\ndocker tag centos:7 iekr/centos:7\ndocker push iekr/centos:7\n\n\n1\n2\n\n\n\n# docker容器与虚拟机比较\n\n\n\n\n\n\n# 本地镜像导出和导入\n\n使用export 和 import 将容器镜像导出和导入操作\n\n\n# 导出\n\ndocker export 镜像id > 压缩文件名.tar\n\ndocker export f299f501774c > hangger_server.tar\n\n\n1\n\n\n\n# 导入\n\ndocker import - 自定义容器名称 < 镜像压缩文件.tar\n\ndocker import - new_hangger_server < hangger_server.tar\n\n\n1\n\n * export命令是从容器（container）中导出tar文件，而save命令则是从镜像（images）中导出\n * save 保存镜像所有的信息-包含历史\n * export 只导出当前的信息 只包含当前版本信息 无法回滚其他版本',normalizedContent:'# docker\n\ndocker 是一个开源的应用容器引擎\n\n\n# 安装\n\nyum install -y docker\n\n\n1\n\n\n\n# 架构\n\n\n\n\n# docker 镜像加速器\n\nhttps://cr.console.aliyun.com/cn-hangzhou/instances/mirrors\n\nsudo mkdir -p /etc/docker\nsudo tee /etc/docker/daemon.json <<-\'eof\'\n{\n  "registry-mirrors": ["https://o7uzc3zp.mirror.aliyuncs.com"]\n}\neof\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# docker 命令\n\n\n# 服务\n\nsystemctl start docker  # 启动\nsystemctl stop docker  # 停止\nsystemctl status docker  #状态\nsystemctl restart docker #重启\nsystemctl enabled docker #开机自启\n\n\n1\n2\n3\n4\n5\n\n\n\n# 镜像\n\n * 查看本地镜像\n   \n   docker images\n   \n   \n   1\n   \n\n * 搜索镜像\n   \n   docker search 镜像名\n   \n   \n   1\n   \n\n * 拉取镜像\n   \n   docker pull 镜像名:版本号 # 版本号默认为latest\n   \n   \n   1\n   \n\n * 删除镜像\n   \n   docker rmi 镜像id    # 也可以通过名称:版本号删除\n   \n   \n   1\n   \n\n * 删除所有镜像\n   \n   docker rmi `docker images -q`\n   \n   \n   1\n   \n\n\n# 容器\n\n * 创建容器并 运行参数run\n   \n   * -i 保持运行\n   * -t 以终端模式运行并进入\n   * -d 后台常驻模式\n   * --name=xxx 此容器名称\n   * 镜像名/id\n   * /bin/bash 初始化命令\n   \n   docker run -it --name=redis redis /bin/bash  #进入容器内部\n   docker run -d --name=redis2 redis # 后台运行容器\n   \n   \n   1\n   2\n   \n\n * 查询容器\n   \n   docker ps  #查询正在运行的容器\n   docker ps -a # 查看所有容器包括已停止的\n   \n   \n   1\n   2\n   \n\n * 进入已经运行的容器内部\n   \n   docker exec -it 名称/id /bin/bash \n   \n   \n   1\n   \n\n * 运行 停止容器\n   \n   docker stop 名称/id  #停止容器\n   docker start 名称/id  #启动容器\n   \n   \n   1\n   2\n   \n\n * 删除00容器\n   \n   docker rm 名称/id # 删除容器 先docker ps -a 查询\n   docker rm `docker ps -aq`   # 删除所有容器 慎用 正在运行的容器无法删除\n   \n   \n   1\n   2\n   \n\n * 查看容器 配置信息\n   \n   docker inspect 名称/id\n   \n   \n   1\n   \n\n * 修改配置信息\n\nsystemctl stop docker\ncd /var/lib/docker/containers/docekr inspect查出的id\nvim config.v2.json\nsystemctl restart docker\n\n\n1\n2\n3\n4\n\n\n * 用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及即无tag的镜像 谨慎使用\n   \n   docker system prune\n   \n   \n   1\n   \n\n * 查看docker自身磁盘使用情况\n   \n   docker system df\n   \n   \n   1\n   \n\n\n# 容器数据卷\n\n数据卷是宿主机中的一个目录或文件 称为挂载目录 一个数据卷可以被多个容器同时挂载\n\n * 创建启动容器时 使用 -v参数 设置数据集\n   \n   docker run -d -v 宿主机目录:容器内目录\n   \n   \n   1\n   \n   \n   目录必须为绝对路径 如果目录不存在则自动创建\n\n\n# 数据卷容器\n\n多容器进行数据交换 之前我们通过多容器绑定宿主机的同目录\n\n 1. 创建一个数据卷容器 单创建容器目录 作为数据卷容器\n    \n    docker run -it --name=c3 -v /volume redis /bin/bash\n    \n    \n    1\n    \n\n 2. 创建 c1 c2 容器 使用 --volume-from 参数 设置数据卷from c3数据卷容器中\n    \n    docker run -it --name=c1 --volume-from c3 redis /bin/bash\n    docker run -it --name=c2 --volume-from c3 redis /bin/bash\n    \n    \n    1\n    2\n    \n\n\n# 应用部署\n\n\n# mysql\n\n创建容器\n\ndocker run -id \\\n-p 3307:3306 \\\n--name=my_mysql \\\n-v $pwd/conf:/etc/mysql \\\n-v $pwd/logs:/logs \\\n-v $pwd/data:/var/lib/mysql \\\n-e mysql_root_password=123456 \\\nmysql:5.6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n进入容器中\n\ndocker exec -it my_mysql /bin/bash\nmysql -uroot -p123456\n\n\n1\n2\n\n\n\n# tomcat\n\n创建容器\n\ndocker run -d \\\n--name=my_tomcat \\\n-p 8080:8080 \\\n-v $pwd:/usr/local/tomcat/webapps \\\ntomcat\n\n\n1\n2\n3\n4\n5\n\n\n\n# nginx\n\ndocker run -id \\\n--name=my_nginx \\\n-p 80:80 \\\n-v $pwd/html:/usr/share/nginx/html \\\nnginx\n\n\n1\n2\n3\n4\n5\n\n\n\n# redis\n\ndocker run -id --name=my_redis -p 6379:6379 redis\n\n\n1\n\n\n\n# docker 镜像\n\n\n\ndocker镜像是由特殊的文件系统叠加而成\n\n最底端是bootfs 并使用宿主机的bootfs\n\n第二层是 root 文件系统 rootfs 称为 base image\n\n\n\n\n# 镜像制作\n\n 1. 容器转为镜像 注意通过映射方式挂载的目录或文件 打包后并不会保存在镜像中\n    \n    docker commit 容器id 镜像名称:版本号\n    \n    \n    1\n    \n\n 2. 镜像转为压缩文件\n    \n    docker save -o 压缩名称.tar 镜像名称:版本号\n    \n    \n    1\n    \n\n 3. 从压缩文件中 加载镜像\n    \n    docker load -i 压缩名称.tar\n    \n    \n    1\n    \n\n\n# dockerfile\n\ndockerfile是一个文本文件 包含许多指令 每一条指令构建一层 基于基础镜像 最终构建出一个新的镜像\n\n命令            说明                 示例                                                       \nfrom          基于这个image开始        from nginx:latest                                        \nenv           环境变量               env localfile /usr/local/nginx                           \nrun           新层中执行命令            run /bin/bash -c \'source $home/.bashrc; echo $home\'      \nlabel         设置metadata         label version="1.0"                                      \nmaintainer    维护者 (deprecated)   maintainer ducafecat                                     \nexpose        声明容器监听端口           expose 80 443                                            \nadd           复制文件               add ./dist ${foo}/html                                   \ncopy          复制文件               copy ./dist ${foo}/html                                  \nentrypoint    容器启动时执行指令          cmd ["ls"]                                               \ncmd           容器启动时执行指令默认值       cmd ["-la"]                                              \nvolume        挂载点                volume ["/data"]                                         \nuser          指定操作用户             user www                                                 \nworkdir       设置工作目录             workdir /path/to/workdir                                 \narg           设置参数               arg user=www                                             \nonbuild       镜像被from时触发         onbuild run /bin/bash -c \'echo onbuild ...\'              \nstopsignal    停止信号退出             stopsignal sigterm                                       \nhealthcheck   检查容器的健康状况          healthcheck --interval=5m --timeout=3s cmd curl -f       exit 1\n                                 http://localhost/\nshell         覆盖默认shell          shell ["powershell", "-command"]                         \n\n# 通过官方centos 自定义\n\n 1. 创建dockerfile文件\n\nfrom centos:7\nmaintainer iekr <iekr_wh@qq.com>\n\nrun yum install -y vim\nworkdir /usr\ncmd /bin/bash\n\n\n1\n2\n3\n4\n5\n6\n\n\n 2. 通过file文件build镜像\n    \n    #  docker buid -f  dockerfile文件 -t 镜像名称:版本号 .\n    # 注意最后有一个 点\n    docker build -f ./my_dockerfile -t my_centos:1 .\n    \n    \n    1\n    2\n    3\n    \n\n 3. 运行容器\n    \n    #docker run --name <容器名称> -d -p 8080:8080 <上一步构建的镜像名称>\n    docker run --name test  -d -p 8080:8080 my_centos:1\n    \n    \n    1\n    2\n    \n\n# spring boot dockerfile\n\nfrom java:8\nmaintainer iekr iekr_wh@qq.com\nvolume /home/iekr\nadd schoolelectricity-1.0-snapshot.jar app.jar\nrun bash -c \'touch /app.jar\'\nentrypoint ["java","-duser.timezone=gmt+8","-djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]\n\n\n1\n2\n3\n4\n5\n6\n\n\ndocker build -t mystory .\ndocker run -d -p 8080:8080 mystory\n\n\n1\n2\n\n\n第二个方案\n\ndocker run -id -p 8080:8080 openjdk:8-jre-alpine\n#添加时区\napk add tzdata\n\n#下载好复制时区过去 就可以了\ncp /usr/share/zoneinfo/asia/shanghai /etc/localtime\n\n\n1\n2\n3\n4\n5\n6\n\n\nnohup java -jar schoolelectricity-1.0-snapshot.jar > /logs.log 2>&1 & #后台启动java\n\n\n1\n\n\n\n# docker compose服务编排\n\n微服务架构的应用系统一般包含若干个微服务,每个微服务一般都会部署多个实例,如果每个微服务都要手动启停,工作量会非常的大\n\ndocker compose是一个编排多容器分布式部署的工具 提供命令集管理容器化应用的完整开发周期,包括服务构建 启动 停止\n\n * 安装 docker compose\n\ncurl -l "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\nln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\ndocker-compose --version\n\n\n1\n2\n3\n4\n\n\n 1. 创建 docker-compose.yml\n    \n    # yaml 配置实例\n    version: \'3\'\n    services:\n      web:  #自定义名称\n        build: .  # builddockerfile\n        ports:    #映射端口\n       - "5000:5000"\n        volumes: #映射目录\n       - .:/code\n        - logvolume01:/var/log\n        links:  #当前容器可以访问下面的容器\n       - redis\n      redis:  # 第二个容器\n        image: redis  #容器名称\n    volumes:  \n      logvolume01: {}\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n\n 2. 在当前目录 构建编写配置\n    \n    docker-compose up  #启动配置\n    docker-compose up -d # 后台启动\n    \n    \n    1\n    2\n    \n\n\n# 私有仓库\n\n服务端\n\ndocker pull registry\ndocker run -id --name=registry -p 5000:5000 registry\n#访问 http://192.168.130.124:5000/v2/_catalog 如果出现{"repositories":[]}则安装成功\n\n\n1\n2\n3\n\n\n客户端\n\nvim /etc/docker/daemon.json  \n#添加为私有仓库服务端地址\n{"insecure-registries":["192.168.130.124:5000"]}\n#重启docker服务\nsystemctl restart docker\n\n\n1\n2\n3\n4\n5\n\n\n上传前先给镜像打上标记\n\n#docker tag 镜像名:版本号 私有仓库地址/镜像名:版本号\ndocker tag centos:7 192.168.130.124:5000/centos:7\n\n\n1\n2\n\n\n上传\n\ndocker push 192.168.130.124:5000/centos:7\n\n\n1\n\n\n从仓库拉取镜像\n\ndocker pull 192.168.130.124:5000/centos:7\n\n\n1\n\n\n\n# docker hub\n\n登陆 输入账号和密码\n\ndocker login \n\n\n1\n\n\n镜像改名 以自己 用户名/镜像名 改\n\ndocker tag centos:7 iekr/centos:7\ndocker push iekr/centos:7\n\n\n1\n2\n\n\n\n# docker容器与虚拟机比较\n\n\n\n\n\n\n# 本地镜像导出和导入\n\n使用export 和 import 将容器镜像导出和导入操作\n\n\n# 导出\n\ndocker export 镜像id > 压缩文件名.tar\n\ndocker export f299f501774c > hangger_server.tar\n\n\n1\n\n\n\n# 导入\n\ndocker import - 自定义容器名称 < 镜像压缩文件.tar\n\ndocker import - new_hangger_server < hangger_server.tar\n\n\n1\n\n * export命令是从容器（container）中导出tar文件，而save命令则是从镜像（images）中导出\n * save 保存镜像所有的信息-包含历史\n * export 只导出当前的信息 只包含当前版本信息 无法回滚其他版本',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Spring Cloud",frontmatter:{title:"Spring Cloud",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/e886f8/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/17.Spring%20Cloud.html",relativePath:"后端/02.JavaEE/17.Spring Cloud.md",key:"v-22d9c6ec",path:"/pages/e886f8/",headers:[{level:2,title:"微服务",slug:"微服务",normalizedTitle:"微服务",charIndex:117},{level:2,title:"Spring Cloud 与 Dubbo 对比",slug:"spring-cloud-与-dubbo-对比",normalizedTitle:"spring cloud 与 dubbo 对比",charIndex:183},{level:2,title:"服务治理",slug:"服务治理",normalizedTitle:"服务治理",charIndex:223},{level:3,title:"Eureka",slug:"eureka",normalizedTitle:"eureka",charIndex:348},{level:3,title:"Consul",slug:"consul",normalizedTitle:"consul",charIndex:8422},{level:3,title:"Nacos",slug:"nacos",normalizedTitle:"nacos",charIndex:11697},{level:2,title:"Ribbon 负载均衡",slug:"ribbon-负载均衡",normalizedTitle:"ribbon 负载均衡",charIndex:13273},{level:3,title:"简化ResTemplate调用",slug:"简化restemplate调用",normalizedTitle:"简化restemplate调用",charIndex:13291},{level:3,title:"负载均衡",slug:"负载均衡",normalizedTitle:"负载均衡",charIndex:420},{level:2,title:"Feign 声明式服务调用",slug:"feign-声明式服务调用",normalizedTitle:"feign 声明式服务调用",charIndex:16556},{level:3,title:"超时设置",slug:"超时设置",normalizedTitle:"超时设置",charIndex:20032},{level:3,title:"日志记录",slug:"日志记录",normalizedTitle:"日志记录",charIndex:20210},{level:2,title:"Hystrix 熔断器",slug:"hystrix-熔断器",normalizedTitle:"hystrix 熔断器",charIndex:22034},{level:3,title:"服务方降级",slug:"服务方降级",normalizedTitle:"服务方降级",charIndex:22188},{level:3,title:"消费方降级",slug:"消费方降级",normalizedTitle:"消费方降级",charIndex:24387},{level:3,title:"熔断机制",slug:"熔断机制",normalizedTitle:"熔断机制",charIndex:25974},{level:3,title:"Turbine  熔断监控",slug:"turbine-熔断监控",normalizedTitle:"turbine  熔断监控",charIndex:null},{level:2,title:"Gateway 网关",slug:"gateway-网关",normalizedTitle:"gateway 网关",charIndex:33426},{level:3,title:"入门案例",slug:"入门案例-2",normalizedTitle:"入门案例",charIndex:11954},{level:3,title:"静态路由",slug:"静态路由",normalizedTitle:"静态路由",charIndex:35274},{level:3,title:"动态路由",slug:"动态路由",normalizedTitle:"动态路由",charIndex:35835},{level:3,title:"微服务名称配置",slug:"微服务名称配置",normalizedTitle:"微服务名称配置",charIndex:36875},{level:3,title:"网关过滤器",slug:"网关过滤器",normalizedTitle:"网关过滤器",charIndex:37931},{level:2,title:"Config 分布式配置中心",slug:"config-分布式配置中心",normalizedTitle:"config 分布式配置中心",charIndex:42740},{level:3,title:"客户端刷新",slug:"客户端刷新",normalizedTitle:"客户端刷新",charIndex:44855},{level:3,title:"集成Eureka",slug:"集成eureka",normalizedTitle:"集成eureka",charIndex:45807},{level:2,title:"Bus 消息总线",slug:"bus-消息总线",normalizedTitle:"bus 消息总线",charIndex:47448},{level:2,title:"Stream 消息驱动",slug:"stream-消息驱动",normalizedTitle:"stream 消息驱动",charIndex:48658},{level:3,title:"消息生产者",slug:"消息生产者",normalizedTitle:"消息生产者",charIndex:48802},{level:3,title:"消息消费者",slug:"消息消费者",normalizedTitle:"消息消费者",charIndex:51968},{level:2,title:"Sleuth+Zipkin 链路追踪",slug:"sleuth-zipkin-链路追踪",normalizedTitle:"sleuth+zipkin 链路追踪",charIndex:54286}],headersStr:"微服务 Spring Cloud 与 Dubbo 对比 服务治理 Eureka Consul Nacos Ribbon 负载均衡 简化ResTemplate调用 负载均衡 Feign 声明式服务调用 超时设置 日志记录 Hystrix 熔断器 服务方降级 消费方降级 熔断机制 Turbine  熔断监控 Gateway 网关 入门案例 静态路由 动态路由 微服务名称配置 网关过滤器 Config 分布式配置中心 客户端刷新 集成Eureka Bus 消息总线 Stream 消息驱动 消息生产者 消息消费者 Sleuth+Zipkin 链路追踪",content:'# Spring Cloud\n\nSpring Cloud是一系列框架的有序集合\n\n通过 Spring Boot 风格进行再封装屏蔽掉了复杂的配置和实现原理 最终给开发者留出了一套简单易懂 易部署 易维护的分布式系统开发工具包\n\n\n# 微服务\n\n\n\n微服务它将一个原本独立的系统 拆分成为多个小型服务 并且这些小型服务都可以独立运行 服务之间可以进行通信协作\n\n\n# Spring Cloud 与 Dubbo 对比\n\n\n\n * Dubbo只是实现了服务治理 而spring cloud子项目分别覆盖了微服务架构下的众多部件\n * Dubbo使用 RPC 通信协议 Spring Cloud 使用 RESTful 完成通信 Dubbo 效率略高于 Spring Cloud\n\n\n# 服务治理\n\n\n# Eureka\n\nEureka 是 Netflix 公司开源的一个服务注册与发现的组件\n\nEureka 和 其他Netflix 公司的服务组件(如负载均衡 熔断器 网关等) 一起 被spring cloud 社区整合为 Spring-Cloud-Netflix 模块\n\nEureka 包含 Eueka Server(注册中心) 和 Eureka Client(服务提供者 服务器消费者)\n\n# server\n\n引入eureka-server 和 spring web\n\n<dependencies>\n\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-web</artifactId>\n    </dependency>\n\n    \x3c!-- eureka-server --\x3e\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\napplication\n\nserver:\n  port: 8761\n\n# eureka 配置\n#1.dashboard  :eureka的web控制台配置\n#2.server  :eureka的服务端配置\n#3.client  :eureka的客户端配置\n#4.instance  :eureka的实例配置\neureka:\n  instance:\n    hostname: localhost # 主机名\n\n  dashboard:\n    enabled: true  # 是否启用web控制台  默认为开启\n    path: /  # 默认路径为/  访问http://localhost:8761/即可\n\n  client:\n    service-url:\n      defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\n    register-with-eureka: false # 是否将自己的路径 注册到eureka上  默认为true  一般情况下只有provider client需要\n    fetch-registry: false  # 是否需要从eureka中抓取路径   默认为true  一般只有consumer client需要\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n启动类\n\npackage com.itheima.eureka;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@SpringBootApplication\n//启用EurekaServer\n@EnableEurekaServer\npublic class EurekaApp {\n    public static void main(String[] args) {\n        SpringApplication.run(EurekaApp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# provider\n\n引入 eureka-client 和 spring web\n\n<dependencies>\n\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-web</artifactId>\n    </dependency>\n\n\n    \x3c!-- eureka-client --\x3e\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n    </dependency>\n\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\napplication\n\nserver:\n  port: 8000\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n  client:\n    service-url:\n      defaultZone: http://localhost:8761/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\nspring:\n  application:\n    name: eureka-provider # 设置当前应用的名称 将来在eureka的web控制台Application显示为该名称  将来需要该名称来获取路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n启动类\n\npackage com.itheima.provider;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n\n@SpringBootApplication\n//标记为 Eureka 客户端 在新版本中自动标记\n@EnableEurekaClient\npublic class ProviderApp {\n    public static void main(String[] args) {\n        SpringApplication.run(ProviderApp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n并编写对应的dao层 domian对象 service层 controller层\n\n并且能够远程调用获取数据 此时为了在Eureka上注册为应用使其他应用能够远程调用该服务\n\n# consumer\n\n引入 eureka-client 和 spring web\n\n<dependencies>\n\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-web</artifactId>\n    </dependency>\n\n\n    \x3c!-- eureka-client --\x3e\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n    </dependency>\n\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\napplication\n\nserver:\n  port: 9000\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n  client:\n    service-url:\n      defaultZone: http://localhost:8761/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\nspring:\n  application:\n    name: eureka-consumer # 设置当前应用的名称 将来在eureka的web控制台Application显示为该名称  将来需要该名称来获取路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n启动类\n\npackage com.itheima.consumer;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n\n@SpringBootApplication\n@EnableEurekaClient\n@EnableDiscoveryClient //激活DiscoveryClient 新版本可以忽略 自动激活\npublic class ConsumerApp {\n    public static void main(String[] args) {\n        SpringApplication.run(ConsumerApp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nRestTemplateConfig类\n\npackage com.itheima.consumer.cnfig;\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.client.RestTemplate;\n\n@Configuration\npublic class RestTemplateConfig {\n\n    @Bean\n    public RestTemplate restTemplate(){\n        return new RestTemplate();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\ncontroller层\n\npackage com.itheima.consumer.controller;\n\n\nimport com.itheima.consumer.domain.Goods;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cloud.client.ServiceInstance;\nimport org.springframework.cloud.client.discovery.DiscoveryClient;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\nimport java.util.List;\n\n@RestController\n@RequestMapping("/order")\npublic class OrderController {\n\n    @Autowired\n    private RestTemplate restTemplate;\n\n    @Autowired\n    private DiscoveryClient discoveryClient;\n\n    @GetMapping("/goods/{id}")\n    public Goods findGoodsById(@PathVariable("id") int id) {\n\n /*\n        远程调用goods服务中的findone接口\n        使用RestTemplate\n        1.定义bean\n        2.注入bean\n        3.调用方法\n         */\n        /*\n        动态从 Eureka server 中获取provide 的 ip 和端口\n        1. 注入 DiscoveryClient 并在启动类注解激活\n        2. 调用方法\n         */\n        List<ServiceInstance> instances = discoveryClient.getInstances("EUREKA-PROVIDER");  //根据应用名在Eureka服务端中获取应用集合\n        if (instances == null || instances.size() == 0) { //判断是否为空\n            return null;\n        }\n        ServiceInstance instance = instances.get(0);  //此处只有一个所以使用第一个\n        String host = instance.getHost();  //获取ip\n        int port = instance.getPort(); //获取端口\n        System.out.println(host);\n        System.out.println(port);\n\n        String url = "http://" + host + ":" + port + "goods/findOne/" + id;\n        Goods goods = restTemplate.getForObject(url, Goods.class);\n        return goods;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n# 配置属性\n\n# instance\n\n\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n    prefer-ip-address: true  # 以ip地址形式注册到eureka server中 默认为false 注册为主机名\n    instance-id: ${eureka.instance.prefer-ip-address}:${spring.application.name}:${server.port}  #设置在web控制台中应用id的名称\n\n\n1\n2\n3\n4\n5\n\n\n# server\n\n\n\n# 高可用\n\n每个server都是镜像相同的注册应用 如果其中一个宕机了 另外一个server仍然存活\n\n设置多个eureka server 相互注册\n\nserver:\n  port: 8762\n\neureka:\n  instance:\n    hostname: eureka-server2 # 主机名\n\n  dashboard:\n    enabled: true  # 是否启用web控制台  默认为开启\n    path: /  # 默认路径为/  访问http://localhost:8761/即可\n\n  client:\n    service-url:\n      defaultZone: http://127.0.0.1:8761/eureka  # 两个eureka相互注册\n\n    register-with-eureka: true # 是否将自己的路径 注册到eureka上  默认为true  一般情况下只有provider client需要\n    fetch-registry: true  # 是否需要从eureka中抓取路径   默认为true  一般只有consumer client需要\nspring:\n  application:\n    name: eureka-server-ha\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n在client中在多个server注册应用\n\nserver:\n  port: 8000\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n  client:\n    service-url:\n      defaultZone: http://localhost:8761/eureka,http://localhost:8762/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\nspring:\n  application:\n    name: eureka-provider # 设置当前应用的名称 将来在eureka的web控制台Application显示为该名称  将来需要该名称来获取路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# Consul\n\nConsul 是 由 HashiCorp 基于 Go 语言开发 支持多数据中心 分布式高可用的服务发布和注册服务软件\n\n这里我们使用windows中的exe执行程序\n\n启动\n\n.\\consul agent -dev\n\n\n1\n\n\n打开web管理界面\n\nhttp://localhost:8500/\n\n 1. 搭建provider 和 Consumer 服务\n 2. 并配置好 RestTemplate 完成远程调用\n\nconsul客户端坐标 必须导入actuator组件\n\n        \x3c!-- consul-client --\x3e\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n不需要在启动类中标价\n\n只需要在application配置好consul属性\n\nprovider配置\n\nserver:\n  port: 8000\nspring:\n  cloud:\n    consul:\n      host: localhost  # consul服务端 ip\n      port: 8500  # 端口\n      discovery:\n        service-name: ${spring.application.name}  # 应用名称\n        prefer-ip-address: true # 以ip注册\n  application:\n    name: consul-provider  # 应用名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nconsumer配置\n\nserver:\n  port: 9000\nspring:\n  cloud:\n    consul:\n      host: localhost  # consul服务端 ip\n      port: 8500  # 端口\n      discovery:\n        service-name: ${spring.application.name}  # 应用名称\n        prefer-ip-address: true # 以ip注册\n  application:\n    name: consul-consumer  # 应用名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nconsumer的controller层还是一致\n\npackage com.itheima.consumer.controller;\n\n\nimport com.itheima.consumer.domain.Goods;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cloud.client.ServiceInstance;\nimport org.springframework.cloud.client.discovery.DiscoveryClient;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\nimport java.util.List;\n\n@RestController\n@RequestMapping("/order")\npublic class OrderController {\n\n    @Autowired\n    private RestTemplate restTemplate;\n\n    @Autowired\n    private DiscoveryClient discoveryClient;\n\n    @GetMapping("/goods/{id}")\n    public Goods findGoodsById(@PathVariable("id") int id) {\n\n /*\n        远程调用goods服务中的findone接口\n        使用RestTemplate\n        1.定义bean\n        2.注入bean\n        3.调用方法\n         */\n        /*\n        动态从 Eureka server 中获取provide 的 ip 和端口\n        1. 注入 DiscoveryClient 并在启动类注解激活\n        2. 调用方法\n         */\n        List<ServiceInstance> instances = discoveryClient.getInstances("consul-provider");  //根据应用名在Eureka服务端中获取应用集合\n        if (instances == null || instances.size() == 0) { //判断是否为空\n            return null;\n        }\n        ServiceInstance instance = instances.get(0);  //此处只有一个所以使用第一个\n        String host = instance.getHost();  //获取ip\n        int port = instance.getPort(); //获取端口\n        System.out.println(host);\n        System.out.println(port);\n\n        String url = "http://" + host + ":" + port + "goods/findOne/" + id;\n        Goods goods = restTemplate.getForObject(url, Goods.class);\n        return goods;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\nconsumer的controller层还是一致\n\n\n# Nacos\n\nNacos 是阿里巴巴的开源项目 专注于服务发现和配置管理微服务\n\nNacos = Spring Cloud 注册中心 + Spring Cloud配置中心\n\nhttps://nacos.io/zh-cn/\n\n# 启动\n\n在bin目录下 以单机形式运行\n\n启动命令(standalone代表着单机模式运行，非集群模式):\n\nstartup.cmd -m standalone\n\n\n1\n\n\nweb管理页面\n\nhttp://localhost:8848/nacos/\n\n默认用户密码 为nacos\n\n# 入门案例\n\nnacos坐标\n\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>\n            <version>0.2.2.RELEASE</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba.nacos</groupId>\n            <artifactId>nacos-client</artifactId>\n            <version>2.0.3</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nprovider配置\n\nserver:\n  port: 8000\nspring:\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848  # 配置nacos服务端地址和端口\n\n  application:\n    name: nacos-provider # 服务名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nconsumer配置\n\nserver:\n  port: 9000\nspring:\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848  # 配置nacos服务端地址和端口\n\n  application:\n    name: nacos-consumer # 服务名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nconsumer启动类\n\npackage com.itheima.consumer;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n\n\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class ConsumerApp {\n    public static void main(String[] args) {\n        SpringApplication.run(ConsumerApp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# Ribbon 负载均衡\n\n\n\n\n# 简化ResTemplate调用\n\n之前我们Eureka通过ResTemplate获取一个应用集合, 获取一个应用对象 的 地址 和端口,并拼接成一个url调用服务方的应用 使用Ribbon只需要将ip:端口换成应用名称即可以远程调用 应用\n\n 1. 在ResTemplate的Bean绑定中加上 @LoadBalanced 注解\n    \n    package com.itheima.consumer.cnfig;\n    \n    import org.springframework.cloud.client.loadbalancer.LoadBalanced;\n    import org.springframework.context.annotation.Bean;\n    import org.springframework.context.annotation.Configuration;\n    import org.springframework.web.client.RestTemplate;\n    \n    @Configuration\n    public class RestTemplateConfig {\n    \n        @LoadBalanced\n        @Bean\n        public RestTemplate restTemplate(){\n            return new RestTemplate();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n\n 2. 将url中的ip:端口 换成应用名称\n    \n    /*\n    使用 Ribbon 简化调用\n    1.在声明restTemplate的Bean时 添加一个@LoadBalanced 注解\n    2.在使用在声明restTemplate 发请求时 需要定义url时 host:port可以换成 服务端提供的应用名称\n     */\n    @GetMapping("/goods2/{id}")\n    public Goods findGoodsById2(@PathVariable("id") int id) {\n    \n        String url = "http://EUREKA-PROVIDER/goods/findOne/" + id;\n        Goods goods = restTemplate.getForObject(url, Goods.class);\n        return goods;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    \n\n\n# 负载均衡\n\n通过在ResTemplate的Bean绑定中加上 @LoadBalanced 注解后,如果当前用于有个多个服务提供方,则Ribbon会自动帮我们负载均衡,默认为轮询策略,策略配置在消费者一端 即客户端负载均衡\n\n\n\n# 配置类形式配置负载策略\n\n 1. 定义配置类 使用Bean绑定并返回一个IRule接口的实现类\n    \n    package com.itheima.consumer.cnfig;\n    \n    import com.netflix.loadbalancer.IRule;\n    import com.netflix.loadbalancer.RandomRule;\n    import org.springframework.context.annotation.Bean;\n    import org.springframework.context.annotation.Configuration;\n    \n    @Configuration\n    public class MyRule {\n    \n        @Bean\n        public IRule rule(){\n            return new RandomRule();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n 2. 在启动类中 使用@RibbonClient 定义应用名称和配置类\n    \n    package com.itheima.consumer;\n    \n    import com.itheima.consumer.cnfig.MyRule;\n    import org.springframework.boot.SpringApplication;\n    import org.springframework.boot.autoconfigure.SpringBootApplication;\n    import org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n    import org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n    import org.springframework.cloud.netflix.ribbon.RibbonClient;\n    \n    @SpringBootApplication\n    @EnableEurekaClient\n    @EnableDiscoveryClient //激活DiscoveryClient 新版本可以忽略 自动激活\n    /*\n        配置Ribbon 负载均衡策略\n        name :设置服务提供方的 应用名称\n        configuration : 设置负载均衡的Bean\n    \n     */\n    @RibbonClient(name ="EUREKA-PROVIDER",configuration = MyRule.class)\n    public class ConsumerApp {\n        public static void main(String[] args) {\n            SpringApplication.run(ConsumerApp.class, args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    \n\n# 配置方式设置\n\n在application中设置该属性\n\nEUREKA-PROVIDER:  # 应用名称\n  ribbon:\n    NFloadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule  # 负载均衡策略实现类的引用路径\n\n\n1\n2\n3\n\n\n\n# Feign 声明式服务调用\n\nFeign 是一个声明式的REST客户端 它基于接口的注解方式 很方便实现客户端配置\n\n 1. 在消费者端导入坐标\n    \n    <dependency>\n         <groupId>org.springframework.cloud</groupId>\n         <artifactId>spring-cloud-starter-openfeign</artifactId>\n     </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 消费者启动类中加上 @EnableFeignClients 启用Feign\n    \n    package com.itheima.consumer;\n    \n    import org.springframework.boot.SpringApplication;\n    import org.springframework.boot.autoconfigure.SpringBootApplication;\n    import org.springframework.cloud.client.discovery.EnableDiscoveryClient;\n    import org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n    import org.springframework.cloud.openfeign.EnableFeignClients;\n    \n    @EnableDiscoveryClient // 激活DiscoveryClient\n    @EnableEurekaClient\n    @SpringBootApplication\n    \n    @EnableFeignClients //开启Feign的功能\n    public class ConsumerApp {\n    \n    \n        public static void main(String[] args) {\n            SpringApplication.run(ConsumerApp.class,args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 3. 在消费者中定义一个feign声明接口\n    \n    package com.itheima.consumer.feign;\n    \n    import com.itheima.consumer.domain.Goods;\n    import org.springframework.cloud.openfeign.FeignClient;\n    import org.springframework.web.bind.annotation.GetMapping;\n    import org.springframework.web.bind.annotation.PathVariable;\n    \n    /**\n     * feign声明式接口  发起远程调用的\n     * 1.定义接口\n     * 2.接口上添加 @FeignClient value属性为 应用名称\n     * 3.编写调用接口 接口的声明规则 和 提供方接口保持一致\n     * 4.注入该接口对象 调用接口方法完成远程调用\n     */\n    \n    @FeignClient(value = "feign-provider")\n    public interface GoodsFeignClient {\n        @GetMapping("/goods/findOne/{id}")\n        Goods findGoodsById(@PathVariable("id") int id);\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 4. 注入 声明接口对象 调用接口方法\n    \n    package com.itheima.consumer.controller;\n    \n    \n    import com.itheima.consumer.domain.Goods;\n    import com.itheima.consumer.feign.GoodsFeignClient;\n    import org.springframework.beans.factory.annotation.Autowired;\n    import org.springframework.web.bind.annotation.GetMapping;\n    import org.springframework.web.bind.annotation.PathVariable;\n    import org.springframework.web.bind.annotation.RequestMapping;\n    import org.springframework.web.bind.annotation.RestController;\n    import org.springframework.web.client.RestTemplate;\n    \n    @RestController\n    @RequestMapping("/order")\n    public class OrderController {\n    \n        @Autowired\n        private RestTemplate restTemplate;\n    \n    \n        @Autowired\n        private GoodsFeignClient goodsFeignClient;\n    \n    \n        @GetMapping("/goods/{id}")\n        public Goods findGoodsById(@PathVariable("id") int id){\n    \n            /*String url = "http://FEIGN-PROVIDER/goods/findOne/"+id;\n            // 3. 调用方法\n            Goods goods = restTemplate.getForObject(url, Goods.class);*/\n    \n            Goods goods = goodsFeignClient.findGoodsById(id);\n    \n    \n            return goods;\n        }\n    \n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    \n\n\n# 超时设置\n\nFeign 底层依赖于 Ribbon 实现负载均衡和远程调用\n\nRibbon默认1秒超时\n\n在consumer的配置文件中配置\n\nribbon:\n  ConnectTimeout: 1000 # 设置连接超时时间 默认为1000毫秒\n  ReadTimeout: 3000 # 逻辑处理的超时时间 默认为1000毫秒\n\n\n1\n2\n3\n\n\n\n# 日志记录\n\nFeign 只能记录 debug 级别的日志信息\n\n 1. 设置consumer的日志级别 为该路径下所有的类为debug级别\n    \n    logging:\n      level:\n        com.itheima: debug # 键位类路径\n    \n    \n    1\n    2\n    3\n    \n\n 2. 在consumer中定义配置类\n    \n    package com.itheima.consumer.config;\n    \n    import feign.Logger;\n    import org.springframework.context.annotation.Bean;\n    import org.springframework.context.annotation.Configuration;\n    \n    @Configuration\n    public class FeignLogConfig {\n    \n        @Bean\n        public Logger.Level level(){\n            /**\n             * NONE 不记录\n             * BASIC 记录基本的请求行 响应状态码数据\n             * HEADERS 记录基本的请求行 响应状态码数据  响应头信息\n             * FULL 记录完整的信息\n             */\n            return Logger.Level.FULL;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 3. 在声明式接口中的@FeignClient注解 引入配置类\n    \n    package com.itheima.consumer.feign;\n    \n    import com.itheima.consumer.config.FeignLogConfig;\n    import com.itheima.consumer.domain.Goods;\n    import org.springframework.cloud.openfeign.FeignClient;\n    import org.springframework.web.bind.annotation.GetMapping;\n    import org.springframework.web.bind.annotation.PathVariable;\n    \n    /**\n     * feign声明式接口  发起远程调用的\n     * 1.定义接口\n     * 2.接口上添加 @FeignClient value属性为 应用名称   configuration 为日志配置类\n     * 3.编写调用接口 接口的声明规则 和 提供方接口保持一致\n     * 4.注入该接口对象 调用接口方法完成远程调用\n     */\n    \n    @FeignClient(value = "feign-provider",configuration = FeignLogConfig.class)\n    public interface GoodsFeignClient {\n        @GetMapping("/goods/findOne/{id}")\n        Goods findGoodsById(@PathVariable("id") int id);\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n\n# Hystrix 熔断器\n\nHystrix 是Netflix 开源的一个延迟和容错库 用于隔离访问远程服务 第三方库 防止出现级联失败(雪崩)\n\n雪崩: 一个服务失败 导致整条链路的服务都失败的情况\n\n * 隔离\n   * 线程池隔离 默认值\n   * 信号量隔离\n * 降级\n * 熔断\n * 限流\n\n\n# 服务方降级\n\n当服务发生异常或调用超时,返回默认数据\n\n 1. 在服务方导入坐标\n    \n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 在服务方启动类中加上@EnableCircuitBreaker注解\n    \n    package com.itheima.provider;\n    \n    import org.springframework.boot.SpringApplication;\n    import org.springframework.boot.autoconfigure.SpringBootApplication;\n    import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;\n    import org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n    \n    /**\n     * 启动类\n     */\n    \n    @EnableEurekaClient //该注解 在新版本中可以省略\n    @SpringBootApplication\n    \n    @EnableCircuitBreaker //开启Hystrix功能\n    public class ProviderApp {\n    \n    \n        public static void main(String[] args) {\n            SpringApplication.run(ProviderApp.class,args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n\n 3. 在服务方controller层 中定降级方法(降级方法需要与原方法参数返回值一致),并且在原方法中加上注解@HystrixCommand(fallbackMethod = "降级方法名")\n    \n    /**\n         *降级\n         *1.出现异常\n         * 2.服务调用超时\n         */\n    \n    @GetMapping("/findOne/{id}")\n    @HystrixCommand(fallbackMethod = "findOne_fallback",commandProperties = {\n        //设置Hystrix的超时时间默认为1000毫秒\n        @HystrixProperty(name = "execution.isolation.thread.interruptOnTimeout",value = "3000")\n    })  //指定降级后调用的方法   commandProperties为设置指定规则\n    public Goods findOne(@PathVariable("id") int id){\n    \n        Goods goods = goodsService.findOne(id);\n    \n        goods.setTitle(goods.getTitle() + ":" + port);//将端口号，设置到了 商品标题上\n        return goods;\n    }\n    \n    /**\n         * 定义降级方法\n         * 1. 方法的返回值和参数要与原方法一致\n         */\n    public Goods findOne_fallback(@PathVariable("id") int id){\n        Goods goods = new Goods();\n        goods.setTitle("降级了");\n        return goods;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    \n\n\n# 消费方降级\n\nFeign已经提供了Hystrix 无需导入 Hystrix 坐标\n\n 1. 在消费方配置文件中开启Hystrix\n    \n    feign:\n      hystrix:\n        enabled: true\n    # 开启 feign对hystrix的支持  默认为false\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 定义声明式接口的实现类\n    \n    package com.itheima.consumer.feign;\n    \n    import com.itheima.consumer.domain.Goods;\n    import org.springframework.stereotype.Component;\n    /*\n    降级处理类\n    1.定义类实现feign客户端接口\n    2.使用@Component注解 加入IOC容器中\n    \n     */\n    @Component\n    public class GoodsFeignClientFallback implements GoodsFeignClient{\n        @Override\n        public Goods findGoodsById(int id) {\n            Goods goods = new Goods();\n            goods.setTitle("降级了");\n            return goods;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n 3. 在声明式接口@FeignClient注解 设置fallback属性为降级类的字节码\n    \n    package com.itheima.consumer.feign;\n    \n    \n    import com.itheima.consumer.domain.Goods;\n    import org.springframework.cloud.openfeign.FeignClient;\n    import org.springframework.web.bind.annotation.GetMapping;\n    import org.springframework.web.bind.annotation.PathVariable;\n    \n    \n    \n    @FeignClient(value = "HYSTRIX-PROVIDER",fallback = GoodsFeignClientFallback.class)\n    public interface GoodsFeignClient {\n    \n    \n        @GetMapping("/goods/findOne/{id}")\n        public Goods findGoodsById(@PathVariable("id") int id);\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    \n\n\n# 熔断机制\n\n当降级的情况情况达到预定的阈值(默认为5秒20次),则会打开断路器,拒绝所有请求,直到服务恢复正常为止\n\n\n\n\n\n@HystrixCommand(fallbackMethod = "findOne_fallback",commandProperties = {\n        //设置Hystrix的超时时间默认为1000毫秒\n        @HystrixProperty(name = "execution.isolation.thread.interruptOnTimeout",value = "3000"),\n        @HystrixProperty(name = "circuitBreaker.sleepWindowInMilliseconds",value = "5000"), //监控时间 默认为5000毫秒\n        @HystrixProperty(name = "circuitBreaker.requestVolumeThreshold",value = "20"),  //失败次数 默认20次\n        @HystrixProperty(name = "circuitBreaker.errorThresholdPercentage",value = "50"),  //失败率  默认为百分之50\n})  //指定降级后调用的方法   commandProperties为设置指定规则\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Turbine 熔断监控\n\nHystrix 提供了 Hystrix-dashboard功能 用于实时监控微服务运行状态 Hystrix-dashboard 只能监控一个微服务\n\nNetflix 提供了Turbine 进行聚合监控\n\n 1. 新建模块并导入坐标\n    \n         <dependencies>\n           <dependency>\n               <groupId>org.springframework.cloud</groupId>\n               <artifactId>spring-cloud-starter-netflix-hystrix-dashboard</artifactId>\n           </dependency>\n    <dependency>\n           <groupId>org.springframework.cloud</groupId>\n           <artifactId>spring-cloud-starter-netflix-turbine</artifactId>\n       </dependency>\n       \n       <dependency>\n           <groupId>org.springframework.boot</groupId>\n           <artifactId>spring-boot-starter-actuator</artifactId>\n       </dependency>\n       \n       <dependency>\n           <groupId>org.springframework.cloud</groupId>\n           <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n       </dependency>\n    <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    \n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    \n\n 2. 创建application.yml\n    \n    spring:\n      application.name: hystrix-monitor\n    server:\n      port: 8769\n    turbine:\n      combine-host-port: true\n      # 配置需要监控的服务名称列表\n      app-config: hystrix-provider,hystrix-consumer\n      cluster-name-expression: "\'default\'"\n      aggregator:\n        cluster-config: default\n      #instanceUrlSuffix: /actuator/hystrix.stream\n    eureka:\n      client:\n        serviceUrl:\n          defaultZone: http://localhost:8761/eureka/\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    \n\n 3. 创建启动类\n    \n    package com.itheima;\n    \n    \n    import org.springframework.boot.SpringApplication;\n    import org.springframework.boot.autoconfigure.SpringBootApplication;\n    import org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n    import org.springframework.cloud.netflix.hystrix.dashboard.EnableHystrixDashboard;\n    import org.springframework.cloud.netflix.turbine.EnableTurbine;\n    \n    @SpringBootApplication\n    @EnableEurekaClient\n    \n    @EnableTurbine //开启Turbine 很聚合监控功能\n    @EnableHystrixDashboard //开启Hystrix仪表盘监控功能\n    public class HystrixMonitorApp {\n    \n        public static void main(String[] args) {\n            SpringApplication.run(HystrixMonitorApp.class, args);\n        }\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n 4. 修改被监控的模块 需要分别修改 hystrix-provider 和 hystrix-consumer 模块\n    \n    1. 导入坐标\n       \n       \t\t<dependency>\n                   <groupId>org.springframework.boot</groupId>\n                   <artifactId>spring-boot-starter-actuator</artifactId>\n               </dependency>\n       \n               <dependency>\n                   <groupId>org.springframework.cloud</groupId>\n                   <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>\n               </dependency>\n       \n               <dependency>\n                   <groupId>org.springframework.cloud</groupId>\n                   <artifactId>spring-cloud-starter-netflix-hystrix-dashboard</artifactId>\n               </dependency>\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       \n    \n    2. 配置Bean\n       \n       @Bean\n           public ServletRegistrationBean getServlet() {\n               HystrixMetricsStreamServlet streamServlet = new HystrixMetricsStreamServlet();\n               ServletRegistrationBean registrationBean = new ServletRegistrationBean(streamServlet);\n               registrationBean.setLoadOnStartup(1);\n               registrationBean.addUrlMappings("/actuator/hystrix.stream");\n               registrationBean.setName("HystrixMetricsStreamServlet");\n               return registrationBean;\n           }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       \n    \n    3. 在启动类上添加注解@EnableHystrixDashboard\n       \n       package com.itheima.provider;\n       \n       import com.netflix.hystrix.contrib.metrics.eventstream.HystrixMetricsStreamServlet;\n       import org.springframework.boot.SpringApplication;\n       import org.springframework.boot.autoconfigure.SpringBootApplication;\n       import org.springframework.boot.web.servlet.ServletRegistrationBean;\n       import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;\n       import org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n       import org.springframework.cloud.netflix.hystrix.dashboard.EnableHystrixDashboard;\n       import org.springframework.context.annotation.Bean;\n       \n       /**\n        * 启动类\n        */\n       \n       @EnableEurekaClient //该注解 在新版本中可以省略\n       @SpringBootApplication\n       \n       @EnableCircuitBreaker //开启Hystrix功能\n       @EnableHystrixDashboard \n       public class ProviderApp {\n       \n       \n           public static void main(String[] args) {\n               SpringApplication.run(ProviderApp.class, args);\n           }\n       \n           @Bean\n           public ServletRegistrationBean getServlet() {\n               HystrixMetricsStreamServlet streamServlet = new HystrixMetricsStreamServlet();\n               ServletRegistrationBean registrationBean = new ServletRegistrationBean(streamServlet);\n               registrationBean.setLoadOnStartup(1);\n               registrationBean.addUrlMappings("/actuator/hystrix.stream");\n               registrationBean.setName("HystrixMetricsStreamServlet");\n               return registrationBean;\n           }\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       15\n       16\n       17\n       18\n       19\n       20\n       21\n       22\n       23\n       24\n       25\n       26\n       27\n       28\n       29\n       30\n       31\n       32\n       33\n       34\n       35\n       36\n       37\n       \n\n 5. 启动服务\n    \n    * eureka-server\n    * hystrix-provider\n    * hystrix-consumer\n    * hystrix-monitor\n\n 6. 访问 http://localhost:8769/hystrix/\n    \n    监控Url地址为 http://localhost:8769/turbine.stream\n    \n    记得访问一次 接口否则没有数据\n\n\n\n\n# Gateway 网关\n\n网关是为微服务架构提供一种简单而有效的统一的API路由管理方式\n\n\n# 入门案例\n\n 1. 创建gateway模块 导入坐标\n    \n        <dependencies>\n            \x3c!--引入gateway 网关--\x3e\n    \n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-starter-gateway</artifactId>\n            </dependency>\n    \n            \x3c!-- eureka-client --\x3e\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n            </dependency>\n        </dependencies>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    \n\n 2. 创建启动类\n    \n    package com.itheima.gateway;\n    \n    import org.springframework.boot.SpringApplication;\n    import org.springframework.boot.autoconfigure.SpringBootApplication;\n    import org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n    \n    @SpringBootApplication\n    @EnableEurekaClient\n    public class ApiGatewayApp {\n        public static void main(String[] args) {\n            SpringApplication.run(ApiGatewayApp.class, args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 3. 编写application.yml\n    \n    server:\n      port: 80\n    spring:\n      application:\n        name: api-geteway-server\n      cloud:\n        # 网关配置\n        gateway:\n          # 路由配置: 转发规则\n          routes: # 集合\n          - id: gateway-provider # 唯一标识 默认为随机UUID\n            uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n            predicates: # 条件 用于请求网关路径的匹配规则\n            - Path=/goods/**\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    \n\n 4. 启动模块和服务治理 服务提供方 消费方 只要访问http://localhost:80/goods/* 地址则转发为\n\nhttp://localhost:8001/goods/*\n\n\n# 静态路由\n\n通过routes转发规则 配置多个集合对象\n\nserver:\n  port: 80\nspring:\n  application:\n    name: api-geteway-server\n  cloud:\n    # 网关配置\n    gateway:\n      # 路由配置: 转发规则\n      routes: # 集合\n      - id: gateway-provider # 唯一标识 默认为随机UUID\n        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - Path=/goods/**\n\n      - id: gateway-consumer # 转发规则2\n        uri: http://localhost:9000/  #  转发路径  服务提供方访问路径\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - Path=/order/**\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n但不推荐此方法配置\n\n\n# 动态路由\n\n从eureka 动态获取\n\n 1. 引入eureka client 并在启动类中加上@EnableEurekaClient注解\n\n 2. 配置eureka服务地址 并将Gateway的uri 改成 lb://应用名称\n    \n    server:\n      port: 80\n    spring:\n      application:\n        name: api-geteway-server\n      cloud:\n        # 网关配置\n        gateway:\n          # 路由配置: 转发规则\n          routes: # 集合\n          - id: gateway-provider # 唯一标识 默认为随机UUID\n    #        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n            uri: lb://gateway-provider\n            predicates: # 条件 用于请求网关路径的匹配规则\n            - Path=/goods/**\n    \n          - id: gateway-consumer # 转发规则2\n    #          uri: http://localhost:9000/  #  转发路径  服务提供方访问路径\n            uri: lb://gateway-consumer\n            predicates: # 条件 用于请求网关路径的匹配规则\n            - Path=/order/**\n    \n    eureka:\n      client:\n        service-url:\n          defaultZone: http://localhost:8761/eureka\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    \n\n\n# 微服务名称配置\n\nGateway网关中 默认不启用由Eureka应用名拼接的URL转发访问\n\n我们通过 sping.cloud.discovery.locator.enabled 开启之后\n\n可以通过 http://localhost/gateway-consumer/order/goods/2 来转发访问 方便我们更好的区分应用 当然http://localhost/order/goods/2 不加应用名称拼接也是可以转发访问\n\nserver:\n  port: 80\nspring:\n  application:\n    name: api-geteway-server\n  cloud:\n    # 网关配置\n    gateway:\n      # 路由配置: 转发规则\n      routes: # 集合\n      - id: gateway-provider # 唯一标识 默认为随机UUID\n#        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n        uri: lb://gateway-provider\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - Path=/goods/**\n\n      - id: gateway-consumer # 转发规则2\n#          uri: http://localhost:9000/  #  转发路径  服务提供方访问路径\n        uri: lb://gateway-consumer\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - Path=/order/**\n      discovery:\n        locator:\n          enabled: true  # 开启微服务发现功能\n          lower-case-service-id: true # 请求路径上的应用名称允许小写\n\neureka:\n  client:\n    service-url:\n      defaultZone: http://localhost:8761/eureka\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 网关过滤器\n\n过滤器 对请求或响应进行拦截 完成一些通用操作\n\nGateway 提供 两种过滤器方式 pre 和 post\n\n * pre 过滤器 在转发之前执行 可以做参数校验 权限校验 流量监控 日志输出 协议转换等\n * post过滤器 在响应之前执行 可以用对响应内容 响应头的修改 日志的输出 流量监控等\n\n同时还提供了两种类型过滤器\n\n * GatewayFilter 局部过滤器 针对单个路由\n * GlobalFilter 全局过滤器 针对所有路由\n\n# 局部过滤器\n\n过滤器工厂                         作用                                                             参数\nAddRequestHeader              为原始请求添加Header                                                  Header的名称及值\nAddRequestParameter           为原始请求添加请求参数                                                    参数名称及值\nAddResponseHeader             为原始响应添加Header                                                  Header的名称及值\nDedupeResponseHeader          剔除响应头中重复的值                                                     需要去重的Header名称及去重策略\nHystrix                       为路由引入Hystrix的断路器保护                                             HystrixCommand的名称\nFallbackHeaders               为fallbackUri的请求头中添加具体的异常信息                                     Header的名称\nPrefixPath                    为原始请求路径添加前缀                                                    前缀路径\nPreserveHostHeader            为请求添加一个preserveHostHeader=true的属性，路由过滤器会检查该属性以决定是否要发送原始的Host   无\nRequestRateLimiter            用于对请求限流，限流算法为令牌桶                                               keyResolver、rateLimiter、statusCode、denyEmptyKey、emptyKeyStatus\nRedirectTo                    将原始请求重定向到指定的URL                                                http状态码及重定向的url\nRemoveHopByHopHeadersFilter   为原始请求删除IETF组织规定的一系列Header                                      默认就会启用，可以通过配置指定仅删除哪些Header\nRemoveRequestHeader           为原始请求删除某个Header                                                Header名称\nRemoveResponseHeader          为原始响应删除某个Header                                                Header名称\nRewritePath                   重写原始的请求路径                                                      原始路径正则表达式以及重写后路径的正则表达式\nRewriteResponseHeader         重写原始响应中的某个Header                                               Header名称，值的正则表达式，重写后的值\nSaveSession                   在转发请求之前，强制执行WebSession::save操作                                 无\nsecureHeaders                 为原始响应添加一系列起安全作用的响应头                                            无，支持修改这些安全响应头的值\nSetPath                       修改原始的请求路径                                                      修改后的路径\nSetResponseHeader             修改原始响应中某个Header的值                                              Header名称，修改后的值\nSetStatus                     修改原始响应的状态码                                                     HTTP 状态码，可以是数字，也可以是字符串\nStripPrefix                   用于截断原始请求的路径                                                    使用数字表示要截断的路径的数量\nRetry                         针对不同的响应进行重试                                                    retries、statuses、methods、series\nRequestSize                   设置允许接收最大请求包的大小。如果请求包大小超过设置的值，则返回 413 Payload Too Large         请求包大小，单位为字节，默认值为5M\nModifyRequestBody             在转发请求之前修改原始请求体内容                                               修改后的请求体内容\nModifyResponseBody            修改原始响应体的内容                                                     修改后的响应体内容\nDefault                       为所有路由添加过滤器                                                     过滤器工厂名称及值\n\n在配置文件中通过 sping.cloud.gateway.routes.filters 设置\n\nspring:\n  application:\n    name: api-geteway-server\n  cloud:\n    # 网关配置\n    gateway:\n      # 路由配置: 转发规则\n      routes: # 集合\n        - id: gateway-provider # 唯一标识 默认为随机UUID\n          #        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n          uri: lb://gateway-provider\n          predicates: # 条件 用于请求网关路径的匹配规则\n            - Path=/goods/**\n          filters:\n            - AddRequestHeader=username,zhansan\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 全局过滤器\n\n全局过滤器不需要在配置文件中配置 系统初始化时加载 并作用在每个路由上\n\n\n\n 1. 在gateway模块中 编写自定义过滤器类 并实现 GlobalFilter, Ordered 接口\n    \n    package com.itheima.gateway.filter;\n    \n    import org.springframework.cloud.gateway.filter.GatewayFilterChain;\n    import org.springframework.cloud.gateway.filter.GlobalFilter;\n    import org.springframework.core.Ordered;\n    import org.springframework.stereotype.Component;\n    import org.springframework.web.server.ServerWebExchange;\n    import reactor.core.publisher.Mono;\n    \n    @Component\n    public class MyFilter implements GlobalFilter, Ordered {\n        @Override\n        public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {\n            System.out.println("自定义全局过滤器");\n            return chain.filter(exchange);  //放行\n        }\n    \n        /**\n         * 过滤器排序\n         * @return 数值越小 越先执行\n         */\n        @Override\n        public int getOrder() {\n            return 0;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    \n\n\n# Config 分布式配置中心\n\nConfig 解决了在分布式场景下多环境配置文件的管理和维护\n\n 1. 在github或者gitee创建一个存放config配置的仓库\n\n 2. 创建config-server 模块 并导入坐标\n    \n        \x3c!-- config-server --\x3e\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-config-server</artifactId>\n        </dependency>\n    </dependencies>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 创建启动类并使用 @EnableConfigServer 注解启用 config server\n    \n    package com.itheima.config;\n    \n    import org.springframework.boot.SpringApplication;\n    import org.springframework.boot.autoconfigure.SpringBootApplication;\n    import org.springframework.cloud.config.server.EnableConfigServer;\n    \n    @SpringBootApplication\n    @EnableConfigServer //启用config server功能\n    public class ConfigServerApp {\n        public static void main(String[] args) {\n            SpringApplication.run(ConfigServerApp.class,args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 4. 配置application.yml\n    \n    server:\n      port: 8888\n    \n    \n    spring:\n      application:\n        name: config-server\n    \n      cloud:\n        config:\n          server:\n            git:\n              uri: https://gitee.com/Iekrwh/configs.git  # 仓库地址\n    #          username:   #如果是私有仓库则需要配置git的账号和密码\n    #          password:\n              default-label: master # 分支配置\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n\n 5. 在服务提供方中 导入 config client坐标\n    \n    \x3c!--     config client   --\x3e\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-config</artifactId>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 6. 在创建配置文件 bootstrap.yml 优先级高于application.yml\n    \n    spring:\n      cloud:\n        config:\n          uri: http://localhost:8888  # 配置config server地址\n          name: config # 文件名称\n          label: master # 分支\n          profile: dev # -后面的版本名称\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 7. 此时在服务提供方中 可以读取config 配置文件提供的值\n\n\n# 客户端刷新\n\n当远程仓库中的配置文件发生改变事 我们的config server 会自动更新\n\n但我们的config client并不会自动更新内容\n\n 1. 客户端引入actuator依赖\n    \n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-actuator</artifactId>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 获取配置信息的类上 加上@RefreshScope 注解\n\n 3. 在bootstrap.yml 添加 management.endpoints.web.exposure.include\n    \n    spring:\n      cloud:\n        config:\n          uri: http://localhost:8888  # 配置config server地址\n          name: config # 文件名称\n          label: master # 分支\n          profile: dev # -后面的版本名称\n    management:\n      endpoints:\n        web:\n          exposure:\n            include: \'*\'\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    \n\n 4. 在cmd中 向http://localhost:8001/actuator/refresh 发送post请求\n    \n    curl -X POST http://localhost:8001/actuator/refresh\n    \n    \n    1\n    \n    \n    但是每次更新 需要发送一次请求 后面结合bus才能解决此问题\n\n\n# 集成Eureka\n\n通过上面例子 我们发现 config客户端访问服务端 地址是固定写死 非常不灵活\n\n我们可以通过Eureka 使服务端在上面注册应用 自动获取应用的地址\n\n 1. config-server 导入Eureka坐标\n    \n    \x3c!-- eureka-client --\x3e\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 2. config-server 配置 加上Eureka的地址\n    \n    server:\n      port: 8888\n    \n    \n    spring:\n      application:\n        name: config-server\n    \n      cloud:\n        config:\n          server:\n            git:\n              uri: https://gitee.com/Iekrwh/configs.git  # 仓库地址\n    #          username:   #如果是私有仓库则需要配置git的账号和密码\n    #          password:\n              default-label: master # 分支配置\n    \n    eureka:\n      client:\n        service-url:\n          defaultZone: http://localhost:8761/eureka\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n 3. 在config serve 启动类加上 @EnableEurekaClient 注解\n\n 4. 在config client 配置文件中 将 config uri 改为 Eurka服务地址\n    \n    spring:\n      cloud:\n        config:\n    #      uri: http://localhost:8888  # 配置config server地址\n          name: config # 文件名称\n          label: master # 分支\n          profile: dev # -后面的版本名称\n          discovery:\n            enabled: true # 从注册中心寻找config server 地址\n            service-id: config-server # config server 注册的应用名\n    management:\n      endpoints:\n        web:\n          exposure:\n            include: \'*\'\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n\n# Bus 消息总线\n\nSpring Cloud 是用轻量的消息中间件将分布式的节点连接起来 用于广播配置文件的更改或者服务的监控管理\n\nSpring Cloud Bus 可选的消息中间件包括RabbitMQ和 Kafka\n\n 1. 在config-server 和 config-client中 引入 bus依赖\n    \n    \x3c!-- bus --\x3e\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-bus-amqp</artifactId>\n    </dependency>\n    <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-actuator</artifactId>\n            </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    \n\n 2. 在config-server 和 config-client中 分别配置RabbitMQ\n    \n    spring:\n      rabbitmq:\n        host: 192.168.130.124 # mq服务器地址\n        port: 5672\n        username: iekr\n        password: iekr\n        virtual-host: /itcast\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 3. 在config-server 中 设置暴露监控断电 bus-refresh\n    \n    management:\n      endpoints:\n        web:\n          exposure:\n            include: \'bus-refresh\'  # 暴露bus的刷新端点\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 4. 更新则发送指令 curl -X post config-server地址:端口/actuator/bus-refresh\n    \n    curl -X post http://localhost:8888/actuator/bus-refresh\n    \n    \n    1\n    \n\n\n# Stream 消息驱动\n\nSpring Clou Stream 是一个构建消息驱动微服务应用的框架\n\nStream 对消息中间件的进一步封装 可以做到代码层面对中间件的无感知 甚至动态切换中间件\n\nStream 构建的应用程序与消息中间件之间是通过绑定器 Binder 相关联的\n\n\n# 消息生产者\n\n 1. 创建生产者模块 导入坐标\n    \n            \x3c!--spring boot web--\x3e\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-web</artifactId>\n            </dependency>\n    \n    \n            \x3c!-- stream --\x3e\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-starter-stream-rabbit</artifactId>\n            </dependency>\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 2. 编写配置文件\n    \n    server:\n      port: 8000\n    \n    \n    \n    spring:\n      cloud:\n        stream:\n          # 定义绑定器，绑定到哪个消息中间件上\n          binders:\n            itheima_binder: # 自定义的绑定器名称\n              type: rabbit # 绑定器类型\n              environment: # 指定mq的环境\n                spring:\n                  rabbitmq:\n                    host: 192.168.130.124\n                    port: 5672\n                    username: guest\n                    password: guest\n                    virtual-host: /\n          bindings:\n            output: # channel名称\n              binder: itheima_binder #指定使用哪一个绑定器(自定义的绑定器名称)\n              destination: itheima_exchange # 消息目的地\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    \n\n 3. 定义发送消息类 并在类上开启Binding绑定为配置文件\n    \n    package com.itheima.stream.producer;\n    \n    \n    import org.springframework.beans.factory.annotation.Autowired;\n    import org.springframework.cloud.stream.annotation.EnableBinding;\n    import org.springframework.cloud.stream.messaging.Source;\n    import org.springframework.messaging.MessageChannel;\n    import org.springframework.messaging.support.MessageBuilder;\n    import org.springframework.stereotype.Component;\n    \n    @Component\n    @EnableBinding(Source.class)\n    public class MessageProducer {\n    \n        @Autowired\n        private MessageChannel output;\n    \n        public void send(){\n            String msessage = "hello stream~~~";\n    \n            //发送消息\n            output.send(MessageBuilder.withPayload(msessage).build());\n    \n            System.out.println("消息发送成功~~~");\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    \n\n 4. 创建一个接口用于测试 是否能发送消息给mq\n    \n    package com.itheima.stream.producer;\n    \n    import org.springframework.beans.factory.annotation.Autowired;\n    import org.springframework.web.bind.annotation.RequestMapping;\n    import org.springframework.web.bind.annotation.RestController;\n    \n    @RestController\n    public class ProducerController {\n    \n        @Autowired\n        private MessageProducer producer;\n    \n    \n        @RequestMapping("/send")\n            public String sendMsg(){\n            producer.send();\n            return "success";\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n\n# 消息消费者\n\n 1. 创建消费者模块 导入坐标\n    \n            \x3c!--spring boot web--\x3e\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-web</artifactId>\n            </dependency>\n    \n    \n            \x3c!-- stream --\x3e\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-starter-stream-rabbit</artifactId>\n            </dependency>\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 2. 编写配置文件 主要把端口和 bindings设置为输出还是输入模式\n    \n    server:\n      port: 9000\n    \n    \n    \n    spring:\n      cloud:\n        stream:\n          # 定义绑定器，绑定到哪个消息中间件上\n          binders:\n            itheima_binder: # 自定义的绑定器名称\n              type: rabbit # 绑定器类型\n              environment: # 指定mq的环境\n                spring:\n                  rabbitmq:\n                    host: localhost\n                    port: 5672\n                    username: guest\n                    password: guest\n                    virtual-host: /\n          bindings:\n            input: # channel名称\n              binder: itheima_binder #指定使用哪一个binder\n              destination: itheima_exchange # 消息目的地\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    \n\n 3. 创建消息接收类 在类开启binding注解 并在接收方法中定义 @StreamListener(Sink.INPUT)\n    \n    package com.itheima.stream.consumer;\n    \n    import org.springframework.cloud.stream.annotation.EnableBinding;\n    import org.springframework.cloud.stream.annotation.StreamListener;\n    import org.springframework.cloud.stream.messaging.Sink;\n    import org.springframework.messaging.Message;\n    import org.springframework.stereotype.Component;\n    \n    /**\n     * 消息接收类\n     */\n    @EnableBinding({Sink.class})\n    @Component\n    public class MessageListener {\n    \n        @StreamListener(Sink.INPUT)\n        public void receive(Message message){\n    \n            System.out.println(message);\n            System.out.println(message.getPayload());\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n\n\n# Sleuth+Zipkin 链路追踪\n\nSpring Cloud Sleuth 它在分布式中能跟踪一个用户请求的过程 捕获这些跟踪数据 就能构建微服务的整个调用链的视图 它是调试和监控微服务的关键工具\n\n 1. 安装启动zipkin 访问http://localhost:9411/\n    \n    java -jar zipkin.jar\n    \n    \n    1\n    \n\n 2. 在服务提供方和消费方法 引入 zipkin 坐标\n    \n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-zipkin</artifactId>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 3. 配置文件\n    \n    server:\n      port: 8001\n    \n    eureka:\n      client:\n        service-url:\n          defaultZone: http://localhost:8761/eureka\n    spring:\n      application:\n        name: feign-provider\n      zipkin:\n        base-url: http://localhost:9411/  # 设置zipkin的服务端路径\n    \n      sleuth:\n        sampler:\n          probability: 1 # 采集率 默认 0.1 百分之十。\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    ',normalizedContent:'# spring cloud\n\nspring cloud是一系列框架的有序集合\n\n通过 spring boot 风格进行再封装屏蔽掉了复杂的配置和实现原理 最终给开发者留出了一套简单易懂 易部署 易维护的分布式系统开发工具包\n\n\n# 微服务\n\n\n\n微服务它将一个原本独立的系统 拆分成为多个小型服务 并且这些小型服务都可以独立运行 服务之间可以进行通信协作\n\n\n# spring cloud 与 dubbo 对比\n\n\n\n * dubbo只是实现了服务治理 而spring cloud子项目分别覆盖了微服务架构下的众多部件\n * dubbo使用 rpc 通信协议 spring cloud 使用 restful 完成通信 dubbo 效率略高于 spring cloud\n\n\n# 服务治理\n\n\n# eureka\n\neureka 是 netflix 公司开源的一个服务注册与发现的组件\n\neureka 和 其他netflix 公司的服务组件(如负载均衡 熔断器 网关等) 一起 被spring cloud 社区整合为 spring-cloud-netflix 模块\n\neureka 包含 eueka server(注册中心) 和 eureka client(服务提供者 服务器消费者)\n\n# server\n\n引入eureka-server 和 spring web\n\n<dependencies>\n\n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-web</artifactid>\n    </dependency>\n\n    \x3c!-- eureka-server --\x3e\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-netflix-eureka-server</artifactid>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\napplication\n\nserver:\n  port: 8761\n\n# eureka 配置\n#1.dashboard  :eureka的web控制台配置\n#2.server  :eureka的服务端配置\n#3.client  :eureka的客户端配置\n#4.instance  :eureka的实例配置\neureka:\n  instance:\n    hostname: localhost # 主机名\n\n  dashboard:\n    enabled: true  # 是否启用web控制台  默认为开启\n    path: /  # 默认路径为/  访问http://localhost:8761/即可\n\n  client:\n    service-url:\n      defaultzone: http://${eureka.instance.hostname}:${server.port}/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\n    register-with-eureka: false # 是否将自己的路径 注册到eureka上  默认为true  一般情况下只有provider client需要\n    fetch-registry: false  # 是否需要从eureka中抓取路径   默认为true  一般只有consumer client需要\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n启动类\n\npackage com.itheima.eureka;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.cloud.netflix.eureka.server.enableeurekaserver;\n\n@springbootapplication\n//启用eurekaserver\n@enableeurekaserver\npublic class eurekaapp {\n    public static void main(string[] args) {\n        springapplication.run(eurekaapp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# provider\n\n引入 eureka-client 和 spring web\n\n<dependencies>\n\n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-web</artifactid>\n    </dependency>\n\n\n    \x3c!-- eureka-client --\x3e\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n    </dependency>\n\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\napplication\n\nserver:\n  port: 8000\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n  client:\n    service-url:\n      defaultzone: http://localhost:8761/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\nspring:\n  application:\n    name: eureka-provider # 设置当前应用的名称 将来在eureka的web控制台application显示为该名称  将来需要该名称来获取路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n启动类\n\npackage com.itheima.provider;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.cloud.netflix.eureka.enableeurekaclient;\n\n@springbootapplication\n//标记为 eureka 客户端 在新版本中自动标记\n@enableeurekaclient\npublic class providerapp {\n    public static void main(string[] args) {\n        springapplication.run(providerapp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n并编写对应的dao层 domian对象 service层 controller层\n\n并且能够远程调用获取数据 此时为了在eureka上注册为应用使其他应用能够远程调用该服务\n\n# consumer\n\n引入 eureka-client 和 spring web\n\n<dependencies>\n\n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-web</artifactid>\n    </dependency>\n\n\n    \x3c!-- eureka-client --\x3e\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n    </dependency>\n\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\napplication\n\nserver:\n  port: 9000\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n  client:\n    service-url:\n      defaultzone: http://localhost:8761/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\nspring:\n  application:\n    name: eureka-consumer # 设置当前应用的名称 将来在eureka的web控制台application显示为该名称  将来需要该名称来获取路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n启动类\n\npackage com.itheima.consumer;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.cloud.client.discovery.enablediscoveryclient;\nimport org.springframework.cloud.netflix.eureka.enableeurekaclient;\n\n@springbootapplication\n@enableeurekaclient\n@enablediscoveryclient //激活discoveryclient 新版本可以忽略 自动激活\npublic class consumerapp {\n    public static void main(string[] args) {\n        springapplication.run(consumerapp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nresttemplateconfig类\n\npackage com.itheima.consumer.cnfig;\n\nimport org.springframework.context.annotation.bean;\nimport org.springframework.context.annotation.configuration;\nimport org.springframework.web.client.resttemplate;\n\n@configuration\npublic class resttemplateconfig {\n\n    @bean\n    public resttemplate resttemplate(){\n        return new resttemplate();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\ncontroller层\n\npackage com.itheima.consumer.controller;\n\n\nimport com.itheima.consumer.domain.goods;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.cloud.client.serviceinstance;\nimport org.springframework.cloud.client.discovery.discoveryclient;\nimport org.springframework.web.bind.annotation.getmapping;\nimport org.springframework.web.bind.annotation.pathvariable;\nimport org.springframework.web.bind.annotation.requestmapping;\nimport org.springframework.web.bind.annotation.restcontroller;\nimport org.springframework.web.client.resttemplate;\n\nimport java.util.list;\n\n@restcontroller\n@requestmapping("/order")\npublic class ordercontroller {\n\n    @autowired\n    private resttemplate resttemplate;\n\n    @autowired\n    private discoveryclient discoveryclient;\n\n    @getmapping("/goods/{id}")\n    public goods findgoodsbyid(@pathvariable("id") int id) {\n\n /*\n        远程调用goods服务中的findone接口\n        使用resttemplate\n        1.定义bean\n        2.注入bean\n        3.调用方法\n         */\n        /*\n        动态从 eureka server 中获取provide 的 ip 和端口\n        1. 注入 discoveryclient 并在启动类注解激活\n        2. 调用方法\n         */\n        list<serviceinstance> instances = discoveryclient.getinstances("eureka-provider");  //根据应用名在eureka服务端中获取应用集合\n        if (instances == null || instances.size() == 0) { //判断是否为空\n            return null;\n        }\n        serviceinstance instance = instances.get(0);  //此处只有一个所以使用第一个\n        string host = instance.gethost();  //获取ip\n        int port = instance.getport(); //获取端口\n        system.out.println(host);\n        system.out.println(port);\n\n        string url = "http://" + host + ":" + port + "goods/findone/" + id;\n        goods goods = resttemplate.getforobject(url, goods.class);\n        return goods;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n# 配置属性\n\n# instance\n\n\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n    prefer-ip-address: true  # 以ip地址形式注册到eureka server中 默认为false 注册为主机名\n    instance-id: ${eureka.instance.prefer-ip-address}:${spring.application.name}:${server.port}  #设置在web控制台中应用id的名称\n\n\n1\n2\n3\n4\n5\n\n\n# server\n\n\n\n# 高可用\n\n每个server都是镜像相同的注册应用 如果其中一个宕机了 另外一个server仍然存活\n\n设置多个eureka server 相互注册\n\nserver:\n  port: 8762\n\neureka:\n  instance:\n    hostname: eureka-server2 # 主机名\n\n  dashboard:\n    enabled: true  # 是否启用web控制台  默认为开启\n    path: /  # 默认路径为/  访问http://localhost:8761/即可\n\n  client:\n    service-url:\n      defaultzone: http://127.0.0.1:8761/eureka  # 两个eureka相互注册\n\n    register-with-eureka: true # 是否将自己的路径 注册到eureka上  默认为true  一般情况下只有provider client需要\n    fetch-registry: true  # 是否需要从eureka中抓取路径   默认为true  一般只有consumer client需要\nspring:\n  application:\n    name: eureka-server-ha\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n在client中在多个server注册应用\n\nserver:\n  port: 8000\n\neureka:\n  instance:\n    hostname: localhost # 主机名\n  client:\n    service-url:\n      defaultzone: http://localhost:8761/eureka,http://localhost:8762/eureka  # eureka服务端地址 客户端用于访问服务端的地址\n\nspring:\n  application:\n    name: eureka-provider # 设置当前应用的名称 将来在eureka的web控制台application显示为该名称  将来需要该名称来获取路径\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# consul\n\nconsul 是 由 hashicorp 基于 go 语言开发 支持多数据中心 分布式高可用的服务发布和注册服务软件\n\n这里我们使用windows中的exe执行程序\n\n启动\n\n.\\consul agent -dev\n\n\n1\n\n\n打开web管理界面\n\nhttp://localhost:8500/\n\n 1. 搭建provider 和 consumer 服务\n 2. 并配置好 resttemplate 完成远程调用\n\nconsul客户端坐标 必须导入actuator组件\n\n        \x3c!-- consul-client --\x3e\n        <dependency>\n            <groupid>org.springframework.cloud</groupid>\n            <artifactid>spring-cloud-starter-consul-discovery</artifactid>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-actuator</artifactid>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n不需要在启动类中标价\n\n只需要在application配置好consul属性\n\nprovider配置\n\nserver:\n  port: 8000\nspring:\n  cloud:\n    consul:\n      host: localhost  # consul服务端 ip\n      port: 8500  # 端口\n      discovery:\n        service-name: ${spring.application.name}  # 应用名称\n        prefer-ip-address: true # 以ip注册\n  application:\n    name: consul-provider  # 应用名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nconsumer配置\n\nserver:\n  port: 9000\nspring:\n  cloud:\n    consul:\n      host: localhost  # consul服务端 ip\n      port: 8500  # 端口\n      discovery:\n        service-name: ${spring.application.name}  # 应用名称\n        prefer-ip-address: true # 以ip注册\n  application:\n    name: consul-consumer  # 应用名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nconsumer的controller层还是一致\n\npackage com.itheima.consumer.controller;\n\n\nimport com.itheima.consumer.domain.goods;\nimport org.springframework.beans.factory.annotation.autowired;\nimport org.springframework.cloud.client.serviceinstance;\nimport org.springframework.cloud.client.discovery.discoveryclient;\nimport org.springframework.web.bind.annotation.getmapping;\nimport org.springframework.web.bind.annotation.pathvariable;\nimport org.springframework.web.bind.annotation.requestmapping;\nimport org.springframework.web.bind.annotation.restcontroller;\nimport org.springframework.web.client.resttemplate;\n\nimport java.util.list;\n\n@restcontroller\n@requestmapping("/order")\npublic class ordercontroller {\n\n    @autowired\n    private resttemplate resttemplate;\n\n    @autowired\n    private discoveryclient discoveryclient;\n\n    @getmapping("/goods/{id}")\n    public goods findgoodsbyid(@pathvariable("id") int id) {\n\n /*\n        远程调用goods服务中的findone接口\n        使用resttemplate\n        1.定义bean\n        2.注入bean\n        3.调用方法\n         */\n        /*\n        动态从 eureka server 中获取provide 的 ip 和端口\n        1. 注入 discoveryclient 并在启动类注解激活\n        2. 调用方法\n         */\n        list<serviceinstance> instances = discoveryclient.getinstances("consul-provider");  //根据应用名在eureka服务端中获取应用集合\n        if (instances == null || instances.size() == 0) { //判断是否为空\n            return null;\n        }\n        serviceinstance instance = instances.get(0);  //此处只有一个所以使用第一个\n        string host = instance.gethost();  //获取ip\n        int port = instance.getport(); //获取端口\n        system.out.println(host);\n        system.out.println(port);\n\n        string url = "http://" + host + ":" + port + "goods/findone/" + id;\n        goods goods = resttemplate.getforobject(url, goods.class);\n        return goods;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\nconsumer的controller层还是一致\n\n\n# nacos\n\nnacos 是阿里巴巴的开源项目 专注于服务发现和配置管理微服务\n\nnacos = spring cloud 注册中心 + spring cloud配置中心\n\nhttps://nacos.io/zh-cn/\n\n# 启动\n\n在bin目录下 以单机形式运行\n\n启动命令(standalone代表着单机模式运行，非集群模式):\n\nstartup.cmd -m standalone\n\n\n1\n\n\nweb管理页面\n\nhttp://localhost:8848/nacos/\n\n默认用户密码 为nacos\n\n# 入门案例\n\nnacos坐标\n\n        <dependency>\n            <groupid>org.springframework.cloud</groupid>\n            <artifactid>spring-cloud-starter-alibaba-nacos-discovery</artifactid>\n            <version>0.2.2.release</version>\n        </dependency>\n        <dependency>\n            <groupid>com.alibaba.nacos</groupid>\n            <artifactid>nacos-client</artifactid>\n            <version>2.0.3</version>\n        </dependency>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nprovider配置\n\nserver:\n  port: 8000\nspring:\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848  # 配置nacos服务端地址和端口\n\n  application:\n    name: nacos-provider # 服务名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nconsumer配置\n\nserver:\n  port: 9000\nspring:\n  cloud:\n    nacos:\n      discovery:\n        server-addr: 127.0.0.1:8848  # 配置nacos服务端地址和端口\n\n  application:\n    name: nacos-consumer # 服务名称\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nconsumer启动类\n\npackage com.itheima.consumer;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\nimport org.springframework.cloud.client.discovery.enablediscoveryclient;\n\n\n@springbootapplication\n@enablediscoveryclient\npublic class consumerapp {\n    public static void main(string[] args) {\n        springapplication.run(consumerapp.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# ribbon 负载均衡\n\n\n\n\n# 简化restemplate调用\n\n之前我们eureka通过restemplate获取一个应用集合, 获取一个应用对象 的 地址 和端口,并拼接成一个url调用服务方的应用 使用ribbon只需要将ip:端口换成应用名称即可以远程调用 应用\n\n 1. 在restemplate的bean绑定中加上 @loadbalanced 注解\n    \n    package com.itheima.consumer.cnfig;\n    \n    import org.springframework.cloud.client.loadbalancer.loadbalanced;\n    import org.springframework.context.annotation.bean;\n    import org.springframework.context.annotation.configuration;\n    import org.springframework.web.client.resttemplate;\n    \n    @configuration\n    public class resttemplateconfig {\n    \n        @loadbalanced\n        @bean\n        public resttemplate resttemplate(){\n            return new resttemplate();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n\n 2. 将url中的ip:端口 换成应用名称\n    \n    /*\n    使用 ribbon 简化调用\n    1.在声明resttemplate的bean时 添加一个@loadbalanced 注解\n    2.在使用在声明resttemplate 发请求时 需要定义url时 host:port可以换成 服务端提供的应用名称\n     */\n    @getmapping("/goods2/{id}")\n    public goods findgoodsbyid2(@pathvariable("id") int id) {\n    \n        string url = "http://eureka-provider/goods/findone/" + id;\n        goods goods = resttemplate.getforobject(url, goods.class);\n        return goods;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    \n\n\n# 负载均衡\n\n通过在restemplate的bean绑定中加上 @loadbalanced 注解后,如果当前用于有个多个服务提供方,则ribbon会自动帮我们负载均衡,默认为轮询策略,策略配置在消费者一端 即客户端负载均衡\n\n\n\n# 配置类形式配置负载策略\n\n 1. 定义配置类 使用bean绑定并返回一个irule接口的实现类\n    \n    package com.itheima.consumer.cnfig;\n    \n    import com.netflix.loadbalancer.irule;\n    import com.netflix.loadbalancer.randomrule;\n    import org.springframework.context.annotation.bean;\n    import org.springframework.context.annotation.configuration;\n    \n    @configuration\n    public class myrule {\n    \n        @bean\n        public irule rule(){\n            return new randomrule();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n 2. 在启动类中 使用@ribbonclient 定义应用名称和配置类\n    \n    package com.itheima.consumer;\n    \n    import com.itheima.consumer.cnfig.myrule;\n    import org.springframework.boot.springapplication;\n    import org.springframework.boot.autoconfigure.springbootapplication;\n    import org.springframework.cloud.client.discovery.enablediscoveryclient;\n    import org.springframework.cloud.netflix.eureka.enableeurekaclient;\n    import org.springframework.cloud.netflix.ribbon.ribbonclient;\n    \n    @springbootapplication\n    @enableeurekaclient\n    @enablediscoveryclient //激活discoveryclient 新版本可以忽略 自动激活\n    /*\n        配置ribbon 负载均衡策略\n        name :设置服务提供方的 应用名称\n        configuration : 设置负载均衡的bean\n    \n     */\n    @ribbonclient(name ="eureka-provider",configuration = myrule.class)\n    public class consumerapp {\n        public static void main(string[] args) {\n            springapplication.run(consumerapp.class, args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    \n\n# 配置方式设置\n\n在application中设置该属性\n\neureka-provider:  # 应用名称\n  ribbon:\n    nfloadbalancerruleclassname: com.netflix.loadbalancer.randomrule  # 负载均衡策略实现类的引用路径\n\n\n1\n2\n3\n\n\n\n# feign 声明式服务调用\n\nfeign 是一个声明式的rest客户端 它基于接口的注解方式 很方便实现客户端配置\n\n 1. 在消费者端导入坐标\n    \n    <dependency>\n         <groupid>org.springframework.cloud</groupid>\n         <artifactid>spring-cloud-starter-openfeign</artifactid>\n     </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 消费者启动类中加上 @enablefeignclients 启用feign\n    \n    package com.itheima.consumer;\n    \n    import org.springframework.boot.springapplication;\n    import org.springframework.boot.autoconfigure.springbootapplication;\n    import org.springframework.cloud.client.discovery.enablediscoveryclient;\n    import org.springframework.cloud.netflix.eureka.enableeurekaclient;\n    import org.springframework.cloud.openfeign.enablefeignclients;\n    \n    @enablediscoveryclient // 激活discoveryclient\n    @enableeurekaclient\n    @springbootapplication\n    \n    @enablefeignclients //开启feign的功能\n    public class consumerapp {\n    \n    \n        public static void main(string[] args) {\n            springapplication.run(consumerapp.class,args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 3. 在消费者中定义一个feign声明接口\n    \n    package com.itheima.consumer.feign;\n    \n    import com.itheima.consumer.domain.goods;\n    import org.springframework.cloud.openfeign.feignclient;\n    import org.springframework.web.bind.annotation.getmapping;\n    import org.springframework.web.bind.annotation.pathvariable;\n    \n    /**\n     * feign声明式接口  发起远程调用的\n     * 1.定义接口\n     * 2.接口上添加 @feignclient value属性为 应用名称\n     * 3.编写调用接口 接口的声明规则 和 提供方接口保持一致\n     * 4.注入该接口对象 调用接口方法完成远程调用\n     */\n    \n    @feignclient(value = "feign-provider")\n    public interface goodsfeignclient {\n        @getmapping("/goods/findone/{id}")\n        goods findgoodsbyid(@pathvariable("id") int id);\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 4. 注入 声明接口对象 调用接口方法\n    \n    package com.itheima.consumer.controller;\n    \n    \n    import com.itheima.consumer.domain.goods;\n    import com.itheima.consumer.feign.goodsfeignclient;\n    import org.springframework.beans.factory.annotation.autowired;\n    import org.springframework.web.bind.annotation.getmapping;\n    import org.springframework.web.bind.annotation.pathvariable;\n    import org.springframework.web.bind.annotation.requestmapping;\n    import org.springframework.web.bind.annotation.restcontroller;\n    import org.springframework.web.client.resttemplate;\n    \n    @restcontroller\n    @requestmapping("/order")\n    public class ordercontroller {\n    \n        @autowired\n        private resttemplate resttemplate;\n    \n    \n        @autowired\n        private goodsfeignclient goodsfeignclient;\n    \n    \n        @getmapping("/goods/{id}")\n        public goods findgoodsbyid(@pathvariable("id") int id){\n    \n            /*string url = "http://feign-provider/goods/findone/"+id;\n            // 3. 调用方法\n            goods goods = resttemplate.getforobject(url, goods.class);*/\n    \n            goods goods = goodsfeignclient.findgoodsbyid(id);\n    \n    \n            return goods;\n        }\n    \n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    \n\n\n# 超时设置\n\nfeign 底层依赖于 ribbon 实现负载均衡和远程调用\n\nribbon默认1秒超时\n\n在consumer的配置文件中配置\n\nribbon:\n  connecttimeout: 1000 # 设置连接超时时间 默认为1000毫秒\n  readtimeout: 3000 # 逻辑处理的超时时间 默认为1000毫秒\n\n\n1\n2\n3\n\n\n\n# 日志记录\n\nfeign 只能记录 debug 级别的日志信息\n\n 1. 设置consumer的日志级别 为该路径下所有的类为debug级别\n    \n    logging:\n      level:\n        com.itheima: debug # 键位类路径\n    \n    \n    1\n    2\n    3\n    \n\n 2. 在consumer中定义配置类\n    \n    package com.itheima.consumer.config;\n    \n    import feign.logger;\n    import org.springframework.context.annotation.bean;\n    import org.springframework.context.annotation.configuration;\n    \n    @configuration\n    public class feignlogconfig {\n    \n        @bean\n        public logger.level level(){\n            /**\n             * none 不记录\n             * basic 记录基本的请求行 响应状态码数据\n             * headers 记录基本的请求行 响应状态码数据  响应头信息\n             * full 记录完整的信息\n             */\n            return logger.level.full;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 3. 在声明式接口中的@feignclient注解 引入配置类\n    \n    package com.itheima.consumer.feign;\n    \n    import com.itheima.consumer.config.feignlogconfig;\n    import com.itheima.consumer.domain.goods;\n    import org.springframework.cloud.openfeign.feignclient;\n    import org.springframework.web.bind.annotation.getmapping;\n    import org.springframework.web.bind.annotation.pathvariable;\n    \n    /**\n     * feign声明式接口  发起远程调用的\n     * 1.定义接口\n     * 2.接口上添加 @feignclient value属性为 应用名称   configuration 为日志配置类\n     * 3.编写调用接口 接口的声明规则 和 提供方接口保持一致\n     * 4.注入该接口对象 调用接口方法完成远程调用\n     */\n    \n    @feignclient(value = "feign-provider",configuration = feignlogconfig.class)\n    public interface goodsfeignclient {\n        @getmapping("/goods/findone/{id}")\n        goods findgoodsbyid(@pathvariable("id") int id);\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n\n# hystrix 熔断器\n\nhystrix 是netflix 开源的一个延迟和容错库 用于隔离访问远程服务 第三方库 防止出现级联失败(雪崩)\n\n雪崩: 一个服务失败 导致整条链路的服务都失败的情况\n\n * 隔离\n   * 线程池隔离 默认值\n   * 信号量隔离\n * 降级\n * 熔断\n * 限流\n\n\n# 服务方降级\n\n当服务发生异常或调用超时,返回默认数据\n\n 1. 在服务方导入坐标\n    \n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-netflix-hystrix</artifactid>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 在服务方启动类中加上@enablecircuitbreaker注解\n    \n    package com.itheima.provider;\n    \n    import org.springframework.boot.springapplication;\n    import org.springframework.boot.autoconfigure.springbootapplication;\n    import org.springframework.cloud.client.circuitbreaker.enablecircuitbreaker;\n    import org.springframework.cloud.netflix.eureka.enableeurekaclient;\n    \n    /**\n     * 启动类\n     */\n    \n    @enableeurekaclient //该注解 在新版本中可以省略\n    @springbootapplication\n    \n    @enablecircuitbreaker //开启hystrix功能\n    public class providerapp {\n    \n    \n        public static void main(string[] args) {\n            springapplication.run(providerapp.class,args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n\n 3. 在服务方controller层 中定降级方法(降级方法需要与原方法参数返回值一致),并且在原方法中加上注解@hystrixcommand(fallbackmethod = "降级方法名")\n    \n    /**\n         *降级\n         *1.出现异常\n         * 2.服务调用超时\n         */\n    \n    @getmapping("/findone/{id}")\n    @hystrixcommand(fallbackmethod = "findone_fallback",commandproperties = {\n        //设置hystrix的超时时间默认为1000毫秒\n        @hystrixproperty(name = "execution.isolation.thread.interruptontimeout",value = "3000")\n    })  //指定降级后调用的方法   commandproperties为设置指定规则\n    public goods findone(@pathvariable("id") int id){\n    \n        goods goods = goodsservice.findone(id);\n    \n        goods.settitle(goods.gettitle() + ":" + port);//将端口号，设置到了 商品标题上\n        return goods;\n    }\n    \n    /**\n         * 定义降级方法\n         * 1. 方法的返回值和参数要与原方法一致\n         */\n    public goods findone_fallback(@pathvariable("id") int id){\n        goods goods = new goods();\n        goods.settitle("降级了");\n        return goods;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    \n\n\n# 消费方降级\n\nfeign已经提供了hystrix 无需导入 hystrix 坐标\n\n 1. 在消费方配置文件中开启hystrix\n    \n    feign:\n      hystrix:\n        enabled: true\n    # 开启 feign对hystrix的支持  默认为false\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 定义声明式接口的实现类\n    \n    package com.itheima.consumer.feign;\n    \n    import com.itheima.consumer.domain.goods;\n    import org.springframework.stereotype.component;\n    /*\n    降级处理类\n    1.定义类实现feign客户端接口\n    2.使用@component注解 加入ioc容器中\n    \n     */\n    @component\n    public class goodsfeignclientfallback implements goodsfeignclient{\n        @override\n        public goods findgoodsbyid(int id) {\n            goods goods = new goods();\n            goods.settitle("降级了");\n            return goods;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n 3. 在声明式接口@feignclient注解 设置fallback属性为降级类的字节码\n    \n    package com.itheima.consumer.feign;\n    \n    \n    import com.itheima.consumer.domain.goods;\n    import org.springframework.cloud.openfeign.feignclient;\n    import org.springframework.web.bind.annotation.getmapping;\n    import org.springframework.web.bind.annotation.pathvariable;\n    \n    \n    \n    @feignclient(value = "hystrix-provider",fallback = goodsfeignclientfallback.class)\n    public interface goodsfeignclient {\n    \n    \n        @getmapping("/goods/findone/{id}")\n        public goods findgoodsbyid(@pathvariable("id") int id);\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    \n\n\n# 熔断机制\n\n当降级的情况情况达到预定的阈值(默认为5秒20次),则会打开断路器,拒绝所有请求,直到服务恢复正常为止\n\n\n\n\n\n@hystrixcommand(fallbackmethod = "findone_fallback",commandproperties = {\n        //设置hystrix的超时时间默认为1000毫秒\n        @hystrixproperty(name = "execution.isolation.thread.interruptontimeout",value = "3000"),\n        @hystrixproperty(name = "circuitbreaker.sleepwindowinmilliseconds",value = "5000"), //监控时间 默认为5000毫秒\n        @hystrixproperty(name = "circuitbreaker.requestvolumethreshold",value = "20"),  //失败次数 默认20次\n        @hystrixproperty(name = "circuitbreaker.errorthresholdpercentage",value = "50"),  //失败率  默认为百分之50\n})  //指定降级后调用的方法   commandproperties为设置指定规则\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# turbine 熔断监控\n\nhystrix 提供了 hystrix-dashboard功能 用于实时监控微服务运行状态 hystrix-dashboard 只能监控一个微服务\n\nnetflix 提供了turbine 进行聚合监控\n\n 1. 新建模块并导入坐标\n    \n         <dependencies>\n           <dependency>\n               <groupid>org.springframework.cloud</groupid>\n               <artifactid>spring-cloud-starter-netflix-hystrix-dashboard</artifactid>\n           </dependency>\n    <dependency>\n           <groupid>org.springframework.cloud</groupid>\n           <artifactid>spring-cloud-starter-netflix-turbine</artifactid>\n       </dependency>\n       \n       <dependency>\n           <groupid>org.springframework.boot</groupid>\n           <artifactid>spring-boot-starter-actuator</artifactid>\n       </dependency>\n       \n       <dependency>\n           <groupid>org.springframework.cloud</groupid>\n           <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n       </dependency>\n    <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    \n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-maven-plugin</artifactid>\n            </plugin>\n        </plugins>\n    </build>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    \n\n 2. 创建application.yml\n    \n    spring:\n      application.name: hystrix-monitor\n    server:\n      port: 8769\n    turbine:\n      combine-host-port: true\n      # 配置需要监控的服务名称列表\n      app-config: hystrix-provider,hystrix-consumer\n      cluster-name-expression: "\'default\'"\n      aggregator:\n        cluster-config: default\n      #instanceurlsuffix: /actuator/hystrix.stream\n    eureka:\n      client:\n        serviceurl:\n          defaultzone: http://localhost:8761/eureka/\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    \n\n 3. 创建启动类\n    \n    package com.itheima;\n    \n    \n    import org.springframework.boot.springapplication;\n    import org.springframework.boot.autoconfigure.springbootapplication;\n    import org.springframework.cloud.netflix.eureka.enableeurekaclient;\n    import org.springframework.cloud.netflix.hystrix.dashboard.enablehystrixdashboard;\n    import org.springframework.cloud.netflix.turbine.enableturbine;\n    \n    @springbootapplication\n    @enableeurekaclient\n    \n    @enableturbine //开启turbine 很聚合监控功能\n    @enablehystrixdashboard //开启hystrix仪表盘监控功能\n    public class hystrixmonitorapp {\n    \n        public static void main(string[] args) {\n            springapplication.run(hystrixmonitorapp.class, args);\n        }\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n 4. 修改被监控的模块 需要分别修改 hystrix-provider 和 hystrix-consumer 模块\n    \n    1. 导入坐标\n       \n       \t\t<dependency>\n                   <groupid>org.springframework.boot</groupid>\n                   <artifactid>spring-boot-starter-actuator</artifactid>\n               </dependency>\n       \n               <dependency>\n                   <groupid>org.springframework.cloud</groupid>\n                   <artifactid>spring-cloud-starter-netflix-hystrix</artifactid>\n               </dependency>\n       \n               <dependency>\n                   <groupid>org.springframework.cloud</groupid>\n                   <artifactid>spring-cloud-starter-netflix-hystrix-dashboard</artifactid>\n               </dependency>\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       \n    \n    2. 配置bean\n       \n       @bean\n           public servletregistrationbean getservlet() {\n               hystrixmetricsstreamservlet streamservlet = new hystrixmetricsstreamservlet();\n               servletregistrationbean registrationbean = new servletregistrationbean(streamservlet);\n               registrationbean.setloadonstartup(1);\n               registrationbean.addurlmappings("/actuator/hystrix.stream");\n               registrationbean.setname("hystrixmetricsstreamservlet");\n               return registrationbean;\n           }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       \n    \n    3. 在启动类上添加注解@enablehystrixdashboard\n       \n       package com.itheima.provider;\n       \n       import com.netflix.hystrix.contrib.metrics.eventstream.hystrixmetricsstreamservlet;\n       import org.springframework.boot.springapplication;\n       import org.springframework.boot.autoconfigure.springbootapplication;\n       import org.springframework.boot.web.servlet.servletregistrationbean;\n       import org.springframework.cloud.client.circuitbreaker.enablecircuitbreaker;\n       import org.springframework.cloud.netflix.eureka.enableeurekaclient;\n       import org.springframework.cloud.netflix.hystrix.dashboard.enablehystrixdashboard;\n       import org.springframework.context.annotation.bean;\n       \n       /**\n        * 启动类\n        */\n       \n       @enableeurekaclient //该注解 在新版本中可以省略\n       @springbootapplication\n       \n       @enablecircuitbreaker //开启hystrix功能\n       @enablehystrixdashboard \n       public class providerapp {\n       \n       \n           public static void main(string[] args) {\n               springapplication.run(providerapp.class, args);\n           }\n       \n           @bean\n           public servletregistrationbean getservlet() {\n               hystrixmetricsstreamservlet streamservlet = new hystrixmetricsstreamservlet();\n               servletregistrationbean registrationbean = new servletregistrationbean(streamservlet);\n               registrationbean.setloadonstartup(1);\n               registrationbean.addurlmappings("/actuator/hystrix.stream");\n               registrationbean.setname("hystrixmetricsstreamservlet");\n               return registrationbean;\n           }\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       15\n       16\n       17\n       18\n       19\n       20\n       21\n       22\n       23\n       24\n       25\n       26\n       27\n       28\n       29\n       30\n       31\n       32\n       33\n       34\n       35\n       36\n       37\n       \n\n 5. 启动服务\n    \n    * eureka-server\n    * hystrix-provider\n    * hystrix-consumer\n    * hystrix-monitor\n\n 6. 访问 http://localhost:8769/hystrix/\n    \n    监控url地址为 http://localhost:8769/turbine.stream\n    \n    记得访问一次 接口否则没有数据\n\n\n\n\n# gateway 网关\n\n网关是为微服务架构提供一种简单而有效的统一的api路由管理方式\n\n\n# 入门案例\n\n 1. 创建gateway模块 导入坐标\n    \n        <dependencies>\n            \x3c!--引入gateway 网关--\x3e\n    \n            <dependency>\n                <groupid>org.springframework.cloud</groupid>\n                <artifactid>spring-cloud-starter-gateway</artifactid>\n            </dependency>\n    \n            \x3c!-- eureka-client --\x3e\n            <dependency>\n                <groupid>org.springframework.cloud</groupid>\n                <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n            </dependency>\n        </dependencies>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    \n\n 2. 创建启动类\n    \n    package com.itheima.gateway;\n    \n    import org.springframework.boot.springapplication;\n    import org.springframework.boot.autoconfigure.springbootapplication;\n    import org.springframework.cloud.netflix.eureka.enableeurekaclient;\n    \n    @springbootapplication\n    @enableeurekaclient\n    public class apigatewayapp {\n        public static void main(string[] args) {\n            springapplication.run(apigatewayapp.class, args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 3. 编写application.yml\n    \n    server:\n      port: 80\n    spring:\n      application:\n        name: api-geteway-server\n      cloud:\n        # 网关配置\n        gateway:\n          # 路由配置: 转发规则\n          routes: # 集合\n          - id: gateway-provider # 唯一标识 默认为随机uuid\n            uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n            predicates: # 条件 用于请求网关路径的匹配规则\n            - path=/goods/**\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    \n\n 4. 启动模块和服务治理 服务提供方 消费方 只要访问http://localhost:80/goods/* 地址则转发为\n\nhttp://localhost:8001/goods/*\n\n\n# 静态路由\n\n通过routes转发规则 配置多个集合对象\n\nserver:\n  port: 80\nspring:\n  application:\n    name: api-geteway-server\n  cloud:\n    # 网关配置\n    gateway:\n      # 路由配置: 转发规则\n      routes: # 集合\n      - id: gateway-provider # 唯一标识 默认为随机uuid\n        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - path=/goods/**\n\n      - id: gateway-consumer # 转发规则2\n        uri: http://localhost:9000/  #  转发路径  服务提供方访问路径\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - path=/order/**\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n但不推荐此方法配置\n\n\n# 动态路由\n\n从eureka 动态获取\n\n 1. 引入eureka client 并在启动类中加上@enableeurekaclient注解\n\n 2. 配置eureka服务地址 并将gateway的uri 改成 lb://应用名称\n    \n    server:\n      port: 80\n    spring:\n      application:\n        name: api-geteway-server\n      cloud:\n        # 网关配置\n        gateway:\n          # 路由配置: 转发规则\n          routes: # 集合\n          - id: gateway-provider # 唯一标识 默认为随机uuid\n    #        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n            uri: lb://gateway-provider\n            predicates: # 条件 用于请求网关路径的匹配规则\n            - path=/goods/**\n    \n          - id: gateway-consumer # 转发规则2\n    #          uri: http://localhost:9000/  #  转发路径  服务提供方访问路径\n            uri: lb://gateway-consumer\n            predicates: # 条件 用于请求网关路径的匹配规则\n            - path=/order/**\n    \n    eureka:\n      client:\n        service-url:\n          defaultzone: http://localhost:8761/eureka\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    \n\n\n# 微服务名称配置\n\ngateway网关中 默认不启用由eureka应用名拼接的url转发访问\n\n我们通过 sping.cloud.discovery.locator.enabled 开启之后\n\n可以通过 http://localhost/gateway-consumer/order/goods/2 来转发访问 方便我们更好的区分应用 当然http://localhost/order/goods/2 不加应用名称拼接也是可以转发访问\n\nserver:\n  port: 80\nspring:\n  application:\n    name: api-geteway-server\n  cloud:\n    # 网关配置\n    gateway:\n      # 路由配置: 转发规则\n      routes: # 集合\n      - id: gateway-provider # 唯一标识 默认为随机uuid\n#        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n        uri: lb://gateway-provider\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - path=/goods/**\n\n      - id: gateway-consumer # 转发规则2\n#          uri: http://localhost:9000/  #  转发路径  服务提供方访问路径\n        uri: lb://gateway-consumer\n        predicates: # 条件 用于请求网关路径的匹配规则\n        - path=/order/**\n      discovery:\n        locator:\n          enabled: true  # 开启微服务发现功能\n          lower-case-service-id: true # 请求路径上的应用名称允许小写\n\neureka:\n  client:\n    service-url:\n      defaultzone: http://localhost:8761/eureka\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 网关过滤器\n\n过滤器 对请求或响应进行拦截 完成一些通用操作\n\ngateway 提供 两种过滤器方式 pre 和 post\n\n * pre 过滤器 在转发之前执行 可以做参数校验 权限校验 流量监控 日志输出 协议转换等\n * post过滤器 在响应之前执行 可以用对响应内容 响应头的修改 日志的输出 流量监控等\n\n同时还提供了两种类型过滤器\n\n * gatewayfilter 局部过滤器 针对单个路由\n * globalfilter 全局过滤器 针对所有路由\n\n# 局部过滤器\n\n过滤器工厂                         作用                                                             参数\naddrequestheader              为原始请求添加header                                                  header的名称及值\naddrequestparameter           为原始请求添加请求参数                                                    参数名称及值\naddresponseheader             为原始响应添加header                                                  header的名称及值\ndeduperesponseheader          剔除响应头中重复的值                                                     需要去重的header名称及去重策略\nhystrix                       为路由引入hystrix的断路器保护                                             hystrixcommand的名称\nfallbackheaders               为fallbackuri的请求头中添加具体的异常信息                                     header的名称\nprefixpath                    为原始请求路径添加前缀                                                    前缀路径\npreservehostheader            为请求添加一个preservehostheader=true的属性，路由过滤器会检查该属性以决定是否要发送原始的host   无\nrequestratelimiter            用于对请求限流，限流算法为令牌桶                                               keyresolver、ratelimiter、statuscode、denyemptykey、emptykeystatus\nredirectto                    将原始请求重定向到指定的url                                                http状态码及重定向的url\nremovehopbyhopheadersfilter   为原始请求删除ietf组织规定的一系列header                                      默认就会启用，可以通过配置指定仅删除哪些header\nremoverequestheader           为原始请求删除某个header                                                header名称\nremoveresponseheader          为原始响应删除某个header                                                header名称\nrewritepath                   重写原始的请求路径                                                      原始路径正则表达式以及重写后路径的正则表达式\nrewriteresponseheader         重写原始响应中的某个header                                               header名称，值的正则表达式，重写后的值\nsavesession                   在转发请求之前，强制执行websession::save操作                                 无\nsecureheaders                 为原始响应添加一系列起安全作用的响应头                                            无，支持修改这些安全响应头的值\nsetpath                       修改原始的请求路径                                                      修改后的路径\nsetresponseheader             修改原始响应中某个header的值                                              header名称，修改后的值\nsetstatus                     修改原始响应的状态码                                                     http 状态码，可以是数字，也可以是字符串\nstripprefix                   用于截断原始请求的路径                                                    使用数字表示要截断的路径的数量\nretry                         针对不同的响应进行重试                                                    retries、statuses、methods、series\nrequestsize                   设置允许接收最大请求包的大小。如果请求包大小超过设置的值，则返回 413 payload too large         请求包大小，单位为字节，默认值为5m\nmodifyrequestbody             在转发请求之前修改原始请求体内容                                               修改后的请求体内容\nmodifyresponsebody            修改原始响应体的内容                                                     修改后的响应体内容\ndefault                       为所有路由添加过滤器                                                     过滤器工厂名称及值\n\n在配置文件中通过 sping.cloud.gateway.routes.filters 设置\n\nspring:\n  application:\n    name: api-geteway-server\n  cloud:\n    # 网关配置\n    gateway:\n      # 路由配置: 转发规则\n      routes: # 集合\n        - id: gateway-provider # 唯一标识 默认为随机uuid\n          #        uri: http://localhost:8001/  #  转发路径  服务提供方访问路径\n          uri: lb://gateway-provider\n          predicates: # 条件 用于请求网关路径的匹配规则\n            - path=/goods/**\n          filters:\n            - addrequestheader=username,zhansan\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 全局过滤器\n\n全局过滤器不需要在配置文件中配置 系统初始化时加载 并作用在每个路由上\n\n\n\n 1. 在gateway模块中 编写自定义过滤器类 并实现 globalfilter, ordered 接口\n    \n    package com.itheima.gateway.filter;\n    \n    import org.springframework.cloud.gateway.filter.gatewayfilterchain;\n    import org.springframework.cloud.gateway.filter.globalfilter;\n    import org.springframework.core.ordered;\n    import org.springframework.stereotype.component;\n    import org.springframework.web.server.serverwebexchange;\n    import reactor.core.publisher.mono;\n    \n    @component\n    public class myfilter implements globalfilter, ordered {\n        @override\n        public mono<void> filter(serverwebexchange exchange, gatewayfilterchain chain) {\n            system.out.println("自定义全局过滤器");\n            return chain.filter(exchange);  //放行\n        }\n    \n        /**\n         * 过滤器排序\n         * @return 数值越小 越先执行\n         */\n        @override\n        public int getorder() {\n            return 0;\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    \n\n\n# config 分布式配置中心\n\nconfig 解决了在分布式场景下多环境配置文件的管理和维护\n\n 1. 在github或者gitee创建一个存放config配置的仓库\n\n 2. 创建config-server 模块 并导入坐标\n    \n        \x3c!-- config-server --\x3e\n        <dependency>\n            <groupid>org.springframework.cloud</groupid>\n            <artifactid>spring-cloud-config-server</artifactid>\n        </dependency>\n    </dependencies>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 创建启动类并使用 @enableconfigserver 注解启用 config server\n    \n    package com.itheima.config;\n    \n    import org.springframework.boot.springapplication;\n    import org.springframework.boot.autoconfigure.springbootapplication;\n    import org.springframework.cloud.config.server.enableconfigserver;\n    \n    @springbootapplication\n    @enableconfigserver //启用config server功能\n    public class configserverapp {\n        public static void main(string[] args) {\n            springapplication.run(configserverapp.class,args);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 4. 配置application.yml\n    \n    server:\n      port: 8888\n    \n    \n    spring:\n      application:\n        name: config-server\n    \n      cloud:\n        config:\n          server:\n            git:\n              uri: https://gitee.com/iekrwh/configs.git  # 仓库地址\n    #          username:   #如果是私有仓库则需要配置git的账号和密码\n    #          password:\n              default-label: master # 分支配置\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n\n 5. 在服务提供方中 导入 config client坐标\n    \n    \x3c!--     config client   --\x3e\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-config</artifactid>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 6. 在创建配置文件 bootstrap.yml 优先级高于application.yml\n    \n    spring:\n      cloud:\n        config:\n          uri: http://localhost:8888  # 配置config server地址\n          name: config # 文件名称\n          label: master # 分支\n          profile: dev # -后面的版本名称\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 7. 此时在服务提供方中 可以读取config 配置文件提供的值\n\n\n# 客户端刷新\n\n当远程仓库中的配置文件发生改变事 我们的config server 会自动更新\n\n但我们的config client并不会自动更新内容\n\n 1. 客户端引入actuator依赖\n    \n    <dependency>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-actuator</artifactid>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 获取配置信息的类上 加上@refreshscope 注解\n\n 3. 在bootstrap.yml 添加 management.endpoints.web.exposure.include\n    \n    spring:\n      cloud:\n        config:\n          uri: http://localhost:8888  # 配置config server地址\n          name: config # 文件名称\n          label: master # 分支\n          profile: dev # -后面的版本名称\n    management:\n      endpoints:\n        web:\n          exposure:\n            include: \'*\'\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    \n\n 4. 在cmd中 向http://localhost:8001/actuator/refresh 发送post请求\n    \n    curl -x post http://localhost:8001/actuator/refresh\n    \n    \n    1\n    \n    \n    但是每次更新 需要发送一次请求 后面结合bus才能解决此问题\n\n\n# 集成eureka\n\n通过上面例子 我们发现 config客户端访问服务端 地址是固定写死 非常不灵活\n\n我们可以通过eureka 使服务端在上面注册应用 自动获取应用的地址\n\n 1. config-server 导入eureka坐标\n    \n    \x3c!-- eureka-client --\x3e\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-netflix-eureka-client</artifactid>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 2. config-server 配置 加上eureka的地址\n    \n    server:\n      port: 8888\n    \n    \n    spring:\n      application:\n        name: config-server\n    \n      cloud:\n        config:\n          server:\n            git:\n              uri: https://gitee.com/iekrwh/configs.git  # 仓库地址\n    #          username:   #如果是私有仓库则需要配置git的账号和密码\n    #          password:\n              default-label: master # 分支配置\n    \n    eureka:\n      client:\n        service-url:\n          defaultzone: http://localhost:8761/eureka\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    \n\n 3. 在config serve 启动类加上 @enableeurekaclient 注解\n\n 4. 在config client 配置文件中 将 config uri 改为 eurka服务地址\n    \n    spring:\n      cloud:\n        config:\n    #      uri: http://localhost:8888  # 配置config server地址\n          name: config # 文件名称\n          label: master # 分支\n          profile: dev # -后面的版本名称\n          discovery:\n            enabled: true # 从注册中心寻找config server 地址\n            service-id: config-server # config server 注册的应用名\n    management:\n      endpoints:\n        web:\n          exposure:\n            include: \'*\'\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n\n# bus 消息总线\n\nspring cloud 是用轻量的消息中间件将分布式的节点连接起来 用于广播配置文件的更改或者服务的监控管理\n\nspring cloud bus 可选的消息中间件包括rabbitmq和 kafka\n\n 1. 在config-server 和 config-client中 引入 bus依赖\n    \n    \x3c!-- bus --\x3e\n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-bus-amqp</artifactid>\n    </dependency>\n    <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-starter-actuator</artifactid>\n            </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    \n\n 2. 在config-server 和 config-client中 分别配置rabbitmq\n    \n    spring:\n      rabbitmq:\n        host: 192.168.130.124 # mq服务器地址\n        port: 5672\n        username: iekr\n        password: iekr\n        virtual-host: /itcast\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 3. 在config-server 中 设置暴露监控断电 bus-refresh\n    \n    management:\n      endpoints:\n        web:\n          exposure:\n            include: \'bus-refresh\'  # 暴露bus的刷新端点\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 4. 更新则发送指令 curl -x post config-server地址:端口/actuator/bus-refresh\n    \n    curl -x post http://localhost:8888/actuator/bus-refresh\n    \n    \n    1\n    \n\n\n# stream 消息驱动\n\nspring clou stream 是一个构建消息驱动微服务应用的框架\n\nstream 对消息中间件的进一步封装 可以做到代码层面对中间件的无感知 甚至动态切换中间件\n\nstream 构建的应用程序与消息中间件之间是通过绑定器 binder 相关联的\n\n\n# 消息生产者\n\n 1. 创建生产者模块 导入坐标\n    \n            \x3c!--spring boot web--\x3e\n            <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-starter-web</artifactid>\n            </dependency>\n    \n    \n            \x3c!-- stream --\x3e\n            <dependency>\n                <groupid>org.springframework.cloud</groupid>\n                <artifactid>spring-cloud-starter-stream-rabbit</artifactid>\n            </dependency>\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 2. 编写配置文件\n    \n    server:\n      port: 8000\n    \n    \n    \n    spring:\n      cloud:\n        stream:\n          # 定义绑定器，绑定到哪个消息中间件上\n          binders:\n            itheima_binder: # 自定义的绑定器名称\n              type: rabbit # 绑定器类型\n              environment: # 指定mq的环境\n                spring:\n                  rabbitmq:\n                    host: 192.168.130.124\n                    port: 5672\n                    username: guest\n                    password: guest\n                    virtual-host: /\n          bindings:\n            output: # channel名称\n              binder: itheima_binder #指定使用哪一个绑定器(自定义的绑定器名称)\n              destination: itheima_exchange # 消息目的地\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    \n\n 3. 定义发送消息类 并在类上开启binding绑定为配置文件\n    \n    package com.itheima.stream.producer;\n    \n    \n    import org.springframework.beans.factory.annotation.autowired;\n    import org.springframework.cloud.stream.annotation.enablebinding;\n    import org.springframework.cloud.stream.messaging.source;\n    import org.springframework.messaging.messagechannel;\n    import org.springframework.messaging.support.messagebuilder;\n    import org.springframework.stereotype.component;\n    \n    @component\n    @enablebinding(source.class)\n    public class messageproducer {\n    \n        @autowired\n        private messagechannel output;\n    \n        public void send(){\n            string msessage = "hello stream~~~";\n    \n            //发送消息\n            output.send(messagebuilder.withpayload(msessage).build());\n    \n            system.out.println("消息发送成功~~~");\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    \n\n 4. 创建一个接口用于测试 是否能发送消息给mq\n    \n    package com.itheima.stream.producer;\n    \n    import org.springframework.beans.factory.annotation.autowired;\n    import org.springframework.web.bind.annotation.requestmapping;\n    import org.springframework.web.bind.annotation.restcontroller;\n    \n    @restcontroller\n    public class producercontroller {\n    \n        @autowired\n        private messageproducer producer;\n    \n    \n        @requestmapping("/send")\n            public string sendmsg(){\n            producer.send();\n            return "success";\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n\n# 消息消费者\n\n 1. 创建消费者模块 导入坐标\n    \n            \x3c!--spring boot web--\x3e\n            <dependency>\n                <groupid>org.springframework.boot</groupid>\n                <artifactid>spring-boot-starter-web</artifactid>\n            </dependency>\n    \n    \n            \x3c!-- stream --\x3e\n            <dependency>\n                <groupid>org.springframework.cloud</groupid>\n                <artifactid>spring-cloud-starter-stream-rabbit</artifactid>\n            </dependency>\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n\n 2. 编写配置文件 主要把端口和 bindings设置为输出还是输入模式\n    \n    server:\n      port: 9000\n    \n    \n    \n    spring:\n      cloud:\n        stream:\n          # 定义绑定器，绑定到哪个消息中间件上\n          binders:\n            itheima_binder: # 自定义的绑定器名称\n              type: rabbit # 绑定器类型\n              environment: # 指定mq的环境\n                spring:\n                  rabbitmq:\n                    host: localhost\n                    port: 5672\n                    username: guest\n                    password: guest\n                    virtual-host: /\n          bindings:\n            input: # channel名称\n              binder: itheima_binder #指定使用哪一个binder\n              destination: itheima_exchange # 消息目的地\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    \n\n 3. 创建消息接收类 在类开启binding注解 并在接收方法中定义 @streamlistener(sink.input)\n    \n    package com.itheima.stream.consumer;\n    \n    import org.springframework.cloud.stream.annotation.enablebinding;\n    import org.springframework.cloud.stream.annotation.streamlistener;\n    import org.springframework.cloud.stream.messaging.sink;\n    import org.springframework.messaging.message;\n    import org.springframework.stereotype.component;\n    \n    /**\n     * 消息接收类\n     */\n    @enablebinding({sink.class})\n    @component\n    public class messagelistener {\n    \n        @streamlistener(sink.input)\n        public void receive(message message){\n    \n            system.out.println(message);\n            system.out.println(message.getpayload());\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n\n\n# sleuth+zipkin 链路追踪\n\nspring cloud sleuth 它在分布式中能跟踪一个用户请求的过程 捕获这些跟踪数据 就能构建微服务的整个调用链的视图 它是调试和监控微服务的关键工具\n\n 1. 安装启动zipkin 访问http://localhost:9411/\n    \n    java -jar zipkin.jar\n    \n    \n    1\n    \n\n 2. 在服务提供方和消费方法 引入 zipkin 坐标\n    \n    <dependency>\n        <groupid>org.springframework.cloud</groupid>\n        <artifactid>spring-cloud-starter-zipkin</artifactid>\n    </dependency>\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 3. 配置文件\n    \n    server:\n      port: 8001\n    \n    eureka:\n      client:\n        service-url:\n          defaultzone: http://localhost:8761/eureka\n    spring:\n      application:\n        name: feign-provider\n      zipkin:\n        base-url: http://localhost:9411/  # 设置zipkin的服务端路径\n    \n      sleuth:\n        sampler:\n          probability: 1 # 采集率 默认 0.1 百分之十。\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"ElasticSearch",frontmatter:{title:"ElasticSearch",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/95da1a/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/19.ElasticSearch.html",relativePath:"后端/02.JavaEE/19.ElasticSearch.md",key:"v-766b181e",path:"/pages/95da1a/",headers:[{level:2,title:"倒排索引",slug:"倒排索引",normalizedTitle:"倒排索引",charIndex:187},{level:2,title:"ElasticSearch搜索和传统数据库查询的区别",slug:"elasticsearch搜索和传统数据库查询的区别",normalizedTitle:"elasticsearch搜索和传统数据库查询的区别",charIndex:244},{level:2,title:"安装ElasticSearch",slug:"安装elasticsearch",normalizedTitle:"安装elasticsearch",charIndex:585},{level:3,title:"安装辅助工具Kibana",slug:"安装辅助工具kibana",normalizedTitle:"安装辅助工具kibana",charIndex:1587},{level:2,title:"ElasticSearch 核心概念",slug:"elasticsearch-核心概念",normalizedTitle:"elasticsearch 核心概念",charIndex:2211},{level:2,title:"操作ElasticSearch",slug:"操作elasticsearch",normalizedTitle:"操作elasticsearch",charIndex:2612},{level:3,title:"RESTful风格",slug:"restful风格",normalizedTitle:"restful风格",charIndex:2632},{level:3,title:"操作索引",slug:"操作索引",normalizedTitle:"操作索引",charIndex:2715},{level:3,title:"操作映射",slug:"操作映射",normalizedTitle:"操作映射",charIndex:3193},{level:3,title:"操作文档",slug:"操作文档",normalizedTitle:"操作文档",charIndex:4197},{level:2,title:"分词器",slug:"分词器",normalizedTitle:"分词器",charIndex:2331},{level:2,title:"IK分词器",slug:"ik分词器",normalizedTitle:"ik分词器",charIndex:4761},{level:3,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:585},{level:3,title:"使用",slug:"使用",normalizedTitle:"使用",charIndex:284},{level:2,title:"JavaApi",slug:"javaapi",normalizedTitle:"javaapi",charIndex:7487},{level:3,title:"添加索引",slug:"添加索引-2",normalizedTitle:"添加索引",charIndex:2744},{level:3,title:"添加/修改文档",slug:"添加-修改文档",normalizedTitle:"添加/修改文档",charIndex:11912},{level:3,title:"查询文档",slug:"查询文档-2",normalizedTitle:"查询文档",charIndex:4423},{level:3,title:"删除文档",slug:"删除文档",normalizedTitle:"删除文档",charIndex:4592}],headersStr:"倒排索引 ElasticSearch搜索和传统数据库查询的区别 安装ElasticSearch 安装辅助工具Kibana ElasticSearch 核心概念 操作ElasticSearch RESTful风格 操作索引 操作映射 操作文档 分词器 IK分词器 安装 使用 JavaApi 添加索引 添加/修改文档 查询文档 删除文档",content:'# ElasticSearch\n\nElasticSearch是基于 Lucene的搜索服务器 是一个分布式 高扩展 高实时的搜索与数据分析引擎\n\n基于RESTful web接口\n\nElasticSearch是用java开发 并作为apache的开源项目\n\nhttps://www.elastic.co/cn/\n\n一般用于 海量数据的查询 日志数据分析 实时数据分析\n\n\n# 倒排索引\n\n将各个文档中内容.进行分词 形成词条 记录词条和数据的唯一标识(id)的对应关系 形成的产物\n\n\n# ElasticSearch搜索和传统数据库查询的区别\n\n 1. 传统关系型数据 使用模糊查询 左边有通配符 不会走索引 会全表扫描 性能低\n 2. 只能以一个关键字作为查询条件 而ElasticSearch会把一个关键字拆分为多个词 进行查询\n 3. ElasticSearch 以关键字 生成的倒排索引 词条会排序 形成一颗树形结构 提升词条的查询速度\n 4. Mysql有事务性 而ElasticSearch 没有事务性 所以删了的数据是无法恢复的\n 5. ElasticSearch 没有物理外键这个特性 如果数据的一致性要求比较高 不建议使用\n 6. ElasticSearch和Mysql 分工不同 Mysql负责存储数据 ElasticSearch负责搜索数据\n\n\n# 安装ElasticSearch\n\ntar -zxvf elasticsearch-7.15.0-linux-x86_64.tar.gz -C /opt\n#编辑配置\nvim /opt/elasticsearch-7.15.0/config/elasticsearch.yml\n\n#追加以下内容\ncluster.name: my-application\nnode.name: node-1\nnetwork.host: 0.0.0.0\nhttp.port: 9200\ncluster.initial_master_nodes: ["node-1"]\n\n#出于安全问题ElasticSearch不允许root用户直接运行\nuseradd iekr\npasswd 123456\n#授权\nchown -R iekr:iekr /opt/elasticsearch-7.15.0/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n新建的用户最大创建文件和最大虚拟内存太小 需要修改配置文件\n\n#修改最大创建文件数\nvim /etc/security/limits.conf\n\n#追加内容\niekr soft nofile 65536\niekr hard nofile 65536\n\nvim /etc/security/limits.d/20-nproc.conf\n#追加内容\niekr soft nofile 65536\niekr hard nofile 65536\n* hard nproc 4096\n\n\n#修改虚拟内容大小\nvim /etc/sysctl.conf \n\n#追加内容\nvm.max_map_count=655360\n#重载\nsysctl -p\n\nfirewall-cmd --zone=public --add-port=9200/tcp --permanent\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n启动\n\nsu iekr\ncd /opt/elasticsearch-7.15.0/bin/\n./elasticsearch -d  #-d后台运行\n\n\n1\n2\n3\n\n\n访问 http://192.168.130.124:9200/ 出现json字符串则启动成功\n\n\n# 安装辅助工具Kibana\n\nhttps://www.elastic.co/cn/kibana/\n\ntar -zxvf kibana-7.15.0-linux-x86_64.tar.gz -C /opt\n#配置\nvim /opt/kibana-7.15.0-linux-x86_64/config/kibana.yml\n\n#追加内容\nserver.port: 5601\nserver.host: "0.0.0.0"\nserver.name: "your-hostname"  #自定义名称\nelasticsearch.hosts: ["http://localhost:9200"]\nelasticsearch.requestTimeout: 30000  #连接ES超时时间\ni18n.locale: "zh-CN"  #设置为中文\n\n#授权\nchown -R iekr:iekr /opt/kibana-7.15.0-linux-x86_64/\nfirewall-cmd --zone=public --add-port=5601/tcp --permanent\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n启动\n\nsu iekr\ncd /opt/kibana-7.15.0-linux-x86_64/bin/\nnohup ./kibana &  #后台运行  前台./kibana\n\n\n1\n2\n3\n\n\n\n# ElasticSearch 核心概念\n\n * 索引(index) ElasticSearch存储数据的地方 可以理解为关系型数据库中的数据库概念\n\n * 映射(mapping)\n   \n   mapping定义了每个字段的类型 字段所使用的分词器等 相定义关系型数据库中的表结构\n\n * 文档(document) ElasticSearch中的最小数据单元 以json格式显示 一个document相当于 关系型数据库的一行数据\n\n * 倒排索引 一个倒排索引由文档中所有不重复此的列表构成 对应其中每个词 对应一个包含它的文档id列表\n\n * 类型(type)\n   \n   一种type就像一类表 如用户表 角色表等 在ElasticSearch7.x type默认为_doc\n   \n   5.x中一个index可以有多种type\n   \n   6.x中一个index只能有一种type\n\n\n# 操作ElasticSearch\n\n\n# RESTful风格\n\nREST 表述性状态转移 是一组架构约束条件和原则 满足这些约束条件和原则的应用程序或设计就是RESTful 是与只能怪定义接口的规范\n\n\n# 操作索引\n\n以下操作使用postman工具发送请求\n\n# 添加索引\n\n使用PUT请求 在uri地址后加上索引名称\n\n192.168.130.124:9200/goods_index\n\n\n1\n\n\n# 查询索引\n\n使用GET请求 在uri地址后加上索引名称\n\n192.168.130.124:9200/goods_index\n\n\n1\n\n\n查询多个使用逗号分隔\n\n192.168.130.124:9200/goods_index,goods_index2\n\n\n1\n\n\n查询全部使用_all\n\n192.168.130.124:9200/_all\n\n\n1\n\n\n# 删除索引\n\n使用DELETE请求 在uri地址后加上索引名称\n\n192.168.130.124:9200/goods_index\n\n\n1\n\n\n# 关闭指定索引\n\n关闭后只是无法使用 并不会删除此索引\n\n192.168.130.124:9200/goods_index/_close\n\n\n1\n\n\n打开索引\n\n192.168.130.124:9200/goods_index/_open\n\n\n1\n\n\n\n# 操作映射\n\n * 简单数据类型\n   * 字符串\n     * text 会分词,不支持聚合\n     * keyword 不会分词 将全部内容作为一个词条 支持聚合\n   * 数组\n     * \n   * 布尔\n   * 二进制\n     * binary\n   * 范围类型\n     * integer_range\n     * float_range\n     * long_range\n     * double_range\n     * date_range\n   * 日期\n * 复杂数据类型\n   * 数组:[]\n   * 对象:{}\n\n# 添加映射\n\n以下操作使用Kibana操作\n\nhttp://192.168.130.124:5601/app/dev_tools#/console\n\n# 创建索引\nPUT person\n# 查询索引\nGET person\n\n# 添加映射\nPUT person/_mapping\n{\n  "properties":{\n    "name":{\n      "type":"keyword"\n    },\n    "age":{\n      "type":"integer"\n    }\n  }\n}\n\n# 查询映射\nGET person/_mapping\n\n# 删除索引\nDELETE person\n\n# 创建索引并添加映射\nPUT person\n{\n  "mappings": {\n    "properties": {\n        "name":{\n      "type":"keyword"\n    },\n    "age":{\n      "type":"integer"\n    }\n    }\n  }\n}\n\n# 查询索引\nGET person\n\n# 添加字段\nPUT person/_mapping\n{\n  "properties":{\n    "address":{\n      "type":"text"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n\n# 操作文档\n\n# 查询索引\nGET person\n\n\n# 添加文档 指定id   可以使用PUT或POST请求\nPUT person/_doc/1\n{\n  "name":"zhangsan",\n  "age":20,\n  "address":"北京"\n}\n\n\n# 添加文档 不指定id  必须为POST请求\nPOST person/_doc\n{\n  "name":"李四",\n  "age":30,\n  "address":"广东"\n}\n\n\n# 根据id查询文档\nGET person/_doc/1\n\n\n# 查询所有文档\nGET person/_search\n\n# 修改文档   必须为PUT请求  如果id存在则修改 不存在则自动创建\nPUT person/_doc/1\n{\n  "name":"wangwu",\n  "age":20,\n  "address":"北京"\n}\n\n# 根据ID删除文档\nDELETE person/_doc/1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 分词器\n\n\n\n但ES原始的分词器对中文不太友好\n\n\n# IK分词器\n\nIKAnalyzer是一个开源的 基于java语言开发的轻量级的中文分词工具包\n\n是一个基于Maven构建的项目 具有60万字/秒的高速处理能力 支持用户词典扩展定义\n\nhttps://github.com/medcl/elasticsearch-analysis-ik\n\n\n# 安装\n\n * 安装JDK 由于ES内置了JDK 我们将ES内置的JDK设置为系统环境变量\n\nvim /etc/profile\n\nexport JAVA_HOME=/opt/elasticsearch-7.15.0/jdk\nexport PATH=$PATH:${JAVA_HOME}/bin\n\nsource /etc/profile\n\n\n1\n2\n3\n4\n5\n6\n\n * 安装Maven -- 已不需要\n\nwget https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz\ntar -zxvf apache-maven-3.8.3-bin.tar.gz\nln -s apache-maven-3.8.3 maven\n#配置环境变量\nvim /etc/profile.d/maven.sh\n\nexport MAVEN_HOME=/root/maven\nexport PATH=${MAVEN_HOME}/bin:${PATH}\n\nsource /etc/profile.d/maven.sh\nmvn -v\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 安装IK分词器 --已不需要\n\nwget https://github.com/medcl/elasticsearch-analysis-ik/archive/refs/tags/v7.15.0.zip  #这里下载的是源码\n# 由于是zip文件所以需要unzip命令\nyum install -y zip\nyum install -y unzip\n#解压\nunzip v7.15.0.zip\n\ncd /root/elasticsearch-analysis-ik-7.15.0/\n#编译打包\nmvn package\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 将编译好的插件导入到ES中\n\ncd /opt/elasticsearch-7.15.0/plugins/\nmkdir analysis-ik\ncd analysis-ik/\n#重启es\n\n\n\n1\n2\n3\n4\n5\n\n\n\n# 使用\n\n\n# ES内置分词器\nGET _analyze\n{\n  "analyzer": "standard",\n  "text": "你是试试水"\n}\n\n# ik分词器,粗粒度分词\nGET _analyze\n{\n  "analyzer": "ik_smart",\n  "text": "你是试试水"\n}\n\n# ik分词器,粗粒度分词\nGET _analyze\n{\n  "analyzer": "ik_max_word",\n  "text": "你是试试水"\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 查询文档\n\n * 词条查询:tern\n   * 词条查询不会分析查询条件 只有当词条和查询字符串完全匹配时才匹配搜索\n * 全文查询:match\n   * 全文查询会分析查询条件 先将查询条件进行分词 然后查询 求并集\n\nPUT person/_doc/2\n{\n  "name":"李四",\n  "age":30,\n  "address":"华为5G手机"\n}\n\nGET person/_doc/2\n\n# term 词条查询 查询条件字符串和词条要完全匹配\n# es默认使用的分词器是standard 一个子一个词\nGET person/_search\n{\n  "query":{\n    "term": {\n      "address": {\n        "value": "手机"\n      }\n    }\n  }\n}\n\nDELETE person\n\n# 创建索引 添加映射 指定使用ik分词器\nPUT person\n{\n  "mappings": {\n    "properties": {\n      "name":{\n        "type": "keyword"\n      },\n      "address":{\n        "type": "text",\n        "analyzer": "ik_max_word"\n      }\n    }\n  }\n}\n\nGET person\n\n# 添加文档\nPUT person/_doc/1\n{\n  "name":"张三",\n  "age":30,\n  "address":"华为5G手机"\n}\nPUT person/_doc/2\n{\n  "name":"李四",\n  "age":30,\n  "address":"广东"\n}\nPUT person/_doc/3\n{\n  "name":"王五",\n  "age":30,\n  "address":"小米5G手机"\n}\n\nGET person/_search\n\n# 查询 term词条查询\nGET person/_search\n{\n  "query":{\n    "term": {\n      "address": {\n        "value": "手机"\n      }\n    }\n  }\n}\n\n# match 先会对查询的字符串进行分词 在查询 求交集\nGET person/_search\n{\n  "query": {\n    "match": {\n      "address": "苹果手机"\n    }\n  }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n\n# JavaApi\n\n 1. 创建springboot项目 并引入ES\n    \n    \n\n 2. 创建application.yml 配置文件\n    \n    elasticsearch:\n      host: 192.168.130.124\n      port: 9200\n    \n    \n    1\n    2\n    3\n    \n\n 3. 创建ElasticSearchConfig配置类 加载配置文件 并返回一个es客户端对象\n    \n    package com.itheima.elasticsearchdemo.config;\n    \n    \n    import org.apache.http.HttpHost;\n    import org.elasticsearch.client.RestClient;\n    import org.elasticsearch.client.RestHighLevelClient;\n    import org.springframework.boot.context.properties.ConfigurationProperties;\n    import org.springframework.context.annotation.Bean;\n    import org.springframework.context.annotation.Configuration;\n    \n    @Configuration\n    @ConfigurationProperties(prefix = "elasticsearch")\n    public class ElasticSearchConfig {\n    \n        private String host;\n        private int port;\n    \n        public String getHost() {\n            return host;\n        }\n    \n        public void setHost(String host) {\n            this.host = host;\n        }\n    \n        public int getPort() {\n            return port;\n        }\n    \n        public void setPort(int port) {\n            this.port = port;\n        }\n    \n        @Bean\n        public RestHighLevelClient client(){\n            return new RestHighLevelClient(RestClient.builder(\n                    new HttpHost(host,port,"http")\n            ));\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    \n\n 4. 注入对象 使用\n    \n    //1.创建es客户端对象\n    @Autowired\n    private RestHighLevelClient restHighLevelClient;\n    \n    \n    1\n    2\n    3\n    \n\n\n# 添加索引\n\n//添加索引\n@Test\nvoid addIndex() throws IOException {\n    //获取操作索引的对象\n    IndicesClient indices = restHighLevelClient.indices();\n    //添加\n    CreateIndexRequest createIndex = new CreateIndexRequest("itheima");  //索引名称\n    CreateIndexResponse createIndexResponse = indices.create(createIndex, RequestOptions.DEFAULT);\n    //根据返回值判断结果\n    System.out.println(createIndexResponse.isAcknowledged());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 添加索引并添加映射\n\n//添加索引并添加映射\n@Test\nvoid addIndexAndMapping() throws IOException {\n    //获取操作索引的对象\n    IndicesClient indices = restHighLevelClient.indices();\n    CreateIndexRequest request = new CreateIndexRequest("twitter");\n\n    // 向索引添加映射\n    request.source("{\\n" +\n            "    \\"settings\\" : {\\n" +\n            "        \\"number_of_shards\\" : 3,\\n" +\n            "        \\"number_of_replicas\\" : 2\\n" +\n            "    },\\n" +\n            "    \\"mappings\\" : {\\n" +\n            "        \\"tweet\\" : {\\n" +\n            "            \\"properties\\" : {\\n" +\n            "                \\"message\\" : { \\"type\\" : \\"text\\" }\\n" +\n            "            }\\n" +\n            "        }\\n" +\n            "    },\\n" +\n            "    \\"aliases\\" : {\\n" +\n            "        \\"twitter_alias\\" : {}\\n" +\n            "    }\\n" +\n            "}", XContentType.JSON);\n    CreateIndexResponse createIndexResponse = indices.create(request, RequestOptions.DEFAULT);\n    //根据返回值判断结果\n    System.out.println(createIndexResponse.isAcknowledged());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n# 查询索引\n\n//查询索引\n@Test\npublic void queryIndex() throws IOException {\n    GetIndexRequest request = new GetIndexRequest("twitter");\n    GetIndexResponse getIndexResponse = restHighLevelClient.indices().get(request, RequestOptions.DEFAULT);\n    System.out.println(getIndexResponse.getAliases());\n    System.out.println(getIndexResponse.getMappings());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 删除索引\n\n//删除索引\n@Test\npublic void deleteIndex() throws IOException {\n    IndicesClient indices = restHighLevelClient.indices();\n    DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest("itheima");\n    AcknowledgedResponse acknowledgedResponse = indices.delete(deleteIndexRequest, RequestOptions.DEFAULT);\n    System.out.println(acknowledgedResponse.isAcknowledged());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 判断索引是否存在\n\n//判断索引是否存在\n@Test\npublic void existIndex() throws IOException {\n    IndicesClient indices = restHighLevelClient.indices();\n    GetIndexRequest getRequest = new GetIndexRequest("itheima");\n    boolean exists = indices.exists(getRequest, RequestOptions.DEFAULT);\n    System.out.println(exists);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 添加/修改文档\n\n当id存在时则修改 不存在时则添加\n\n# Map\n\n//添加/修改 map\n@Test\npublic void addDoc() throws IOException {\n    //添加数据对象 map\n    Map<String, String> data = new HashMap<>();\n    data.put("address","背景");\n    data.put("name","钻石");\n    data.put("age","11");\n    //1.获取文档对象\n    IndexRequest indexRequest = new IndexRequest("twitter").id("1").source(data);\n    //添加数据\n    IndexResponse index = restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT);\n    //打印响应结果\n    System.out.println(index.getId());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# JSON\n\n//添加/修改 json\n@Test\npublic void addDoc2() throws IOException {\n   //创建对象\n    Person p =new Person();\n    p.setId("2");\n    p.setName("iekr");\n    p.setAge(15);\n    p.setAddress("广东");\n    String json = JSON.toJSONString(p);\n    //1.获取文档对象\n    IndexRequest indexRequest = new IndexRequest("twitter").id(p.getId()).source(json,XContentType.JSON);\n    //添加数据\n    IndexResponse index = restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT);\n    //打印响应结果\n    System.out.println(index.getId());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 查询文档\n\n    //根据id查询文档\n    @Test\n    public void findDocById() throws IOException {\n        GetRequest getRequest = new GetRequest("twitter", "1");\n//        getRequest.id("1");  //单独指定id\n        GetResponse documentFields = restHighLevelClient.get(getRequest, RequestOptions.DEFAULT);\n        System.out.println(documentFields.getSourceAsString()); //获取JSON字符串\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 删除文档\n\n//根据id删除文档\n@Test\npublic void delDoc() throws IOException {\n    DeleteRequest delete = new DeleteRequest("twitter", "1");\n    DeleteResponse response = restHighLevelClient.delete(delete, RequestOptions.DEFAULT);\n    System.out.println(response.getId()); \n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',normalizedContent:'# elasticsearch\n\nelasticsearch是基于 lucene的搜索服务器 是一个分布式 高扩展 高实时的搜索与数据分析引擎\n\n基于restful web接口\n\nelasticsearch是用java开发 并作为apache的开源项目\n\nhttps://www.elastic.co/cn/\n\n一般用于 海量数据的查询 日志数据分析 实时数据分析\n\n\n# 倒排索引\n\n将各个文档中内容.进行分词 形成词条 记录词条和数据的唯一标识(id)的对应关系 形成的产物\n\n\n# elasticsearch搜索和传统数据库查询的区别\n\n 1. 传统关系型数据 使用模糊查询 左边有通配符 不会走索引 会全表扫描 性能低\n 2. 只能以一个关键字作为查询条件 而elasticsearch会把一个关键字拆分为多个词 进行查询\n 3. elasticsearch 以关键字 生成的倒排索引 词条会排序 形成一颗树形结构 提升词条的查询速度\n 4. mysql有事务性 而elasticsearch 没有事务性 所以删了的数据是无法恢复的\n 5. elasticsearch 没有物理外键这个特性 如果数据的一致性要求比较高 不建议使用\n 6. elasticsearch和mysql 分工不同 mysql负责存储数据 elasticsearch负责搜索数据\n\n\n# 安装elasticsearch\n\ntar -zxvf elasticsearch-7.15.0-linux-x86_64.tar.gz -c /opt\n#编辑配置\nvim /opt/elasticsearch-7.15.0/config/elasticsearch.yml\n\n#追加以下内容\ncluster.name: my-application\nnode.name: node-1\nnetwork.host: 0.0.0.0\nhttp.port: 9200\ncluster.initial_master_nodes: ["node-1"]\n\n#出于安全问题elasticsearch不允许root用户直接运行\nuseradd iekr\npasswd 123456\n#授权\nchown -r iekr:iekr /opt/elasticsearch-7.15.0/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n新建的用户最大创建文件和最大虚拟内存太小 需要修改配置文件\n\n#修改最大创建文件数\nvim /etc/security/limits.conf\n\n#追加内容\niekr soft nofile 65536\niekr hard nofile 65536\n\nvim /etc/security/limits.d/20-nproc.conf\n#追加内容\niekr soft nofile 65536\niekr hard nofile 65536\n* hard nproc 4096\n\n\n#修改虚拟内容大小\nvim /etc/sysctl.conf \n\n#追加内容\nvm.max_map_count=655360\n#重载\nsysctl -p\n\nfirewall-cmd --zone=public --add-port=9200/tcp --permanent\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n启动\n\nsu iekr\ncd /opt/elasticsearch-7.15.0/bin/\n./elasticsearch -d  #-d后台运行\n\n\n1\n2\n3\n\n\n访问 http://192.168.130.124:9200/ 出现json字符串则启动成功\n\n\n# 安装辅助工具kibana\n\nhttps://www.elastic.co/cn/kibana/\n\ntar -zxvf kibana-7.15.0-linux-x86_64.tar.gz -c /opt\n#配置\nvim /opt/kibana-7.15.0-linux-x86_64/config/kibana.yml\n\n#追加内容\nserver.port: 5601\nserver.host: "0.0.0.0"\nserver.name: "your-hostname"  #自定义名称\nelasticsearch.hosts: ["http://localhost:9200"]\nelasticsearch.requesttimeout: 30000  #连接es超时时间\ni18n.locale: "zh-cn"  #设置为中文\n\n#授权\nchown -r iekr:iekr /opt/kibana-7.15.0-linux-x86_64/\nfirewall-cmd --zone=public --add-port=5601/tcp --permanent\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n启动\n\nsu iekr\ncd /opt/kibana-7.15.0-linux-x86_64/bin/\nnohup ./kibana &  #后台运行  前台./kibana\n\n\n1\n2\n3\n\n\n\n# elasticsearch 核心概念\n\n * 索引(index) elasticsearch存储数据的地方 可以理解为关系型数据库中的数据库概念\n\n * 映射(mapping)\n   \n   mapping定义了每个字段的类型 字段所使用的分词器等 相定义关系型数据库中的表结构\n\n * 文档(document) elasticsearch中的最小数据单元 以json格式显示 一个document相当于 关系型数据库的一行数据\n\n * 倒排索引 一个倒排索引由文档中所有不重复此的列表构成 对应其中每个词 对应一个包含它的文档id列表\n\n * 类型(type)\n   \n   一种type就像一类表 如用户表 角色表等 在elasticsearch7.x type默认为_doc\n   \n   5.x中一个index可以有多种type\n   \n   6.x中一个index只能有一种type\n\n\n# 操作elasticsearch\n\n\n# restful风格\n\nrest 表述性状态转移 是一组架构约束条件和原则 满足这些约束条件和原则的应用程序或设计就是restful 是与只能怪定义接口的规范\n\n\n# 操作索引\n\n以下操作使用postman工具发送请求\n\n# 添加索引\n\n使用put请求 在uri地址后加上索引名称\n\n192.168.130.124:9200/goods_index\n\n\n1\n\n\n# 查询索引\n\n使用get请求 在uri地址后加上索引名称\n\n192.168.130.124:9200/goods_index\n\n\n1\n\n\n查询多个使用逗号分隔\n\n192.168.130.124:9200/goods_index,goods_index2\n\n\n1\n\n\n查询全部使用_all\n\n192.168.130.124:9200/_all\n\n\n1\n\n\n# 删除索引\n\n使用delete请求 在uri地址后加上索引名称\n\n192.168.130.124:9200/goods_index\n\n\n1\n\n\n# 关闭指定索引\n\n关闭后只是无法使用 并不会删除此索引\n\n192.168.130.124:9200/goods_index/_close\n\n\n1\n\n\n打开索引\n\n192.168.130.124:9200/goods_index/_open\n\n\n1\n\n\n\n# 操作映射\n\n * 简单数据类型\n   * 字符串\n     * text 会分词,不支持聚合\n     * keyword 不会分词 将全部内容作为一个词条 支持聚合\n   * 数组\n     * \n   * 布尔\n   * 二进制\n     * binary\n   * 范围类型\n     * integer_range\n     * float_range\n     * long_range\n     * double_range\n     * date_range\n   * 日期\n * 复杂数据类型\n   * 数组:[]\n   * 对象:{}\n\n# 添加映射\n\n以下操作使用kibana操作\n\nhttp://192.168.130.124:5601/app/dev_tools#/console\n\n# 创建索引\nput person\n# 查询索引\nget person\n\n# 添加映射\nput person/_mapping\n{\n  "properties":{\n    "name":{\n      "type":"keyword"\n    },\n    "age":{\n      "type":"integer"\n    }\n  }\n}\n\n# 查询映射\nget person/_mapping\n\n# 删除索引\ndelete person\n\n# 创建索引并添加映射\nput person\n{\n  "mappings": {\n    "properties": {\n        "name":{\n      "type":"keyword"\n    },\n    "age":{\n      "type":"integer"\n    }\n    }\n  }\n}\n\n# 查询索引\nget person\n\n# 添加字段\nput person/_mapping\n{\n  "properties":{\n    "address":{\n      "type":"text"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n\n# 操作文档\n\n# 查询索引\nget person\n\n\n# 添加文档 指定id   可以使用put或post请求\nput person/_doc/1\n{\n  "name":"zhangsan",\n  "age":20,\n  "address":"北京"\n}\n\n\n# 添加文档 不指定id  必须为post请求\npost person/_doc\n{\n  "name":"李四",\n  "age":30,\n  "address":"广东"\n}\n\n\n# 根据id查询文档\nget person/_doc/1\n\n\n# 查询所有文档\nget person/_search\n\n# 修改文档   必须为put请求  如果id存在则修改 不存在则自动创建\nput person/_doc/1\n{\n  "name":"wangwu",\n  "age":20,\n  "address":"北京"\n}\n\n# 根据id删除文档\ndelete person/_doc/1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 分词器\n\n\n\n但es原始的分词器对中文不太友好\n\n\n# ik分词器\n\nikanalyzer是一个开源的 基于java语言开发的轻量级的中文分词工具包\n\n是一个基于maven构建的项目 具有60万字/秒的高速处理能力 支持用户词典扩展定义\n\nhttps://github.com/medcl/elasticsearch-analysis-ik\n\n\n# 安装\n\n * 安装jdk 由于es内置了jdk 我们将es内置的jdk设置为系统环境变量\n\nvim /etc/profile\n\nexport java_home=/opt/elasticsearch-7.15.0/jdk\nexport path=$path:${java_home}/bin\n\nsource /etc/profile\n\n\n1\n2\n3\n4\n5\n6\n\n * 安装maven -- 已不需要\n\nwget https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz\ntar -zxvf apache-maven-3.8.3-bin.tar.gz\nln -s apache-maven-3.8.3 maven\n#配置环境变量\nvim /etc/profile.d/maven.sh\n\nexport maven_home=/root/maven\nexport path=${maven_home}/bin:${path}\n\nsource /etc/profile.d/maven.sh\nmvn -v\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 安装ik分词器 --已不需要\n\nwget https://github.com/medcl/elasticsearch-analysis-ik/archive/refs/tags/v7.15.0.zip  #这里下载的是源码\n# 由于是zip文件所以需要unzip命令\nyum install -y zip\nyum install -y unzip\n#解压\nunzip v7.15.0.zip\n\ncd /root/elasticsearch-analysis-ik-7.15.0/\n#编译打包\nmvn package\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 将编译好的插件导入到es中\n\ncd /opt/elasticsearch-7.15.0/plugins/\nmkdir analysis-ik\ncd analysis-ik/\n#重启es\n\n\n\n1\n2\n3\n4\n5\n\n\n\n# 使用\n\n\n# es内置分词器\nget _analyze\n{\n  "analyzer": "standard",\n  "text": "你是试试水"\n}\n\n# ik分词器,粗粒度分词\nget _analyze\n{\n  "analyzer": "ik_smart",\n  "text": "你是试试水"\n}\n\n# ik分词器,粗粒度分词\nget _analyze\n{\n  "analyzer": "ik_max_word",\n  "text": "你是试试水"\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 查询文档\n\n * 词条查询:tern\n   * 词条查询不会分析查询条件 只有当词条和查询字符串完全匹配时才匹配搜索\n * 全文查询:match\n   * 全文查询会分析查询条件 先将查询条件进行分词 然后查询 求并集\n\nput person/_doc/2\n{\n  "name":"李四",\n  "age":30,\n  "address":"华为5g手机"\n}\n\nget person/_doc/2\n\n# term 词条查询 查询条件字符串和词条要完全匹配\n# es默认使用的分词器是standard 一个子一个词\nget person/_search\n{\n  "query":{\n    "term": {\n      "address": {\n        "value": "手机"\n      }\n    }\n  }\n}\n\ndelete person\n\n# 创建索引 添加映射 指定使用ik分词器\nput person\n{\n  "mappings": {\n    "properties": {\n      "name":{\n        "type": "keyword"\n      },\n      "address":{\n        "type": "text",\n        "analyzer": "ik_max_word"\n      }\n    }\n  }\n}\n\nget person\n\n# 添加文档\nput person/_doc/1\n{\n  "name":"张三",\n  "age":30,\n  "address":"华为5g手机"\n}\nput person/_doc/2\n{\n  "name":"李四",\n  "age":30,\n  "address":"广东"\n}\nput person/_doc/3\n{\n  "name":"王五",\n  "age":30,\n  "address":"小米5g手机"\n}\n\nget person/_search\n\n# 查询 term词条查询\nget person/_search\n{\n  "query":{\n    "term": {\n      "address": {\n        "value": "手机"\n      }\n    }\n  }\n}\n\n# match 先会对查询的字符串进行分词 在查询 求交集\nget person/_search\n{\n  "query": {\n    "match": {\n      "address": "苹果手机"\n    }\n  }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n\n# javaapi\n\n 1. 创建springboot项目 并引入es\n    \n    \n\n 2. 创建application.yml 配置文件\n    \n    elasticsearch:\n      host: 192.168.130.124\n      port: 9200\n    \n    \n    1\n    2\n    3\n    \n\n 3. 创建elasticsearchconfig配置类 加载配置文件 并返回一个es客户端对象\n    \n    package com.itheima.elasticsearchdemo.config;\n    \n    \n    import org.apache.http.httphost;\n    import org.elasticsearch.client.restclient;\n    import org.elasticsearch.client.resthighlevelclient;\n    import org.springframework.boot.context.properties.configurationproperties;\n    import org.springframework.context.annotation.bean;\n    import org.springframework.context.annotation.configuration;\n    \n    @configuration\n    @configurationproperties(prefix = "elasticsearch")\n    public class elasticsearchconfig {\n    \n        private string host;\n        private int port;\n    \n        public string gethost() {\n            return host;\n        }\n    \n        public void sethost(string host) {\n            this.host = host;\n        }\n    \n        public int getport() {\n            return port;\n        }\n    \n        public void setport(int port) {\n            this.port = port;\n        }\n    \n        @bean\n        public resthighlevelclient client(){\n            return new resthighlevelclient(restclient.builder(\n                    new httphost(host,port,"http")\n            ));\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    \n\n 4. 注入对象 使用\n    \n    //1.创建es客户端对象\n    @autowired\n    private resthighlevelclient resthighlevelclient;\n    \n    \n    1\n    2\n    3\n    \n\n\n# 添加索引\n\n//添加索引\n@test\nvoid addindex() throws ioexception {\n    //获取操作索引的对象\n    indicesclient indices = resthighlevelclient.indices();\n    //添加\n    createindexrequest createindex = new createindexrequest("itheima");  //索引名称\n    createindexresponse createindexresponse = indices.create(createindex, requestoptions.default);\n    //根据返回值判断结果\n    system.out.println(createindexresponse.isacknowledged());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 添加索引并添加映射\n\n//添加索引并添加映射\n@test\nvoid addindexandmapping() throws ioexception {\n    //获取操作索引的对象\n    indicesclient indices = resthighlevelclient.indices();\n    createindexrequest request = new createindexrequest("twitter");\n\n    // 向索引添加映射\n    request.source("{\\n" +\n            "    \\"settings\\" : {\\n" +\n            "        \\"number_of_shards\\" : 3,\\n" +\n            "        \\"number_of_replicas\\" : 2\\n" +\n            "    },\\n" +\n            "    \\"mappings\\" : {\\n" +\n            "        \\"tweet\\" : {\\n" +\n            "            \\"properties\\" : {\\n" +\n            "                \\"message\\" : { \\"type\\" : \\"text\\" }\\n" +\n            "            }\\n" +\n            "        }\\n" +\n            "    },\\n" +\n            "    \\"aliases\\" : {\\n" +\n            "        \\"twitter_alias\\" : {}\\n" +\n            "    }\\n" +\n            "}", xcontenttype.json);\n    createindexresponse createindexresponse = indices.create(request, requestoptions.default);\n    //根据返回值判断结果\n    system.out.println(createindexresponse.isacknowledged());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n# 查询索引\n\n//查询索引\n@test\npublic void queryindex() throws ioexception {\n    getindexrequest request = new getindexrequest("twitter");\n    getindexresponse getindexresponse = resthighlevelclient.indices().get(request, requestoptions.default);\n    system.out.println(getindexresponse.getaliases());\n    system.out.println(getindexresponse.getmappings());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 删除索引\n\n//删除索引\n@test\npublic void deleteindex() throws ioexception {\n    indicesclient indices = resthighlevelclient.indices();\n    deleteindexrequest deleteindexrequest = new deleteindexrequest("itheima");\n    acknowledgedresponse acknowledgedresponse = indices.delete(deleteindexrequest, requestoptions.default);\n    system.out.println(acknowledgedresponse.isacknowledged());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 判断索引是否存在\n\n//判断索引是否存在\n@test\npublic void existindex() throws ioexception {\n    indicesclient indices = resthighlevelclient.indices();\n    getindexrequest getrequest = new getindexrequest("itheima");\n    boolean exists = indices.exists(getrequest, requestoptions.default);\n    system.out.println(exists);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 添加/修改文档\n\n当id存在时则修改 不存在时则添加\n\n# map\n\n//添加/修改 map\n@test\npublic void adddoc() throws ioexception {\n    //添加数据对象 map\n    map<string, string> data = new hashmap<>();\n    data.put("address","背景");\n    data.put("name","钻石");\n    data.put("age","11");\n    //1.获取文档对象\n    indexrequest indexrequest = new indexrequest("twitter").id("1").source(data);\n    //添加数据\n    indexresponse index = resthighlevelclient.index(indexrequest, requestoptions.default);\n    //打印响应结果\n    system.out.println(index.getid());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# json\n\n//添加/修改 json\n@test\npublic void adddoc2() throws ioexception {\n   //创建对象\n    person p =new person();\n    p.setid("2");\n    p.setname("iekr");\n    p.setage(15);\n    p.setaddress("广东");\n    string json = json.tojsonstring(p);\n    //1.获取文档对象\n    indexrequest indexrequest = new indexrequest("twitter").id(p.getid()).source(json,xcontenttype.json);\n    //添加数据\n    indexresponse index = resthighlevelclient.index(indexrequest, requestoptions.default);\n    //打印响应结果\n    system.out.println(index.getid());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 查询文档\n\n    //根据id查询文档\n    @test\n    public void finddocbyid() throws ioexception {\n        getrequest getrequest = new getrequest("twitter", "1");\n//        getrequest.id("1");  //单独指定id\n        getresponse documentfields = resthighlevelclient.get(getrequest, requestoptions.default);\n        system.out.println(documentfields.getsourceasstring()); //获取json字符串\n\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 删除文档\n\n//根据id删除文档\n@test\npublic void deldoc() throws ioexception {\n    deleterequest delete = new deleterequest("twitter", "1");\n    deleteresponse response = resthighlevelclient.delete(delete, requestoptions.default);\n    system.out.println(response.getid()); \n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"ElasticSearch 高级",frontmatter:{title:"ElasticSearch 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/cf92bb/",categories:["后端","JavaEE"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/02.JavaEE/20.ElasticSearch%20%E9%AB%98%E7%BA%A7.html",relativePath:"后端/02.JavaEE/20.ElasticSearch 高级.md",key:"v-bf6ac024",path:"/pages/cf92bb/",headers:[{level:2,title:"批量操作",slug:"批量操作",normalizedTitle:"批量操作",charIndex:23},{level:3,title:"脚本操作",slug:"脚本操作",normalizedTitle:"脚本操作",charIndex:78},{level:3,title:"API",slug:"api",normalizedTitle:"api",charIndex:337},{level:2,title:"导入数据",slug:"导入数据",normalizedTitle:"导入数据",charIndex:1228},{level:3,title:"使用fastjson 转换对象不转换该成员变量",slug:"使用fastjson-转换对象不转换该成员变量",normalizedTitle:"使用fastjson 转换对象不转换该成员变量",charIndex:1333},{level:2,title:"matchALL查询",slug:"matchall查询",normalizedTitle:"matchall查询",charIndex:1418},{level:3,title:"脚本操作",slug:"脚本操作-2",normalizedTitle:"脚本操作",charIndex:78},{level:3,title:"Api操作",slug:"api操作",normalizedTitle:"api操作",charIndex:1623},{level:2,title:"term查询",slug:"term查询",normalizedTitle:"term查询",charIndex:2722},{level:3,title:"脚本查询",slug:"脚本查询",normalizedTitle:"脚本查询",charIndex:2733},{level:3,title:"api查询",slug:"api查询",normalizedTitle:"api查询",charIndex:2903},{level:2,title:"match查询",slug:"match查询",normalizedTitle:"match查询",charIndex:3782},{level:3,title:"脚本操作",slug:"脚本操作-3",normalizedTitle:"脚本操作",charIndex:78},{level:3,title:"API操作",slug:"api操作-2",normalizedTitle:"api操作",charIndex:4044},{level:2,title:"模糊查询",slug:"模糊查询",normalizedTitle:"模糊查询",charIndex:4972},{level:3,title:"脚本查询",slug:"脚本查询-2",normalizedTitle:"脚本查询",charIndex:2733},{level:3,title:"api操作",slug:"api操作-3",normalizedTitle:"api操作",charIndex:5539},{level:2,title:"范围查询",slug:"范围查询",normalizedTitle:"范围查询",charIndex:8214},{level:3,title:"脚本查询",slug:"脚本查询-3",normalizedTitle:"脚本查询",charIndex:2733},{level:3,title:"api查询",slug:"api查询-2",normalizedTitle:"api查询",charIndex:2903},{level:2,title:"queryString查询",slug:"querystring查询",normalizedTitle:"querystring查询",charIndex:9521},{level:3,title:"脚本查询",slug:"脚本查询-4",normalizedTitle:"脚本查询",charIndex:2733},{level:3,title:"Api查询",slug:"api查询-3",normalizedTitle:"api查询",charIndex:10099},{level:2,title:"布尔查询",slug:"布尔查询",normalizedTitle:"布尔查询",charIndex:11061},{level:3,title:"脚本查询",slug:"脚本查询-5",normalizedTitle:"脚本查询",charIndex:2733},{level:3,title:"API查询",slug:"api查询-4",normalizedTitle:"api查询",charIndex:12047},{level:2,title:"聚合查询",slug:"聚合查询",normalizedTitle:"聚合查询",charIndex:13213},{level:3,title:"脚本查询",slug:"脚本查询-6",normalizedTitle:"脚本查询",charIndex:2733},{level:3,title:"Api查询",slug:"api查询-5",normalizedTitle:"api查询",charIndex:10099},{level:2,title:"高亮查询",slug:"高亮查询",normalizedTitle:"高亮查询",charIndex:15473},{level:3,title:"脚本操作",slug:"脚本操作-4",normalizedTitle:"脚本操作",charIndex:78},{level:3,title:"Api操作",slug:"api操作-4",normalizedTitle:"api操作",charIndex:1623},{level:2,title:"重建索引&索引别名",slug:"重建索引-索引别名",normalizedTitle:"重建索引&amp;索引别名",charIndex:null},{level:2,title:"ES集群",slug:"es集群",normalizedTitle:"es集群",charIndex:18453},{level:3,title:"搭建",slug:"搭建",normalizedTitle:"搭建",charIndex:18695},{level:3,title:"使用Kibana配置和管理集群",slug:"使用kibana配置和管理集群",normalizedTitle:"使用kibana配置和管理集群",charIndex:22189},{level:3,title:"JavaApi访问集群",slug:"javaapi访问集群",normalizedTitle:"javaapi访问集群",charIndex:22750},{level:3,title:"集群原理",slug:"集群原理",normalizedTitle:"集群原理",charIndex:24689},{level:3,title:"脑裂",slug:"脑裂",normalizedTitle:"脑裂",charIndex:25480},{level:2,title:"集群扩容",slug:"集群扩容",normalizedTitle:"集群扩容",charIndex:26065}],headersStr:"批量操作 脚本操作 API 导入数据 使用fastjson 转换对象不转换该成员变量 matchALL查询 脚本操作 Api操作 term查询 脚本查询 api查询 match查询 脚本操作 API操作 模糊查询 脚本查询 api操作 范围查询 脚本查询 api查询 queryString查询 脚本查询 Api查询 布尔查询 脚本查询 API查询 聚合查询 脚本查询 Api查询 高亮查询 脚本操作 Api操作 重建索引&索引别名 ES集群 搭建 使用Kibana配置和管理集群 JavaApi访问集群 集群原理 脑裂 集群扩容",content:'# ElasticSearch 高级\n\n\n# 批量操作\n\nBulk 批量操作是将文档的增删改查一系列操作 通过一次请求 全部完成 减少网络传输次数\n\n\n# 脚本操作\n\nGET person/_search\n\n# 批量操作\nPOST _bulk\n{"delete":{"_index":"person","_id":"3"}}\n{"create":{"_index":"person","_id":"8"}}\n{"name":"hhh","age":88,"address":"qqqq"}\n{"update":{"_index":"person","_id":"2"}}\n{"doc":{"name":"qwedqd"}}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# API\n\n//批量操作 bulk\n@Test\npublic void testBulk() throws IOException {\n    //创建bulkrequest对象 整合所有操作\n    BulkRequest bulkRequest = new BulkRequest();\n    //添加操作\n    //删除1号操作\n    DeleteRequest deleteRequest = new DeleteRequest("person","1");\n    bulkRequest.add(deleteRequest);\n    //添加6号操作\n    Map map = new HashMap();\n    map.put("name","测试");\n    IndexRequest indexRequest = new IndexRequest("person").id("6").source(map);\n    bulkRequest.add(indexRequest);\n    //修改3号操作\n    Map map2 = new HashMap();\n    map2.put("name","测试3号");\n    UpdateRequest updateReqeust = new UpdateRequest("person","3").doc(map2);\n    bulkRequest.add(updateReqeust);\n    //执行批量操作\n    BulkResponse response = restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT);\n    RestStatus status = response.status();\n    System.out.println(status);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 导入数据\n\n将数据库中表数据导入到ElasticSearch中\n\n 1. 创建索引 并添加mapping\n 2. 创建pojo类 映射mybatis\n 3. 查询数据库\n 4. 使用Bulk 批量导入\n\n\n# 使用fastjson 转换对象不转换该成员变量\n\n在成员变量加上注解 使用@JSONField(serialize =false) 此成员变量不参与json转换\n\n\n# matchALL查询\n\nmatchALL查询 查询所有文档\n\n\n# 脚本操作\n\n# 默认情况下,es一次只展示10条数据  通过from控制页码 size控制每页展示条数\n\nGET person/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "from": 0,\n  "size": 100\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n# Api操作\n\n//matchall查询所有 分页操作\n@Test\npublic void testMatchALL() throws IOException {\n    //2通过索引查询\n    SearchRequest searchRequest = new SearchRequest("person");\n    //4创建查询条件构建器\n    SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n    //5查询条件\n    QueryBuilder query = QueryBuilders.matchAllQuery();  //查询所有文档\n    //6指定查询条件\n    sourceBuilder.query(query);\n    //分页查询\n    sourceBuilder.from(0); //第几页\n    sourceBuilder.size(100); //每页显示数\n    //3添加查询条件构建器\n    searchRequest.source(sourceBuilder);\n\n    //1查询 获取查询结果\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n\n    //7获取命中对象 hits\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# term查询\n\n\n# 脚本查询\n\n# term查询 词条查询 一般用于查分类\n\nGET person/_search\n{\n  "query": {\n    "term": {\n      "name": {\n        "value": "hhh"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# api查询\n\n//termQuery 词条查询\n@Test\npublic void testTermQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBulider=new SearchSourceBuilder();\n    QueryBuilder query= QueryBuilders.termQuery("name","hhh");\n    sourceBulider.query(query);\n    searchRequest.source(sourceBulider);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# match查询\n\n * 会对查询条件进行分词\n * 然后将分词后的查询条件和词条进行等值匹配\n * 默认取并集(OR) 交集为(AND)\n\n\n# 脚本操作\n\n# match 查询\n\nGET person/_search\n{\n  "query": {\n    "match": {\n      "name": {\n        "query": "hhh",\n        "operator": "or"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# API操作\n\n//match 词条查询\n@Test\npublic void testMatchQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBulider=new SearchSourceBuilder();\n    MatchQueryBuilder query= QueryBuilders.matchQuery("name","hhh");\n    query.operator(Operator.AND); //并集\n\n    sourceBulider.query(query);\n    searchRequest.source(sourceBulider);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 模糊查询\n\n * wildcard查询 会对查询条件进行分词 可以使用通配符 ?(任意单个字符) 和 * (0或多个字符)\n * regexp 查询 正则查询\n * prefix 前缀查询\n\n\n# 脚本查询\n\n# wildcard 查询 查询条件分词 模糊查询\nGET person/_search\n{\n  "query": {\n    "wildcard": {\n      "name": {\n        "value": "h"\n      }\n    }\n  }  \n}\n\n# 正则查询 查询条件分词 模糊查询\nGET person/_search\n{\n  "query": {\n    "regexp": {\n      "name": "\\\\q+(.)*"\n    }\n  }  \n}\n\n# 前缀查询\nGET person/_search\n{\n  "query": {\n    "prefix": {\n      "name": {\n        "value": "qwe"\n      }\n    }\n  }  \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# api操作\n\n//wildcard 模糊查询\n@Test\npublic void testWildcardQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBulider=new SearchSourceBuilder();\n    WildcardQueryBuilder query = QueryBuilders.wildcardQuery("name", "h*");\n    sourceBulider.query(query);\n    searchRequest.source(sourceBulider);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n//regexp  正则查询\n@Test\npublic void testrRegexpQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBulider=new SearchSourceBuilder();\n    RegexpQueryBuilder query = QueryBuilders.regexpQuery("name", "\\\\h*");\n    sourceBulider.query(query);\n    searchRequest.source(sourceBulider);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n//prefix   前缀查询\n@Test\npublic void testrPrefixQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBulider=new SearchSourceBuilder();\n    PrefixQueryBuilder query = QueryBuilders.prefixQuery("name", "qwe");\n    sourceBulider.query(query);\n    searchRequest.source(sourceBulider);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n\n\n\n# 范围查询\n\nrange范围查询 查询指定字段在指定范围内包含值\n\n\n# 脚本查询\n\n#范围查询\nGET perso/_search\n{\n  "query": {\n    "range": {\n      "age": {\n        "gte": 10,\n        "lte": 30\n      }\n    }\n  },\n  "sort": [\n    {\n      "age": {\n        "order": "desc"\n      }\n    }\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# api查询\n\n//range   范围查询\n@Test\npublic void testRangeQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBulider=new SearchSourceBuilder();\n    RangeQueryBuilder query = QueryBuilders.rangeQuery("age");//指定字段\n    query.gte("10"); //小于等于\n    query.lte("30"); //大于等于\n\n\n    sourceBulider.query(query);\n    sourceBulider.sort("age", SortOrder.ASC); //排序\n    searchRequest.source(sourceBulider);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# queryString查询\n\n * 会对查询条件进行分词\n * 然后将分词后的查询条件和词条进行等值匹配\n * 默认取并集\n * 可以指定多个查询字段\n\n\n# 脚本查询\n\n# queryString 查询\nGET person/_search\n{\n  "query": {\n    "query_string": {\n      "fields": ["name","address"],\n      "query": "华为 OR 手机"\n    }\n  }  \n}\n\n# SimpleQueryStringQuery是QueryStringQuery的简化版，其本身不支持 AND OR NOT 布尔运算关键字，这些关键字会被当做普通词语进行处理。\n# 可以通过 default_operator 指定查询字符串默认使用的运算方式，默认为 OR\nGET person/_search\n{\n  "query": {\n    "simple_query_string": {\n      "fields": ["name","address"],\n      "query": "华为 OR 手机"\n    }\n  }  \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# Api查询\n\n//queryString\n@Test\npublic void testQueryStringQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBulider=new SearchSourceBuilder();\n    QueryStringQueryBuilder query = QueryBuilders.queryStringQuery("华为").field("name").field("address").defaultOperator(Operator.OR);\n\n\n    sourceBulider.query(query);\n    searchRequest.source(sourceBulider);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 布尔查询\n\nboolQuery 对多个查询条件连接 组合查询\n\n * must (and) 条件必须成立\n * must_not (not) 条件必须不成立\n * should(or) 条件可以成立\n * filter 条件必须成功 性能比must高 不会计算得分 (当查询结果符合查询条件越多则得分越多)\n\n\n# 脚本查询\n\n# bool查询\nGET person/_search\n{\n  "query": {\n    "bool": {\n      "must": [\n        {"term": {\n          "name": {\n            "value": "张三"\n          }\n        }}\n      ]\n    }\n  }\n}\n\n#filter\nGET person/_search\n{\n  "query": {\n    "bool": {\n      "filter": [\n        {"term": {\n          "name": {\n            "value": "张三"\n          }\n        }}\n      ]\n    }\n  }\n}\n\n# 组合多条件查询\nGET person/_search\n{\n  "query": {\n    "bool": {\n      "must": [\n        {"term": {\n          "name": {\n            "value": "张三"\n          }\n        }}\n      ],\n      "filter": [\n        {\n          "term": {\n            "address": "5G"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n\n\n\n# API查询\n\n//boolQuery\n@Test\npublic void testBoolQueryQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBuilder=new SearchSourceBuilder();\n\n    //构建boolQuery\n    BoolQueryBuilder query = QueryBuilders.boolQuery();\n\n    //构建各个查询条件\n    TermQueryBuilder termQuery = QueryBuilders.termQuery("name", "张三");//查询名字为张三的\n    query.must(termQuery);\n    MatchQueryBuilder matchQuery = QueryBuilders.matchQuery("address", "5G"); //查询地址包含5g的\n    query.must(matchQuery);\n\n\n    sourceBuilder.query(query);\n    searchRequest.source(sourceBuilder);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 聚合查询\n\n * 指标聚合 相当于mysql的聚合函数 max min avg sum等\n * 桶聚合 相当于mysql的group by . 不要对text类型的数据进行分组 会失败\n\n\n# 脚本查询\n\n# 聚合查询\n\n# 指标聚合 聚合函数\nGET person/_search\n{\n  "query": {\n    "match": {\n      "name": "张三"\n    }\n  },\n  "aggs": {\n    "NAME": {\n      "max": {\n        "field": "age" \n      }\n    }\n  }\n}\n\n \n# 桶聚合 分组  通过 aggs\nGET person/_search\n{\n  "query": {\n    "match": {\n      "name": "张三"\n    }\n  },\n  "aggs": {\n    "zdymc": {\n      "terms": {\n        "field": "age",\n        "size": 10\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n\n\n\n# Api查询\n\n//聚合查询 桶聚合 分组\n@Test\npublic void testAggsQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBuilder=new SearchSourceBuilder();\n    MatchQueryBuilder query = QueryBuilders.matchQuery("name", "张三");\n    sourceBuilder.query(query);\n\n    /**\n     * terms 查询后结果名称\n     * field 条件字段\n     * size 每页展示的条数\n     */\n    TermsAggregationBuilder aggs = AggregationBuilders.terms("自定义名称").field("age").size(10);\n    sourceBuilder.aggregation(aggs);\n\n    searchRequest.source(sourceBuilder);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        System.out.println(sourceAsString);\n    }\n\n\n    //获取聚合结果\n    Aggregations aggregations = search.getAggregations();\n    Map<String, Aggregation> aggregationMap = aggregations.asMap();  //将结果转为map\n    Terms zdymc = (Terms) aggregationMap.get("自定义名称");\n    List<? extends Terms.Bucket> buckets = zdymc.getBuckets();\n\n    ArrayList list = new ArrayList<>();\n    for (Terms.Bucket bucket : buckets) {\n        Object key = bucket.getKey();\n        list.add(key);\n    }\n\n    for (Object o : list) {\n        System.out.println(o);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 高亮查询\n\n * 高亮字段\n * 前缀\n * 后缀 如果不设置前后缀 默认为em标签\n\n\n# 脚本操作\n\n# 高亮查询\nGET person/_search\n{\n  "query": {\n    "match": {\n      "address": "手机"\n    }\n  },\n  "highlight": {\n    "fields": {\n      "address": {\n        "pre_tags": "<font color=\'red\'>",\n        "post_tags": "</font>"\n      }\n    }\n  }\n  \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# Api操作\n\n//highlight 高亮查询\n@Test\npublic void testHighlightQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest("person");\n\n    SearchSourceBuilder sourceBuilder=new SearchSourceBuilder();\n    MatchQueryBuilder query = QueryBuilders.matchQuery("address", "手机");\n    sourceBuilder.query(query);\n\n    HighlightBuilder highlightBuilder = new HighlightBuilder();//高亮对象\n    highlightBuilder.field("address");  //字段\n    highlightBuilder.preTags("<font color=\'red\'>");  //前缀\n    highlightBuilder.postTags("</font>");  //后缀\n    sourceBuilder.highlighter(highlightBuilder);\n\n    searchRequest.source(sourceBuilder);\n\n\n    SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHits searchHits = search.getHits();\n    //获取总条数\n    long value = searchHits.getTotalHits().value;\n    System.out.println("总记录数" + value);\n    //获取hits数组\n    SearchHit[] hits = searchHits.getHits();\n    for (SearchHit hit : hits) {\n        String sourceAsString = hit.getSourceAsString(); //获取json字符串格式的数据\n        //todo 将对象转为json\n\n\n        Map<String, HighlightField> highlightFields = hit.getHighlightFields();  //获取高亮中的对象元素\n        HighlightField address = highlightFields.get("address");\n        Text[] fragments = address.fragments();  //获取元素中的高亮数组结果\n        //替换json中的成员变量数据\n        //todo\n\n        System.out.println(sourceAsString);\n        System.out.println(Arrays.toString(fragments));\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n# 重建索引&索引别名\n\nES的索引一旦创建,只允许添加字段 不允许改变字段 因为改变字段 需要重建倒排索引 影响内部缓存结构\n\n此时需要重建一个新的索引 并将原有索引的数据导入到新索引中\n\n# 重建索引\n\n# 新建索引 索引名称必须全部小写\nPUT stdent_index_v1\n{\n  "mappings": {\n    "properties": {\n      "birthday":{\n        "type": "date"\n      }\n    }\n  }\n}\n\nGET stdent_index_v1\n\nPUT stdent_index_v1/_doc/1\n{\n  "birthday":"1999-01-01"\n}\n\nGET stdent_index_v1/_search\n\n# 现在stdent_index_v1需要存储birthday为一个字符串\n# 1.创建新的索引v2\nPUT stdent_index_v2\n{\n  "mappings": {\n    "properties": {\n      "birthday":{\n        "type": "text"\n      }\n    }\n  }\n}\n#2.将旧索引的数据拷贝到新索引 使用_reindex\nPOST _reindex\n{\n  "source": {\n    "index": "stdent_index_v1"\n  },\n  "dest": {\n    "index": "stdent_index_v2"\n  }\n}\n\n\nGET stdent_index_v2/_search\n\n\nPUT stdent_index_v2/_doc/2\n{\n  "birthday":"199年124日"\n}\n\n\n# 索引别名  因为旧索引已经不使用 而我们代码中写的是旧索引名 无法正常运行 则需要别名\n# 1.删除旧索引\nDELETE stdent_index_v1\n# 2. 给新索引起别名为旧索引名\nPOST stdent_index_v2/_alias/stdent_index_v1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n\n# ES集群\n\nES天然支持分布式,并且分布式自动配置\n\n * 集群(cluster) 一组拥有共同的cluster name 节点\n * 节点(node) 集合中的一个ES实例\n * 索引(index) es存储数据的地方\n * 分片(shard) 索引可以被拆分为不同的部分进行存储 称为分片 在集群环境下 一个索引的不同可以拆分到不同节点中\n * 主分片(Primary shard) 相当于副本分片的定义\n * 副本分片 每个主分片可以有一个或多个副本 数据与主分片一样\n\n\n# 搭建\n\n 1. 准备3个集群 此处作伪集群 使用端口号区分\n    \n    cp -r elasticsearch-7.15.0 elasticsearch-7.15.0-1\n    cp -r elasticsearch-7.15.0 elasticsearch-7.15.0-2\n    cp -r elasticsearch-7.15.0 elasticsearch-7.15.0-3\n    \n    \n    1\n    2\n    3\n    \n\n 2. 创建日志和data目录 并授权给iekr用户\n    \n    cd /opt\n    mkdir logs\n    mkdir data\n    \n    #授权\n    chown -R iekr:iekr ./logs\n    chown -R iekr:iekr ./data\n    \n    chown -R iekr:iekr ./elasticsearch-7.15.0-1\n    chown -R iekr:iekr ./elasticsearch-7.15.0-2\n    chown -R iekr:iekr ./elasticsearch-7.15.0-3\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    \n\n 3. 修改三个集群的配置文件\n    \n    vim /opt/elasticsearch-7.15.0-1/config/elasticsearch.yml\n    \n    \n    1\n    \n    \n    # 集群名称  各个集群必须一致\n    cluster.name: itcast-es\n    # 节点名称  不能一致\n    node.name: iekr-1\n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    #最大集群数\n    node.max_local_storage_nodes: 3\n    #ip地址\n    network.host: 0.0.0.0\n    # 端口\n    http.port: 9201\n    #内部节点之间沟通端口\n    transport.tcp.port: 9700\n    #节点发现 es7.x才有\n    discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"]\n    #初始化一个新的集群时需要此配置来选举master\n    cluster.initial_master_nodes: ["iekr-1","iekr-2","iekr-3"]\n    #数据和存储路径\n    path.data: /opt/data\n    path.logs: /opt/logs\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n    \n    vim /opt/elasticsearch-7.15.0-2/config/elasticsearch.yml\n    \n    \n    1\n    \n    \n    # 集群名称  各个集群必须一致\n    cluster.name: itcast-es\n    # 节点名称  不能一致\n    node.name: iekr-2\n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    #最大集群数\n    node.max_local_storage_nodes: 3\n    #ip地址\n    network.host: 0.0.0.0\n    # 端口\n    http.port: 9202\n    #内部节点之间沟通端口\n    transport.tcp.port: 9800\n    #节点发现 es7.x才有\n    discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"]\n    #初始化一个新的集群时需要此配置来选举master\n    cluster.initial_master_nodes: ["iekr-1","iekr-2","iekr-3"]\n    #数据和存储路径\n    path.data: /opt/data\n    path.logs: /opt/logs\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n    \n    vim /opt/elasticsearch-7.15.0-3/config/elasticsearch.yml\n    \n    \n    1\n    \n    \n    # 集群名称  各个集群必须一致\n    cluster.name: itcast-es\n    # 节点名称  不能一致\n    node.name: iekr-3\n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    #最大集群数\n    node.max_local_storage_nodes: 3\n    #ip地址\n    network.host: 0.0.0.0\n    # 端口\n    http.port: 9203\n    #内部节点之间沟通端口\n    transport.tcp.port: 9900\n    #节点发现 es7.x才有\n    discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"]\n    #初始化一个新的集群时需要此配置来选举master\n    cluster.initial_master_nodes: ["iekr-1","iekr-2","iekr-3"]\n    #数据和存储路径\n    path.data: /opt/data\n    path.logs: /opt/logs\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n\n 4. ES默认占用1G 我们通过配置文件修改\n    \n    vim /opt/elasticsearch-7.15.0-1/config/jvm.options\n    \n    \n    1\n    \n    \n    -Xms256m\n    -Xmx256m\n    \n    \n    1\n    2\n    \n\n 5. 分别启动\n    \n    systemctl stop firewalld\n    su iekr\n    cd /opt/elasticsearch-7.15.0-1/bin/\n    ./elasticsearch\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 6. 访问 http://192.168.130.124:9201/_cat/health?v 节点状态\n\n\n\n\n# 使用Kibana配置和管理集群\n\n 1. 复制kibana\n    \n    cd /opt\n    cp -r kibana-7.15.0-linux-x86_64 kibana-7.15.0-linux-x86_64-cluster\n    \n    \n    1\n    2\n    \n\n 2. 修改kibana集群配置\n    \n    vim /opt/kibana-7.15.0-linux-x86_64-cluster/config/kibana.yml\n    #修改以下内容\n    elasticsearch.hosts: ["http://localhost:9201","http://localhost:9202","http://localhost:9203"]\n    \n    \n    1\n    2\n    3\n    \n\n 3. 启动\n    \n    cd /opt/kibana-7.15.0-linux-x86_64-cluster/bin/\n    ./kibana --allow-root\n    \n    \n    1\n    2\n    \n\n 4. 访问 http://192.168.130.124:5601/app/monitoring 查询集群节点信息\n\n\n# JavaApi访问集群\n\n 1. application.yml\n    \n    elasticsearch:\n      host: 192.168.130.124\n      port: 9201\n      host2: 192.168.130.124\n      port2: 9202\n      host3: 192.168.130.124\n      port3: 9203\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 2. config类 并注册ioc容器\n    \n    package com.itheima.elasticsearchdemo.config;\n    \n    \n    import org.apache.http.HttpHost;\n    import org.elasticsearch.client.RestClient;\n    import org.elasticsearch.client.RestHighLevelClient;\n    import org.springframework.boot.context.properties.ConfigurationProperties;\n    import org.springframework.context.annotation.Bean;\n    import org.springframework.context.annotation.Configuration;\n    \n    @Configuration\n    @ConfigurationProperties(prefix = "elasticsearch")\n    public class ElasticSearchConfig {\n    \n        private String host;\n        private int port;\n        private String host2;\n        private int port2;\n        private String host3;\n        private int port3;\n    \n        public String getHost() {\n            return host;\n        }\n    \n        public void setHost(String host) {\n            this.host = host;\n        }\n    \n        public int getPort() {\n            return port;\n        }\n    \n        public void setPort(int port) {\n            this.port = port;\n        }\n    \n        @Bean\n        public RestHighLevelClient client(){\n            return new RestHighLevelClient(RestClient.builder(\n                    new HttpHost(host,port,"http"),\n            new HttpHost(host2,port2,"http"),\n            new HttpHost(host3,port3,"http")\n            ));\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    \n\n\n# 集群原理\n\n# 分片配置\n\n * 在创建索引时 如果不指定分配配置 默认主分片1 副本分片1\n   \n   * \n\n * 在创建索引时 可以通过settings设置分片\n   \n   * PUT stdent_index_v3\n     {\n       "mappings": {\n         "properties": {\n           "birthday":{\n             "type": "date"\n           }\n         }\n       },\n       "settings": {\n         "number_of_shards": 3,\n         "number_of_replicas": 1\n       }\n     }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 分片与自平衡: es默认会交错存储分片 如果其中一个节点失效不影响访问 并且会自动将失效的分片归并到目前仍在线的节点上 节点重新上线归还分片\n\n * ES每个查询在每个分片中是单线程执行 但是可以并行处理多个分片\n\n * 分片数量一旦确定好了 不能修改 但是可以通过重建索引和索引别名来迁移\n\n * 索引分片推荐配置方案\n   \n   1. 每个分片推荐大小10-30GB\n   2. 分片数量推荐 = 节点数 * 1 ~ 3 倍\n\n# 路由原理\n\n * 文档存入对应的分片 es计算分片编号的过程 称为路由\n * 路由算法 shard_index = hash(id) % number_of_shards\n\n\n\n\n# 脑裂\n\n * 一个正常es集群中只有一个主节点 主节点负责管理整个集群 如创建或删除索引 跟踪哪些节点是集群的一部分 并决定哪些分片分配给相关的节点\n * 集群的所有节点都会选择同一个节点作为主节点\n * 脑裂问题的出现是因为从节点在选择主节点上出现分歧导致一个集群出现多个主节点从而集群分离,使得集群处于异常状态\n\n# 脑裂原因\n\n 1. 网络原因: 网络延迟 一般出现在外网集群\n\n 2. 节点负责 主节点的角色即为master又为data 当数据访问量较大时 可能导致Master节点停止响应(假死状态)\n    \n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 3. JVM内存回收\n    \n    * 当Master节点设置的JVM内存较小时 引发JVM的大规模内存回收 造成ES进程失去响应\n\n# 避免脑裂\n\n 1. 网络原因: discovery.zen.ping.timeout 超时时间配置大一些 默认为3S\n 2. 节点负责 角色分离 当主节点就不要当数据存储 当数据存储的就不要当主节点\n 3. 修改 jvm.options 的最大内存和最小内存 为服务器的内存一半\n\n\n# 集群扩容\n\n 1. 修改所有集群中的 配置文件 添加新的集群\n 2. 全部启动',normalizedContent:'# elasticsearch 高级\n\n\n# 批量操作\n\nbulk 批量操作是将文档的增删改查一系列操作 通过一次请求 全部完成 减少网络传输次数\n\n\n# 脚本操作\n\nget person/_search\n\n# 批量操作\npost _bulk\n{"delete":{"_index":"person","_id":"3"}}\n{"create":{"_index":"person","_id":"8"}}\n{"name":"hhh","age":88,"address":"qqqq"}\n{"update":{"_index":"person","_id":"2"}}\n{"doc":{"name":"qwedqd"}}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# api\n\n//批量操作 bulk\n@test\npublic void testbulk() throws ioexception {\n    //创建bulkrequest对象 整合所有操作\n    bulkrequest bulkrequest = new bulkrequest();\n    //添加操作\n    //删除1号操作\n    deleterequest deleterequest = new deleterequest("person","1");\n    bulkrequest.add(deleterequest);\n    //添加6号操作\n    map map = new hashmap();\n    map.put("name","测试");\n    indexrequest indexrequest = new indexrequest("person").id("6").source(map);\n    bulkrequest.add(indexrequest);\n    //修改3号操作\n    map map2 = new hashmap();\n    map2.put("name","测试3号");\n    updaterequest updatereqeust = new updaterequest("person","3").doc(map2);\n    bulkrequest.add(updatereqeust);\n    //执行批量操作\n    bulkresponse response = resthighlevelclient.bulk(bulkrequest, requestoptions.default);\n    reststatus status = response.status();\n    system.out.println(status);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 导入数据\n\n将数据库中表数据导入到elasticsearch中\n\n 1. 创建索引 并添加mapping\n 2. 创建pojo类 映射mybatis\n 3. 查询数据库\n 4. 使用bulk 批量导入\n\n\n# 使用fastjson 转换对象不转换该成员变量\n\n在成员变量加上注解 使用@jsonfield(serialize =false) 此成员变量不参与json转换\n\n\n# matchall查询\n\nmatchall查询 查询所有文档\n\n\n# 脚本操作\n\n# 默认情况下,es一次只展示10条数据  通过from控制页码 size控制每页展示条数\n\nget person/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "from": 0,\n  "size": 100\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n# api操作\n\n//matchall查询所有 分页操作\n@test\npublic void testmatchall() throws ioexception {\n    //2通过索引查询\n    searchrequest searchrequest = new searchrequest("person");\n    //4创建查询条件构建器\n    searchsourcebuilder sourcebuilder = new searchsourcebuilder();\n    //5查询条件\n    querybuilder query = querybuilders.matchallquery();  //查询所有文档\n    //6指定查询条件\n    sourcebuilder.query(query);\n    //分页查询\n    sourcebuilder.from(0); //第几页\n    sourcebuilder.size(100); //每页显示数\n    //3添加查询条件构建器\n    searchrequest.source(sourcebuilder);\n\n    //1查询 获取查询结果\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n\n    //7获取命中对象 hits\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# term查询\n\n\n# 脚本查询\n\n# term查询 词条查询 一般用于查分类\n\nget person/_search\n{\n  "query": {\n    "term": {\n      "name": {\n        "value": "hhh"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# api查询\n\n//termquery 词条查询\n@test\npublic void testtermquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebulider=new searchsourcebuilder();\n    querybuilder query= querybuilders.termquery("name","hhh");\n    sourcebulider.query(query);\n    searchrequest.source(sourcebulider);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# match查询\n\n * 会对查询条件进行分词\n * 然后将分词后的查询条件和词条进行等值匹配\n * 默认取并集(or) 交集为(and)\n\n\n# 脚本操作\n\n# match 查询\n\nget person/_search\n{\n  "query": {\n    "match": {\n      "name": {\n        "query": "hhh",\n        "operator": "or"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# api操作\n\n//match 词条查询\n@test\npublic void testmatchquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebulider=new searchsourcebuilder();\n    matchquerybuilder query= querybuilders.matchquery("name","hhh");\n    query.operator(operator.and); //并集\n\n    sourcebulider.query(query);\n    searchrequest.source(sourcebulider);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 模糊查询\n\n * wildcard查询 会对查询条件进行分词 可以使用通配符 ?(任意单个字符) 和 * (0或多个字符)\n * regexp 查询 正则查询\n * prefix 前缀查询\n\n\n# 脚本查询\n\n# wildcard 查询 查询条件分词 模糊查询\nget person/_search\n{\n  "query": {\n    "wildcard": {\n      "name": {\n        "value": "h"\n      }\n    }\n  }  \n}\n\n# 正则查询 查询条件分词 模糊查询\nget person/_search\n{\n  "query": {\n    "regexp": {\n      "name": "\\\\q+(.)*"\n    }\n  }  \n}\n\n# 前缀查询\nget person/_search\n{\n  "query": {\n    "prefix": {\n      "name": {\n        "value": "qwe"\n      }\n    }\n  }  \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# api操作\n\n//wildcard 模糊查询\n@test\npublic void testwildcardquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebulider=new searchsourcebuilder();\n    wildcardquerybuilder query = querybuilders.wildcardquery("name", "h*");\n    sourcebulider.query(query);\n    searchrequest.source(sourcebulider);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n//regexp  正则查询\n@test\npublic void testrregexpquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebulider=new searchsourcebuilder();\n    regexpquerybuilder query = querybuilders.regexpquery("name", "\\\\h*");\n    sourcebulider.query(query);\n    searchrequest.source(sourcebulider);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n//prefix   前缀查询\n@test\npublic void testrprefixquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebulider=new searchsourcebuilder();\n    prefixquerybuilder query = querybuilders.prefixquery("name", "qwe");\n    sourcebulider.query(query);\n    searchrequest.source(sourcebulider);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n\n\n\n# 范围查询\n\nrange范围查询 查询指定字段在指定范围内包含值\n\n\n# 脚本查询\n\n#范围查询\nget perso/_search\n{\n  "query": {\n    "range": {\n      "age": {\n        "gte": 10,\n        "lte": 30\n      }\n    }\n  },\n  "sort": [\n    {\n      "age": {\n        "order": "desc"\n      }\n    }\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# api查询\n\n//range   范围查询\n@test\npublic void testrangequery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebulider=new searchsourcebuilder();\n    rangequerybuilder query = querybuilders.rangequery("age");//指定字段\n    query.gte("10"); //小于等于\n    query.lte("30"); //大于等于\n\n\n    sourcebulider.query(query);\n    sourcebulider.sort("age", sortorder.asc); //排序\n    searchrequest.source(sourcebulider);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# querystring查询\n\n * 会对查询条件进行分词\n * 然后将分词后的查询条件和词条进行等值匹配\n * 默认取并集\n * 可以指定多个查询字段\n\n\n# 脚本查询\n\n# querystring 查询\nget person/_search\n{\n  "query": {\n    "query_string": {\n      "fields": ["name","address"],\n      "query": "华为 or 手机"\n    }\n  }  \n}\n\n# simplequerystringquery是querystringquery的简化版，其本身不支持 and or not 布尔运算关键字，这些关键字会被当做普通词语进行处理。\n# 可以通过 default_operator 指定查询字符串默认使用的运算方式，默认为 or\nget person/_search\n{\n  "query": {\n    "simple_query_string": {\n      "fields": ["name","address"],\n      "query": "华为 or 手机"\n    }\n  }  \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# api查询\n\n//querystring\n@test\npublic void testquerystringquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebulider=new searchsourcebuilder();\n    querystringquerybuilder query = querybuilders.querystringquery("华为").field("name").field("address").defaultoperator(operator.or);\n\n\n    sourcebulider.query(query);\n    searchrequest.source(sourcebulider);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 布尔查询\n\nboolquery 对多个查询条件连接 组合查询\n\n * must (and) 条件必须成立\n * must_not (not) 条件必须不成立\n * should(or) 条件可以成立\n * filter 条件必须成功 性能比must高 不会计算得分 (当查询结果符合查询条件越多则得分越多)\n\n\n# 脚本查询\n\n# bool查询\nget person/_search\n{\n  "query": {\n    "bool": {\n      "must": [\n        {"term": {\n          "name": {\n            "value": "张三"\n          }\n        }}\n      ]\n    }\n  }\n}\n\n#filter\nget person/_search\n{\n  "query": {\n    "bool": {\n      "filter": [\n        {"term": {\n          "name": {\n            "value": "张三"\n          }\n        }}\n      ]\n    }\n  }\n}\n\n# 组合多条件查询\nget person/_search\n{\n  "query": {\n    "bool": {\n      "must": [\n        {"term": {\n          "name": {\n            "value": "张三"\n          }\n        }}\n      ],\n      "filter": [\n        {\n          "term": {\n            "address": "5g"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n\n\n\n# api查询\n\n//boolquery\n@test\npublic void testboolqueryquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebuilder=new searchsourcebuilder();\n\n    //构建boolquery\n    boolquerybuilder query = querybuilders.boolquery();\n\n    //构建各个查询条件\n    termquerybuilder termquery = querybuilders.termquery("name", "张三");//查询名字为张三的\n    query.must(termquery);\n    matchquerybuilder matchquery = querybuilders.matchquery("address", "5g"); //查询地址包含5g的\n    query.must(matchquery);\n\n\n    sourcebuilder.query(query);\n    searchrequest.source(sourcebuilder);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 聚合查询\n\n * 指标聚合 相当于mysql的聚合函数 max min avg sum等\n * 桶聚合 相当于mysql的group by . 不要对text类型的数据进行分组 会失败\n\n\n# 脚本查询\n\n# 聚合查询\n\n# 指标聚合 聚合函数\nget person/_search\n{\n  "query": {\n    "match": {\n      "name": "张三"\n    }\n  },\n  "aggs": {\n    "name": {\n      "max": {\n        "field": "age" \n      }\n    }\n  }\n}\n\n \n# 桶聚合 分组  通过 aggs\nget person/_search\n{\n  "query": {\n    "match": {\n      "name": "张三"\n    }\n  },\n  "aggs": {\n    "zdymc": {\n      "terms": {\n        "field": "age",\n        "size": 10\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n\n\n\n# api查询\n\n//聚合查询 桶聚合 分组\n@test\npublic void testaggsquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebuilder=new searchsourcebuilder();\n    matchquerybuilder query = querybuilders.matchquery("name", "张三");\n    sourcebuilder.query(query);\n\n    /**\n     * terms 查询后结果名称\n     * field 条件字段\n     * size 每页展示的条数\n     */\n    termsaggregationbuilder aggs = aggregationbuilders.terms("自定义名称").field("age").size(10);\n    sourcebuilder.aggregation(aggs);\n\n    searchrequest.source(sourcebuilder);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        system.out.println(sourceasstring);\n    }\n\n\n    //获取聚合结果\n    aggregations aggregations = search.getaggregations();\n    map<string, aggregation> aggregationmap = aggregations.asmap();  //将结果转为map\n    terms zdymc = (terms) aggregationmap.get("自定义名称");\n    list<? extends terms.bucket> buckets = zdymc.getbuckets();\n\n    arraylist list = new arraylist<>();\n    for (terms.bucket bucket : buckets) {\n        object key = bucket.getkey();\n        list.add(key);\n    }\n\n    for (object o : list) {\n        system.out.println(o);\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n\n\n\n# 高亮查询\n\n * 高亮字段\n * 前缀\n * 后缀 如果不设置前后缀 默认为em标签\n\n\n# 脚本操作\n\n# 高亮查询\nget person/_search\n{\n  "query": {\n    "match": {\n      "address": "手机"\n    }\n  },\n  "highlight": {\n    "fields": {\n      "address": {\n        "pre_tags": "<font color=\'red\'>",\n        "post_tags": "</font>"\n      }\n    }\n  }\n  \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# api操作\n\n//highlight 高亮查询\n@test\npublic void testhighlightquery() throws ioexception {\n    searchrequest searchrequest = new searchrequest("person");\n\n    searchsourcebuilder sourcebuilder=new searchsourcebuilder();\n    matchquerybuilder query = querybuilders.matchquery("address", "手机");\n    sourcebuilder.query(query);\n\n    highlightbuilder highlightbuilder = new highlightbuilder();//高亮对象\n    highlightbuilder.field("address");  //字段\n    highlightbuilder.pretags("<font color=\'red\'>");  //前缀\n    highlightbuilder.posttags("</font>");  //后缀\n    sourcebuilder.highlighter(highlightbuilder);\n\n    searchrequest.source(sourcebuilder);\n\n\n    searchresponse search = resthighlevelclient.search(searchrequest, requestoptions.default);\n    searchhits searchhits = search.gethits();\n    //获取总条数\n    long value = searchhits.gettotalhits().value;\n    system.out.println("总记录数" + value);\n    //获取hits数组\n    searchhit[] hits = searchhits.gethits();\n    for (searchhit hit : hits) {\n        string sourceasstring = hit.getsourceasstring(); //获取json字符串格式的数据\n        //todo 将对象转为json\n\n\n        map<string, highlightfield> highlightfields = hit.gethighlightfields();  //获取高亮中的对象元素\n        highlightfield address = highlightfields.get("address");\n        text[] fragments = address.fragments();  //获取元素中的高亮数组结果\n        //替换json中的成员变量数据\n        //todo\n\n        system.out.println(sourceasstring);\n        system.out.println(arrays.tostring(fragments));\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n# 重建索引&索引别名\n\nes的索引一旦创建,只允许添加字段 不允许改变字段 因为改变字段 需要重建倒排索引 影响内部缓存结构\n\n此时需要重建一个新的索引 并将原有索引的数据导入到新索引中\n\n# 重建索引\n\n# 新建索引 索引名称必须全部小写\nput stdent_index_v1\n{\n  "mappings": {\n    "properties": {\n      "birthday":{\n        "type": "date"\n      }\n    }\n  }\n}\n\nget stdent_index_v1\n\nput stdent_index_v1/_doc/1\n{\n  "birthday":"1999-01-01"\n}\n\nget stdent_index_v1/_search\n\n# 现在stdent_index_v1需要存储birthday为一个字符串\n# 1.创建新的索引v2\nput stdent_index_v2\n{\n  "mappings": {\n    "properties": {\n      "birthday":{\n        "type": "text"\n      }\n    }\n  }\n}\n#2.将旧索引的数据拷贝到新索引 使用_reindex\npost _reindex\n{\n  "source": {\n    "index": "stdent_index_v1"\n  },\n  "dest": {\n    "index": "stdent_index_v2"\n  }\n}\n\n\nget stdent_index_v2/_search\n\n\nput stdent_index_v2/_doc/2\n{\n  "birthday":"199年124日"\n}\n\n\n# 索引别名  因为旧索引已经不使用 而我们代码中写的是旧索引名 无法正常运行 则需要别名\n# 1.删除旧索引\ndelete stdent_index_v1\n# 2. 给新索引起别名为旧索引名\npost stdent_index_v2/_alias/stdent_index_v1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n\n# es集群\n\nes天然支持分布式,并且分布式自动配置\n\n * 集群(cluster) 一组拥有共同的cluster name 节点\n * 节点(node) 集合中的一个es实例\n * 索引(index) es存储数据的地方\n * 分片(shard) 索引可以被拆分为不同的部分进行存储 称为分片 在集群环境下 一个索引的不同可以拆分到不同节点中\n * 主分片(primary shard) 相当于副本分片的定义\n * 副本分片 每个主分片可以有一个或多个副本 数据与主分片一样\n\n\n# 搭建\n\n 1. 准备3个集群 此处作伪集群 使用端口号区分\n    \n    cp -r elasticsearch-7.15.0 elasticsearch-7.15.0-1\n    cp -r elasticsearch-7.15.0 elasticsearch-7.15.0-2\n    cp -r elasticsearch-7.15.0 elasticsearch-7.15.0-3\n    \n    \n    1\n    2\n    3\n    \n\n 2. 创建日志和data目录 并授权给iekr用户\n    \n    cd /opt\n    mkdir logs\n    mkdir data\n    \n    #授权\n    chown -r iekr:iekr ./logs\n    chown -r iekr:iekr ./data\n    \n    chown -r iekr:iekr ./elasticsearch-7.15.0-1\n    chown -r iekr:iekr ./elasticsearch-7.15.0-2\n    chown -r iekr:iekr ./elasticsearch-7.15.0-3\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    \n\n 3. 修改三个集群的配置文件\n    \n    vim /opt/elasticsearch-7.15.0-1/config/elasticsearch.yml\n    \n    \n    1\n    \n    \n    # 集群名称  各个集群必须一致\n    cluster.name: itcast-es\n    # 节点名称  不能一致\n    node.name: iekr-1\n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    #最大集群数\n    node.max_local_storage_nodes: 3\n    #ip地址\n    network.host: 0.0.0.0\n    # 端口\n    http.port: 9201\n    #内部节点之间沟通端口\n    transport.tcp.port: 9700\n    #节点发现 es7.x才有\n    discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"]\n    #初始化一个新的集群时需要此配置来选举master\n    cluster.initial_master_nodes: ["iekr-1","iekr-2","iekr-3"]\n    #数据和存储路径\n    path.data: /opt/data\n    path.logs: /opt/logs\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n    \n    vim /opt/elasticsearch-7.15.0-2/config/elasticsearch.yml\n    \n    \n    1\n    \n    \n    # 集群名称  各个集群必须一致\n    cluster.name: itcast-es\n    # 节点名称  不能一致\n    node.name: iekr-2\n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    #最大集群数\n    node.max_local_storage_nodes: 3\n    #ip地址\n    network.host: 0.0.0.0\n    # 端口\n    http.port: 9202\n    #内部节点之间沟通端口\n    transport.tcp.port: 9800\n    #节点发现 es7.x才有\n    discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"]\n    #初始化一个新的集群时需要此配置来选举master\n    cluster.initial_master_nodes: ["iekr-1","iekr-2","iekr-3"]\n    #数据和存储路径\n    path.data: /opt/data\n    path.logs: /opt/logs\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n    \n    vim /opt/elasticsearch-7.15.0-3/config/elasticsearch.yml\n    \n    \n    1\n    \n    \n    # 集群名称  各个集群必须一致\n    cluster.name: itcast-es\n    # 节点名称  不能一致\n    node.name: iekr-3\n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    #最大集群数\n    node.max_local_storage_nodes: 3\n    #ip地址\n    network.host: 0.0.0.0\n    # 端口\n    http.port: 9203\n    #内部节点之间沟通端口\n    transport.tcp.port: 9900\n    #节点发现 es7.x才有\n    discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"]\n    #初始化一个新的集群时需要此配置来选举master\n    cluster.initial_master_nodes: ["iekr-1","iekr-2","iekr-3"]\n    #数据和存储路径\n    path.data: /opt/data\n    path.logs: /opt/logs\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n\n 4. es默认占用1g 我们通过配置文件修改\n    \n    vim /opt/elasticsearch-7.15.0-1/config/jvm.options\n    \n    \n    1\n    \n    \n    -xms256m\n    -xmx256m\n    \n    \n    1\n    2\n    \n\n 5. 分别启动\n    \n    systemctl stop firewalld\n    su iekr\n    cd /opt/elasticsearch-7.15.0-1/bin/\n    ./elasticsearch\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 6. 访问 http://192.168.130.124:9201/_cat/health?v 节点状态\n\n\n\n\n# 使用kibana配置和管理集群\n\n 1. 复制kibana\n    \n    cd /opt\n    cp -r kibana-7.15.0-linux-x86_64 kibana-7.15.0-linux-x86_64-cluster\n    \n    \n    1\n    2\n    \n\n 2. 修改kibana集群配置\n    \n    vim /opt/kibana-7.15.0-linux-x86_64-cluster/config/kibana.yml\n    #修改以下内容\n    elasticsearch.hosts: ["http://localhost:9201","http://localhost:9202","http://localhost:9203"]\n    \n    \n    1\n    2\n    3\n    \n\n 3. 启动\n    \n    cd /opt/kibana-7.15.0-linux-x86_64-cluster/bin/\n    ./kibana --allow-root\n    \n    \n    1\n    2\n    \n\n 4. 访问 http://192.168.130.124:5601/app/monitoring 查询集群节点信息\n\n\n# javaapi访问集群\n\n 1. application.yml\n    \n    elasticsearch:\n      host: 192.168.130.124\n      port: 9201\n      host2: 192.168.130.124\n      port2: 9202\n      host3: 192.168.130.124\n      port3: 9203\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n 2. config类 并注册ioc容器\n    \n    package com.itheima.elasticsearchdemo.config;\n    \n    \n    import org.apache.http.httphost;\n    import org.elasticsearch.client.restclient;\n    import org.elasticsearch.client.resthighlevelclient;\n    import org.springframework.boot.context.properties.configurationproperties;\n    import org.springframework.context.annotation.bean;\n    import org.springframework.context.annotation.configuration;\n    \n    @configuration\n    @configurationproperties(prefix = "elasticsearch")\n    public class elasticsearchconfig {\n    \n        private string host;\n        private int port;\n        private string host2;\n        private int port2;\n        private string host3;\n        private int port3;\n    \n        public string gethost() {\n            return host;\n        }\n    \n        public void sethost(string host) {\n            this.host = host;\n        }\n    \n        public int getport() {\n            return port;\n        }\n    \n        public void setport(int port) {\n            this.port = port;\n        }\n    \n        @bean\n        public resthighlevelclient client(){\n            return new resthighlevelclient(restclient.builder(\n                    new httphost(host,port,"http"),\n            new httphost(host2,port2,"http"),\n            new httphost(host3,port3,"http")\n            ));\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    \n\n\n# 集群原理\n\n# 分片配置\n\n * 在创建索引时 如果不指定分配配置 默认主分片1 副本分片1\n   \n   * \n\n * 在创建索引时 可以通过settings设置分片\n   \n   * put stdent_index_v3\n     {\n       "mappings": {\n         "properties": {\n           "birthday":{\n             "type": "date"\n           }\n         }\n       },\n       "settings": {\n         "number_of_shards": 3,\n         "number_of_replicas": 1\n       }\n     }\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 分片与自平衡: es默认会交错存储分片 如果其中一个节点失效不影响访问 并且会自动将失效的分片归并到目前仍在线的节点上 节点重新上线归还分片\n\n * es每个查询在每个分片中是单线程执行 但是可以并行处理多个分片\n\n * 分片数量一旦确定好了 不能修改 但是可以通过重建索引和索引别名来迁移\n\n * 索引分片推荐配置方案\n   \n   1. 每个分片推荐大小10-30gb\n   2. 分片数量推荐 = 节点数 * 1 ~ 3 倍\n\n# 路由原理\n\n * 文档存入对应的分片 es计算分片编号的过程 称为路由\n * 路由算法 shard_index = hash(id) % number_of_shards\n\n\n\n\n# 脑裂\n\n * 一个正常es集群中只有一个主节点 主节点负责管理整个集群 如创建或删除索引 跟踪哪些节点是集群的一部分 并决定哪些分片分配给相关的节点\n * 集群的所有节点都会选择同一个节点作为主节点\n * 脑裂问题的出现是因为从节点在选择主节点上出现分歧导致一个集群出现多个主节点从而集群分离,使得集群处于异常状态\n\n# 脑裂原因\n\n 1. 网络原因: 网络延迟 一般出现在外网集群\n\n 2. 节点负责 主节点的角色即为master又为data 当数据访问量较大时 可能导致master节点停止响应(假死状态)\n    \n    #是否有资格主节点\n    node.master: true\n    #是否存储数据\n    node.data: true\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 3. jvm内存回收\n    \n    * 当master节点设置的jvm内存较小时 引发jvm的大规模内存回收 造成es进程失去响应\n\n# 避免脑裂\n\n 1. 网络原因: discovery.zen.ping.timeout 超时时间配置大一些 默认为3s\n 2. 节点负责 角色分离 当主节点就不要当数据存储 当数据存储的就不要当主节点\n 3. 修改 jvm.options 的最大内存和最小内存 为服务器的内存一半\n\n\n# 集群扩容\n\n 1. 修改所有集群中的 配置文件 添加新的集群\n 2. 全部启动',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Nginx",frontmatter:{title:"Nginx",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/0aeef9/",categories:["后端","Linux"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/03.Linux/03.Nginx.html",relativePath:"后端/03.Linux/03.Nginx.md",key:"v-44752817",path:"/pages/0aeef9/",headers:[{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:12},{level:2,title:"发布项目",slug:"发布项目",normalizedTitle:"发布项目",charIndex:570}],headersStr:"安装 发布项目",content:"# Nginx\n\n\n# 安装\n\n上传Nginx文件\n\ntar -zxvf nginx-1.21.1.tar.gz\ncd nginx-1.21.1\nyum -y install gcc\nyum -y install pcre pcre-devel\nyum -y install zlib zlib-devel\nyum -y install openssl openssl-devel\nmake\nmake install\ncd /usr/local/nginx/sbin\n./nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 停止\n./nginx -s stop\n\n# 重启\n./nginx -s reload\n\n# 查看进程\nps -ef |grep nginx\n\n# 查看80端口\nnetstat -ntlp|grep 80\n\n# 开放80端口\nfirewall-cmd --zone=public --add-port=80/tcp --permanent\n\n# 重启防火墙\nfirewall-cmd --reload  \n\n# 查看已经开放的端口\nfirewall-cmd --list-ports  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 发布项目\n\n编辑nginx配置文件\n\nvi /usr/local/nginx/conf/nginx.conf\n\n\n1\n\n\n修改 server 下的location-root路径为要发布项目的路径\n\nserver {\n        listen       80;\n        server_name  localhost;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n            root   html;\n            index  index.html index.htm;\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n重启nginx服务器\n\n ./nginx -s stop\n\n ./nginx\n \n # 也可以另外指定nginx.conf\n /usr/local/nginx/conf/nginx -c /home/xxx/nginx.conf\n\n\n1\n2\n3\n4\n5\n6\n",normalizedContent:"# nginx\n\n\n# 安装\n\n上传nginx文件\n\ntar -zxvf nginx-1.21.1.tar.gz\ncd nginx-1.21.1\nyum -y install gcc\nyum -y install pcre pcre-devel\nyum -y install zlib zlib-devel\nyum -y install openssl openssl-devel\nmake\nmake install\ncd /usr/local/nginx/sbin\n./nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 停止\n./nginx -s stop\n\n# 重启\n./nginx -s reload\n\n# 查看进程\nps -ef |grep nginx\n\n# 查看80端口\nnetstat -ntlp|grep 80\n\n# 开放80端口\nfirewall-cmd --zone=public --add-port=80/tcp --permanent\n\n# 重启防火墙\nfirewall-cmd --reload  \n\n# 查看已经开放的端口\nfirewall-cmd --list-ports  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 发布项目\n\n编辑nginx配置文件\n\nvi /usr/local/nginx/conf/nginx.conf\n\n\n1\n\n\n修改 server 下的location-root路径为要发布项目的路径\n\nserver {\n        listen       80;\n        server_name  localhost;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n            root   html;\n            index  index.html index.htm;\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n重启nginx服务器\n\n ./nginx -s stop\n\n ./nginx\n \n # 也可以另外指定nginx.conf\n /usr/local/nginx/conf/nginx -c /home/xxx/nginx.conf\n\n\n1\n2\n3\n4\n5\n6\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Linux",frontmatter:{title:"Linux",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/124a07/",categories:["后端","Linux"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/03.Linux/01.Linux.html",relativePath:"后端/03.Linux/01.Linux.md",key:"v-06144067",path:"/pages/124a07/",headers:[{level:2,title:"帮助命令",slug:"帮助命令",normalizedTitle:"帮助命令",charIndex:12},{level:3,title:"man",slug:"man",normalizedTitle:"man",charIndex:21},{level:3,title:"help",slug:"help",normalizedTitle:"help",charIndex:51},{level:3,title:"XXX–help",slug:"xxx-help",normalizedTitle:"xxx–help",charIndex:81},{level:2,title:"文件",slug:"文件",normalizedTitle:"文件",charIndex:120},{level:3,title:"ls",slug:"ls",normalizedTitle:"ls",charIndex:38},{level:3,title:"显示当前目录",slug:"显示当前目录",normalizedTitle:"显示当前目录",charIndex:203},{level:3,title:"打开指定目录",slug:"打开指定目录",normalizedTitle:"打开指定目录",charIndex:219},{level:3,title:"创建目录",slug:"创建目录",normalizedTitle:"创建目录",charIndex:265},{level:3,title:"创建文件",slug:"创建文件",normalizedTitle:"创建文件",charIndex:324},{level:3,title:"删除目录",slug:"删除目录",normalizedTitle:"删除目录",charIndex:414},{level:2,title:"复制文件",slug:"复制文件",normalizedTitle:"复制文件",charIndex:513},{level:2,title:"移动文件",slug:"移动文件",normalizedTitle:"移动文件",charIndex:609},{level:2,title:"更改文件拥有者",slug:"更改文件拥有者",normalizedTitle:"更改文件拥有者",charIndex:695},{level:2,title:"更改文件用户组",slug:"更改文件用户组",normalizedTitle:"更改文件用户组",charIndex:761},{level:2,title:"清屏",slug:"清屏",normalizedTitle:"清屏",charIndex:794},{level:2,title:"重命名",slug:"重命名",normalizedTitle:"重命名",charIndex:544},{level:2,title:"打印字符",slug:"打印字符",normalizedTitle:"打印字符",charIndex:859},{level:2,title:"显示文本文件中的内容",slug:"显示文本文件中的内容",normalizedTitle:"显示文本文件中的内容",charIndex:1073},{level:2,title:"awk",slug:"awk",normalizedTitle:"awk",charIndex:1344},{level:2,title:"连接",slug:"连接",normalizedTitle:"连接",charIndex:1466},{level:2,title:"修改文件权限",slug:"修改文件权限",normalizedTitle:"修改文件权限",charIndex:1732},{level:2,title:"用户管理",slug:"用户管理",normalizedTitle:"用户管理",charIndex:2138},{level:2,title:"用户组",slug:"用户组",normalizedTitle:"用户组",charIndex:737},{level:2,title:"历史命令",slug:"历史命令",normalizedTitle:"历史命令",charIndex:2572},{level:2,title:"日期",slug:"日期",normalizedTitle:"日期",charIndex:2590},{level:3,title:"查看日期",slug:"查看日期",normalizedTitle:"查看日期",charIndex:2743},{level:2,title:"克隆修改Linux网卡属性",slug:"克隆修改linux网卡属性",normalizedTitle:"克隆修改linux网卡属性",charIndex:2881},{level:2,title:"VIM编辑",slug:"vim编辑",normalizedTitle:"vim编辑",charIndex:3171},{level:2,title:"查看当前进程",slug:"查看当前进程",normalizedTitle:"查看当前进程",charIndex:3397},{level:2,title:"iptables",slug:"iptables",normalizedTitle:"iptables",charIndex:3417},{level:2,title:"开放端口",slug:"开放端口",normalizedTitle:"开放端口",charIndex:3448},{level:2,title:"防火墙",slug:"防火墙",normalizedTitle:"防火墙",charIndex:4056},{level:2,title:"查看系统版本",slug:"查看系统版本",normalizedTitle:"查看系统版本",charIndex:4493},{level:2,title:"后台运行",slug:"后台运行",normalizedTitle:"后台运行",charIndex:4522},{level:2,title:"添加新的硬盘并挂载",slug:"添加新的硬盘并挂载",normalizedTitle:"添加新的硬盘并挂载",charIndex:4619},{level:3,title:"删除挂载",slug:"删除挂载",normalizedTitle:"删除挂载",charIndex:6402},{level:2,title:"搜索",slug:"搜索",normalizedTitle:"搜索",charIndex:6452},{level:3,title:"创建查找文件数据库索引",slug:"创建查找文件数据库索引",normalizedTitle:"创建查找文件数据库索引",charIndex:6620},{level:3,title:"查找文件指定内容",slug:"查找文件指定内容",normalizedTitle:"查找文件指定内容",charIndex:6673},{level:2,title:"压缩",slug:"压缩",normalizedTitle:"压缩",charIndex:6891},{level:3,title:"gzip",slug:"gzip",normalizedTitle:"gzip",charIndex:6898},{level:3,title:"zip",slug:"zip",normalizedTitle:"zip",charIndex:6899},{level:3,title:"tar打包",slug:"tar打包",normalizedTitle:"tar打包",charIndex:7103},{level:3,title:"bzip2",slug:"bzip2",normalizedTitle:"bzip2",charIndex:7358},{level:2,title:"进程",slug:"进程",normalizedTitle:"进程",charIndex:3401},{level:3,title:"结束进程",slug:"结束进程",normalizedTitle:"结束进程",charIndex:7506},{level:3,title:"进程树",slug:"进程树",normalizedTitle:"进程树",charIndex:7552},{level:3,title:"top 健康",slug:"top-健康",normalizedTitle:"top 健康",charIndex:7593},{level:2,title:"定时任务",slug:"定时任务",normalizedTitle:"定时任务",charIndex:7658},{level:3,title:"crond 启动定时服务",slug:"crond-启动定时服务",normalizedTitle:"crond 启动定时服务",charIndex:7667},{level:3,title:"crontab",slug:"crontab",normalizedTitle:"crontab",charIndex:7767},{level:3,title:"* 执行的任务",slug:"执行的任务",normalizedTitle:"* 执行的任务",charIndex:7846},{level:2,title:"安装软件",slug:"安装软件",normalizedTitle:"安装软件",charIndex:8145},{level:3,title:"rpm",slug:"rpm",normalizedTitle:"rpm",charIndex:8154},{level:3,title:"yum",slug:"yum",normalizedTitle:"yum",charIndex:8305},{level:2,title:"shutdown",slug:"shutdown",normalizedTitle:"shutdown",charIndex:8589}],headersStr:"帮助命令 man help XXX–help 文件 ls 显示当前目录 打开指定目录 创建目录 创建文件 删除目录 复制文件 移动文件 更改文件拥有者 更改文件用户组 清屏 重命名 打印字符 显示文本文件中的内容 awk 连接 修改文件权限 用户管理 用户组 历史命令 日期 查看日期 克隆修改Linux网卡属性 VIM编辑 查看当前进程 iptables 开放端口 防火墙 查看系统版本 后台运行 添加新的硬盘并挂载 删除挂载 搜索 创建查找文件数据库索引 查找文件指定内容 压缩 gzip zip tar打包 bzip2 进程 结束进程 进程树 top 健康 定时任务 crond 启动定时服务 crontab * 执行的任务 安装软件 rpm yum shutdown",content:"# Linux\n\n\n# 帮助命令\n\n\n# man\n\n帮助命令\n\n如:man ls\n\n按Q退出\n\n\n# help\n\n针对内部命令帮助\n\n如:help cd\n\n\n# XXX–help\n\n如:ls –help\n\n查看该命令内部的帮助文档\n\n\n# 文件\n\n\n# ls\n\n列出当前目录中的文件和目录\n\nls -a 显示所有文件和目录包括隐藏的\n\nls -l 所有文件和隐藏文件的 详细信息 可以缩写为 ll\n\n\n# 显示当前目录\n\npwd\n\n\n# 打开指定目录\n\ncd / #返回到root根目录\n\ncd ../ #返回上一级目录\n\n\n# 创建目录\n\nmkdir 目录名 如:mkdir a\n\n创建多级目录\n\ncd a/b/c -p #加入参数-p\n\n\n# 创建文件\n\ntouch 文件名 如:touch test.txt\n\ntouch a{1..10}.txt 创建 a1-a10的文件\n\nstat 文件名 查看文件的详细信息\n\n\n# 删除目录\n\nrmdir a\n\n删除多级目录\n\nrmdir a/b/c -p\n\n可以缩写为rm\n\nrm 目录名 -r 层叠递归的询问方式删除目录\n\nrm 目录名 -rf 以递归删除并且不询问\n\n\n# 复制文件\n\ncp 文件名 文件目录\n\ncp 文件名 文件目录/重命名文件名 (如果不指定则为原名称)\n\ncp xxx.txt ./home\n\ncp -r 复制该文件夹下的子目录和文件\n\n\n# 移动文件\n\nmv(move) 文件名 目录\n\nmv test.txt ./a\n\n移动并重命名\n\nmv 文件名 目录重命名名字 mv test.txt ../qaq\n\n\n# 更改文件拥有者\n\nchown 用户名 文件名 更改文件拥有者\n\nchown 用户名:用户组 文件名 更改文件拥有者和用户组\n\n\n# 更改文件用户组\n\nchgrp [选项] 用户组 文件路径\n\n\n# 清屏\n\nclear 或者 ctrl+l\n\n\n# 重命名\n\nmv 原文件名 重命名文件名\n\nmv test.txt qaq\n\n\n# 打印字符\n\necho 字符串\n\n如echo “abc”\n\n包含特殊字符\n\necho -e “abc \\t cd\\n” #加上-e参数\n\n重定向(覆盖文本中的内容)\n\necho “字符串” > 文件名(可写入文件)\n\n将字符串写入到文本中\n\n追加\n\necho “字符串” > >文件名(可写入文件)\n\n将字符串插入到文本中\n\ncat 不存在的目录 &>> error.log 将命令的失败结果 追加到error.log中\n\n\n# 显示文本文件中的内容\n\ncat 文件名 如:cat test.txt 一次性显示所有内容\n\nmore 文件名 如:more test.txt 当按下回车后展示下一行,按q退出,按空格展示下一屏\n\nless 文件名 如:less test.txt 一页一页展示文本内容,用paup和padn进行翻页\n\ntail 文件名 -n X 如:tail test.txt -n 2 只看该文件最后的X行\n\ntail 文件名 -f 可以动态查看文件变化\n\nhead 文件名 -n X 如:head test.txt -n 2 只看该文件前面的X行\n\n\n# awk\n\nark [选项] '语法' 文件名 查询文件中包含awk语法的文件\n\n * -F '字符' 使用 指定字符串切割\n\n * $ + 数字 获取第几段内容\n\n * $0 获取当前行内容\n\n * OFS=\"字符\" 将切割后的内容 以指定字符连接\n\n * print 打印\n\n * toupper() 将指定内容转换为大写 cat a.txt | awk -F ' ' '{print toupper($1)}'\n\n * tolower() 将指定内容转换为小写\n\n * length() 返回字符长度\n\n * 'BEGIN{初始化操作}\n   {每行都执行}\n   END{结束时操作}'\n   文件名\n   \n   \n   1\n   2\n   3\n   4\n   \n\n\n# 连接\n\nln 源文件名 自定义名字 硬连接\n\nln -s 源文件名 自定义名字 软连接\n\n\n# 修改文件权限\n\nchmod 用户组操作权限文件\n\n用户组:u g o a u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。\n\n操作: + - = + 表示增加权限、- 表示取消权限、= 表示唯一设定权限。\n\n权限:r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。\n\n如 chmod u-rw aa\n\nchmod可以使用数字代替chmod abc aa\n\n如chmod 777 aa 代表 u rwx g rwx o rwx\n\n其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。\n\n# r=4，w=2，x=1\n\n * 若要 rwx 属性则 4+2+1=7；\n * 若要 rw- 属性则 4+2=6；\n * 若要 r-x 属性则 4+1=5。\n\n\n# 用户管理\n\nuseradd 用户名 添加新用户\n\npasswd 新用户名 为新用户添加密码\n\nid 用户名 查看指定用户是否存在\n\ncat /etc/passwd 查看该centos中所有用户和组\n\nuserdel 用户名 删除该用户\n\nuserdel 用户名 -r 删除该用户和该用户家目录\n\nsu 用户名 切换用户\n\nsu - 用户名 切换用户并进入该用户家目录\n\nsudo 设置普通用户具有root权限\n\nusermod -g root(欲要加入的组) 用户名 修改用户组\n\nwhoaimi 当前登录用户信息\n\nlogname 当前登录用户信息\n\n\n# 用户组\n\ngroupadd (选项) 用户组名 创建用户组\n\ngroupmod (选项) 用户组名 修改用户组\n\ngrops 用户名 查询用户所属组\n\ngropdel 用户组名 删除用户组\n\ngpasswd (选项) 组名 管理指定组的用户\n\ngpasswd -a 用户名 用户组 将用户加入指定组\n\n\n# 历史命令\n\nhistory\n\n\n# 日期\n\ntimedatectl 根据时区校正时间\n\ntimedatectl list-timezones 查看当前国家下的时区\n\ntimedatectl set-timezone \"Asia/Shanghai\" 设置指定时区\n\ntimedatectl set-ntp true 关闭ntp同步时间\n\n\n# 查看日期\n\ndate +%Y 年\n\ndate +%m 月\n\ndate +%d 日\n\ndate +%Y-%m-%d 年月日\n\ndate +%H 时 M分 S秒\n\ndate -s \"2021-04-19\" 更改系统时间\n\ncal 当月日历\n\ncal 2021 当前年日历\n\n\n# 克隆修改Linux网卡属性\n\nvim /etc/udev/rules.d/70-persistent-ipoib.rules 修改mac\n\nvim /etc/sysconfig/network-scripts/ifcfg-ensXX 修改ip地址\n\nhostname:查看主机名\n\nhostname 主机名:修改主机名 重启后恢复 临时更改 如:hostname admin\n\n如要永久修改需要 更改 /etc/sysconfig/network文件\n\nvim /etc/sysconfig/network 修改主机名称\n\nvim /etc/hostname 修改主机名\n\n\n# VIM编辑\n\nvim 文件名 进入vim一般模式\n\n按I、a、o进入输入模式 按esc退出到一般模式\n\n:wq 退出vim编辑\n\n:q! 不保存退出\n\nvim 文件名 +指定行数 进入vim并光标在指定行数 如:vim a.txt +5\n\nyy 复制光标当前行\n\ny数字y 从当前行复制到n行\n\nu 撤回上一步\n\ndd 删除当前行\n\nd数字d 从当前行删除到n行\n\nx 删除一个字母\n\nX 退格键\n\nyw 复制一个单词\n\ndw 删除一个词\n\n\n\n\n# 查看当前进程\n\nps -aux\n\n\n# iptables\n\nnetstat -tnl #查看服务器目前开放端口\n\niptables -I INPUT -p tcp --dport 8000 -j ACCEPT #开启8000端口\n\niptables -L -n -v #查看已添加的iptables规则 查询编号id\n\niptables -D INPUT 编号id #删除指定编码的规则\n\niptables -F #一键清空所有规则\n\niptables -A INPUT -p tcp --dport 80 -j DROP #封指定的端口\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 开放端口\n\nfirewall-cmd --zone=public --add-port=8888/tcp --permanent\n# 命令含义：\n--zone #作用域\n--add-port=1935/tcp  #添加端口，格式为：端口/通讯协议\n--permanent  #永久生效，没有此参数重启后失效\n#重启\nfirewall-cmd --reload\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 查询端口是否开放\nfirewall-cmd --query-port=8080/tcp\n# 开放80端口\nfirewall-cmd --permanent --add-port=80/tcp\n# 移除端口\nfirewall-cmd --permanent --remove-port=8080/tcp\n#重启防火墙(修改配置后要重启防火墙)\nfirewall-cmd --reload #ubuntu 我干要重启\nservice iptables stop #停止\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n开放所有端口\n\n iptables -P INPUT ACCEPT\n iptables -P FORWARD ACCEPT\n iptables -P OUTPUT ACCEPT\n iptables -F\n iptables-save\n #建议删除 iptables\n sudoapt-get remove iptables\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 防火墙\n\nservice iptables status 防火墙状态\n\nservice iptables stop 关闭防火墙\n\nservice iptables start 启动防火墙\n\nchkconfig iptables off 禁止防火墙自启\n\nnetstat -nltp 查看聆听的端口\n\n\n# 查看系统版本\n\ngetconf LONG_BIT\n\n\n# 后台运行\n\nnohup 运行文件名/命令名 &\n\nnohup 命令 用途：执行命令，忽略挂起指令，可用于后台运行，关闭shell不受影响 语法： nohup COMMAND [ARG]\n\n\n# 添加新的硬盘并挂载\n\n我将使用VM来进行模拟\n\n先使用df看下我的电脑硬盘信息：\n\ndf -h\n\n\n\n可以看到只有一个sda1分区装载/boot,还有一个扩展分区\n\n查看dev下的硬盘：\n\n\n\n只有一个硬盘（两个分区）\n\n注意：\n\n如果你是IDE 接 口 硬 盘 :/dev/dh[a-z]，这里的硬盘名字应该是dh[a-z]开头\n\n如果你是SCSI 接 口 硬 盘 : / dev/[ a -z ],这里的硬盘名字应该是sd[a-z]开头\n\n接下来将进行另一块硬盘的安装：\n\n1.将硬盘装在电脑上，重启电脑，后查看/dev/ 下有没多了一块硬盘\n\n\n\n\n\n2.用fdisk对这块硬盘分区\n\nfdisk /dev/sdb\n\n\n\n按下m显示菜单：\n\n\n\n因为要新建分区选择n\n\n\n\n这里是问你是要建立主分区还是扩展分区，这里是第一次建立选择主分区p\n\n\n\n因为是MBR分区只能有4个分区，这里建立第一个分区，输入1\n\n\n\n这里问你个分区的起始扇区，这里直接回车（默认），相当于输入了2048\n\n\n\n这里问你的结束扇区，这里不需要计算，直接输入+1G 加号后面为这个分区的大小\n\n\n\n此时第一个分区已经建立，但还是在内存中并没有写到硬盘sdb中，所以直接输入w\n\n注意这里可以继续创建分区，完了再输入w，我这里只建立一个分区\n\n\n\n经过以上步骤后分区的建立已经完成，但是此时系统还无法识别分区表\n\n3.内核重新读取分区表\n\npartprobe /dev/sdb\n\n\n\n注意：这里是整个磁盘sdb，不是磁盘分区sdb1\n\n4.创建文件系统（格式化分区）\n\nLinux 中的主流的文件系统有：ext4和xfsd等\n\n\n\n这里我建立ext4文件系统\n\nmkfs.ext4 /dev/sdb1\n\n注意：这里是磁盘分区sdb1，不是整个磁盘sdb\n\n\n\n5.挂载\n\n在挂载之前你需要确定挂载的目录，我这里是/mnt/sdb1-zhi　　,没有目录的自己mkdir,这里目录最好建立在/mnt下，这个目录是专门挂载的，可以任意。\n\n将来这个分区就会与这个/mnt/sdb1-zhi目录建立联系\n\n手动挂载\n\nmount /dev/sdb1 /mnt/sdb1-zhi/\n\n\n\ndf一下：\n\n\n\n这里已经挂载成功，但是这只是一次性的，重启后就会消失\n\n永久挂载：\n\n要对/etc/fstab文件编辑\n\nvim /etc/fstab\n\n\n\n比如第一行中\n\n　硬盘路径　　　　　　　　　　　　　　　　　　　　　　　　　文件路径（挂载点）　　　　文件系统类型　　　　设备的自定义选项　　是否转存　　fsck的顺序\n\n\n1\n\n\n　/dev/mapper/CentOS-root　　　　　　　　　　　　　　　　　　 /         　xfs 　　　　　　 　defaults    0 　　　　　0\n\n\n1\n\n\n　UUID=e4ef36e1-0840-4a58-a4f7-c26f52ead6f1 　　　　　　　　/boot　　　　　　　　　xfs 　　　　　　　　defaults 　　　 0　　　　　 0\n\n\n1\n\n\n我们要在最后一行写入自己的分区与文件路径，可以仿照上面的写\n\n\n1\n\n\n　　/dev/sdb1 　　　　　　　　　　　　　　　　　　　　　　　　/mnt/sdb1-zhi 　　　　　ext4 　　　　　　　　defaults 　　　　0　　　　　 0\n\n\n1\n\n\n\n\n这里第一列也可以写入UUID\n\nUUID的查询：\n\nblkid\n\n\n\n转存：0 不转存，不备份　　1转存，备份\n\nfsck：开机检查磁盘的顺序　　0表示不检查　　1234....为检查顺序\n\n以上步骤完成后，还需要判断是否正确\n\nmount -a\n\n\n\n如果没有其他信息出现，表示你插入的正确，否则错误。\n\n如果错误且没有检查，开机后将进入紧急模式，无法开机\n\n最后开机重启后df一下，看看是否正常\n\n\n\n最后总结一下\n\n1.fdisk /dev/sdb\n\n2.partprobe　　/dev/sdb\n\n\\3. mkfs.ext4　　 /dev/sdb1\n\n4.挂载　　mount 　　/dev/sdb1　　/mnt/sdb1-zhi\n\nvim /etc/fstab\n\n5.mount -a\n\n\n# 删除挂载\n\numount 磁盘名 挂载点 如:umount /dev/sdb1 /sdb1\n\n\n# 搜索\n\nfind 搜索范围 选项(name-查询方式、user-用户名、size-文件大小、ctime-文件时间)\n\n查询指定文件:\n\nfind -name 文件名\n\nfind -name 文件名前面*(匹配包含此名字的文件)\n\n文件大小\n\nfind -size +1k(大于1k)\n\nfind -size -1k(小于1k)\n\n\n# 创建查找文件数据库索引\n\nupdatedb 创建locate数据库初始化\n\nlocate 文件名\n\n\n# 查找文件指定内容\n\ngrep 查找内容 源文件\n\n * -n(显示指定行号可省略)\n * --color 高亮显示关键字\n * -An 显示后面的n行\n * -Bn 显示前面的n行\n * -c 统计个数\n * -v 查询不包含该关键字的\n * -i 忽略大小写\n\ngrep - n 123 abc.txt\n\n管道符 | 将前一个命令的结果传递给后一个命令\n\n如:\n\nll | grep ini (会根据ll的结果查找指定内容)\n\n\n# 压缩\n\n\n# gzip\n\ngzip 文件名\n\ngunzip 文件名.gz 解压gz文件\n\n不保留原文件,只压缩文件不压缩文件夹\n\ngzip -dv 压缩文件 解压gz文件\n\n\n# zip\n\nzip -r(可省 递归操作目录) 文件名.zip 要压缩的内容 压缩zip文件\n\nunzip -d(可省指定解压文件的存放路径) 压缩文件.zip 解压zip文件\n\nunzip -l 压缩文件.zip 查看zip里面的文件\n\n\n# tar打包\n\ntar 选项 xxx.tar.gz 将要打包进去的内容\n\n * -c产生.tar打包文件\n * -v显示详情信息\n * -f指定压缩后的文件名\n * -z打包同时压缩\n * -x解包.tar文件\n * -C解包目录\n\n打包: tar -cvf xxx.tar ./*\n\n打包并压缩: tar -zcvf xxx.tar.gz ./*\n\n查看压缩包的文件：tar -ztvf xxx.gz\n\n解压:tar -xvf xxx.tar\n\ntar -zxvf xxx.tar.gz -C ./*\n\n\n# bzip2\n\nbzip2 文件 压缩成.bz2文件 并删除原来的文件\n\nbunzip2 -v(可省解压过程) 文件.bz2 解压\n\n\n# 进程\n\nps 查看当前系统进程状态\n\n-a 显示所有进程\n\n-u 显示所有用户的所有进程\n\n-x 显示没有终端的进程\n\nps -aux 查看所有进程\n\n\n# 结束进程\n\nkill -9(可省,强制结束进程) 进程号\n\nkillall 进程名\n\n\n# 进程树\n\npstree 选项(-p显示进程id,-u显示进程的所属用户)\n\n\n# top 健康\n\ntop 显示所有进程信息\n\ntop -c 显示进程的详细路径\n\ntop -p PID 显示指定的进程信息\n\n\n# 定时任务\n\n\n# crond 启动定时服务\n\n重新启动:service crond restart\n\n启动:systemctl start crond\n\n查看状态:systemctl status crond\n\n\n# crontab\n\ncrontab -e 编辑定时任务\n\ncrontab -l 查询任务\n\ncrontab -r 删除当前用户所有任务\n\n\n# * * * * * 执行的任务\n\n\n\n 1. * 一小时当中的第几分钟 0-59\n 2. * 小时 0-23\n 3. * 当月第几天 1-31\n 4. * 几月 1-12\n 5. * 星期几 0-7(0和7代表星期日)\n\n* 代表任何时间. 如第一个星则为 每分钟执行\n\n, 代表不连续时间 0 8,12,16 * * * 代表每天8点12点16点执行一次\n\n- 代表连续时间 0 5 * * 1-6 代表周一到周六 5点执行\n\n*/x 代表每隔多久执行一次 */10 * * * * 代表每隔10分钟执行一次\n\n30 * * * * /root/qb.sh > /dev/null 2>& &\n\n\n1\n\n\n\n# 安装软件\n\n\n# rpm\n\n# 查询\n\nrpm -qa 查询所有rpm软件包\n\nrpm -qa | grep rmp软件包 查看指定软件是否安装\n\n# 卸载\n\nrpm -e rpm软件包 卸载指定软件包\n\nrpm -e --nodeps 软件包 卸载rpm时不检测依赖\n\n# 安装\n\nrpm -ivh 软件包名\n\n\n# yum\n\nyum [选项] [参数]\n\n选项: -y 对所有提问都回答yes\n\n参数: install 安装、update 更新、check-update 检测是否有可用更新、remove 删除指定包、list 显示软件包信息、clean清理yum过期缓存、deplist显示yum软件包所有依赖更新\n\n如安装mysql:\n\n 1. 检测是否安装了mysql:rpm -qa | grep mysql\n 2. 下载mysql的repo源 wget http.....下载地址\n 3. rpm -ivh mysql软件包名\n 4. yum install 软件\n\n\n# shutdown\n\nshutdown 默认为1分钟关机\n\nshutdown -c 取消关机\n\nshutdown -h now 立刻关机\n\nshutdown -r 重启\n\nreboot",normalizedContent:"# linux\n\n\n# 帮助命令\n\n\n# man\n\n帮助命令\n\n如:man ls\n\n按q退出\n\n\n# help\n\n针对内部命令帮助\n\n如:help cd\n\n\n# xxx–help\n\n如:ls –help\n\n查看该命令内部的帮助文档\n\n\n# 文件\n\n\n# ls\n\n列出当前目录中的文件和目录\n\nls -a 显示所有文件和目录包括隐藏的\n\nls -l 所有文件和隐藏文件的 详细信息 可以缩写为 ll\n\n\n# 显示当前目录\n\npwd\n\n\n# 打开指定目录\n\ncd / #返回到root根目录\n\ncd ../ #返回上一级目录\n\n\n# 创建目录\n\nmkdir 目录名 如:mkdir a\n\n创建多级目录\n\ncd a/b/c -p #加入参数-p\n\n\n# 创建文件\n\ntouch 文件名 如:touch test.txt\n\ntouch a{1..10}.txt 创建 a1-a10的文件\n\nstat 文件名 查看文件的详细信息\n\n\n# 删除目录\n\nrmdir a\n\n删除多级目录\n\nrmdir a/b/c -p\n\n可以缩写为rm\n\nrm 目录名 -r 层叠递归的询问方式删除目录\n\nrm 目录名 -rf 以递归删除并且不询问\n\n\n# 复制文件\n\ncp 文件名 文件目录\n\ncp 文件名 文件目录/重命名文件名 (如果不指定则为原名称)\n\ncp xxx.txt ./home\n\ncp -r 复制该文件夹下的子目录和文件\n\n\n# 移动文件\n\nmv(move) 文件名 目录\n\nmv test.txt ./a\n\n移动并重命名\n\nmv 文件名 目录重命名名字 mv test.txt ../qaq\n\n\n# 更改文件拥有者\n\nchown 用户名 文件名 更改文件拥有者\n\nchown 用户名:用户组 文件名 更改文件拥有者和用户组\n\n\n# 更改文件用户组\n\nchgrp [选项] 用户组 文件路径\n\n\n# 清屏\n\nclear 或者 ctrl+l\n\n\n# 重命名\n\nmv 原文件名 重命名文件名\n\nmv test.txt qaq\n\n\n# 打印字符\n\necho 字符串\n\n如echo “abc”\n\n包含特殊字符\n\necho -e “abc \\t cd\\n” #加上-e参数\n\n重定向(覆盖文本中的内容)\n\necho “字符串” > 文件名(可写入文件)\n\n将字符串写入到文本中\n\n追加\n\necho “字符串” > >文件名(可写入文件)\n\n将字符串插入到文本中\n\ncat 不存在的目录 &>> error.log 将命令的失败结果 追加到error.log中\n\n\n# 显示文本文件中的内容\n\ncat 文件名 如:cat test.txt 一次性显示所有内容\n\nmore 文件名 如:more test.txt 当按下回车后展示下一行,按q退出,按空格展示下一屏\n\nless 文件名 如:less test.txt 一页一页展示文本内容,用paup和padn进行翻页\n\ntail 文件名 -n x 如:tail test.txt -n 2 只看该文件最后的x行\n\ntail 文件名 -f 可以动态查看文件变化\n\nhead 文件名 -n x 如:head test.txt -n 2 只看该文件前面的x行\n\n\n# awk\n\nark [选项] '语法' 文件名 查询文件中包含awk语法的文件\n\n * -f '字符' 使用 指定字符串切割\n\n * $ + 数字 获取第几段内容\n\n * $0 获取当前行内容\n\n * ofs=\"字符\" 将切割后的内容 以指定字符连接\n\n * print 打印\n\n * toupper() 将指定内容转换为大写 cat a.txt | awk -f ' ' '{print toupper($1)}'\n\n * tolower() 将指定内容转换为小写\n\n * length() 返回字符长度\n\n * 'begin{初始化操作}\n   {每行都执行}\n   end{结束时操作}'\n   文件名\n   \n   \n   1\n   2\n   3\n   4\n   \n\n\n# 连接\n\nln 源文件名 自定义名字 硬连接\n\nln -s 源文件名 自定义名字 软连接\n\n\n# 修改文件权限\n\nchmod 用户组操作权限文件\n\n用户组:u g o a u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。\n\n操作: + - = + 表示增加权限、- 表示取消权限、= 表示唯一设定权限。\n\n权限:r 表示可读取，w 表示可写入，x 表示可执行，x 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。\n\n如 chmod u-rw aa\n\nchmod可以使用数字代替chmod abc aa\n\n如chmod 777 aa 代表 u rwx g rwx o rwx\n\n其中a,b,c各为一个数字，分别表示user、group、及other的权限。\n\n# r=4，w=2，x=1\n\n * 若要 rwx 属性则 4+2+1=7；\n * 若要 rw- 属性则 4+2=6；\n * 若要 r-x 属性则 4+1=5。\n\n\n# 用户管理\n\nuseradd 用户名 添加新用户\n\npasswd 新用户名 为新用户添加密码\n\nid 用户名 查看指定用户是否存在\n\ncat /etc/passwd 查看该centos中所有用户和组\n\nuserdel 用户名 删除该用户\n\nuserdel 用户名 -r 删除该用户和该用户家目录\n\nsu 用户名 切换用户\n\nsu - 用户名 切换用户并进入该用户家目录\n\nsudo 设置普通用户具有root权限\n\nusermod -g root(欲要加入的组) 用户名 修改用户组\n\nwhoaimi 当前登录用户信息\n\nlogname 当前登录用户信息\n\n\n# 用户组\n\ngroupadd (选项) 用户组名 创建用户组\n\ngroupmod (选项) 用户组名 修改用户组\n\ngrops 用户名 查询用户所属组\n\ngropdel 用户组名 删除用户组\n\ngpasswd (选项) 组名 管理指定组的用户\n\ngpasswd -a 用户名 用户组 将用户加入指定组\n\n\n# 历史命令\n\nhistory\n\n\n# 日期\n\ntimedatectl 根据时区校正时间\n\ntimedatectl list-timezones 查看当前国家下的时区\n\ntimedatectl set-timezone \"asia/shanghai\" 设置指定时区\n\ntimedatectl set-ntp true 关闭ntp同步时间\n\n\n# 查看日期\n\ndate +%y 年\n\ndate +%m 月\n\ndate +%d 日\n\ndate +%y-%m-%d 年月日\n\ndate +%h 时 m分 s秒\n\ndate -s \"2021-04-19\" 更改系统时间\n\ncal 当月日历\n\ncal 2021 当前年日历\n\n\n# 克隆修改linux网卡属性\n\nvim /etc/udev/rules.d/70-persistent-ipoib.rules 修改mac\n\nvim /etc/sysconfig/network-scripts/ifcfg-ensxx 修改ip地址\n\nhostname:查看主机名\n\nhostname 主机名:修改主机名 重启后恢复 临时更改 如:hostname admin\n\n如要永久修改需要 更改 /etc/sysconfig/network文件\n\nvim /etc/sysconfig/network 修改主机名称\n\nvim /etc/hostname 修改主机名\n\n\n# vim编辑\n\nvim 文件名 进入vim一般模式\n\n按i、a、o进入输入模式 按esc退出到一般模式\n\n:wq 退出vim编辑\n\n:q! 不保存退出\n\nvim 文件名 +指定行数 进入vim并光标在指定行数 如:vim a.txt +5\n\nyy 复制光标当前行\n\ny数字y 从当前行复制到n行\n\nu 撤回上一步\n\ndd 删除当前行\n\nd数字d 从当前行删除到n行\n\nx 删除一个字母\n\nx 退格键\n\nyw 复制一个单词\n\ndw 删除一个词\n\n\n\n\n# 查看当前进程\n\nps -aux\n\n\n# iptables\n\nnetstat -tnl #查看服务器目前开放端口\n\niptables -i input -p tcp --dport 8000 -j accept #开启8000端口\n\niptables -l -n -v #查看已添加的iptables规则 查询编号id\n\niptables -d input 编号id #删除指定编码的规则\n\niptables -f #一键清空所有规则\n\niptables -a input -p tcp --dport 80 -j drop #封指定的端口\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 开放端口\n\nfirewall-cmd --zone=public --add-port=8888/tcp --permanent\n# 命令含义：\n--zone #作用域\n--add-port=1935/tcp  #添加端口，格式为：端口/通讯协议\n--permanent  #永久生效，没有此参数重启后失效\n#重启\nfirewall-cmd --reload\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 查询端口是否开放\nfirewall-cmd --query-port=8080/tcp\n# 开放80端口\nfirewall-cmd --permanent --add-port=80/tcp\n# 移除端口\nfirewall-cmd --permanent --remove-port=8080/tcp\n#重启防火墙(修改配置后要重启防火墙)\nfirewall-cmd --reload #ubuntu 我干要重启\nservice iptables stop #停止\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n开放所有端口\n\n iptables -p input accept\n iptables -p forward accept\n iptables -p output accept\n iptables -f\n iptables-save\n #建议删除 iptables\n sudoapt-get remove iptables\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 防火墙\n\nservice iptables status 防火墙状态\n\nservice iptables stop 关闭防火墙\n\nservice iptables start 启动防火墙\n\nchkconfig iptables off 禁止防火墙自启\n\nnetstat -nltp 查看聆听的端口\n\n\n# 查看系统版本\n\ngetconf long_bit\n\n\n# 后台运行\n\nnohup 运行文件名/命令名 &\n\nnohup 命令 用途：执行命令，忽略挂起指令，可用于后台运行，关闭shell不受影响 语法： nohup command [arg]\n\n\n# 添加新的硬盘并挂载\n\n我将使用vm来进行模拟\n\n先使用df看下我的电脑硬盘信息：\n\ndf -h\n\n\n\n可以看到只有一个sda1分区装载/boot,还有一个扩展分区\n\n查看dev下的硬盘：\n\n\n\n只有一个硬盘（两个分区）\n\n注意：\n\n如果你是ide 接 口 硬 盘 :/dev/dh[a-z]，这里的硬盘名字应该是dh[a-z]开头\n\n如果你是scsi 接 口 硬 盘 : / dev/[ a -z ],这里的硬盘名字应该是sd[a-z]开头\n\n接下来将进行另一块硬盘的安装：\n\n1.将硬盘装在电脑上，重启电脑，后查看/dev/ 下有没多了一块硬盘\n\n\n\n\n\n2.用fdisk对这块硬盘分区\n\nfdisk /dev/sdb\n\n\n\n按下m显示菜单：\n\n\n\n因为要新建分区选择n\n\n\n\n这里是问你是要建立主分区还是扩展分区，这里是第一次建立选择主分区p\n\n\n\n因为是mbr分区只能有4个分区，这里建立第一个分区，输入1\n\n\n\n这里问你个分区的起始扇区，这里直接回车（默认），相当于输入了2048\n\n\n\n这里问你的结束扇区，这里不需要计算，直接输入+1g 加号后面为这个分区的大小\n\n\n\n此时第一个分区已经建立，但还是在内存中并没有写到硬盘sdb中，所以直接输入w\n\n注意这里可以继续创建分区，完了再输入w，我这里只建立一个分区\n\n\n\n经过以上步骤后分区的建立已经完成，但是此时系统还无法识别分区表\n\n3.内核重新读取分区表\n\npartprobe /dev/sdb\n\n\n\n注意：这里是整个磁盘sdb，不是磁盘分区sdb1\n\n4.创建文件系统（格式化分区）\n\nlinux 中的主流的文件系统有：ext4和xfsd等\n\n\n\n这里我建立ext4文件系统\n\nmkfs.ext4 /dev/sdb1\n\n注意：这里是磁盘分区sdb1，不是整个磁盘sdb\n\n\n\n5.挂载\n\n在挂载之前你需要确定挂载的目录，我这里是/mnt/sdb1-zhi　　,没有目录的自己mkdir,这里目录最好建立在/mnt下，这个目录是专门挂载的，可以任意。\n\n将来这个分区就会与这个/mnt/sdb1-zhi目录建立联系\n\n手动挂载\n\nmount /dev/sdb1 /mnt/sdb1-zhi/\n\n\n\ndf一下：\n\n\n\n这里已经挂载成功，但是这只是一次性的，重启后就会消失\n\n永久挂载：\n\n要对/etc/fstab文件编辑\n\nvim /etc/fstab\n\n\n\n比如第一行中\n\n　硬盘路径　　　　　　　　　　　　　　　　　　　　　　　　　文件路径（挂载点）　　　　文件系统类型　　　　设备的自定义选项　　是否转存　　fsck的顺序\n\n\n1\n\n\n　/dev/mapper/centos-root　　　　　　　　　　　　　　　　　　 /         　xfs 　　　　　　 　defaults    0 　　　　　0\n\n\n1\n\n\n　uuid=e4ef36e1-0840-4a58-a4f7-c26f52ead6f1 　　　　　　　　/boot　　　　　　　　　xfs 　　　　　　　　defaults 　　　 0　　　　　 0\n\n\n1\n\n\n我们要在最后一行写入自己的分区与文件路径，可以仿照上面的写\n\n\n1\n\n\n　　/dev/sdb1 　　　　　　　　　　　　　　　　　　　　　　　　/mnt/sdb1-zhi 　　　　　ext4 　　　　　　　　defaults 　　　　0　　　　　 0\n\n\n1\n\n\n\n\n这里第一列也可以写入uuid\n\nuuid的查询：\n\nblkid\n\n\n\n转存：0 不转存，不备份　　1转存，备份\n\nfsck：开机检查磁盘的顺序　　0表示不检查　　1234....为检查顺序\n\n以上步骤完成后，还需要判断是否正确\n\nmount -a\n\n\n\n如果没有其他信息出现，表示你插入的正确，否则错误。\n\n如果错误且没有检查，开机后将进入紧急模式，无法开机\n\n最后开机重启后df一下，看看是否正常\n\n\n\n最后总结一下\n\n1.fdisk /dev/sdb\n\n2.partprobe　　/dev/sdb\n\n\\3. mkfs.ext4　　 /dev/sdb1\n\n4.挂载　　mount 　　/dev/sdb1　　/mnt/sdb1-zhi\n\nvim /etc/fstab\n\n5.mount -a\n\n\n# 删除挂载\n\numount 磁盘名 挂载点 如:umount /dev/sdb1 /sdb1\n\n\n# 搜索\n\nfind 搜索范围 选项(name-查询方式、user-用户名、size-文件大小、ctime-文件时间)\n\n查询指定文件:\n\nfind -name 文件名\n\nfind -name 文件名前面*(匹配包含此名字的文件)\n\n文件大小\n\nfind -size +1k(大于1k)\n\nfind -size -1k(小于1k)\n\n\n# 创建查找文件数据库索引\n\nupdatedb 创建locate数据库初始化\n\nlocate 文件名\n\n\n# 查找文件指定内容\n\ngrep 查找内容 源文件\n\n * -n(显示指定行号可省略)\n * --color 高亮显示关键字\n * -an 显示后面的n行\n * -bn 显示前面的n行\n * -c 统计个数\n * -v 查询不包含该关键字的\n * -i 忽略大小写\n\ngrep - n 123 abc.txt\n\n管道符 | 将前一个命令的结果传递给后一个命令\n\n如:\n\nll | grep ini (会根据ll的结果查找指定内容)\n\n\n# 压缩\n\n\n# gzip\n\ngzip 文件名\n\ngunzip 文件名.gz 解压gz文件\n\n不保留原文件,只压缩文件不压缩文件夹\n\ngzip -dv 压缩文件 解压gz文件\n\n\n# zip\n\nzip -r(可省 递归操作目录) 文件名.zip 要压缩的内容 压缩zip文件\n\nunzip -d(可省指定解压文件的存放路径) 压缩文件.zip 解压zip文件\n\nunzip -l 压缩文件.zip 查看zip里面的文件\n\n\n# tar打包\n\ntar 选项 xxx.tar.gz 将要打包进去的内容\n\n * -c产生.tar打包文件\n * -v显示详情信息\n * -f指定压缩后的文件名\n * -z打包同时压缩\n * -x解包.tar文件\n * -c解包目录\n\n打包: tar -cvf xxx.tar ./*\n\n打包并压缩: tar -zcvf xxx.tar.gz ./*\n\n查看压缩包的文件：tar -ztvf xxx.gz\n\n解压:tar -xvf xxx.tar\n\ntar -zxvf xxx.tar.gz -c ./*\n\n\n# bzip2\n\nbzip2 文件 压缩成.bz2文件 并删除原来的文件\n\nbunzip2 -v(可省解压过程) 文件.bz2 解压\n\n\n# 进程\n\nps 查看当前系统进程状态\n\n-a 显示所有进程\n\n-u 显示所有用户的所有进程\n\n-x 显示没有终端的进程\n\nps -aux 查看所有进程\n\n\n# 结束进程\n\nkill -9(可省,强制结束进程) 进程号\n\nkillall 进程名\n\n\n# 进程树\n\npstree 选项(-p显示进程id,-u显示进程的所属用户)\n\n\n# top 健康\n\ntop 显示所有进程信息\n\ntop -c 显示进程的详细路径\n\ntop -p pid 显示指定的进程信息\n\n\n# 定时任务\n\n\n# crond 启动定时服务\n\n重新启动:service crond restart\n\n启动:systemctl start crond\n\n查看状态:systemctl status crond\n\n\n# crontab\n\ncrontab -e 编辑定时任务\n\ncrontab -l 查询任务\n\ncrontab -r 删除当前用户所有任务\n\n\n# * * * * * 执行的任务\n\n\n\n 1. * 一小时当中的第几分钟 0-59\n 2. * 小时 0-23\n 3. * 当月第几天 1-31\n 4. * 几月 1-12\n 5. * 星期几 0-7(0和7代表星期日)\n\n* 代表任何时间. 如第一个星则为 每分钟执行\n\n, 代表不连续时间 0 8,12,16 * * * 代表每天8点12点16点执行一次\n\n- 代表连续时间 0 5 * * 1-6 代表周一到周六 5点执行\n\n*/x 代表每隔多久执行一次 */10 * * * * 代表每隔10分钟执行一次\n\n30 * * * * /root/qb.sh > /dev/null 2>& &\n\n\n1\n\n\n\n# 安装软件\n\n\n# rpm\n\n# 查询\n\nrpm -qa 查询所有rpm软件包\n\nrpm -qa | grep rmp软件包 查看指定软件是否安装\n\n# 卸载\n\nrpm -e rpm软件包 卸载指定软件包\n\nrpm -e --nodeps 软件包 卸载rpm时不检测依赖\n\n# 安装\n\nrpm -ivh 软件包名\n\n\n# yum\n\nyum [选项] [参数]\n\n选项: -y 对所有提问都回答yes\n\n参数: install 安装、update 更新、check-update 检测是否有可用更新、remove 删除指定包、list 显示软件包信息、clean清理yum过期缓存、deplist显示yum软件包所有依赖更新\n\n如安装mysql:\n\n 1. 检测是否安装了mysql:rpm -qa | grep mysql\n 2. 下载mysql的repo源 wget http.....下载地址\n 3. rpm -ivh mysql软件包名\n 4. yum install 软件\n\n\n# shutdown\n\nshutdown 默认为1分钟关机\n\nshutdown -c 取消关机\n\nshutdown -h now 立刻关机\n\nshutdown -r 重启\n\nreboot",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"java",frontmatter:{title:"java",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/a70c4f/",categories:["后端","Linux"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/03.Linux/04.java.html",relativePath:"后端/03.Linux/04.java.md",key:"v-424bdfb7",path:"/pages/a70c4f/",headers:[{level:2,title:"yum安装",slug:"yum安装",normalizedTitle:"yum安装",charIndex:11}],headersStr:"yum安装",content:"# java\n\n\n# yum安装\n\n# 查询java1.8版本\nyum list | grep java-1.8.0-openjdk\n\nyum -y install java-1.8.0-openjdk*\n\njava -version\n\njavac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nyum默认为我们配置环境变量 但是有部分应用无法使用 我们需要配置文件的全局变量\n\nvim /etc/profile\n\n\n1\n\n\nexport JAVA_HOME=/usr/lib/jvm/java\n\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/jre/lib/rt.jar\n\nexport PATH=$PATH:$JAVA_HOME/bin\n\n\n1\n2\n3\n4\n5\n\n\n更新配置\n\n. /etc/profile\necho $JAVA_HOME  \n\n\n1\n2\n",normalizedContent:"# java\n\n\n# yum安装\n\n# 查询java1.8版本\nyum list | grep java-1.8.0-openjdk\n\nyum -y install java-1.8.0-openjdk*\n\njava -version\n\njavac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nyum默认为我们配置环境变量 但是有部分应用无法使用 我们需要配置文件的全局变量\n\nvim /etc/profile\n\n\n1\n\n\nexport java_home=/usr/lib/jvm/java\n\nexport classpath=.:$java_home/lib/dt.jar:$java_home/lib/tools.jar:$java_home/jre/lib/rt.jar\n\nexport path=$path:$java_home/bin\n\n\n1\n2\n3\n4\n5\n\n\n更新配置\n\n. /etc/profile\necho $java_home  \n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Tomcat",frontmatter:{title:"Tomcat",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/f323f1/",categories:["后端","Linux"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/03.Linux/05.Tomcat.html",relativePath:"后端/03.Linux/05.Tomcat.md",key:"v-73daf8d7",path:"/pages/f323f1/",headers:[{level:2,title:"yum安装",slug:"yum安装",normalizedTitle:"yum安装",charIndex:117}],headersStr:"yum安装",content:"# Tomcat\n\n上传linux\n\ntar -zxvf apache-tomcat-9.0.52.tar.gz\n\ncd apache-tomcat-9.0.52/bin\n\n./startup.sh\n\n\n1\n2\n3\n4\n5\n\n\n\n# yum安装\n\nyum -y install tomcat\n\ncd /usr/share/tomcat\n\nsystemctl status tomcat\n\nsystemctl start tomcat\n\n# 安装web插件包\nyum install tomcat-webapps tomcat-admin-webapps\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nhttps://www.cnblogs.com/yoyoketang/p/10186513.html",normalizedContent:"# tomcat\n\n上传linux\n\ntar -zxvf apache-tomcat-9.0.52.tar.gz\n\ncd apache-tomcat-9.0.52/bin\n\n./startup.sh\n\n\n1\n2\n3\n4\n5\n\n\n\n# yum安装\n\nyum -y install tomcat\n\ncd /usr/share/tomcat\n\nsystemctl status tomcat\n\nsystemctl start tomcat\n\n# 安装web插件包\nyum install tomcat-webapps tomcat-admin-webapps\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nhttps://www.cnblogs.com/yoyoketang/p/10186513.html",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"常见的数据库产品",frontmatter:{title:"常见的数据库产品",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/e051f6/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/01.%E5%B8%B8%E8%A7%81%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%A7%E5%93%81.html",relativePath:"后端/04.SQL/01.常见的数据库产品.md",key:"v-a6e0fd06",path:"/pages/e051f6/",headersStr:null,content:"# 常见的数据库产品\n\n> Oracle:甲骨文==> 收购sun公司java==>sun收购mysql\n> \n> DB2:IBM\n> \n> SQL Sever:微软\n> \n> MySQL:AB 后被甲骨文收购",normalizedContent:"# 常见的数据库产品\n\n> oracle:甲骨文==> 收购sun公司java==>sun收购mysql\n> \n> db2:ibm\n> \n> sql sever:微软\n> \n> mysql:ab 后被甲骨文收购",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"shell",frontmatter:{title:"shell",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/f9e4a1/",categories:["后端","Linux"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/03.Linux/02.shell.html",relativePath:"后端/03.Linux/02.shell.md",key:"v-3d56846d",path:"/pages/f9e4a1/",headers:[{level:2,title:"变量",slug:"变量",normalizedTitle:"变量",charIndex:101},{level:2,title:"数组",slug:"数组",normalizedTitle:"数组",charIndex:391},{level:2,title:"运算符",slug:"运算符",normalizedTitle:"运算符",charIndex:572},{level:3,title:"字符串运算符",slug:"字符串运算符",normalizedTitle:"字符串运算符",charIndex:604},{level:3,title:"关系运算符",slug:"关系运算符",normalizedTitle:"关系运算符",charIndex:633},{level:3,title:"布尔运算符",slug:"布尔运算符",normalizedTitle:"布尔运算符",charIndex:645},{level:3,title:"$?",slug:"",normalizedTitle:"$?",charIndex:659},{level:2,title:"判断语句",slug:"判断语句",normalizedTitle:"判断语句",charIndex:699},{level:3,title:"选择语句",slug:"选择语句",normalizedTitle:"选择语句",charIndex:860},{level:2,title:"循环语句",slug:"循环语句",normalizedTitle:"循环语句",charIndex:1045},{level:2,title:"函数",slug:"函数",normalizedTitle:"函数",charIndex:1126},{level:2,title:"键盘录入",slug:"键盘录入",normalizedTitle:"键盘录入",charIndex:1329}],headersStr:"变量 数组 运算符 字符串运算符 关系运算符 布尔运算符 $? 判断语句 选择语句 循环语句 函数 键盘录入",content:'# shell\n\n以.sh后缀的文件夹\n\n使用vim编辑\n\n第一句恒定是#!bin/bash\n\nlinux语句\n\n获取.sh文件权限\n\n./文件名.sh 运行\n\nbash 文件名.sh 运行2\n\n\n# 变量\n\n * 普通变量\n   * 变量名=变量值 必须为一个整体,中间没有特殊字符\n   * 变量名=\'变量值\' 单引号中的内容原样赋值\n   * 变量名="变量值" 如果双引号里面有其他变量,那么会进行拼接后,再赋值\n * 命令变量 把命令执行的结果赋值给变量\n   * 变量名=`命令` 注意为反引号\n   * 变量名=$(命令)\n * 使用变量\n   * $变量名\n   * "$变量名"\n   * ${变量名}\n   * "${变量名}" 标准用法\n * 只读变量\n   * readonly 变量名 常量\n * 删除变量\n   * unset 变量名 删除变量\n\n\n# 数组\n\n * 定义数组\n   * 数组名=(值 值2...) arr=(1 2 3 4 5)\n * 使用索引赋值\n   * 数组名[索引]=值 arr[0]=2\n * 获取元素\n   * ${数组名[下标]} ${arr[0]}\n * 获取长度\n   * ${#数组名[*]} ${#arr[*]}\n   * ${#数组名[@]} ${#arr[@]}\n\n\n# 运算符\n\n\n\n需要在运算的数字 前加上关键字 expr\n\n\n# 字符串运算符\n\n\n\n字符串长度: "${#a}"\n\n\n# 关系运算符\n\n\n\n\n# 布尔运算符\n\n\n\n\n\n\n# $?\n\n$? 可以获取sh脚本上一条命令执行的结果,减少赋值和创建变量\n\n\n# 判断语句\n\nif[条件]\nthen\n\t语句体\nfi\n\nif[条件]\nthen\n\t语句体\nelse\n\t语句体\nfi\n\nif[条件]\nthen\n\t语句体\nelif[条件2]\n\t语句体\nelse\n\t语句体\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 选择语句\n\ncase 值 in\n模式1)\n\t语句体\n\t;;\n模式2)\n\t语句体\n\t;;\nesac\n\n\nv = "test"\ncase "${v}" in\n"123")\n\techo "123"\n\t;;\n"test")\n\techo "test"\n\t;;\nesac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 循环语句\n\nfor 变量 in 范围\ndo\n\t循环体\ndone\n\n\n1\n2\n3\n4\n\n\nwhile 条件\ndo\n\t循环体\ndone\n\n\n1\n2\n3\n4\n\n\n\n# 函数\n\n函数名(){\n\t函数体\n}\n\nfunction(){\n\techo "function"\n}\n\n# 直接用函数名调用函数\nfunction\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nfa(){\n\techo "$1"\n\techo "$2"\n\t# 返回值直接返回就好\n\treturn $(($1+$2))\n}\n\nfa 10 20\necho $?\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 键盘录入\n\nread 变量名',normalizedContent:'# shell\n\n以.sh后缀的文件夹\n\n使用vim编辑\n\n第一句恒定是#!bin/bash\n\nlinux语句\n\n获取.sh文件权限\n\n./文件名.sh 运行\n\nbash 文件名.sh 运行2\n\n\n# 变量\n\n * 普通变量\n   * 变量名=变量值 必须为一个整体,中间没有特殊字符\n   * 变量名=\'变量值\' 单引号中的内容原样赋值\n   * 变量名="变量值" 如果双引号里面有其他变量,那么会进行拼接后,再赋值\n * 命令变量 把命令执行的结果赋值给变量\n   * 变量名=`命令` 注意为反引号\n   * 变量名=$(命令)\n * 使用变量\n   * $变量名\n   * "$变量名"\n   * ${变量名}\n   * "${变量名}" 标准用法\n * 只读变量\n   * readonly 变量名 常量\n * 删除变量\n   * unset 变量名 删除变量\n\n\n# 数组\n\n * 定义数组\n   * 数组名=(值 值2...) arr=(1 2 3 4 5)\n * 使用索引赋值\n   * 数组名[索引]=值 arr[0]=2\n * 获取元素\n   * ${数组名[下标]} ${arr[0]}\n * 获取长度\n   * ${#数组名[*]} ${#arr[*]}\n   * ${#数组名[@]} ${#arr[@]}\n\n\n# 运算符\n\n\n\n需要在运算的数字 前加上关键字 expr\n\n\n# 字符串运算符\n\n\n\n字符串长度: "${#a}"\n\n\n# 关系运算符\n\n\n\n\n# 布尔运算符\n\n\n\n\n\n\n# $?\n\n$? 可以获取sh脚本上一条命令执行的结果,减少赋值和创建变量\n\n\n# 判断语句\n\nif[条件]\nthen\n\t语句体\nfi\n\nif[条件]\nthen\n\t语句体\nelse\n\t语句体\nfi\n\nif[条件]\nthen\n\t语句体\nelif[条件2]\n\t语句体\nelse\n\t语句体\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 选择语句\n\ncase 值 in\n模式1)\n\t语句体\n\t;;\n模式2)\n\t语句体\n\t;;\nesac\n\n\nv = "test"\ncase "${v}" in\n"123")\n\techo "123"\n\t;;\n"test")\n\techo "test"\n\t;;\nesac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 循环语句\n\nfor 变量 in 范围\ndo\n\t循环体\ndone\n\n\n1\n2\n3\n4\n\n\nwhile 条件\ndo\n\t循环体\ndone\n\n\n1\n2\n3\n4\n\n\n\n# 函数\n\n函数名(){\n\t函数体\n}\n\nfunction(){\n\techo "function"\n}\n\n# 直接用函数名调用函数\nfunction\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nfa(){\n\techo "$1"\n\techo "$2"\n\t# 返回值直接返回就好\n\treturn $(($1+$2))\n}\n\nfa 10 20\necho $?\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 键盘录入\n\nread 变量名',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"MySQL服务端",frontmatter:{title:"MySQL服务端",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/641ffa/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/04.MySQL%E6%9C%8D%E5%8A%A1%E7%AB%AF.html",relativePath:"后端/04.SQL/04.MySQL服务端.md",key:"v-3072118a",path:"/pages/641ffa/",headersStr:null,content:"# MySQL服务端\n\n> DBMS分两类:\n> \n>  1. 基于共享文件系统的DBMS(Access)\n> \n>  2. 基于客户机，服务器的DBMS C/S(client/sever)，底层是TCP/IP协议的程序\n>     \n>     Mysql、Oracle、sqlsever",normalizedContent:"# mysql服务端\n\n> dbms分两类:\n> \n>  1. 基于共享文件系统的dbms(access)\n> \n>  2. 基于客户机，服务器的dbms c/s(client/sever)，底层是tcp/ip协议的程序\n>     \n>     mysql、oracle、sqlsever",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数据库存储数据的特点",frontmatter:{title:"数据库存储数据的特点",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/d2b3d3/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/03.%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E7%82%B9.html",relativePath:"后端/04.SQL/03.数据库存储数据的特点.md",key:"v-31cc3bd2",path:"/pages/d2b3d3/",headersStr:null,content:"# 数据库存储数据的特点\n\n> 将数据放到表中，表再放到库中\n> \n> 一个数据库可以有多个表，每个表都有一个名字，用来标识自己。表名具有唯一性。",normalizedContent:"# 数据库存储数据的特点\n\n> 将数据放到表中，表再放到库中\n> \n> 一个数据库可以有多个表，每个表都有一个名字，用来标识自己。表名具有唯一性。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数据库相关概念",frontmatter:{title:"数据库相关概念",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4b2b60/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/02.%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5.html",relativePath:"后端/04.SQL/02.数据库相关概念.md",key:"v-b2a04030",path:"/pages/4b2b60/",headersStr:null,content:"# 数据库相关概念\n\n> DB\n> \n> 数据库(databese):存储数据的仓库。它保存了一系列有组织的数据\n> \n> DBMS\n> \n> 数据库管理系统(Datebese Management system)。数据库是通过DBMS创建和操作容器\n> \n> SQL\n> \n> 结构化查询语言(Structure Query Language):专门用来与数据库通信的语言",normalizedContent:"# 数据库相关概念\n\n> db\n> \n> 数据库(databese):存储数据的仓库。它保存了一系列有组织的数据\n> \n> dbms\n> \n> 数据库管理系统(datebese management system)。数据库是通过dbms创建和操作容器\n> \n> sql\n> \n> 结构化查询语言(structure query language):专门用来与数据库通信的语言",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"环境变量",frontmatter:{title:"环境变量",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/a4dffe/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/05.%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F.html",relativePath:"后端/04.SQL/05.环境变量.md",key:"v-3a8393e0",path:"/pages/a4dffe/",headersStr:null,content:"# 环境变量\n\n> 在用户里面添加 MYSQL_HOME = MYSQL的安装目录\n> \n> 在系统变量 Path 里面添加 MYSQL = MYSQL的安装目录/bin\n> \n> 即可在cmd里面输入mysql -uroot -p\n> \n> 如想连其他主机mysql则使用，mysql -hIP -P3306(端口) -uroot -p密码",normalizedContent:"# 环境变量\n\n> 在用户里面添加 mysql_home = mysql的安装目录\n> \n> 在系统变量 path 里面添加 mysql = mysql的安装目录/bin\n> \n> 即可在cmd里面输入mysql -uroot -p\n> \n> 如想连其他主机mysql则使用，mysql -hip -p3306(端口) -uroot -p密码",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"命令符指令",frontmatter:{title:"命令符指令",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/8c64bc/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/06.%E5%91%BD%E4%BB%A4%E7%AC%A6%E6%8C%87%E4%BB%A4.html",relativePath:"后端/04.SQL/06.命令符指令.md",key:"v-4e1ae1c0",path:"/pages/8c64bc/",headersStr:null,content:"# 命令符指令\n\n> show databases; 查看数据库\n> \n> use 库的名字; 进入指定的库\n> \n> show tables; 查看指定库的表\n> \n> show tables from 库的名字; 查看某个库的表，不进入库\n> \n> select database(); 查看当前在那个库\n> \n> create table 表的名字( 在当前库创建新的表\n> \n>  1. mysql里面没有字符串，单字符多字符都叫字符 varchar() 括号里面控制有多少个字符\n>     \n>     如需要限制单个字符 可以填 char， 或者varchar(1)\n> \n>  2. datetime, 时间类型\n> \n>  3. int 数字类型\n> \n>  4. 创建完表在后面加上); ，命令符里面都是以分号作结束字段\n> \n> desc 表的名字; 查看表的结构 (describe 描述)\n> \n> selct * from 表的名字; 查看表的数据\n> \n> insert into 表的名字 values(); 在表中插入新的数据，括号里面填入要插入的数据\n> \n> set names gbk; 如果插入失败可能是编码问题，如果改gbk不行尝试改成utf8\n> \n> update 表的名字 set 列的名字=‘XXXX’ where(相当于if) 其中一项列的名字(判断条件);\n> \n> delete from 表的名字 where 列的名字=“XXXX” 删除指定列\n> \n> alter table 表的名字 add column 列的名字 数据类型 在表中新建类别列\n> \n> drop table 表的名字 删除指定表的\n> \n> exit; 退出\n> \n> #注释\n> \n> –- 注释\n> \n> /*注释* #注释不能嵌套",normalizedContent:"# 命令符指令\n\n> show databases; 查看数据库\n> \n> use 库的名字; 进入指定的库\n> \n> show tables; 查看指定库的表\n> \n> show tables from 库的名字; 查看某个库的表，不进入库\n> \n> select database(); 查看当前在那个库\n> \n> create table 表的名字( 在当前库创建新的表\n> \n>  1. mysql里面没有字符串，单字符多字符都叫字符 varchar() 括号里面控制有多少个字符\n>     \n>     如需要限制单个字符 可以填 char， 或者varchar(1)\n> \n>  2. datetime, 时间类型\n> \n>  3. int 数字类型\n> \n>  4. 创建完表在后面加上); ，命令符里面都是以分号作结束字段\n> \n> desc 表的名字; 查看表的结构 (describe 描述)\n> \n> selct * from 表的名字; 查看表的数据\n> \n> insert into 表的名字 values(); 在表中插入新的数据，括号里面填入要插入的数据\n> \n> set names gbk; 如果插入失败可能是编码问题，如果改gbk不行尝试改成utf8\n> \n> update 表的名字 set 列的名字=‘xxxx’ where(相当于if) 其中一项列的名字(判断条件);\n> \n> delete from 表的名字 where 列的名字=“xxxx” 删除指定列\n> \n> alter table 表的名字 add column 列的名字 数据类型 在表中新建类别列\n> \n> drop table 表的名字 删除指定表的\n> \n> exit; 退出\n> \n> #注释\n> \n> –- 注释\n> \n> /*注释* #注释不能嵌套",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数据库",frontmatter:{title:"数据库",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/04fedb/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/08.%E6%95%B0%E6%8D%AE%E5%BA%93.html",relativePath:"后端/04.SQL/08.数据库.md",key:"v-3049bd10",path:"/pages/04fedb/",headersStr:null,content:"# 数据库\n\n * 查询所有数据库 show database;\n * 查询数据库创建时的语句 show create database 数据库名;\n * 创建数据库 create database 数据库名;\n * 创建时判断是否存在 create database if not exists 数据库名;\n * 指定字符 create database 数据库名 character set utf8;\n * 修改数据库字符 alter database 数据库名 character utf8;\n * 删除数据库 drop database 数据库名;\n * 删除判断 drop database if exists 数据库名;\n * 使用数据库 use 数据库名;\n * 当前数据库 select database();",normalizedContent:"# 数据库\n\n * 查询所有数据库 show database;\n * 查询数据库创建时的语句 show create database 数据库名;\n * 创建数据库 create database 数据库名;\n * 创建时判断是否存在 create database if not exists 数据库名;\n * 指定字符 create database 数据库名 character set utf8;\n * 修改数据库字符 alter database 数据库名 character utf8;\n * 删除数据库 drop database 数据库名;\n * 删除判断 drop database if exists 数据库名;\n * 使用数据库 use 数据库名;\n * 当前数据库 select database();",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"SQL语句",frontmatter:{title:"SQL语句",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4c9417/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/07.SQL%E8%AF%AD%E5%8F%A5.html",relativePath:"后端/04.SQL/07.SQL语句.md",key:"v-2b35c500",path:"/pages/4c9417/",headersStr:null,content:"# SQL语句\n\nSQL基本可以用在所有关系型数据库中，除了某些特有的指令\n\nDDL(Data Definition Language):数据定义语言，用来定义数据库对象:库、表、列等； 常见指令有:create/drop/alter\n\nDML(Data Manipulation language):数据操作语言，用来定义数据库记录(数据)； Insert/updata/delete\n\nDCL(Data Control Language):数据控制语言，用来定义访问权限和安全级别；\n\nTCL(Transaction Control Language)\n\nDQL(Data Query Language):数据查询语言，用来查询记录(数据)。 select",normalizedContent:"# sql语句\n\nsql基本可以用在所有关系型数据库中，除了某些特有的指令\n\nddl(data definition language):数据定义语言，用来定义数据库对象:库、表、列等； 常见指令有:create/drop/alter\n\ndml(data manipulation language):数据操作语言，用来定义数据库记录(数据)； insert/updata/delete\n\ndcl(data control language):数据控制语言，用来定义访问权限和安全级别；\n\ntcl(transaction control language)\n\ndql(data query language):数据查询语言，用来查询记录(数据)。 select",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"约束",frontmatter:{title:"约束",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/de8702/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/10.%E7%BA%A6%E6%9D%9F.html",relativePath:"后端/04.SQL/10.约束.md",key:"v-64db6885",path:"/pages/de8702/",headersStr:null,content:"# 约束\n\n\n\n * 主键约束 默认包含非空和唯一 一张表只有一个主键\n   \n   * 创建 create table 表名 (列名 数据类型 primary key)\n   * 删除pk alter table 表名 drop primary key;\n   * 建表后添加主键 alter table 表名 modify 列名 数据类型 primary key;\n\n * 自增约束 必须要配合其他约束一起使用 如主键\n   \n   * 创建 create table 表名(列名 数据类型 primary key auto_increment);\n   * 删除 alter table 表名 modify 列名 数据类型\n   * 建表后添加 alter table 表名 modify 列名 数据类型 auto_increment;\n\n * 唯一约束 不重复\n   \n   * 创建 create table 表名(列名 数据类型 unique);\n   * 删除 alter table 表名 drop index 列名;\n   * 建表后添加 alter table 表名 modify 列名 数据类型 unique;\n\n * 非空约束 不允许为NULL\n   \n   * 创建 create table 表名(列名 数据类型 not null);\n   * 删除 alter table 表名 modify 列名 数据类型;\n   * 建表后添加 alter table 表名 modify 列名 数据类型 not null;\n\n * 外键约束 表与表之间有关联性 保证数据的准确性\n   \n   * 创建 create table 表名(列名 数据类型 约束) constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名);\n   * 删除 alter table 表名drop foreign key 外键名;\n   * 建表后添加 alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名);\n\n * 外键级联更新 当主表的主键更新时 从表的外键也更新\n   \n   * alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名) on update cascade;\n\n * 外键级联删除 当主表的主键删除时 从表的外键也删除\n   \n   * alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名) on delete cascade;\n\n * 级联同时更新和删除\n   \n   * alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名) on update cascade on delete cascade;",normalizedContent:"# 约束\n\n\n\n * 主键约束 默认包含非空和唯一 一张表只有一个主键\n   \n   * 创建 create table 表名 (列名 数据类型 primary key)\n   * 删除pk alter table 表名 drop primary key;\n   * 建表后添加主键 alter table 表名 modify 列名 数据类型 primary key;\n\n * 自增约束 必须要配合其他约束一起使用 如主键\n   \n   * 创建 create table 表名(列名 数据类型 primary key auto_increment);\n   * 删除 alter table 表名 modify 列名 数据类型\n   * 建表后添加 alter table 表名 modify 列名 数据类型 auto_increment;\n\n * 唯一约束 不重复\n   \n   * 创建 create table 表名(列名 数据类型 unique);\n   * 删除 alter table 表名 drop index 列名;\n   * 建表后添加 alter table 表名 modify 列名 数据类型 unique;\n\n * 非空约束 不允许为null\n   \n   * 创建 create table 表名(列名 数据类型 not null);\n   * 删除 alter table 表名 modify 列名 数据类型;\n   * 建表后添加 alter table 表名 modify 列名 数据类型 not null;\n\n * 外键约束 表与表之间有关联性 保证数据的准确性\n   \n   * 创建 create table 表名(列名 数据类型 约束) constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名);\n   * 删除 alter table 表名drop foreign key 外键名;\n   * 建表后添加 alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名);\n\n * 外键级联更新 当主表的主键更新时 从表的外键也更新\n   \n   * alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名) on update cascade;\n\n * 外键级联删除 当主表的主键删除时 从表的外键也删除\n   \n   * alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名) on delete cascade;\n\n * 级联同时更新和删除\n   \n   * alter table 表名 add constraint 外键名 foreign key (本表外键列名) references 主表名(主表主键列名) on update cascade on delete cascade;",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"多表操作",frontmatter:{title:"多表操作",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/89e8d8/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/11.%E5%A4%9A%E8%A1%A8%E6%93%8D%E4%BD%9C.html",relativePath:"后端/04.SQL/11.多表操作.md",key:"v-16aed12b",path:"/pages/89e8d8/",headers:[{level:2,title:"内连接查询",slug:"内连接查询",normalizedTitle:"内连接查询",charIndex:93},{level:2,title:"外连接查询",slug:"外连接查询",normalizedTitle:"外连接查询",charIndex:243},{level:3,title:"左外连接",slug:"左外连接",normalizedTitle:"左外连接",charIndex:253},{level:3,title:"右外连接",slug:"右外连接",normalizedTitle:"右外连接",charIndex:359},{level:2,title:"子查询",slug:"子查询",normalizedTitle:"子查询",charIndex:467},{level:2,title:"自关联查询",slug:"自关联查询",normalizedTitle:"自关联查询",charIndex:588}],headersStr:"内连接查询 外连接查询 左外连接 右外连接 子查询 自关联查询",content:"# 多表操作\n\n * 一对一 主键和外键 唯一性(不可重复)\n * 一对多 外键 可重复\n * 多对多 需要借助第三张表中间表 中间表至少包含两列作为外键 分别关联两张表的主键\n\n\n# 内连接查询\n\n内连接查询的是两张表有交集的部分数据（有主外键关联的数据） 使用 inner join 关键字\n\n * 显式查询 select 列名 from 表名1 inner join 表名2 on 条件；\n * 隐式查询 select 列名 from 表名1，表名2 where 条件；\n\n\n# 外连接查询\n\n\n# 左外连接\n\n查询左表的全部数据，和左右两张表有交集部分的数据。 使用 关键字 left outer join\n\nselect 列名 from 表名1 left outer join 表名2 on 条件；\n\n\n# 右外连接\n\n查询右表的全部数据，和左右两张表有交集部分的数据。 使用 关键字 right outer join\n\nselect 列名 from 表名1 right outer join 表名2 on 条件；\n\n\n# 子查询\n\n查询语句中嵌套了查询语句，我们称为子查询\n\n * 结果是单行单列 可以将结果加上运算符作为另外一条的查询条件\n * 结果是多行单列 使用运算符 in 或者 not in进行判断\n * 结果是多行多列 作为一张虚拟表参与查询\n\n\n# 自关联查询\n\n在同一张表中数据有关联性，我们可以当成多个表来查询\n\n配合内外连接使用",normalizedContent:"# 多表操作\n\n * 一对一 主键和外键 唯一性(不可重复)\n * 一对多 外键 可重复\n * 多对多 需要借助第三张表中间表 中间表至少包含两列作为外键 分别关联两张表的主键\n\n\n# 内连接查询\n\n内连接查询的是两张表有交集的部分数据（有主外键关联的数据） 使用 inner join 关键字\n\n * 显式查询 select 列名 from 表名1 inner join 表名2 on 条件；\n * 隐式查询 select 列名 from 表名1，表名2 where 条件；\n\n\n# 外连接查询\n\n\n# 左外连接\n\n查询左表的全部数据，和左右两张表有交集部分的数据。 使用 关键字 left outer join\n\nselect 列名 from 表名1 left outer join 表名2 on 条件；\n\n\n# 右外连接\n\n查询右表的全部数据，和左右两张表有交集部分的数据。 使用 关键字 right outer join\n\nselect 列名 from 表名1 right outer join 表名2 on 条件；\n\n\n# 子查询\n\n查询语句中嵌套了查询语句，我们称为子查询\n\n * 结果是单行单列 可以将结果加上运算符作为另外一条的查询条件\n * 结果是多行单列 使用运算符 in 或者 not in进行判断\n * 结果是多行多列 作为一张虚拟表参与查询\n\n\n# 自关联查询\n\n在同一张表中数据有关联性，我们可以当成多个表来查询\n\n配合内外连接使用",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"表",frontmatter:{title:"表",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/5a2265/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/09.%E8%A1%A8.html",relativePath:"后端/04.SQL/09.表.md",key:"v-6265be4e",path:"/pages/5a2265/",headers:[{level:2,title:"数据类型",slug:"数据类型",normalizedTitle:"数据类型",charIndex:206},{level:2,title:"表数据操作",slug:"表数据操作",normalizedTitle:"表数据操作",charIndex:581},{level:2,title:"查询",slug:"查询",normalizedTitle:"查询",charIndex:8},{level:2,title:"集合函数",slug:"集合函数",normalizedTitle:"集合函数",charIndex:847},{level:2,title:"排序查询",slug:"排序查询",normalizedTitle:"排序查询",charIndex:888},{level:2,title:"分组查询",slug:"分组查询",normalizedTitle:"分组查询",charIndex:1023},{level:2,title:"分页查询",slug:"分页查询",normalizedTitle:"分页查询",charIndex:1118}],headersStr:"数据类型 表数据操作 查询 集合函数 排序查询 分组查询 分页查询",content:"# 表\n\n * 查询表 show tables;\n * 查询表结构 desc 表名;\n * 查询表字符集 show table status from 库名 like '表名';\n * 修改表名 clter table 表名 rename to 新表名;\n * 修改表的字符集 alter table 表名 character set utf8;\n * 单独添加一列 alter table 表名 add 列名 数据类型;\n * 修改指定列类型 alter table 表名 modify 列名 新数据类型;\n * 修改列名和数据类型 alter table 表名 change 列名 新列名 新数据类型;\n * 删除某一列 alter table 表名 drop 列名;\n * 删除表 drop table 表名;\n * 删除判断 drop table if exists 表名;\n\n\n# 数据类型\n\n * int 整数型\n * douban 小数\n * date 日期类型 yyyy-MM-dd\n * datetime 日期时间类型 yyyy-MM-dd HH:mm:ss\n * timestamp 时间戳类型 yyyy-MM-dd HH:mm:ss 如果不赋值或赋值为null 则使用当前系统时间自动赋值\n * varchar(长度) 字符串类型\n\n\n# 表数据操作\n\n * 添加数据 insert into 表名(列名) values(值1,值2);\n * 更新 update 表名 set 列名=值1,列名2=值2 where\n * 删除表指定数据 delete from 表名 where\n\n\n# 查询\n\n * 去重复 select distinct 列名1,列名2 from 表名;\n * 四则运算 select 列名1*10 frome 表名;\n * 起别名 select 列表 as 别名 from 表名;\n * 判断null ifnull(列名,替换的值)\n * \n\n\n# 集合函数\n\n\n\nselect 函数名(列名) from 表名 where\n\n\n# 排序查询\n\n * ORDER BY select 列名 frome 表名 where order by 列名 排序方式,列名2 排序方式\n   \n   * ASC 升序 默认值 如果是多个条件 只有当判断值一样时才会执行第二个条件排序\n   * DESC 降序\n\n\n# 分组查询\n\n * GROUP BY select 列名 from 表名 group by 指定以什么分组的列名\n   * 分组前过滤 where\n   * 分组后过滤 having\n\n\n# 分页查询\n\n * LIMIT select 列名 from 表名 LIMIT 当前页,每页个数\n   * 当前页 = (当前页 - 1 ) * 每页个数",normalizedContent:"# 表\n\n * 查询表 show tables;\n * 查询表结构 desc 表名;\n * 查询表字符集 show table status from 库名 like '表名';\n * 修改表名 clter table 表名 rename to 新表名;\n * 修改表的字符集 alter table 表名 character set utf8;\n * 单独添加一列 alter table 表名 add 列名 数据类型;\n * 修改指定列类型 alter table 表名 modify 列名 新数据类型;\n * 修改列名和数据类型 alter table 表名 change 列名 新列名 新数据类型;\n * 删除某一列 alter table 表名 drop 列名;\n * 删除表 drop table 表名;\n * 删除判断 drop table if exists 表名;\n\n\n# 数据类型\n\n * int 整数型\n * douban 小数\n * date 日期类型 yyyy-mm-dd\n * datetime 日期时间类型 yyyy-mm-dd hh:mm:ss\n * timestamp 时间戳类型 yyyy-mm-dd hh:mm:ss 如果不赋值或赋值为null 则使用当前系统时间自动赋值\n * varchar(长度) 字符串类型\n\n\n# 表数据操作\n\n * 添加数据 insert into 表名(列名) values(值1,值2);\n * 更新 update 表名 set 列名=值1,列名2=值2 where\n * 删除表指定数据 delete from 表名 where\n\n\n# 查询\n\n * 去重复 select distinct 列名1,列名2 from 表名;\n * 四则运算 select 列名1*10 frome 表名;\n * 起别名 select 列表 as 别名 from 表名;\n * 判断null ifnull(列名,替换的值)\n * \n\n\n# 集合函数\n\n\n\nselect 函数名(列名) from 表名 where\n\n\n# 排序查询\n\n * order by select 列名 frome 表名 where order by 列名 排序方式,列名2 排序方式\n   \n   * asc 升序 默认值 如果是多个条件 只有当判断值一样时才会执行第二个条件排序\n   * desc 降序\n\n\n# 分组查询\n\n * group by select 列名 from 表名 group by 指定以什么分组的列名\n   * 分组前过滤 where\n   * 分组后过滤 having\n\n\n# 分页查询\n\n * limit select 列名 from 表名 limit 当前页,每页个数\n   * 当前页 = (当前页 - 1 ) * 每页个数",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"MySQL 存储过程和函数",frontmatter:{title:"MySQL 存储过程和函数",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/336822/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/14.MySQL%20%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%92%8C%E5%87%BD%E6%95%B0.html",relativePath:"后端/04.SQL/14.MySQL 存储过程和函数.md",key:"v-d9c864d4",path:"/pages/336822/",headers:[{level:2,title:"存储过程",slug:"存储过程",normalizedTitle:"存储过程",charIndex:8},{level:3,title:"变量",slug:"变量",normalizedTitle:"变量",charIndex:511},{level:3,title:"if语句",slug:"if语句",normalizedTitle:"if语句",charIndex:624},{level:3,title:"传参",slug:"传参",normalizedTitle:"传参",charIndex:711},{level:3,title:"while 循环",slug:"while-循环",normalizedTitle:"while 循环",charIndex:930},{level:2,title:"存储函数",slug:"存储函数",normalizedTitle:"存储函数",charIndex:26}],headersStr:"存储过程 变量 if语句 传参 while 循环 存储函数",content:"# MySQL 存储过程和函数\n\n相对应函数/方法，存储函数必须有返回值，存储过程可以没有返回值\n\n\n# 存储过程\n\n存储过程可以没有返回值\n\n-- 修改结束分隔符这里指定为$ 可以自定义\ndelimiter $\n\n-- 创建存储过程\ncreate procedure 存储过程名称(参数列表[可为空]\nbegin\n                        sql 语句;\nend$\n                        \n-- 恢复结束分割符为分号\ndelimiter;\n           \n-- 调用存储过程\ncall 存储过程名称(参数);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n * 查询数据中所有的存储 过程 必须是root权限用户\n   \n   * select * from mysql.proc where db='数据库名称';\n     \n     \n     1\n     \n\n * 删除存储过程\n   \n   * drop procedure [if exists] 存储过程名称;\n     \n     \n     1\n     \n\n\n# 变量\n\n * 定义变量 declare 变量名 数据类型 [default 默认值];\n * 赋值 set 变量名 = 值;\n * 赋值2 select 列名 into 变量名 from 表名 [where 条件];\n\n\n# if语句\n\nif 判断条件1 then sql语句;\nelseif 判断条件2 then sql语句;\nelse sql语句;\nend if;\n\n\n1\n2\n3\n4\n\n\n\n# 传参\n\n * IN 默认值 输入参数关键字 由调用者传递实参\n * OUT 输出参数,作为返回值返回\n * INOUT 即可以为输入也可以为输出\n\ncreate procedure 存储过程名([IN|OUT|INOUT] 参数名 数据类型)\nBEGIN\n\t\t\tSQL 语句;\nEND$\n\n-- 调用\ncall 存储过程名(@输出参数名);\n-- 查询返回值\nselect @输出参数名\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# while 循环\n\nwhile 判断语句 do\n\t循环体语句;\nend while;\n\n\n1\n2\n3\n\n\n\n# 存储函数\n\n存储函数必须要有返回值 即有out参数\n\ndelimiter $\ncreate function 函数名称(参数列表)\nreturns 返回值类型\nbegin\n\tsql语句;\n\treturn 返回值;\nend$\ndelimiter ;\n\n-- 调用\nselect 函数名称(实际参数);\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n * 删除函数 drop function 函数名;",normalizedContent:"# mysql 存储过程和函数\n\n相对应函数/方法，存储函数必须有返回值，存储过程可以没有返回值\n\n\n# 存储过程\n\n存储过程可以没有返回值\n\n-- 修改结束分隔符这里指定为$ 可以自定义\ndelimiter $\n\n-- 创建存储过程\ncreate procedure 存储过程名称(参数列表[可为空]\nbegin\n                        sql 语句;\nend$\n                        \n-- 恢复结束分割符为分号\ndelimiter;\n           \n-- 调用存储过程\ncall 存储过程名称(参数);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n * 查询数据中所有的存储 过程 必须是root权限用户\n   \n   * select * from mysql.proc where db='数据库名称';\n     \n     \n     1\n     \n\n * 删除存储过程\n   \n   * drop procedure [if exists] 存储过程名称;\n     \n     \n     1\n     \n\n\n# 变量\n\n * 定义变量 declare 变量名 数据类型 [default 默认值];\n * 赋值 set 变量名 = 值;\n * 赋值2 select 列名 into 变量名 from 表名 [where 条件];\n\n\n# if语句\n\nif 判断条件1 then sql语句;\nelseif 判断条件2 then sql语句;\nelse sql语句;\nend if;\n\n\n1\n2\n3\n4\n\n\n\n# 传参\n\n * in 默认值 输入参数关键字 由调用者传递实参\n * out 输出参数,作为返回值返回\n * inout 即可以为输入也可以为输出\n\ncreate procedure 存储过程名([in|out|inout] 参数名 数据类型)\nbegin\n\t\t\tsql 语句;\nend$\n\n-- 调用\ncall 存储过程名(@输出参数名);\n-- 查询返回值\nselect @输出参数名\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# while 循环\n\nwhile 判断语句 do\n\t循环体语句;\nend while;\n\n\n1\n2\n3\n\n\n\n# 存储函数\n\n存储函数必须要有返回值 即有out参数\n\ndelimiter $\ncreate function 函数名称(参数列表)\nreturns 返回值类型\nbegin\n\tsql语句;\n\treturn 返回值;\nend$\ndelimiter ;\n\n-- 调用\nselect 函数名称(实际参数);\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n * 删除函数 drop function 函数名;",charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"备份",frontmatter:{title:"备份",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/f561ab/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/13.%E5%A4%87%E4%BB%BD.html",relativePath:"后端/04.SQL/13.备份.md",key:"v-95af4b10",path:"/pages/f561ab/",headersStr:null,content:"# 备份\n\nmysqldump -uroot -p 数据库名 > 文件保存路径\n\n\n1\n\n\n删除原有数据库,创建空数据库\n\nuse 数据库;\n\nsource sql文件路径;\n\n\n1\n2\n3\n",normalizedContent:"# 备份\n\nmysqldump -uroot -p 数据库名 > 文件保存路径\n\n\n1\n\n\n删除原有数据库,创建空数据库\n\nuse 数据库;\n\nsource sql文件路径;\n\n\n1\n2\n3\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"视图",frontmatter:{title:"视图",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/3293cf/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/12.%E8%A7%86%E5%9B%BE.html",relativePath:"后端/04.SQL/12.视图.md",key:"v-168103c0",path:"/pages/3293cf/",headersStr:null,content:"# 视图\n\n虚拟表,将查询结果 保存为一个虚拟表\n\n * 创建视图 create view 视图名称 [(列名列表)] as 查询语句;\n * 查询视图 select * from 视图名称;\n * 修改视图数据 update 视图名称 set 列名=值 where 条件; 注意修改虚拟表的数据 原表数据也会修改\n * 修改视图 alter view 视图名称 (列名列表) as 查询语句;\n * 删除视图 drop view [if exists] 视图名称;",normalizedContent:"# 视图\n\n虚拟表,将查询结果 保存为一个虚拟表\n\n * 创建视图 create view 视图名称 [(列名列表)] as 查询语句;\n * 查询视图 select * from 视图名称;\n * 修改视图数据 update 视图名称 set 列名=值 where 条件; 注意修改虚拟表的数据 原表数据也会修改\n * 修改视图 alter view 视图名称 (列名列表) as 查询语句;\n * 删除视图 drop view [if exists] 视图名称;",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"事务",frontmatter:{title:"事务",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/4c8706/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/16.%E4%BA%8B%E5%8A%A1.html",relativePath:"后端/04.SQL/16.事务.md",key:"v-6dca1220",path:"/pages/4c8706/",headers:[{level:2,title:"提交方式",slug:"提交方式",normalizedTitle:"提交方式",charIndex:191},{level:2,title:"事务的四大特征(ACID)",slug:"事务的四大特征-acid",normalizedTitle:"事务的四大特征(acid)",charIndex:344},{level:2,title:"事务的隔离性级别",slug:"事务的隔离性级别",normalizedTitle:"事务的隔离性级别",charIndex:710},{level:3,title:"引发的问题",slug:"引发的问题",normalizedTitle:"引发的问题",charIndex:725}],headersStr:"提交方式 事务的四大特征(ACID) 事务的隔离性级别 引发的问题",content:"# 事务\n\n事务:一条或者多条SQL语句组成一个执行单元,特点是要么同时成功要么同时失败,每条语句都相互依赖,形成一个整体,如果失败或者出现错误,那么状态会撤回到事务最初状态(回退)\n\n * 开启事务 start transaction; 开启事务后所有的操作都是虚拟化的\n * 回滚事务 rollback; 结束事务并回退\n * 提交事务 commit; 结束事务并提交\n\n\n# 提交方式\n\n * 自动提交 1 默认值\n * 手动添加 0\n * 查询事务提交方式 select @@autocommit;\n * 修改事务提交方式 set @@autocommit=0; 1为自动提交 0为手动添加\n\n如果修改为手动提交 不开始事务 做增删改查 操作也需要commit操作才生效\n\n\n# 事务的四大特征(ACID)\n\n * 原子性（Atomicity）\n   * 指事务包含的所有操作要么全部成功,要么全部失败回滚 因此事务的操作成功则要应用到数据，操作失败则不能对数据库有影响\n * 一致性 （Consistency）\n   * 一致性指事务必须使数据库从一个一致性变化到另外一个一致性 也就是说一个事务执行之前和执行之后都必须处于一个一致性\n * 隔离性（isolcation）\n   * 隔离性是当多个用户并发访问数据库时，如操作同一张表时，数据库为每个用户开启事务 不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。\n * 持久性（durability）\n   * 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的 即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作\n\n\n# 事务的隔离性级别\n\n\n\n\n# 引发的问题\n\n\n\n * 查询数据库隔离级别 select @@TX_ISOLATION;\n * 修改数据库隔离级别 set global transaction isolation level 级别; 修改后需要重新连接数据库\n   * REPEATABLE READ 默认级别\n * 脏读 指的事务未被提交 另外一个事务已经可以查询到该事务的数据\n * 不可重复读 指两个事务开启 其中一个事务已经提交了 另外一个事务读取到该事务的数据 正常来说应该是读取不到的,需要结束事务后才能读取\n * 幻读 指查询某个数据不存在 但插入时无法无插入此数据,并且之后发现数据已经存在表中 或 某数据不存在 但执行删除操作 缺成功了\n\n",normalizedContent:"# 事务\n\n事务:一条或者多条sql语句组成一个执行单元,特点是要么同时成功要么同时失败,每条语句都相互依赖,形成一个整体,如果失败或者出现错误,那么状态会撤回到事务最初状态(回退)\n\n * 开启事务 start transaction; 开启事务后所有的操作都是虚拟化的\n * 回滚事务 rollback; 结束事务并回退\n * 提交事务 commit; 结束事务并提交\n\n\n# 提交方式\n\n * 自动提交 1 默认值\n * 手动添加 0\n * 查询事务提交方式 select @@autocommit;\n * 修改事务提交方式 set @@autocommit=0; 1为自动提交 0为手动添加\n\n如果修改为手动提交 不开始事务 做增删改查 操作也需要commit操作才生效\n\n\n# 事务的四大特征(acid)\n\n * 原子性（atomicity）\n   * 指事务包含的所有操作要么全部成功,要么全部失败回滚 因此事务的操作成功则要应用到数据，操作失败则不能对数据库有影响\n * 一致性 （consistency）\n   * 一致性指事务必须使数据库从一个一致性变化到另外一个一致性 也就是说一个事务执行之前和执行之后都必须处于一个一致性\n * 隔离性（isolcation）\n   * 隔离性是当多个用户并发访问数据库时，如操作同一张表时，数据库为每个用户开启事务 不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。\n * 持久性（durability）\n   * 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的 即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作\n\n\n# 事务的隔离性级别\n\n\n\n\n# 引发的问题\n\n\n\n * 查询数据库隔离级别 select @@tx_isolation;\n * 修改数据库隔离级别 set global transaction isolation level 级别; 修改后需要重新连接数据库\n   * repeatable read 默认级别\n * 脏读 指的事务未被提交 另外一个事务已经可以查询到该事务的数据\n * 不可重复读 指两个事务开启 其中一个事务已经提交了 另外一个事务读取到该事务的数据 正常来说应该是读取不到的,需要结束事务后才能读取\n * 幻读 指查询某个数据不存在 但插入时无法无插入此数据,并且之后发现数据已经存在表中 或 某数据不存在 但执行删除操作 缺成功了\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"存储引擎",frontmatter:{title:"存储引擎",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/7d59ad/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/17.%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E.html",relativePath:"后端/04.SQL/17.存储引擎.md",key:"v-36a8fca6",path:"/pages/7d59ad/",headersStr:null,content:'# 存储引擎\n\nMySql 支持的存储引擎有很多,常用的有三个\n\n * InnoDB 访问速度快,不支持事务和外键\n * MyISAM 支持事务和外键 支持并发控制 占用磁盘空间大 MySQL 5.5默认为这个引擎\n * MEMORY 内存存储 速度快 不安全\n\n\n\n * 查询数据支持的存储引擎 show engines;\n * 查询某个库中所有数据表的存储引擎 show table status from 库名;\n * 查询指定库中指定表的引擎 show table status from 库名 where name ="表名";\n * 创表时 指定存储引擎 create table 表名(列名 数据类型)engine = 引擎名;\n * 修改表 存储引擎 alter table 表名 engine = 引擎名;\n\n',normalizedContent:'# 存储引擎\n\nmysql 支持的存储引擎有很多,常用的有三个\n\n * innodb 访问速度快,不支持事务和外键\n * myisam 支持事务和外键 支持并发控制 占用磁盘空间大 mysql 5.5默认为这个引擎\n * memory 内存存储 速度快 不安全\n\n\n\n * 查询数据支持的存储引擎 show engines;\n * 查询某个库中所有数据表的存储引擎 show table status from 库名;\n * 查询指定库中指定表的引擎 show table status from 库名 where name ="表名";\n * 创表时 指定存储引擎 create table 表名(列名 数据类型)engine = 引擎名;\n * 修改表 存储引擎 alter table 表名 engine = 引擎名;\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"触发器",frontmatter:{title:"触发器",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/108b35/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/15.%E8%A7%A6%E5%8F%91%E5%99%A8.html",relativePath:"后端/04.SQL/15.触发器.md",key:"v-341ec21e",path:"/pages/108b35/",headersStr:null,content:"# 触发器\n\n触发器会把我们 添加 修改 删除 操作之前或之后 触发执行 触发器 中定义的SQL语句\n\n这些特性可以作为 日志记录 数据校验 确保数据完整性\n\n使用 别名 NEW 和 OLD 关键字来引用触发器 发生变化的内容记录\n\nNEW 为触发后表的数据 为一个单行多列结果\n\nOLD 为触发器前表的数据\n\nOLD 是只读的，而 NEW 则可以在触发器中使用 SET 赋值，这样不会再次触发触发器，造成循环调用（如每插入一个学生前，都在其学号前加“2013”）。\n\n\n\n-- 创建触发器\ndelimiter $\n\ncreate trigger 触发器名称\nbefore|after(定义操作前/后)     insert|update|delete(定义触发器类型)\non 表名\nfor each row\nbegin\n\t\tsql语句;\n\t\t-- 可以使用 NEW / OLD 关键字来获取更新前后的数据 如new.name old.name\n\t\t\nend$\n\ndelimiter$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n样例\n\ndelimiter $$\nCREATE TRIGGER upd_check \nBEFORE UPDATE ON account\nFOR EACH ROW\nBEGIN\n 　　IF NEW.amount < 0 THEN\n 　　　　SET NEW.amount = 0;\n 　　ELSEIF NEW.amount > 100 THEN\n 　　　　SET NEW.amount = 100;\n 　　END IF;\nEND$$\ndelimiter ;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n * 查看触发器 show triggers;\n * 删除触发器 drop trigger 触发器名称;",normalizedContent:"# 触发器\n\n触发器会把我们 添加 修改 删除 操作之前或之后 触发执行 触发器 中定义的sql语句\n\n这些特性可以作为 日志记录 数据校验 确保数据完整性\n\n使用 别名 new 和 old 关键字来引用触发器 发生变化的内容记录\n\nnew 为触发后表的数据 为一个单行多列结果\n\nold 为触发器前表的数据\n\nold 是只读的，而 new 则可以在触发器中使用 set 赋值，这样不会再次触发触发器，造成循环调用（如每插入一个学生前，都在其学号前加“2013”）。\n\n\n\n-- 创建触发器\ndelimiter $\n\ncreate trigger 触发器名称\nbefore|after(定义操作前/后)     insert|update|delete(定义触发器类型)\non 表名\nfor each row\nbegin\n\t\tsql语句;\n\t\t-- 可以使用 new / old 关键字来获取更新前后的数据 如new.name old.name\n\t\t\nend$\n\ndelimiter$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n样例\n\ndelimiter $$\ncreate trigger upd_check \nbefore update on account\nfor each row\nbegin\n 　　if new.amount < 0 then\n 　　　　set new.amount = 0;\n 　　elseif new.amount > 100 then\n 　　　　set new.amount = 100;\n 　　end if;\nend$$\ndelimiter ;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n * 查看触发器 show triggers;\n * 删除触发器 drop trigger 触发器名称;",charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"索引",frontmatter:{title:"索引",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/acc2dd/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/18.%E7%B4%A2%E5%BC%95.html",relativePath:"后端/04.SQL/18.索引.md",key:"v-1cd8dcd0",path:"/pages/acc2dd/",headers:[{level:2,title:"索引操作",slug:"索引操作",normalizedTitle:"索引操作",charIndex:36},{level:2,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:604},{level:3,title:"1. 磁盘存储",slug:"_1-磁盘存储",normalizedTitle:"1. 磁盘存储",charIndex:710},{level:3,title:"2. BTree",slug:"_2-btree",normalizedTitle:"2. btree",charIndex:724},{level:3,title:"3. B+Tree",slug:"_3-b-tree",normalizedTitle:"3. b+tree",charIndex:799},{level:2,title:"索引原则",slug:"索引原则",normalizedTitle:"索引原则",charIndex:884}],headersStr:"索引操作 原理 1. 磁盘存储 2. BTree 3. B+Tree 索引原则",content:"# 索引\n\n索引是帮助MySQL高效获取数据的一种数据结构\n\n\n\n\n# 索引操作\n\n * 创建索引\n   \n   * create [unique | fulltext] index 索引名称  --unique 唯一索引 \n     [using 索引类型] -- 默认为 btree\n     on 表名(列名);\n     \n     \n     1\n     2\n     3\n     \n\n * 查看索引 show index from 表名;\n\n * 添加索引\n   \n   * 普通索引: alter table 表名 add index 索引名称(列名);\n   * 组合索引: alter table 表名 add index 索引名称(列名,列名);\n   * 主键索引: alter table 表名 add primary key (主键列名);\n   * 外键索引: alter table 表名 add constraint 外键名 foreign key (外键列名) references 主表名(主键列名);\n   * 唯一索引: alter table 表名 add unique 索引名称(列名);\n   * 全文索引: alter table 表名 add fulltext 索引名称(列名);\n\n * 删除索引 drop index 索引名称 on 表名;\n\n\n# 原理\n\n索引是在存储引擎中实现的\n\nBTree 索引类型是基于 B+Tree 数据结构的 , 而B+Tree 是 Btree 的变种, 通常在数据库 和 系统中的文件系统中 特点是能够 保持数据稳定有序\n\n\n# 1. 磁盘存储\n\n\n\n\n# 2. BTree\n\n\n\n由于每次读取到一个磁盘块中的数据 深度 导致效率低\n\n每个节点中包含key值 和 数据 会增加查询时 磁盘IO 次数\n\n\n# 3. B+Tree\n\n\n\n数据只会保持在叶子节点上 并且叶子节点 之间指针相连 方便进行范围查询\n\n非叶子节点只存储 key值 减少磁盘IO的次数 树形结构较小\n\n\n# 索引原则\n\n 1. 对查询次数较高 并且数据量比较大的表 建立索引\n 2. 建议使用唯一索引 区分度高 索引的效率高\n 3. 索引字段的选择 应当从 where 子句的条件中提取\n 4. 索引虽然提供效率 但过多不便于维护\n 5. 最左匹配原则(只适合于组合索引)\n    * 当我们为表中 name age height 添加组合索引 实质上建立了 name 列索引 (name、age) 索引 （name、age、height）索引 使用这三个索引时不区分顺序",normalizedContent:"# 索引\n\n索引是帮助mysql高效获取数据的一种数据结构\n\n\n\n\n# 索引操作\n\n * 创建索引\n   \n   * create [unique | fulltext] index 索引名称  --unique 唯一索引 \n     [using 索引类型] -- 默认为 btree\n     on 表名(列名);\n     \n     \n     1\n     2\n     3\n     \n\n * 查看索引 show index from 表名;\n\n * 添加索引\n   \n   * 普通索引: alter table 表名 add index 索引名称(列名);\n   * 组合索引: alter table 表名 add index 索引名称(列名,列名);\n   * 主键索引: alter table 表名 add primary key (主键列名);\n   * 外键索引: alter table 表名 add constraint 外键名 foreign key (外键列名) references 主表名(主键列名);\n   * 唯一索引: alter table 表名 add unique 索引名称(列名);\n   * 全文索引: alter table 表名 add fulltext 索引名称(列名);\n\n * 删除索引 drop index 索引名称 on 表名;\n\n\n# 原理\n\n索引是在存储引擎中实现的\n\nbtree 索引类型是基于 b+tree 数据结构的 , 而b+tree 是 btree 的变种, 通常在数据库 和 系统中的文件系统中 特点是能够 保持数据稳定有序\n\n\n# 1. 磁盘存储\n\n\n\n\n# 2. btree\n\n\n\n由于每次读取到一个磁盘块中的数据 深度 导致效率低\n\n每个节点中包含key值 和 数据 会增加查询时 磁盘io 次数\n\n\n# 3. b+tree\n\n\n\n数据只会保持在叶子节点上 并且叶子节点 之间指针相连 方便进行范围查询\n\n非叶子节点只存储 key值 减少磁盘io的次数 树形结构较小\n\n\n# 索引原则\n\n 1. 对查询次数较高 并且数据量比较大的表 建立索引\n 2. 建议使用唯一索引 区分度高 索引的效率高\n 3. 索引字段的选择 应当从 where 子句的条件中提取\n 4. 索引虽然提供效率 但过多不便于维护\n 5. 最左匹配原则(只适合于组合索引)\n    * 当我们为表中 name age height 添加组合索引 实质上建立了 name 列索引 (name、age) 索引 （name、age、height）索引 使用这三个索引时不区分顺序",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"MyCat 中间件",frontmatter:{title:"MyCat 中间件",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/3ca264/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/20.MyCat%20%E4%B8%AD%E9%97%B4%E4%BB%B6.html",relativePath:"后端/04.SQL/20.MyCat 中间件.md",key:"v-c42b5e22",path:"/pages/3ca264/",headers:[{level:2,title:"mysql忘记密码",slug:"mysql忘记密码",normalizedTitle:"mysql忘记密码",charIndex:54},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:384},{level:2,title:"克隆",slug:"克隆",normalizedTitle:"克隆",charIndex:1679},{level:2,title:"主重复制",slug:"主重复制",normalizedTitle:"主重复制",charIndex:1753},{level:2,title:"读写分离",slug:"读写分离",normalizedTitle:"读写分离",charIndex:1766},{level:2,title:"分库分表",slug:"分库分表",normalizedTitle:"分库分表",charIndex:4540},{level:3,title:"水平拆分",slug:"水平拆分",normalizedTitle:"水平拆分",charIndex:4574},{level:3,title:"垂直拆分",slug:"垂直拆分",normalizedTitle:"垂直拆分",charIndex:6607}],headersStr:"mysql忘记密码 安装 克隆 主重复制 读写分离 分库分表 水平拆分 垂直拆分",content:'# MyCat 中间件\n\nMyCat 是一款 数据集群软件 ,支持MySQL 和常用的关系型数据库\n\n\n# mysql忘记密码\n\nvi /etc/my.cnf\n#在[mysqld]下添加编码配置\nskip-grant-tables  #无需密码进入mysql\n\n\nmysql -uroot -p #直接回车进入\n\n#修改密码\nuse mysql;\nupdate user set authentication_string=password(\'123456\') where user=\'root\';\n#刷新权限\nflush privileges;\nexit;\n#删除之前的无密码\nvi /etc/my.cnf\n\nsystemctl restart mysqld #重启\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 安装\n\nmysql\n\nwget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\nyum -y install mysql57-community-release-el7-10.noarch.rpm\nyum -y install mysql-community-server\nsystemctl start  mysqld.service\ngrep "password" /var/log/mysqld.log\n \nmysql -uroot -p\n\n#修改密码\nALTER USER \'root\'@\'localhost\' IDENTIFIED BY \'new password\';\n#关闭密码强度检测\ngrant all privileges on *.* to \'root\'@\'%\' identified by \'password\' with grant option;\nflush privileges; \n#退出\nexit\n#防火墙\nfirewall-cmd --zone=public --add-port=3306/tcp --permanent\nfirewall-cmd --reload  \n\n#配置默认编码为UTF-8 修改/etc/my.cnf配置文件，在[mysqld]下添加编码配置，如下所示\nvi /etc/my.cnf\n#在[mysqld]下添加\n\ncharacter_set_server=utf8\ncollation-server=utf8_general_ci\ninit_connect=\'SET NAMES utf8\'\n\nsystemctl restart mysqld\n\n# 开机启动\nsystemctl enable mysqld\nsystemctl daemon-reload\n\n#用户名为root 密码为A37\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\nmycat\n\n#上传mycat到linux上\ntar -xvf Mycat-server-1.6.7.6-release-20210730131311-linux.tar.gz \n\n# 编辑环境变量\nvi /etc/profile\n\n#添加以下内容\nexport MYCAT_HOME=/root/mycat\n\nsource /etc/profile\n\ncd /root/mycat/bin\n./mycat start\n\nfirewall-cmd --zone=public --add-port=8066/tcp --permanent\nfirewall-cmd --reload  \n\n#用户root 密码123456 端口8066\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 克隆\n\n克隆重新生成mac地址\n\n并修改mysql的uuid 随机改一下\n\nvi /var/lib/mysql/auto.cnf\n\n\n1\n\n\n\n# 主重复制\n\nMyCat 的读写分离 需要用到主从复制 从表会根据主表进行同步\n\n我们在写数据时 写入主表 而读取从从表中读取\n\n#在主服务器上\nvi /etc/my.cnf\n\n#在[mysqld]加上 log-bin 开启主从复制 server-id 主从服务器的唯一标识\nlog-bin=mysql-bin\nserver-id=1\ninnodb_flush_log_at_trx_commit=1\nsync_binlog=1\n\nservice mysqld restart\n\nmysql -uroot -p\n\n# 需要 file和position的值 配置从服务器用\nshow master status;\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n#在从服务器上\nvi /etc/my.cnf\n\n#在[mysqld]加上\nserver-id=2\n\nmysql -uroot -p\n\nuse mysql;\ndrop table slave_master_info;\ndrop table slave_relat_log_info;\ndrop table slave_worker_info;\ndrop table innodb_index_stats;\ndrop table innodb_table_stats;\nsorce /usr/share/mysql/mysql_system_tables.sql;\n\nserveice mysqld restart\n\nmysql -uroot -p\n\nchange master to master_host=\'主服务器ip\',master_port=3306,master_user=\'root\',master_password=\'password\',master_log_file=\'刚刚在主服务器查到的值\',master_log_pos=同样也是之前查到的;\n\n# 开启从节点\nstart slave;\n\n# 查询结果 Slave_IO_Runing和Slave_SQL_Running都为yes才成功\nshow slave status\\G;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 读写分离\n\n需要先配置主从复制 写操作只写主服务器 而读取是从从服务器中数据\n\n# 修改server.xml\nvi /root/mycat/conf/server.xml\n\n\n1\n2\n\n\n        <user name="root" defaultAccount="true">\n          \t\t\t\t  \x3c!-- 密码 --\x3e\n                <property name="password">123456</property>\n         \t\t\t   \x3c!-- 库名 --\x3e\n                <property name="schemas">TESTDB</property>\n                <property name="defaultSchema">TESTDB</property>\n        </user>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 修改schema.xml\nvi /root/mycat/conf/schema.xml\n\n\n1\n2\n\n\n\x3c!-- 建议百度找配置文件这个不太准确 --\x3e\n<mycat:schema xmlns:mycat="http://io.mycat/">\n     \x3c!-- name为主服务器 server文件的虚拟库名与name必须一致   --\x3e\n\t<schema name="TESTDB" checkSQLschema="true" sqlMaxLimit="100" randomDataNode="dn1">\n\t\t<table name="customer" primaryKey="id" dataNode="dn1,dn2" rule="sharding-by-intfile" autoIncrement="true" fetchStoreNodeByJdbc="true">\n\t\t\t<childTable name="customer_addr" primaryKey="id" joinKey="customer_id" parentKey="id"> </childTable>\n\t\t</table>\n\t</schema>\n    \x3c!-- name必须与datenode一致  database为真实的库名  --\x3e\n\t<dataNode name="dn1" dataHost="localhost1" database="db1" />\n\t<dataNode name="dn2" dataHost="localhost1" database="db2" />\n\t<dataNode name="dn3" dataHost="localhost1" database="db3" />\n\t<dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"\n\t\t\t  writeType="0" dbType="mysql" dbDriver="jdbc" switchType="1"  slaveThreshold="100">\n\t\t<heartbeat>select user()</heartbeat>\n\t\t\x3c!-- 主服务器 host自定义  url为写的数据的地址+端口  user passwd --\x3e\n\t\t<writeHost host="hostM1" url="jdbc:mysql://localhost:3306" user="root"\n\t\t\t\t   password="root">\n            \x3c!-- 从服务器 负责读 --\x3e\n            <readHost host="hostS1" url="从服务器+端口" user="" password="">\n\t\t</writeHost>\n\t</dataHost>\n</mycat:schema>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 重启 mycat\n./mycat restart\n\n\n1\n2\n\n\n使用时连接mycat的地址使用即可\n\n\n# 分库分表\n\n将庞大数据量拆分为不同的数据库和数据表进行存储\n\n\n# 水平拆分\n\n根据表的数据逻辑关系,将同一表中的数据按某种条件,拆分到多台服务器上,也叫横向拆分\n\n * 修改主服务器中 server.xml vi /root/mycat/conf/server.xml\n   \n   * \x3c!-- 添加property 配置主键方式 0为本地文件方式 --\x3e\n     <property name="sequnceHandlerType">0</property>\n     \n     \n     1\n     2\n     \n\n * 修改主服务器的 sequence_conf.properties vi /root/mycat/conf/sequence_conf.properties\n   \n   * GLOBAL.HISIDS=     #可以自定义关键字\n     GLOBAL.MINID=10001   #最小值\n     GLOBAL.MAXID=20000   #最大值\n     \n     \n     1\n     2\n     3\n     \n\n * 修改schema.xml vi /root/mycat/conf/schema.xml\n   \n   * <mycat:schema xmlns:mycat="http://io.mycat/">\n     \t<schema name="TESTDB" checkSQLschema="false" sqlMaxLimit="100">\n     \t\t<table name="customer" primaryKey="id" dataNode="dn1,dn2,dn3" rule="mod-long">\n     \t\t</table>\n     \t</schema>\n         \x3c!-- name必须与datenode一致  database为真实的库名  --\x3e\n     \t<dataNode name="dn1" dataHost="localhost1" database="db1" />\n     \t<dataNode name="dn2" dataHost="localhost1" database="db2" />\n     \t<dataNode name="dn3" dataHost="localhost1" database="db3" />\n         \n     \t<dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"\n     \t\t\t  writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">\n     \t\t<heartbeat>select user()</heartbeat>\n     \t\t\x3c!-- 主服务器 host自定义  url为写的数据的地址+端口  user passwd --\x3e\n     \t\t<writeHost host="hostM1" url="localhost:3306" user="root"\n     \t\t\t\t   password="root">\n                 \x3c!-- 从服务器 负责读 --\x3e\n                 <readHost host="hostS1" url="从服务器+端口" user="" password="">\n     \t\t</writeHost>\n     \t</dataHost>\n     </mycat:schema>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     \n\n * 修改 rule.xml vi /root/mycat/conf/rule.xml\n   \n   * <function name=\'mod-log\' class=\'io.mycat.route.function.PartitionBymod\'>\n         <property name=\'count\'>3</property>\n     </function>\n     \n     \n     1\n     2\n     3\n     \n\n插入数据列名必须完整 如果是自增键 则需要用 NEXT VALUE FOR MYCATSEQ_GLOBAL 不能用null\n\n以上配置用的是根据主键取模方式拆分\n\n\n# 垂直拆分\n\n根据业务的维度,将不同的表切分到不同数据库上,也叫纵向拆分\n\n * 修改schema.xml vi /root/mycat/conf/schema.xml\n   \n   * <mycat:schema xmlns:mycat="http://io.mycat/">\n     \t<schema name="TESTDB" checkSQLschema="false" sqlMaxLimit="100">\n     \t\n     \t\t</table>\n             <table name="dog" primaryKey="id" autoIncrement=\'true\' dataNode=\'dn4\'>\n     \t\t</table>\n             <table name="cat" primaryKey="id" autoIncrement=\'true\' dataNode=\'dn5\'>\n     \t\t</table>\n     \t</schema>\n         \x3c!-- name必须与datenode一致  database为真实的库名  --\x3e\n     \t<dataNode name="dn4" dataHost="localhost1" database="db1" />\n     \t<dataNode name="dn5" dataHost="localhost1" database="db2" />\n         \n     \t<dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"\n     \t\t\t  writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">\n     \t\t<heartbeat>select user()</heartbeat>\n     \t\t\x3c!-- 主服务器 host自定义  url为写的数据的地址+端口  user passwd --\x3e\n     \t\t<writeHost host="hostM1" url="localhost:3306" user="root"\n     \t\t\t\t   password="root">\n                 \x3c!-- 从服务器 负责读 --\x3e\n                 <readHost host="hostS1" url="从服务器+端口" user="" password="">\n     \t\t</writeHost>\n     \t</dataHost>\n     </mycat:schema>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     \n     \n     根据 表名 区分库 如dog 存放在db1中 cat存放在db2中',normalizedContent:'# mycat 中间件\n\nmycat 是一款 数据集群软件 ,支持mysql 和常用的关系型数据库\n\n\n# mysql忘记密码\n\nvi /etc/my.cnf\n#在[mysqld]下添加编码配置\nskip-grant-tables  #无需密码进入mysql\n\n\nmysql -uroot -p #直接回车进入\n\n#修改密码\nuse mysql;\nupdate user set authentication_string=password(\'123456\') where user=\'root\';\n#刷新权限\nflush privileges;\nexit;\n#删除之前的无密码\nvi /etc/my.cnf\n\nsystemctl restart mysqld #重启\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 安装\n\nmysql\n\nwget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\nyum -y install mysql57-community-release-el7-10.noarch.rpm\nyum -y install mysql-community-server\nsystemctl start  mysqld.service\ngrep "password" /var/log/mysqld.log\n \nmysql -uroot -p\n\n#修改密码\nalter user \'root\'@\'localhost\' identified by \'new password\';\n#关闭密码强度检测\ngrant all privileges on *.* to \'root\'@\'%\' identified by \'password\' with grant option;\nflush privileges; \n#退出\nexit\n#防火墙\nfirewall-cmd --zone=public --add-port=3306/tcp --permanent\nfirewall-cmd --reload  \n\n#配置默认编码为utf-8 修改/etc/my.cnf配置文件，在[mysqld]下添加编码配置，如下所示\nvi /etc/my.cnf\n#在[mysqld]下添加\n\ncharacter_set_server=utf8\ncollation-server=utf8_general_ci\ninit_connect=\'set names utf8\'\n\nsystemctl restart mysqld\n\n# 开机启动\nsystemctl enable mysqld\nsystemctl daemon-reload\n\n#用户名为root 密码为a37\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\nmycat\n\n#上传mycat到linux上\ntar -xvf mycat-server-1.6.7.6-release-20210730131311-linux.tar.gz \n\n# 编辑环境变量\nvi /etc/profile\n\n#添加以下内容\nexport mycat_home=/root/mycat\n\nsource /etc/profile\n\ncd /root/mycat/bin\n./mycat start\n\nfirewall-cmd --zone=public --add-port=8066/tcp --permanent\nfirewall-cmd --reload  \n\n#用户root 密码123456 端口8066\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 克隆\n\n克隆重新生成mac地址\n\n并修改mysql的uuid 随机改一下\n\nvi /var/lib/mysql/auto.cnf\n\n\n1\n\n\n\n# 主重复制\n\nmycat 的读写分离 需要用到主从复制 从表会根据主表进行同步\n\n我们在写数据时 写入主表 而读取从从表中读取\n\n#在主服务器上\nvi /etc/my.cnf\n\n#在[mysqld]加上 log-bin 开启主从复制 server-id 主从服务器的唯一标识\nlog-bin=mysql-bin\nserver-id=1\ninnodb_flush_log_at_trx_commit=1\nsync_binlog=1\n\nservice mysqld restart\n\nmysql -uroot -p\n\n# 需要 file和position的值 配置从服务器用\nshow master status;\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n#在从服务器上\nvi /etc/my.cnf\n\n#在[mysqld]加上\nserver-id=2\n\nmysql -uroot -p\n\nuse mysql;\ndrop table slave_master_info;\ndrop table slave_relat_log_info;\ndrop table slave_worker_info;\ndrop table innodb_index_stats;\ndrop table innodb_table_stats;\nsorce /usr/share/mysql/mysql_system_tables.sql;\n\nserveice mysqld restart\n\nmysql -uroot -p\n\nchange master to master_host=\'主服务器ip\',master_port=3306,master_user=\'root\',master_password=\'password\',master_log_file=\'刚刚在主服务器查到的值\',master_log_pos=同样也是之前查到的;\n\n# 开启从节点\nstart slave;\n\n# 查询结果 slave_io_runing和slave_sql_running都为yes才成功\nshow slave status\\g;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n\n# 读写分离\n\n需要先配置主从复制 写操作只写主服务器 而读取是从从服务器中数据\n\n# 修改server.xml\nvi /root/mycat/conf/server.xml\n\n\n1\n2\n\n\n        <user name="root" defaultaccount="true">\n          \t\t\t\t  \x3c!-- 密码 --\x3e\n                <property name="password">123456</property>\n         \t\t\t   \x3c!-- 库名 --\x3e\n                <property name="schemas">testdb</property>\n                <property name="defaultschema">testdb</property>\n        </user>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 修改schema.xml\nvi /root/mycat/conf/schema.xml\n\n\n1\n2\n\n\n\x3c!-- 建议百度找配置文件这个不太准确 --\x3e\n<mycat:schema xmlns:mycat="http://io.mycat/">\n     \x3c!-- name为主服务器 server文件的虚拟库名与name必须一致   --\x3e\n\t<schema name="testdb" checksqlschema="true" sqlmaxlimit="100" randomdatanode="dn1">\n\t\t<table name="customer" primarykey="id" datanode="dn1,dn2" rule="sharding-by-intfile" autoincrement="true" fetchstorenodebyjdbc="true">\n\t\t\t<childtable name="customer_addr" primarykey="id" joinkey="customer_id" parentkey="id"> </childtable>\n\t\t</table>\n\t</schema>\n    \x3c!-- name必须与datenode一致  database为真实的库名  --\x3e\n\t<datanode name="dn1" datahost="localhost1" database="db1" />\n\t<datanode name="dn2" datahost="localhost1" database="db2" />\n\t<datanode name="dn3" datahost="localhost1" database="db3" />\n\t<datahost name="localhost1" maxcon="1000" mincon="10" balance="0"\n\t\t\t  writetype="0" dbtype="mysql" dbdriver="jdbc" switchtype="1"  slavethreshold="100">\n\t\t<heartbeat>select user()</heartbeat>\n\t\t\x3c!-- 主服务器 host自定义  url为写的数据的地址+端口  user passwd --\x3e\n\t\t<writehost host="hostm1" url="jdbc:mysql://localhost:3306" user="root"\n\t\t\t\t   password="root">\n            \x3c!-- 从服务器 负责读 --\x3e\n            <readhost host="hosts1" url="从服务器+端口" user="" password="">\n\t\t</writehost>\n\t</datahost>\n</mycat:schema>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 重启 mycat\n./mycat restart\n\n\n1\n2\n\n\n使用时连接mycat的地址使用即可\n\n\n# 分库分表\n\n将庞大数据量拆分为不同的数据库和数据表进行存储\n\n\n# 水平拆分\n\n根据表的数据逻辑关系,将同一表中的数据按某种条件,拆分到多台服务器上,也叫横向拆分\n\n * 修改主服务器中 server.xml vi /root/mycat/conf/server.xml\n   \n   * \x3c!-- 添加property 配置主键方式 0为本地文件方式 --\x3e\n     <property name="sequncehandlertype">0</property>\n     \n     \n     1\n     2\n     \n\n * 修改主服务器的 sequence_conf.properties vi /root/mycat/conf/sequence_conf.properties\n   \n   * global.hisids=     #可以自定义关键字\n     global.minid=10001   #最小值\n     global.maxid=20000   #最大值\n     \n     \n     1\n     2\n     3\n     \n\n * 修改schema.xml vi /root/mycat/conf/schema.xml\n   \n   * <mycat:schema xmlns:mycat="http://io.mycat/">\n     \t<schema name="testdb" checksqlschema="false" sqlmaxlimit="100">\n     \t\t<table name="customer" primarykey="id" datanode="dn1,dn2,dn3" rule="mod-long">\n     \t\t</table>\n     \t</schema>\n         \x3c!-- name必须与datenode一致  database为真实的库名  --\x3e\n     \t<datanode name="dn1" datahost="localhost1" database="db1" />\n     \t<datanode name="dn2" datahost="localhost1" database="db2" />\n     \t<datanode name="dn3" datahost="localhost1" database="db3" />\n         \n     \t<datahost name="localhost1" maxcon="1000" mincon="10" balance="0"\n     \t\t\t  writetype="0" dbtype="mysql" dbdriver="native" switchtype="1"  slavethreshold="100">\n     \t\t<heartbeat>select user()</heartbeat>\n     \t\t\x3c!-- 主服务器 host自定义  url为写的数据的地址+端口  user passwd --\x3e\n     \t\t<writehost host="hostm1" url="localhost:3306" user="root"\n     \t\t\t\t   password="root">\n                 \x3c!-- 从服务器 负责读 --\x3e\n                 <readhost host="hosts1" url="从服务器+端口" user="" password="">\n     \t\t</writehost>\n     \t</datahost>\n     </mycat:schema>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     \n\n * 修改 rule.xml vi /root/mycat/conf/rule.xml\n   \n   * <function name=\'mod-log\' class=\'io.mycat.route.function.partitionbymod\'>\n         <property name=\'count\'>3</property>\n     </function>\n     \n     \n     1\n     2\n     3\n     \n\n插入数据列名必须完整 如果是自增键 则需要用 next value for mycatseq_global 不能用null\n\n以上配置用的是根据主键取模方式拆分\n\n\n# 垂直拆分\n\n根据业务的维度,将不同的表切分到不同数据库上,也叫纵向拆分\n\n * 修改schema.xml vi /root/mycat/conf/schema.xml\n   \n   * <mycat:schema xmlns:mycat="http://io.mycat/">\n     \t<schema name="testdb" checksqlschema="false" sqlmaxlimit="100">\n     \t\n     \t\t</table>\n             <table name="dog" primarykey="id" autoincrement=\'true\' datanode=\'dn4\'>\n     \t\t</table>\n             <table name="cat" primarykey="id" autoincrement=\'true\' datanode=\'dn5\'>\n     \t\t</table>\n     \t</schema>\n         \x3c!-- name必须与datenode一致  database为真实的库名  --\x3e\n     \t<datanode name="dn4" datahost="localhost1" database="db1" />\n     \t<datanode name="dn5" datahost="localhost1" database="db2" />\n         \n     \t<datahost name="localhost1" maxcon="1000" mincon="10" balance="0"\n     \t\t\t  writetype="0" dbtype="mysql" dbdriver="native" switchtype="1"  slavethreshold="100">\n     \t\t<heartbeat>select user()</heartbeat>\n     \t\t\x3c!-- 主服务器 host自定义  url为写的数据的地址+端口  user passwd --\x3e\n     \t\t<writehost host="hostm1" url="localhost:3306" user="root"\n     \t\t\t\t   password="root">\n                 \x3c!-- 从服务器 负责读 --\x3e\n                 <readhost host="hosts1" url="从服务器+端口" user="" password="">\n     \t\t</writehost>\n     \t</datahost>\n     </mycat:schema>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     \n     \n     根据 表名 区分库 如dog 存放在db1中 cat存放在db2中',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Nosql",frontmatter:{title:"Nosql",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4a12e8/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/21.Nosql.html",relativePath:"后端/04.SQL/21.Nosql.md",key:"v-b10c11a2",path:"/pages/4a12e8/",headers:[{level:2,title:"关系型数据库遵循ACID规则",slug:"关系型数据库遵循acid规则",normalizedTitle:"关系型数据库遵循acid规则",charIndex:171}],headersStr:"关系型数据库遵循ACID规则",content:"# Nosql\n\n关系型数据 缺点 1. 性能瓶颈:磁盘IO性能低下 2. 扩展瓶颈:数据关系复杂, 扩展性差,不便于大规模集群\n\nNoSQL:即 Not-Only SQL ( 泛指非关系型数据) 作为关系型数据的补充\n\n常见 Nosql数据:\n\n * Redis\n * memcache\n * HBase\n * MongoDB\n\n\n\n\n# 关系型数据库遵循ACID规则\n\n事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性：\n\n1、A (Atomicity) 原子性\n\n原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。\n\n比如银行转账，从A账户转100元至B账户，分为两个步骤：1）从A账户取100元；2）存入100元至B账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。\n\n2、C (Consistency) 一致性\n\n一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。\n\n例如现有完整性约束a+b=10，如果一个事务改变了a，那么必须得改变b，使得事务结束后依然满足a+b=10，否则事务失败。\n\n3、I (Isolation) 独立性\n\n所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。\n\n比如现在有个交易是从A账户转100元至B账户，在这个交易还未完成的情况下，如果此时B查询自己的账户，是看不到新增加的100元的。\n\n4、D (Durability) 持久性\n\n持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。",normalizedContent:"# nosql\n\n关系型数据 缺点 1. 性能瓶颈:磁盘io性能低下 2. 扩展瓶颈:数据关系复杂, 扩展性差,不便于大规模集群\n\nnosql:即 not-only sql ( 泛指非关系型数据) 作为关系型数据的补充\n\n常见 nosql数据:\n\n * redis\n * memcache\n * hbase\n * mongodb\n\n\n\n\n# 关系型数据库遵循acid规则\n\n事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性：\n\n1、a (atomicity) 原子性\n\n原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。\n\n比如银行转账，从a账户转100元至b账户，分为两个步骤：1）从a账户取100元；2）存入100元至b账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。\n\n2、c (consistency) 一致性\n\n一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。\n\n例如现有完整性约束a+b=10，如果一个事务改变了a，那么必须得改变b，使得事务结束后依然满足a+b=10，否则事务失败。\n\n3、i (isolation) 独立性\n\n所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。\n\n比如现在有个交易是从a账户转100元至b账户，在这个交易还未完成的情况下，如果此时b查询自己的账户，是看不到新增加的100元的。\n\n4、d (durability) 持久性\n\n持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"锁",frontmatter:{title:"锁",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/3c034d/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/19.%E9%94%81.html",relativePath:"后端/04.SQL/19.锁.md",key:"v-3a75541f",path:"/pages/3c034d/",headers:[{level:2,title:"共享锁",slug:"共享锁",normalizedTitle:"共享锁",charIndex:52},{level:2,title:"排他锁",slug:"排他锁",normalizedTitle:"排他锁",charIndex:101},{level:2,title:"MyISAM 读锁",slug:"myisam-读锁",normalizedTitle:"myisam 读锁",charIndex:570},{level:2,title:"MyISAM  写锁",slug:"myisam-写锁",normalizedTitle:"myisam  写锁",charIndex:null},{level:2,title:"悲观锁和乐观锁",slug:"悲观锁和乐观锁",normalizedTitle:"悲观锁和乐观锁",charIndex:767}],headersStr:"共享锁 排他锁 MyISAM 读锁 MyISAM  写锁 悲观锁和乐观锁",content:"# 锁\n\n为了保证数据的一致性 在共享数据被并发访问时变得安全的规则\n\n * 按操作\n   \n   * 共享锁 读锁 针对同一份数据 多个事务读取操作可以同时加锁而不互相影响 但不能修改数据\n   * 排他锁 写锁 当前操作没有完成前 会阻断其他操作的读取和写入\n\n * 按力度\n   \n   * 表级锁 会锁定整个表 开销小 加锁快 锁定力度大 发生锁冲突概率高 并发度低 不会出现死锁情况\n   * 行级锁 会锁定当前行 开销大 加锁慢 锁定力度小 发生锁冲突概率低 并发高 会出现死锁情况\n\n * 按使用方式\n   \n   * 悲观锁 每次查询数据都认为别人会修改 查询时加锁\n   * 乐观锁 每次查询数据都认为别人不会修改 但更新时会判断一下期间别人有没有去更新这个数据\n\n\n\n\n# 共享锁\n\n特点: 数据可以被多个事务查询 但不能修改\n\nInnoDB 默认的是加行锁 如果列没有索引则加的是表锁\n\n如在有锁情况下修改 锁释放 修改数据失败\n\n * 创建 select语句 lock in share mode;\n\n\n# 排他锁\n\n加锁后 不能被其他事务加锁查询或者修改 (即不能存在第二把锁)\n\n普通查询可以 加锁查询不能 普通和加锁修改都无法执行 必须提交事务\n\n * 创建 select语句 for update;\n\n\n# MyISAM 读锁\n\n所有连接只能查询不能修改数据\n\nMyISAM 不支持事务 和 行锁 当前表也不能修改\n\n * 加锁 lock table 表名 read;\n * 解锁 unlock tables;\n\n\n# MyISAM 写锁\n\n其他连接不能查询和修改数据 当前表可以查询和修改数据\n\n * 加锁 lock table 表名 write;\n * 解锁 unlock tables;\n\n\n# 悲观锁和乐观锁\n\n以上都都是悲观锁 悲观锁一般都是依靠关系型数据库提供的锁机制\n\n悲观锁和乐观锁 差别是一个执行前上锁 一个是不上锁 只是在更新时查看是否与被修改\n\n * 添加一个版本号列 每次更新版本号+1 并在更新时检测版本号是否与查询时相同",normalizedContent:"# 锁\n\n为了保证数据的一致性 在共享数据被并发访问时变得安全的规则\n\n * 按操作\n   \n   * 共享锁 读锁 针对同一份数据 多个事务读取操作可以同时加锁而不互相影响 但不能修改数据\n   * 排他锁 写锁 当前操作没有完成前 会阻断其他操作的读取和写入\n\n * 按力度\n   \n   * 表级锁 会锁定整个表 开销小 加锁快 锁定力度大 发生锁冲突概率高 并发度低 不会出现死锁情况\n   * 行级锁 会锁定当前行 开销大 加锁慢 锁定力度小 发生锁冲突概率低 并发高 会出现死锁情况\n\n * 按使用方式\n   \n   * 悲观锁 每次查询数据都认为别人会修改 查询时加锁\n   * 乐观锁 每次查询数据都认为别人不会修改 但更新时会判断一下期间别人有没有去更新这个数据\n\n\n\n\n# 共享锁\n\n特点: 数据可以被多个事务查询 但不能修改\n\ninnodb 默认的是加行锁 如果列没有索引则加的是表锁\n\n如在有锁情况下修改 锁释放 修改数据失败\n\n * 创建 select语句 lock in share mode;\n\n\n# 排他锁\n\n加锁后 不能被其他事务加锁查询或者修改 (即不能存在第二把锁)\n\n普通查询可以 加锁查询不能 普通和加锁修改都无法执行 必须提交事务\n\n * 创建 select语句 for update;\n\n\n# myisam 读锁\n\n所有连接只能查询不能修改数据\n\nmyisam 不支持事务 和 行锁 当前表也不能修改\n\n * 加锁 lock table 表名 read;\n * 解锁 unlock tables;\n\n\n# myisam 写锁\n\n其他连接不能查询和修改数据 当前表可以查询和修改数据\n\n * 加锁 lock table 表名 write;\n * 解锁 unlock tables;\n\n\n# 悲观锁和乐观锁\n\n以上都都是悲观锁 悲观锁一般都是依靠关系型数据库提供的锁机制\n\n悲观锁和乐观锁 差别是一个执行前上锁 一个是不上锁 只是在更新时查看是否与被修改\n\n * 添加一个版本号列 每次更新版本号+1 并在更新时检测版本号是否与查询时相同",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Redis高级",frontmatter:{title:"Redis高级",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4e4f7f/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/23.Redis%E9%AB%98%E7%BA%A7.html",relativePath:"后端/04.SQL/23.Redis高级.md",key:"v-47095aee",path:"/pages/4e4f7f/",headers:[{level:2,title:"过期数据",slug:"过期数据",normalizedTitle:"过期数据",charIndex:14},{level:3,title:"定时删除",slug:"定时删除",normalizedTitle:"定时删除",charIndex:194},{level:3,title:"惰性删除",slug:"惰性删除",normalizedTitle:"惰性删除",charIndex:309},{level:3,title:"定期删除",slug:"定期删除",normalizedTitle:"定期删除",charIndex:485},{level:3,title:"淘汰策略",slug:"淘汰策略",normalizedTitle:"淘汰策略",charIndex:991},{level:2,title:"主从复制",slug:"主从复制",normalizedTitle:"主从复制",charIndex:1810},{level:3,title:'互联网 "三高" 架构',slug:"互联网-三高-架构",normalizedTitle:"互联网 &quot;三高&quot; 架构",charIndex:null},{level:3,title:"阶段一:建立连接阶段",slug:"阶段一-建立连接阶段",normalizedTitle:"阶段一:建立连接阶段",charIndex:1952},{level:3,title:"阶段二:数据同步阶段工作流程",slug:"阶段二-数据同步阶段工作流程",normalizedTitle:"阶段二:数据同步阶段工作流程",charIndex:2569},{level:3,title:"阶段三:命令传播阶段",slug:"阶段三-命令传播阶段",normalizedTitle:"阶段三:命令传播阶段",charIndex:3245},{level:3,title:"频繁的全量复制",slug:"频繁的全量复制",normalizedTitle:"频繁的全量复制",charIndex:3980},{level:2,title:"哨兵模式",slug:"哨兵模式",normalizedTitle:"哨兵模式",charIndex:5286},{level:3,title:"配置哨兵",slug:"配置哨兵",normalizedTitle:"配置哨兵",charIndex:5421},{level:3,title:"阶段一:监控阶段",slug:"阶段一-监控阶段",normalizedTitle:"阶段一:监控阶段",charIndex:5893},{level:3,title:"阶段二:通知阶段",slug:"阶段二-通知阶段",normalizedTitle:"阶段二:通知阶段",charIndex:5986},{level:3,title:"阶段三:故障转移",slug:"阶段三-故障转移",normalizedTitle:"阶段三:故障转移",charIndex:6067},{level:2,title:"Cluster集群",slug:"cluster集群",normalizedTitle:"cluster集群",charIndex:6418},{level:3,title:"Cluster集群结构设计",slug:"cluster集群结构设计",normalizedTitle:"cluster集群结构设计",charIndex:6480},{level:3,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:1196},{level:3,title:"启动Culster",slug:"启动culster",normalizedTitle:"启动culster",charIndex:7046},{level:3,title:"节点增删",slug:"节点增删",normalizedTitle:"节点增删",charIndex:7343},{level:2,title:"缓存预热",slug:"缓存预热",normalizedTitle:"缓存预热",charIndex:8372},{level:2,title:"缓存雪崩",slug:"缓存雪崩",normalizedTitle:"缓存雪崩",charIndex:8815},{level:2,title:"缓存击穿",slug:"缓存击穿",normalizedTitle:"缓存击穿",charIndex:9509},{level:2,title:"缓存穿透",slug:"缓存穿透",normalizedTitle:"缓存穿透",charIndex:10115},{level:2,title:"性能指标监控",slug:"性能指标监控",normalizedTitle:"性能指标监控",charIndex:10988},{level:2,title:"性能指标工具",slug:"性能指标工具",normalizedTitle:"性能指标工具",charIndex:11645}],headersStr:'过期数据 定时删除 惰性删除 定期删除 淘汰策略 主从复制 互联网 "三高" 架构 阶段一:建立连接阶段 阶段二:数据同步阶段工作流程 阶段三:命令传播阶段 频繁的全量复制 哨兵模式 配置哨兵 阶段一:监控阶段 阶段二:通知阶段 阶段三:故障转移 Cluster集群 Cluster集群结构设计 配置 启动Culster 节点增删 缓存预热 缓存雪崩 缓存击穿 缓存穿透 性能指标监控 性能指标工具',content:'# Redis高级\n\n\n# 过期数据\n\nRedis是一个内存级别的数据库 所有数据都存放在内存中 我们可以通过指令TTL获取指定数据状态\n\n * XX 具有时效性的数据\n * -1 永久有效的数据\n * -2 已经过期的数据 或被 已删除 或 未定义的数据\n\n\n\n我们通过SETEX设置的有效时间 redis会新开辟一个键值对 用来存储key对应的value地址值 以及存活时间\n\n\n# 定时删除\n\n创建一个定时器,当key设置的过期时间到达时,由定时器执行的对键删除操作\n\n优化:节约内存 快速释放不必要的内容\n\n缺点:cpu负载量大 会影响redis服务器响应时间和指令吞吐量\n\n使用处理性能换取存储空间\n\n\n# 惰性删除\n\n数据到达过期时间,不做处理,等下次访问该数据时删除\n\n * 如果未过期 返回数据\n * 如果已过期 删除 返回不存在\n\n通过get方法查询时,get绑定expirelfNeeded()方法 此方法会检测数据是否过期\n\n优点:节约cpu性能 只有访问时才进行判别删除\n\n缺点:内存压力大 出现长期占用内容\n\n用存储空间换取处理器性能\n\n\n# 定期删除\n\n通过设置redis.config文件中的 hz 属性 默认为10\n\n为每秒执行hz的次数\n\n设置后服务器每秒执行hz次 serverCron() --\x3e databasesCron() --\x3e activeExpireCycle()\n\nactiveExpireCycle() 对每个expires[*](分成若干区) 逐一进行检测 每次执行250ms/hz\n\n对某个expires[*]检测时,随机挑选W个key检测\n\n * 如果key超时,删除key\n * 如果一轮中删除key的数据>W*25%,循环该过程\n * 如果一轮中删除key的数据<=W*25%,则检查下一个expires[*]\n * W取值=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP属性值\n\n参数current_db用于记录activeExpireCycle()进入哪个expires[*]执行\n\n如果activeExpireCyle()执行时间到期,下次从current_db继续向下执行\n\n优点:cpu性能占用设置峰值 检测频度可自定义\n\n内容压力不是很大 长期占用内存的冷数据会被清理掉\n\n\n# 淘汰策略\n\n当redis使用内容存储数据,执行每一个命令前,会调用freeMemoryIfNeeded()检测内存是否充足.如果不满足加入数据的最低存储要求,则redis会为当前指令清理存储空间.清理数据的策略又称为逐出算法\n\n逐出数据的过程不是百分百能够清理出足够的可使用空间,如果不成功则反复执行.当对所有数据尝试完毕,如果仍然不能到达内存清理要求,将出现错误信息\n\n而redis中内存空间是根据物理内存和配置信息来指定\n\n * maxmenory ?mb 默认值为0不做限制 通常设置在50%以上\n\n * maxmemory-samples count 每次选取待删除数据的个数,采用随机获取数据的方式作为待检测删除数据\n\n * maxmemory-policy policy 对数据进行删除的选择策略\n   \n   * 检测易失数据(可能会过期的数据集server.db[i].expires)\n     * volatile-lru 挑选最近最少使用的数据淘汰\n     * valatile-lfu 挑选最近使用次数最少的数据淘汰\n     * valatile-tfl 挑选将要过期的数据淘汰\n     * volatile-random 任意选择数据淘汰\n     * \n   * 检测全库数据(所有数据集server.db[i].dict)\n     * allkeys-lru 挑选最近最少使用的数据淘汰\n     * allkeys-lfu 挑选最近使用次数最少的数据淘汰\n     * allkeys-random 任意选择数据淘汰\n   * 放弃数据驱逐\n     * no-enviction(驱逐) 禁止驱逐数据(redis4.0中默认策略) 会引发错误OOM(Out Of Memory)\n   \n   使用INFO命令输出监控信息,查询缓存hit和miss的次数,根据业务需求调用Redis配置\n\n\n# 主从复制\n\n\n\n\n# 互联网 "三高" 架构\n\n * 高并发\n * 高性能\n * 高可用\n   * 可用性 = (1年的总时间 - 1年宕机的总时间) / 1年的总时间 * 100% : 业界可用性目标5个9 即99.999% 即服务器年宕机时长低于315秒 约5.25分钟\n\n\n# 阶段一:建立连接阶段\n\n建立slave到master的连接,使用master能够识别slave,并保存slave端口号\n\n 1.  saveof ip port : salve发送指令给master\n 2.  master接受到指令,响应对方\n 3.  保存master的ip与端口 masterhost 和 masterport\n 4.  根据保存的信息创建连接master的socket\n 5.  周期性发送ping指令\n 6.  响应pong\n 7.  发送指令 auth password (如无密码则忽略)\n 8.  验证授权\n 9.  replconf listening-port <port-number>\n 10. 保存slave的端口号\n\n# 主从连接 (slave连接mazster)\n\n * 方式一 从机客户端发送命令 slaveof masterip masterport\n * 方式二 启动服务器时连接 redis-server -slaveof masterip masterport\n * 方式三 服务器配置 在从机服务器的redis服务器配置文件 加上 slaveof masterip masterport\n\n# 主从断开连接\n\n * 断开slave与master的连接 不会删除已有数据 只是不再接受master发送的数据 salveof no one 在从机上执行\n\n# 授权访问\n\n\n\n\n# 阶段二:数据同步阶段工作流程\n\n\n\n 1. 如果master数据量巨大,数据同步阶段应该避开流量高峰期,避免造成阻塞\n\n 2. 如果复制缓冲区设置不合理,会导致数据溢出. 如果全量复制周期太长,进行部分复制时发现数据已经存在丢失,必须进行第二次全量复制 导致slave陷入死循环\n    \n    #在主服务器配置文件配置 配置缓存区大小\n    repl-backlog-size ?mb\n    \n    \n    1\n    2\n    \n\n 3. master单机内存占用主机内存比例应在50%-70%,留下30-50作为bgsave命令和创建复制缓冲区\n\n 4. 在slave进行全量复制/部分复制时 建议关闭此时对外服务\n    \n    #在从服务器配置文件中配置  关闭和开启\n    slave-serve-stale-data yes|no\n    \n    \n    1\n    2\n    \n\n 5. 数据同步阶段 master发送给slave信息可以理解为master是slave的一个客户端 主动向slave发送命令\n\n 6. 多个slave同时对master请求数据同步 master发送的rdb文件增多 如果master带宽不足 会对带宽造成巨大冲击 建议适量错峰请求同步\n\n 7. salve过多时 建议调整拓扑结构 由一主多从结构变为树状结构 中间的节点即使master 也salve 使用树状结构时 由于层级深度 导致深度越高的slave与最顶层master间数据同步延迟较大 数据一致性变差 应谨慎选择\n\n\n# 阶段三:命令传播阶段\n\n命令传播阶段出现了断网现象：\n\n网络闪断闪连：忽略\n\n短时间网络中断：部分复制\n\n长时间网络中断：全量复制\n\n工作原理\n\n * 通过offset区分不同的slave当前数据传播的差异\n * master记录已发送的信息对应的offset\n * slave记录已接收的信息对应的offset\n\n\n\n# 流程更新\n\n\n\n# 心跳机制\n\n进入命令传播阶段候，master与slave间需要进行信息交换，使用心跳机制进行维护，实现双方连接保持在线\n\nmaster心跳：\n\n * 内部指令：PING\n * 周期：由repl-ping-slave-period决定，默认10秒\n * 作用：判断slave是否在线\n * 查询：INFO replication 获取slave最后一次连接时间间隔，lag项维持在0或1视为正常\n\nslave心跳任务\n\n * 内部指令：REPLCONF ACK {offset}\n * 周期：1秒\n * 作用1：汇报slave自己的复制偏移量，获取最新的数据变更指令\n * 作用2：判断master是否在线\n\n心跳阶段注意事项：\n\n * 当slave多数掉线，或延迟过高时，master为保障数据稳定性，将拒绝所有信息同步\n\n#在主服务器配置文件中配置\nmin-slaves-to-write 2\nmin-slaves-max-lag 8\n\n\n1\n2\n3\n\n\nslave数量少于2个，或者所有slave的延迟都大于等于8秒时，强制关闭master写功能，停止数据同步\n\n * slave数量由slave发送REPLCONF ACK命令做确认\n\n * slave延迟由slave发送REPLCONF ACK命令做确认\n\n\n\n\n# 频繁的全量复制\n\n * 伴随着系统的运行，master的数据量会越来越大，一旦master重启，runid将发生变化，会导致全部slave的全量复制操作\n\n内部优化调整方案：\n\n1：master内部创建master_replid变量，使用runid相同的策略生成，长度41位，并发送给所有slave\n\n2：在master关闭时执行命令shutdown save，进行RDB持久化,将runid与offset保存到RDB文件中\n\n * repl-id repl-offset\n\n * 通过redis-check-rdb命令可以查看该信息\n\n3：master重启后加载RDB文件，恢复数据，重启后，将RDB文件中保存的repl-id与repl-offset加载到内存中\n\n * master_repl_id=repl master_repl_offset =repl-offset\n\n * 通过info命令可以查看该信息\n\n作用：本机保存上次runid，重启后恢复该值，使所有slave认为还是之前的master\n\n * 第二种出现频繁全量复制的问题现象：网络环境不佳，出现网络中断，slave不提供服务\n\n问题原因：复制缓冲区过小，断网后slave的offset越界，触发全量复制\n\n最终结果：slave反复进行全量复制\n\n解决方案：修改复制缓冲区大小\n\nrepl-backlog-size ?mb\n\n\n1\n\n\n建议设置如下：\n\n1.测算从master到slave的重连平均时长second\n\n2.获取master平均每秒产生写命令数据总量write_size_per_second\n\n3.最优复制缓冲区空间 = 2 * second * write_size_per_second\n\n# 频繁的网络中断\n\n * 问题现象：master的CPU占用过高 或 slave频繁断开连接\n\n问题原因\n\n * slave每1秒发送REPLCONFACK命令到master\n * 当slave接到了慢查询时（keys * ，hgetall等），会大量占用CPU性能\n * master每1秒调用复制定时函数replicationCron()，比对slave发现长时间没有进行响应\n\n最终结果：master各种资源（输出缓冲区、带宽、连接等）被严重占用\n\n解决方案：通过设置合理的超时时间，确认是否释放slave\n\nrepl-timeout seconds\n\n\n1\n\n\n该参数定义了超时时间的阈值（默认60秒），超过该值，释放slave\n\n# 数据不一致\n\n问题现象：多个slave获取相同数据不同步\n\n问题原因：网络信息不同步，数据发送有延迟\n\n解决方案\n\n * 优化主从间的网络环境，通常放置在同一个机房部署，如使用阿里云等云服务器时要注意此现象\n * 监控主从节点延迟（通过offset）判断，如果slave延迟过大，暂时屏蔽程序对该slave的数据访问\n\nslave-serve-stale-data\tyes|no\n\n\n1\n\n\n开启后仅响应info、slaveof等少数命令（慎用，除非对数据一致性要求很高）\n\n\n# 哨兵模式\n\n哨兵(sentinel) 是一个分布式系统,用于对主从结构中的每台服务器进行监控,当出现故障时通过投票机制选择新的master并将所有slave连接到新的master\n\n哨兵也是一台redis服务器 只是不提供数据相关服务 通常哨兵的数量配置为单数\n\n\n# 配置哨兵\n\n通过sentinel.conf 配置\n\n * sentinel monitor master_name master_host master_port sentinel_number 设置哨兵监听的主服务器信息 sentinel_number表示参与投票的哨兵数量 设置超过哨兵半数即可\n * sentinel down-after-milliseconds master_name million_seconds 设置判定服务器宕机时长 该设置控制是否进行主从切换\n * sentinel failover-timeout master_name million_seconds 设置故障切换的最大超时时长\n * sentinel parallel-syncs master_name sync_slave_number 设置主从切换后,同时进行数据的slave数量 值越大 要求网络资源越高 值越小 同步时间越长\n * redis-sentinel filename(sentinel.conf配置文件路径) 启动哨兵\n\n\n# 阶段一:监控阶段\n\n * 获取各个sentinel的状态（是否在线）\n\n * 获取master的状态\n\n * 获取所有slave的状态（根据master中的slave信息）\n\n\n\n\n# 阶段二:通知阶段\n\nsentinel在通知阶段要不断的去获取master/slave的信息，然后在各个sentinel之间进行共享，具体的流程如下：\n\n\n\n\n# 阶段三:故障转移\n\n# 判断master宕机\n\n\n\n主观下线:任意一个sentine认为master已经下线\n\n客观下线:半数以上的sentine认为master已经下线\n\n# 选举master\n\n每个sentine都有一票 最终票最多的称为处理事故的哨兵服务\n\n\n\n首先它有一个在服务器列表中挑选备选master的原则\n\n * 不在线的OUT\n\n * 响应慢的OUT\n\n * 与原master断开时间久的OUT\n\n * 优先原则\n   \n   优先级 offset runid\n\n总结：故障转移阶段\n\n 1. 发现问题，主观下线与客观下线\n 2. 竞选负责人\n 3. 优选新master\n 4. 新master上任，其他slave切换master，原master作为slave故障恢复后连接\n\n\n# Cluster集群\n\n集群就是使用网络将若干台计算机联通起来，并提供统一的管理方式，使其对外呈现单机的服务效果\n\n\n\n\n# Cluster集群结构设计\n\n数据存储设计：\n\n 1. 通过算法设计，计算出key应该保存的位置\n\n 2. 将所有的存储空间计划切割成16384份，每台主机保存一部分\n    \n    注意：每份代表的是一个存储空间，不是一个key的保存空间\n\n 3. 将key按照计算出的结果放到对应的存储空间\n\n那redis的集群是如何增强可扩展性的呢？譬如我们要增加一个集群节点\n\n\n\n当我们查找数据时，集群是如何操作的呢？\n\n * 各个数据库相互通信，保存各个库中槽的编号数据\n * 一次命中，直接返回\n * 一次未命中，告知具体位置\n\n\n\n\n# 配置\n\n在redis-6401.conf中配置\n\n * cluster-enabled yes|no 是否启用cluster，加入cluster节点\n * cluster-config-file cluster-6503.conf cluster配置文件名，该文件属于自动生成，仅用于快速查找文件并查询文件内容\n * cluster-node-timeout milliseconds 节点服务响应超时时间，用于判定该节点是否下线或切换为从节点 毫秒数\n * cluster-migration-barrier min_slave_number master连接的slave最小数量\n\n\n# 启动Culster\n\nredis-cli –-cluster create masterhost1:masterport1 masterhost2:masterport2  masterhost3:masterport3 [masterhostn:masterportn …] slavehost1:slaveport1  slavehost2:slaveport2 slavehost3:slaveport3 -–cluster-replicas n\n\n\n1\n\n\nn为多少个主服务器 前n个ip为主服务器 后面为slave服务器\n\n * cluster nodes 查询集群节点信息\n\n\n# 节点增删\n\n * 添加master到当前集群中，连接时可以指定任意现有节点地址与端口\n   \n   redis-cli --cluster add-node new-master-host:new-master-port now-host:now-port\n   \n   \n   1\n   \n\n * 添加slave\n   \n   redis-cli --cluster add-node new-slave-host:new-slave-port master-host:master-port --cluster-slave --cluster-master-id masterid\n   \n   \n   1\n   \n\n * 删除节点，如果删除的节点是master，必须保障其中没有槽slot\n   \n   redis-cli --cluster del-node del-slave-host:del-slave-port del-slave-id\n   \n   \n   1\n   \n\n * 重新分槽，分槽是从具有槽的master中划分一部分给其他master，过程中不创建新的槽\n   \n   redis-cli --cluster reshard new-master-host:new-master:port --cluster-from src-  master-id1, src-master-id2, src-master-idn --cluster-to target-master-id --  cluster-slots slots\n   #将需要参与分槽的所有masterid不分先后顺序添加到参数中，使用，分隔\n   #指定目标得到的槽的数量，所有的槽将平均从每个来源的master处获取\n   \n   \n   1\n   2\n   3\n   \n\n * 重新分配槽，从具有槽的master中分配指定数量的槽到另一个master中，常用于清空指定master中的槽\n   \n   redis-cli --cluster reshard src-master-host:src-master-port --cluster-from src-  master-id --cluster-to target-master-id --cluster-slots slots --cluster-yes\n   \n   \n   1\n   \n\n\n# 缓存预热\n\n1.请求数量较高，大量的请求过来之后都需要去从缓存中获取数据，但是缓存中又没有，此时从数据库中查找数据然后将数据再存入缓存，造成了短期内对redis的高强度操作从而导致问题\n\n2.主从之间数据吞吐量较大，数据同步操作频度较高\n\n * 前置准备工作：\n\n1.日常例行统计数据访问记录，统计访问频度较高的热点数据\n\n2.利用LRU数据删除策略，构建数据留存队列例如：storm与kafka配合\n\n * 准备工作：\n\n1.将统计结果中的数据分类，根据级别，redis优先加载级别较高的热点数据\n\n2.利用分布式多服务器同时进行数据读取，提速数据加载过程\n\n3.热点数据主从同时预热\n\n * 实施：\n\n4.使用脚本程序固定触发数据预热过程\n\n5.如果条件允许，使用了CDN（内容分发网络），效果会更好\n\n总的来说：缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！\n\n\n# 缓存雪崩\n\n1.系统平稳运行过程中，忽然数据库连接量激增\n\n2.应用服务器无法及时处理请求\n\n3.大量408，500错误页面出现\n\n4.客户反复刷新页面获取数据\n\n5.数据库崩溃\n\n6.应用服务器崩溃\n\n7.重启应用服务器无效\n\n8.Redis服务器崩溃\n\n9.Redis集群崩溃\n\n10.重启数据库后再次被瞬间流量放倒\n\n解决方案\n\n * 思路：\n\n1.更多的页面静态化处理\n\n2.构建多级缓存架构\n\nNginx缓存+redis缓存+ehcache缓存\n\n3.检测Mysql严重耗时业务进行优化\n\n对数据库的瓶颈排查：例如超时查询、耗时较高事务等\n\n4.灾难预警机制\n\n监控redis服务器性能指标\n\nCPU占用、CPU使用率\n\n内存容量\n\n查询平均响应时间\n\n线程数\n\n5.限流、降级\n\n短时间范围内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后再逐步放开访问\n\n * 落地实践：\n\n1.LRU与LFU切换\n\n2.数据有效期策略调整\n\n根据业务数据有效期进行分类错峰，A类90分钟，B类80分钟，C类70分钟\n\n过期时间使用固定时间+随机值的形式，稀释集中到期的key的数量\n\n3.超热数据使用永久key\n\n4.定期维护（自动+人工）\n\n对即将过期数据做访问量分析，确认是否延时，配合访问量统计，做热点数据的延时\n\n5.加锁：慎用！\n\n总的来说：缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力。如能够有效避免过期时间集中，可以有效解决雪崩现象的 出现（约40%），配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整。\n\n\n# 缓存击穿\n\n1.系统平稳运行过程中\n\n2.数据库连接量瞬间激增\n\n3.Redis服务器无大量key过期\n\n4.Redis内存平稳，无波动\n\n5.Redis服务器CPU正常\n\n6.数据库崩溃\n\n问题排查：\n\n1.Redis中某个key过期，该key访问量巨大\n\n2.多个数据请求从服务器直接压到Redis后，均未命中\n\n3.Redis在短时间内发起了大量对数据库中同一数据的访问\n\n总而言之就两点：单个key高热数据，key过期\n\n解决方案：\n\n1.预先设定\n\n以电商为例，每个商家根据店铺等级，指定若干款主打商品，在购物节期间，加大此类信息key的过期时长 注意：购物节不仅仅指当天，以及后续若干天，访问峰值呈现逐渐降低的趋势\n\n2.现场调整\n\n监控访问量，对自然流量激增的数据延长过期时间或设置为永久性key\n\n3.后台刷新数据\n\n启动定时任务，高峰期来临之前，刷新数据有效期，确保不丢失\n\n4.二级缓存\n\n设置不同的失效时间，保障不会被同时淘汰就行\n\n5.加锁\n\n分布式锁，防止被击穿，但是要注意也是性能瓶颈，慎重！\n\n总的来说：缓存击穿就是单个高热数据过期的瞬间，数据访问量较大，未命中redis后，发起了大量对同一数据的数据库访问，导致对数 据库服务器造成压力。应对策略应该在业务数据分析与预防方面进行，配合运行监控测试与即时调整策略，毕竟单个key的过 期监控难度较高，配合雪崩处理策略即可。\n\n\n# 缓存穿透\n\n1.系统平稳运行过程中\n\n2.应用服务器流量随时间增量较大\n\n3.Redis服务器命中率随时间逐步降低\n\n4.Redis内存平稳，内存无压力\n\n5.Redis服务器CPU占用激增\n\n6.数据库服务器压力激增\n\n7.数据库崩溃\n\n问题排查：\n\n1.Redis中大面积出现未命中\n\n2.出现非正常URL访问\n\n问题分析：\n\n * 获取的数据在数据库中也不存在，数据库查询未得到对应数据\n * Redis获取到null数据未进行持久化，直接返回\n * 下次此类数据到达重复上述过程\n * 出现黑客攻击服务器\n\n解决方案：\n\n1.缓存null\n\n对查询结果为null的数据进行缓存（长期使用，定期清理），设定短时限，例如30-60秒，最高5分钟\n\n2.白名单策略\n\n提前预热各种分类数据id对应的bitmaps，id作为bitmaps的offset，相当于设置了数据白名单。当加载正常数据时放行，加载异常数据时直接拦截（效率偏低）\n\n使用布隆过滤器（有关布隆过滤器的命中问题对当前状况可以忽略）\n\n2.实施监控\n\n实时监控redis命中率（业务正常范围时，通常会有一个波动值）与null数据的占比\n\n非活动时段波动：通常检测3-5倍，超过5倍纳入重点排查对象\n\n活动时段波动：通常检测10-50倍，超过50倍纳入重点排查对象\n\n根据倍数不同，启动不同的排查流程。然后使用黑名单进行防控（运营）\n\n4.key加密\n\n问题出现后，临时启动防灾业务key，对key进行业务层传输加密服务，设定校验程序，过来的key校验\n\n例如每天随机分配60个加密串，挑选2到3个，混淆到页面数据id中，发现访问key不满足规则，驳回数据访问\n\n总的来说：缓存击穿是指访问了不存在的数据，跳过了合法数据的redis数据缓存阶段，每次访问数据库，导致对数据库服务器造成压力。通常此类数据的出现量是一个较低的值，当出现此类情况以毒攻毒，并及时报警。应对策略应该在临时预案防范方面多做文章。\n\n无论是黑名单还是白名单，都是对整体系统的压力，警报解除后尽快移除。\n\n\n# 性能指标监控\n\n性能指标\n\n * latency 响应请求的平均时间\n * instantaneous_ops_per_sec 平均每秒处理请求总数\n * hit_rate(calculated) 缓存查询命中率（通过查询总次数与查询得到非nil数据总次数计算而来）\n\n内存指标\n\n * used_memory 当前内存使用量\n * mem_fragmentation_ratio 内存碎片率（关系到是否进行碎片整理）\n * evicted_keys 为避免内存溢出删除的key的总数量\n * blocked_clients 基于阻塞操作（BLPOP等）影响的客户端数量\n\n活动指标\n\n * connected_clients 当前客户端连接总数\n * connected_slaves 当前连接slave总数\n * master_last_io_seconds_ago 最后一次主从信息交换距现在的秒\n * keyspace key的总数\n\n持久性指标\n\n * rdb_last_save_time 当前服务器最后一次RDB持久化的时间\n * rdb_changes_since_last_save 当前服务器最后一次RDB持久化后数据变化总量\n\n错误指标\n\n * rejected_connections 被拒绝连接的客户端总数（基于达到最大连接值的因素）\n * keyspace_misses key未命中的总次数\n * master_link_down_since_seconds 主从断开的秒数\n\n\n# 性能指标工具\n\n测试当前服务器的并发性能\n\nredis-benchmark [-h ] [-p ] [-c ] [-n <requests]> [-k ]\n\n\n1\n\n\n\n\n启动服务器调试信息\n\nmonitor\n\n\n1\n\n\n慢日志 记录查询慢的日志\n\n * slowlog [operator]\n   * get 获取慢日志信息\n   * len 获取慢日志条目数\n   * reset 重置慢查询日志\n\n慢日志通过redis-xxx.conf配置\n\nslowlog-log-slower-than 1000 #设置慢查询的时间下线，单位：微妙\nslowlog-max-len 100\t#设置慢查询命令对应的日志显示长度，单位：命令数\n\n\n1\n2\n',normalizedContent:'# redis高级\n\n\n# 过期数据\n\nredis是一个内存级别的数据库 所有数据都存放在内存中 我们可以通过指令ttl获取指定数据状态\n\n * xx 具有时效性的数据\n * -1 永久有效的数据\n * -2 已经过期的数据 或被 已删除 或 未定义的数据\n\n\n\n我们通过setex设置的有效时间 redis会新开辟一个键值对 用来存储key对应的value地址值 以及存活时间\n\n\n# 定时删除\n\n创建一个定时器,当key设置的过期时间到达时,由定时器执行的对键删除操作\n\n优化:节约内存 快速释放不必要的内容\n\n缺点:cpu负载量大 会影响redis服务器响应时间和指令吞吐量\n\n使用处理性能换取存储空间\n\n\n# 惰性删除\n\n数据到达过期时间,不做处理,等下次访问该数据时删除\n\n * 如果未过期 返回数据\n * 如果已过期 删除 返回不存在\n\n通过get方法查询时,get绑定expirelfneeded()方法 此方法会检测数据是否过期\n\n优点:节约cpu性能 只有访问时才进行判别删除\n\n缺点:内存压力大 出现长期占用内容\n\n用存储空间换取处理器性能\n\n\n# 定期删除\n\n通过设置redis.config文件中的 hz 属性 默认为10\n\n为每秒执行hz的次数\n\n设置后服务器每秒执行hz次 servercron() --\x3e databasescron() --\x3e activeexpirecycle()\n\nactiveexpirecycle() 对每个expires[*](分成若干区) 逐一进行检测 每次执行250ms/hz\n\n对某个expires[*]检测时,随机挑选w个key检测\n\n * 如果key超时,删除key\n * 如果一轮中删除key的数据>w*25%,循环该过程\n * 如果一轮中删除key的数据<=w*25%,则检查下一个expires[*]\n * w取值=active_expire_cycle_lookups_per_loop属性值\n\n参数current_db用于记录activeexpirecycle()进入哪个expires[*]执行\n\n如果activeexpirecyle()执行时间到期,下次从current_db继续向下执行\n\n优点:cpu性能占用设置峰值 检测频度可自定义\n\n内容压力不是很大 长期占用内存的冷数据会被清理掉\n\n\n# 淘汰策略\n\n当redis使用内容存储数据,执行每一个命令前,会调用freememoryifneeded()检测内存是否充足.如果不满足加入数据的最低存储要求,则redis会为当前指令清理存储空间.清理数据的策略又称为逐出算法\n\n逐出数据的过程不是百分百能够清理出足够的可使用空间,如果不成功则反复执行.当对所有数据尝试完毕,如果仍然不能到达内存清理要求,将出现错误信息\n\n而redis中内存空间是根据物理内存和配置信息来指定\n\n * maxmenory ?mb 默认值为0不做限制 通常设置在50%以上\n\n * maxmemory-samples count 每次选取待删除数据的个数,采用随机获取数据的方式作为待检测删除数据\n\n * maxmemory-policy policy 对数据进行删除的选择策略\n   \n   * 检测易失数据(可能会过期的数据集server.db[i].expires)\n     * volatile-lru 挑选最近最少使用的数据淘汰\n     * valatile-lfu 挑选最近使用次数最少的数据淘汰\n     * valatile-tfl 挑选将要过期的数据淘汰\n     * volatile-random 任意选择数据淘汰\n     * \n   * 检测全库数据(所有数据集server.db[i].dict)\n     * allkeys-lru 挑选最近最少使用的数据淘汰\n     * allkeys-lfu 挑选最近使用次数最少的数据淘汰\n     * allkeys-random 任意选择数据淘汰\n   * 放弃数据驱逐\n     * no-enviction(驱逐) 禁止驱逐数据(redis4.0中默认策略) 会引发错误oom(out of memory)\n   \n   使用info命令输出监控信息,查询缓存hit和miss的次数,根据业务需求调用redis配置\n\n\n# 主从复制\n\n\n\n\n# 互联网 "三高" 架构\n\n * 高并发\n * 高性能\n * 高可用\n   * 可用性 = (1年的总时间 - 1年宕机的总时间) / 1年的总时间 * 100% : 业界可用性目标5个9 即99.999% 即服务器年宕机时长低于315秒 约5.25分钟\n\n\n# 阶段一:建立连接阶段\n\n建立slave到master的连接,使用master能够识别slave,并保存slave端口号\n\n 1.  saveof ip port : salve发送指令给master\n 2.  master接受到指令,响应对方\n 3.  保存master的ip与端口 masterhost 和 masterport\n 4.  根据保存的信息创建连接master的socket\n 5.  周期性发送ping指令\n 6.  响应pong\n 7.  发送指令 auth password (如无密码则忽略)\n 8.  验证授权\n 9.  replconf listening-port <port-number>\n 10. 保存slave的端口号\n\n# 主从连接 (slave连接mazster)\n\n * 方式一 从机客户端发送命令 slaveof masterip masterport\n * 方式二 启动服务器时连接 redis-server -slaveof masterip masterport\n * 方式三 服务器配置 在从机服务器的redis服务器配置文件 加上 slaveof masterip masterport\n\n# 主从断开连接\n\n * 断开slave与master的连接 不会删除已有数据 只是不再接受master发送的数据 salveof no one 在从机上执行\n\n# 授权访问\n\n\n\n\n# 阶段二:数据同步阶段工作流程\n\n\n\n 1. 如果master数据量巨大,数据同步阶段应该避开流量高峰期,避免造成阻塞\n\n 2. 如果复制缓冲区设置不合理,会导致数据溢出. 如果全量复制周期太长,进行部分复制时发现数据已经存在丢失,必须进行第二次全量复制 导致slave陷入死循环\n    \n    #在主服务器配置文件配置 配置缓存区大小\n    repl-backlog-size ?mb\n    \n    \n    1\n    2\n    \n\n 3. master单机内存占用主机内存比例应在50%-70%,留下30-50作为bgsave命令和创建复制缓冲区\n\n 4. 在slave进行全量复制/部分复制时 建议关闭此时对外服务\n    \n    #在从服务器配置文件中配置  关闭和开启\n    slave-serve-stale-data yes|no\n    \n    \n    1\n    2\n    \n\n 5. 数据同步阶段 master发送给slave信息可以理解为master是slave的一个客户端 主动向slave发送命令\n\n 6. 多个slave同时对master请求数据同步 master发送的rdb文件增多 如果master带宽不足 会对带宽造成巨大冲击 建议适量错峰请求同步\n\n 7. salve过多时 建议调整拓扑结构 由一主多从结构变为树状结构 中间的节点即使master 也salve 使用树状结构时 由于层级深度 导致深度越高的slave与最顶层master间数据同步延迟较大 数据一致性变差 应谨慎选择\n\n\n# 阶段三:命令传播阶段\n\n命令传播阶段出现了断网现象：\n\n网络闪断闪连：忽略\n\n短时间网络中断：部分复制\n\n长时间网络中断：全量复制\n\n工作原理\n\n * 通过offset区分不同的slave当前数据传播的差异\n * master记录已发送的信息对应的offset\n * slave记录已接收的信息对应的offset\n\n\n\n# 流程更新\n\n\n\n# 心跳机制\n\n进入命令传播阶段候，master与slave间需要进行信息交换，使用心跳机制进行维护，实现双方连接保持在线\n\nmaster心跳：\n\n * 内部指令：ping\n * 周期：由repl-ping-slave-period决定，默认10秒\n * 作用：判断slave是否在线\n * 查询：info replication 获取slave最后一次连接时间间隔，lag项维持在0或1视为正常\n\nslave心跳任务\n\n * 内部指令：replconf ack {offset}\n * 周期：1秒\n * 作用1：汇报slave自己的复制偏移量，获取最新的数据变更指令\n * 作用2：判断master是否在线\n\n心跳阶段注意事项：\n\n * 当slave多数掉线，或延迟过高时，master为保障数据稳定性，将拒绝所有信息同步\n\n#在主服务器配置文件中配置\nmin-slaves-to-write 2\nmin-slaves-max-lag 8\n\n\n1\n2\n3\n\n\nslave数量少于2个，或者所有slave的延迟都大于等于8秒时，强制关闭master写功能，停止数据同步\n\n * slave数量由slave发送replconf ack命令做确认\n\n * slave延迟由slave发送replconf ack命令做确认\n\n\n\n\n# 频繁的全量复制\n\n * 伴随着系统的运行，master的数据量会越来越大，一旦master重启，runid将发生变化，会导致全部slave的全量复制操作\n\n内部优化调整方案：\n\n1：master内部创建master_replid变量，使用runid相同的策略生成，长度41位，并发送给所有slave\n\n2：在master关闭时执行命令shutdown save，进行rdb持久化,将runid与offset保存到rdb文件中\n\n * repl-id repl-offset\n\n * 通过redis-check-rdb命令可以查看该信息\n\n3：master重启后加载rdb文件，恢复数据，重启后，将rdb文件中保存的repl-id与repl-offset加载到内存中\n\n * master_repl_id=repl master_repl_offset =repl-offset\n\n * 通过info命令可以查看该信息\n\n作用：本机保存上次runid，重启后恢复该值，使所有slave认为还是之前的master\n\n * 第二种出现频繁全量复制的问题现象：网络环境不佳，出现网络中断，slave不提供服务\n\n问题原因：复制缓冲区过小，断网后slave的offset越界，触发全量复制\n\n最终结果：slave反复进行全量复制\n\n解决方案：修改复制缓冲区大小\n\nrepl-backlog-size ?mb\n\n\n1\n\n\n建议设置如下：\n\n1.测算从master到slave的重连平均时长second\n\n2.获取master平均每秒产生写命令数据总量write_size_per_second\n\n3.最优复制缓冲区空间 = 2 * second * write_size_per_second\n\n# 频繁的网络中断\n\n * 问题现象：master的cpu占用过高 或 slave频繁断开连接\n\n问题原因\n\n * slave每1秒发送replconfack命令到master\n * 当slave接到了慢查询时（keys * ，hgetall等），会大量占用cpu性能\n * master每1秒调用复制定时函数replicationcron()，比对slave发现长时间没有进行响应\n\n最终结果：master各种资源（输出缓冲区、带宽、连接等）被严重占用\n\n解决方案：通过设置合理的超时时间，确认是否释放slave\n\nrepl-timeout seconds\n\n\n1\n\n\n该参数定义了超时时间的阈值（默认60秒），超过该值，释放slave\n\n# 数据不一致\n\n问题现象：多个slave获取相同数据不同步\n\n问题原因：网络信息不同步，数据发送有延迟\n\n解决方案\n\n * 优化主从间的网络环境，通常放置在同一个机房部署，如使用阿里云等云服务器时要注意此现象\n * 监控主从节点延迟（通过offset）判断，如果slave延迟过大，暂时屏蔽程序对该slave的数据访问\n\nslave-serve-stale-data\tyes|no\n\n\n1\n\n\n开启后仅响应info、slaveof等少数命令（慎用，除非对数据一致性要求很高）\n\n\n# 哨兵模式\n\n哨兵(sentinel) 是一个分布式系统,用于对主从结构中的每台服务器进行监控,当出现故障时通过投票机制选择新的master并将所有slave连接到新的master\n\n哨兵也是一台redis服务器 只是不提供数据相关服务 通常哨兵的数量配置为单数\n\n\n# 配置哨兵\n\n通过sentinel.conf 配置\n\n * sentinel monitor master_name master_host master_port sentinel_number 设置哨兵监听的主服务器信息 sentinel_number表示参与投票的哨兵数量 设置超过哨兵半数即可\n * sentinel down-after-milliseconds master_name million_seconds 设置判定服务器宕机时长 该设置控制是否进行主从切换\n * sentinel failover-timeout master_name million_seconds 设置故障切换的最大超时时长\n * sentinel parallel-syncs master_name sync_slave_number 设置主从切换后,同时进行数据的slave数量 值越大 要求网络资源越高 值越小 同步时间越长\n * redis-sentinel filename(sentinel.conf配置文件路径) 启动哨兵\n\n\n# 阶段一:监控阶段\n\n * 获取各个sentinel的状态（是否在线）\n\n * 获取master的状态\n\n * 获取所有slave的状态（根据master中的slave信息）\n\n\n\n\n# 阶段二:通知阶段\n\nsentinel在通知阶段要不断的去获取master/slave的信息，然后在各个sentinel之间进行共享，具体的流程如下：\n\n\n\n\n# 阶段三:故障转移\n\n# 判断master宕机\n\n\n\n主观下线:任意一个sentine认为master已经下线\n\n客观下线:半数以上的sentine认为master已经下线\n\n# 选举master\n\n每个sentine都有一票 最终票最多的称为处理事故的哨兵服务\n\n\n\n首先它有一个在服务器列表中挑选备选master的原则\n\n * 不在线的out\n\n * 响应慢的out\n\n * 与原master断开时间久的out\n\n * 优先原则\n   \n   优先级 offset runid\n\n总结：故障转移阶段\n\n 1. 发现问题，主观下线与客观下线\n 2. 竞选负责人\n 3. 优选新master\n 4. 新master上任，其他slave切换master，原master作为slave故障恢复后连接\n\n\n# cluster集群\n\n集群就是使用网络将若干台计算机联通起来，并提供统一的管理方式，使其对外呈现单机的服务效果\n\n\n\n\n# cluster集群结构设计\n\n数据存储设计：\n\n 1. 通过算法设计，计算出key应该保存的位置\n\n 2. 将所有的存储空间计划切割成16384份，每台主机保存一部分\n    \n    注意：每份代表的是一个存储空间，不是一个key的保存空间\n\n 3. 将key按照计算出的结果放到对应的存储空间\n\n那redis的集群是如何增强可扩展性的呢？譬如我们要增加一个集群节点\n\n\n\n当我们查找数据时，集群是如何操作的呢？\n\n * 各个数据库相互通信，保存各个库中槽的编号数据\n * 一次命中，直接返回\n * 一次未命中，告知具体位置\n\n\n\n\n# 配置\n\n在redis-6401.conf中配置\n\n * cluster-enabled yes|no 是否启用cluster，加入cluster节点\n * cluster-config-file cluster-6503.conf cluster配置文件名，该文件属于自动生成，仅用于快速查找文件并查询文件内容\n * cluster-node-timeout milliseconds 节点服务响应超时时间，用于判定该节点是否下线或切换为从节点 毫秒数\n * cluster-migration-barrier min_slave_number master连接的slave最小数量\n\n\n# 启动culster\n\nredis-cli –-cluster create masterhost1:masterport1 masterhost2:masterport2  masterhost3:masterport3 [masterhostn:masterportn …] slavehost1:slaveport1  slavehost2:slaveport2 slavehost3:slaveport3 -–cluster-replicas n\n\n\n1\n\n\nn为多少个主服务器 前n个ip为主服务器 后面为slave服务器\n\n * cluster nodes 查询集群节点信息\n\n\n# 节点增删\n\n * 添加master到当前集群中，连接时可以指定任意现有节点地址与端口\n   \n   redis-cli --cluster add-node new-master-host:new-master-port now-host:now-port\n   \n   \n   1\n   \n\n * 添加slave\n   \n   redis-cli --cluster add-node new-slave-host:new-slave-port master-host:master-port --cluster-slave --cluster-master-id masterid\n   \n   \n   1\n   \n\n * 删除节点，如果删除的节点是master，必须保障其中没有槽slot\n   \n   redis-cli --cluster del-node del-slave-host:del-slave-port del-slave-id\n   \n   \n   1\n   \n\n * 重新分槽，分槽是从具有槽的master中划分一部分给其他master，过程中不创建新的槽\n   \n   redis-cli --cluster reshard new-master-host:new-master:port --cluster-from src-  master-id1, src-master-id2, src-master-idn --cluster-to target-master-id --  cluster-slots slots\n   #将需要参与分槽的所有masterid不分先后顺序添加到参数中，使用，分隔\n   #指定目标得到的槽的数量，所有的槽将平均从每个来源的master处获取\n   \n   \n   1\n   2\n   3\n   \n\n * 重新分配槽，从具有槽的master中分配指定数量的槽到另一个master中，常用于清空指定master中的槽\n   \n   redis-cli --cluster reshard src-master-host:src-master-port --cluster-from src-  master-id --cluster-to target-master-id --cluster-slots slots --cluster-yes\n   \n   \n   1\n   \n\n\n# 缓存预热\n\n1.请求数量较高，大量的请求过来之后都需要去从缓存中获取数据，但是缓存中又没有，此时从数据库中查找数据然后将数据再存入缓存，造成了短期内对redis的高强度操作从而导致问题\n\n2.主从之间数据吞吐量较大，数据同步操作频度较高\n\n * 前置准备工作：\n\n1.日常例行统计数据访问记录，统计访问频度较高的热点数据\n\n2.利用lru数据删除策略，构建数据留存队列例如：storm与kafka配合\n\n * 准备工作：\n\n1.将统计结果中的数据分类，根据级别，redis优先加载级别较高的热点数据\n\n2.利用分布式多服务器同时进行数据读取，提速数据加载过程\n\n3.热点数据主从同时预热\n\n * 实施：\n\n4.使用脚本程序固定触发数据预热过程\n\n5.如果条件允许，使用了cdn（内容分发网络），效果会更好\n\n总的来说：缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！\n\n\n# 缓存雪崩\n\n1.系统平稳运行过程中，忽然数据库连接量激增\n\n2.应用服务器无法及时处理请求\n\n3.大量408，500错误页面出现\n\n4.客户反复刷新页面获取数据\n\n5.数据库崩溃\n\n6.应用服务器崩溃\n\n7.重启应用服务器无效\n\n8.redis服务器崩溃\n\n9.redis集群崩溃\n\n10.重启数据库后再次被瞬间流量放倒\n\n解决方案\n\n * 思路：\n\n1.更多的页面静态化处理\n\n2.构建多级缓存架构\n\nnginx缓存+redis缓存+ehcache缓存\n\n3.检测mysql严重耗时业务进行优化\n\n对数据库的瓶颈排查：例如超时查询、耗时较高事务等\n\n4.灾难预警机制\n\n监控redis服务器性能指标\n\ncpu占用、cpu使用率\n\n内存容量\n\n查询平均响应时间\n\n线程数\n\n5.限流、降级\n\n短时间范围内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后再逐步放开访问\n\n * 落地实践：\n\n1.lru与lfu切换\n\n2.数据有效期策略调整\n\n根据业务数据有效期进行分类错峰，a类90分钟，b类80分钟，c类70分钟\n\n过期时间使用固定时间+随机值的形式，稀释集中到期的key的数量\n\n3.超热数据使用永久key\n\n4.定期维护（自动+人工）\n\n对即将过期数据做访问量分析，确认是否延时，配合访问量统计，做热点数据的延时\n\n5.加锁：慎用！\n\n总的来说：缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力。如能够有效避免过期时间集中，可以有效解决雪崩现象的 出现（约40%），配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整。\n\n\n# 缓存击穿\n\n1.系统平稳运行过程中\n\n2.数据库连接量瞬间激增\n\n3.redis服务器无大量key过期\n\n4.redis内存平稳，无波动\n\n5.redis服务器cpu正常\n\n6.数据库崩溃\n\n问题排查：\n\n1.redis中某个key过期，该key访问量巨大\n\n2.多个数据请求从服务器直接压到redis后，均未命中\n\n3.redis在短时间内发起了大量对数据库中同一数据的访问\n\n总而言之就两点：单个key高热数据，key过期\n\n解决方案：\n\n1.预先设定\n\n以电商为例，每个商家根据店铺等级，指定若干款主打商品，在购物节期间，加大此类信息key的过期时长 注意：购物节不仅仅指当天，以及后续若干天，访问峰值呈现逐渐降低的趋势\n\n2.现场调整\n\n监控访问量，对自然流量激增的数据延长过期时间或设置为永久性key\n\n3.后台刷新数据\n\n启动定时任务，高峰期来临之前，刷新数据有效期，确保不丢失\n\n4.二级缓存\n\n设置不同的失效时间，保障不会被同时淘汰就行\n\n5.加锁\n\n分布式锁，防止被击穿，但是要注意也是性能瓶颈，慎重！\n\n总的来说：缓存击穿就是单个高热数据过期的瞬间，数据访问量较大，未命中redis后，发起了大量对同一数据的数据库访问，导致对数 据库服务器造成压力。应对策略应该在业务数据分析与预防方面进行，配合运行监控测试与即时调整策略，毕竟单个key的过 期监控难度较高，配合雪崩处理策略即可。\n\n\n# 缓存穿透\n\n1.系统平稳运行过程中\n\n2.应用服务器流量随时间增量较大\n\n3.redis服务器命中率随时间逐步降低\n\n4.redis内存平稳，内存无压力\n\n5.redis服务器cpu占用激增\n\n6.数据库服务器压力激增\n\n7.数据库崩溃\n\n问题排查：\n\n1.redis中大面积出现未命中\n\n2.出现非正常url访问\n\n问题分析：\n\n * 获取的数据在数据库中也不存在，数据库查询未得到对应数据\n * redis获取到null数据未进行持久化，直接返回\n * 下次此类数据到达重复上述过程\n * 出现黑客攻击服务器\n\n解决方案：\n\n1.缓存null\n\n对查询结果为null的数据进行缓存（长期使用，定期清理），设定短时限，例如30-60秒，最高5分钟\n\n2.白名单策略\n\n提前预热各种分类数据id对应的bitmaps，id作为bitmaps的offset，相当于设置了数据白名单。当加载正常数据时放行，加载异常数据时直接拦截（效率偏低）\n\n使用布隆过滤器（有关布隆过滤器的命中问题对当前状况可以忽略）\n\n2.实施监控\n\n实时监控redis命中率（业务正常范围时，通常会有一个波动值）与null数据的占比\n\n非活动时段波动：通常检测3-5倍，超过5倍纳入重点排查对象\n\n活动时段波动：通常检测10-50倍，超过50倍纳入重点排查对象\n\n根据倍数不同，启动不同的排查流程。然后使用黑名单进行防控（运营）\n\n4.key加密\n\n问题出现后，临时启动防灾业务key，对key进行业务层传输加密服务，设定校验程序，过来的key校验\n\n例如每天随机分配60个加密串，挑选2到3个，混淆到页面数据id中，发现访问key不满足规则，驳回数据访问\n\n总的来说：缓存击穿是指访问了不存在的数据，跳过了合法数据的redis数据缓存阶段，每次访问数据库，导致对数据库服务器造成压力。通常此类数据的出现量是一个较低的值，当出现此类情况以毒攻毒，并及时报警。应对策略应该在临时预案防范方面多做文章。\n\n无论是黑名单还是白名单，都是对整体系统的压力，警报解除后尽快移除。\n\n\n# 性能指标监控\n\n性能指标\n\n * latency 响应请求的平均时间\n * instantaneous_ops_per_sec 平均每秒处理请求总数\n * hit_rate(calculated) 缓存查询命中率（通过查询总次数与查询得到非nil数据总次数计算而来）\n\n内存指标\n\n * used_memory 当前内存使用量\n * mem_fragmentation_ratio 内存碎片率（关系到是否进行碎片整理）\n * evicted_keys 为避免内存溢出删除的key的总数量\n * blocked_clients 基于阻塞操作（blpop等）影响的客户端数量\n\n活动指标\n\n * connected_clients 当前客户端连接总数\n * connected_slaves 当前连接slave总数\n * master_last_io_seconds_ago 最后一次主从信息交换距现在的秒\n * keyspace key的总数\n\n持久性指标\n\n * rdb_last_save_time 当前服务器最后一次rdb持久化的时间\n * rdb_changes_since_last_save 当前服务器最后一次rdb持久化后数据变化总量\n\n错误指标\n\n * rejected_connections 被拒绝连接的客户端总数（基于达到最大连接值的因素）\n * keyspace_misses key未命中的总次数\n * master_link_down_since_seconds 主从断开的秒数\n\n\n# 性能指标工具\n\n测试当前服务器的并发性能\n\nredis-benchmark [-h ] [-p ] [-c ] [-n <requests]> [-k ]\n\n\n1\n\n\n\n\n启动服务器调试信息\n\nmonitor\n\n\n1\n\n\n慢日志 记录查询慢的日志\n\n * slowlog [operator]\n   * get 获取慢日志信息\n   * len 获取慢日志条目数\n   * reset 重置慢查询日志\n\n慢日志通过redis-xxx.conf配置\n\nslowlog-log-slower-than 1000 #设置慢查询的时间下线，单位：微妙\nslowlog-max-len 100\t#设置慢查询命令对应的日志显示长度，单位：命令数\n\n\n1\n2\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"Redis",frontmatter:{title:"Redis",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/a002c8/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/22.Redis.html",relativePath:"后端/04.SQL/22.Redis.md",key:"v-309ca6de",path:"/pages/a002c8/",headers:[{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:220},{level:2,title:"配置文件",slug:"配置文件",normalizedTitle:"配置文件",charIndex:611},{level:2,title:"数据类型指令",slug:"数据类型指令",normalizedTitle:"数据类型指令",charIndex:1116},{level:3,title:"String",slug:"string",normalizedTitle:"string",charIndex:1193},{level:3,title:"hash",slug:"hash",normalizedTitle:"hash",charIndex:145},{level:3,title:"list",slug:"list",normalizedTitle:"list",charIndex:129},{level:3,title:"set",slug:"set",normalizedTitle:"set",charIndex:161},{level:2,title:"常用指令",slug:"常用指令",normalizedTitle:"常用指令",charIndex:4502},{level:3,title:"key",slug:"key",normalizedTitle:"key",charIndex:1179},{level:3,title:"db",slug:"db",normalizedTitle:"db",charIndex:5046}],headersStr:"安装 配置文件 数据类型指令 String hash list set 常用指令 key db",content:'# Redis\n\nRedis 是用C语言开发的一个开源的高性能键值对数据库\n\n特征:\n\n 1. 数据间没有必然的关联关系\n 2. 内部采用单线程机制进行工作\n 3. 高性能\n 4. 多数据类型支持\n    * 字符串类型 string\n    * 列表类型 list\n    * 双列类型 hash\n    * 集合类型 set\n    * 有序集合类型 zset/sorted_set\n 5. 支持持久化, 可以进行数据灾难恢复\n\n\n# 安装\n\nwget https://download.redis.io/releases/redis-6.2.5.tar.gz\ntar -xvf redis-6.2.5.tar.gz \ncd redis-6.2.5\nmake\nmake install\n\nredis-server  \n# redis-server --port 6380  指定端口 默认为6379\n\n#客户端\nredis-cli \n# redis-cli -p 6380\n\n#创建存放配置和数据目录\nmkdir conf\nmkdir data\ncp redis.conf ./conf\n\nfirewall-cmd --zone=public --add-port=6379/tcp --permanent\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 配置文件\n\nredis.conf\n\nbind 192.168.130.128 #绑定对外访问的ip  建议为本机ip  也可注释掉任何主机都可访问\nprotected-mode no  #关闭主机保护模式 yes为开启\nrequirepass 123456  #设置密码\nport 6379\ntimeout 0 #客户端闲置等待时间 到时间关闭\ndaemonize yes #后台运行\nlogfile "log-6379.log"  #日志文件名\ndir root/redis-6.2.5/data\nmaxclients 0  #服务器允许最大连接数 默认为0无限\nloglevel verbose  #debug verbose notice waring 四个级别 开发期间为verbose  生产环境配置为notice 降低写日志IO的频度\n\nredis-server /root/redis-6.2.5/conf/redis.conf \nauth 123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n#连接\nredis-cli \nauth 12\n\n\n1\n2\n3\n\n\n\n# 数据类型指令\n\n\n\nredis 自身是一个 MAP 所有的数据类型都是 键值对形式存储\n\n而数据类型指的value部分的类型 key永远是字符串\n\n\n# String\n\n一个存储空间保存一个数据\n\n如果是数字也可以作为数字操作\n\n * set key value 添加/修改数据\n * get key 获取数据\n * del key 删除数据\n * setnx key value 判断性添加数据\n * mset key1 value1 key2 value2 ... m是Multiple缩写\n * mget key1 key2 ... 获取多个数据\n * strlen key 获取数据字符长度\n * append key value 追加信息到原始信息后面 (如存在则追加 否则新建)\n\n----------------------------------------\n\n * incr key 设置数值+1 如果key不存在 则从0开始并新建 只有数字字符串可以\n * incrby key increment 设置数值+n\n * incrbyfloat key increment 设置小数增加+n\n * decr key 设置数值-1\n * decrby key increment 设置数值减少指定范围\n * setex key seconds value 设置数值具有指定的生命周期秒 如:setex next 10 hello\n * psetex key milliseconds value 设置数值具有指定的生命周期毫秒\n\n----------------------------------------\n\n注意事项:\n\n 1. (integer)0 为false 1为成功 也可能为运行的结果值\n 2. 数据为获取到是 对应为(nil) === null\n 3. 数据最大的存储量为 512MB\n 4. string在内部存储就是一个字符串 当遇到增减类操作 incr,decr时会转成数值类型进行计算\n 5. 在操作数值时,如果原始数据不能转成数值或超出了redis数值的范围 将报错 java.long的MAX_VALUE\n 6. redis所有操作都是原子性,采用单线程处理所有业务,命令都是一个一个执行的,因此无需考虑并发带来的数据影响\n\n\n\n\n# hash\n\n一个存储空间保存多个键值对数据 hash空间里面的key我们通常称为field\n\n如果field数比较少,存储结构会优化为数组,较多时为HashMap结构\n\n * hset key field value 添加/修改数据\n * hget key field 获取数据\n * hgetall key 获取整个hash\n * hdel key field1 [field] 删除hash指定的field\n * hsetnx key field value 设置field, 如果field存在则不做任何操作\n * hmset key field1 value1 field2 value2 设置多个filed\n * hmget key field1 field2 获取多个数据\n * hlen key 获取哈希表中field的数量\n * hexists key field 判断哈希表是否存在指定的field\n\n----------------------------------------\n\n * hkeys key 获取哈希所有的字段名\n * hvals key 获取哈希所有的字段值\n * hincrby key field increment 设置指定字段的数值增加指定范围 可以为负数\n * hincrbyfloat key field increment 设置指定字段的数值增加指定小数范围\n\n----------------------------------------\n\n注意事项:\n\n 1. hash类型中的value只能存储字符串 不允许其他数据类型\n 2. 每个hash可以存储 2的32次方-1个键值对\n 3. hash设计初衷不是为了存储大量对象而设计的 不可以将hash转为对象列表使用\n 4. hgetall 操作可以获取全部属性 如果男人field过多 遍历整体数效率就会很低 造成数据访问瓶颈\n\n\n# list\n\n一个存储空间保存多个数据 并且根据存储顺序一致 底层使用了双向链表存储结构实现\n\n * lpush key value1 [value2] ... 从左添加/删除数据 可以存储多个值\n * rpush key value1 [value2] ... 从右添加/删除数据\n * lrange key start stop 获取指定范围的数据 下标从0开 -1为最后一个元素或list长度-1\n * lindex key index 获取索引的数据 如索引越界则查询为nil\n * llen key 获取list长度\n * lpop key 从左获取并移除数据\n * rpop key 从右获取并移除数据\n\n----------------------------------------\n\n * lrem key count value 移除指定个数的指定数据 如删除个数超出总个数 则删除全部指定数据\n * blpop key1 [key2] timeout 规定时间内获取并移除数据 秒单位 如果有多key则按顺序获取并移除 只有前面为nil后面的key才会执行 每次执行一次 是一个阻塞队列\n * brpop key1 [key2] timeout 规定时间内获取并移除数据\n * brpoppush source destination timeout 将a列表push到b列表中\n\n----------------------------------------\n\n注意事项:\n\n 1. list数据都是string类型 最多2的32次方-1个元素\n 2. list有索引的概念, 但操作数据通常以队列形式入队出队 或以栈的形式\n 3. 获取全部数据的操作结束索引为-1\n 4. list可以对数据进行分页操作 第一页数据来源list 而第二页后的通常以数据库形式加载\n\n\n# set\n\nset与hash存储结构完全相同,但set只存储键,不存储值 值为nil 并且值是不允许重复的\n\n * sadd key member1 [member2] 添加数据\n * smembers key 获取全部数据\n * srem key member1 [member2] 删除指定数据\n * scard key 获取集合数据总量\n * sismember key member 判断集合中是否包含指定数据\n * srandmember key [count] 随机获取集合中指定数量的数据\n * spop key [count] 随机获取集合中的某个数据并移除出集合\n\n----------------------------------------\n\n * sinter key1 [key2 ...] 求两个集合的交集\n * sunion key1 [key2 ...] 求两个集合的并集\n * sdiff key1 [key2 ...] 求两个集合的差集\n * sinterstore destination key [key2 ...] 求两个集合的交集并存储到指定集合中\n * sunionstore destination key [key2 ...] 求两个集合的并集并存储到指定集合中\n * sdiffstore destination key [key2 ...] 求两个集合的差集并存储到指定集合中\n * smove source destination member 将指定数据从A集合移动到B集合\n\n----------------------------------------\n\n注意事项:\n\n 1. set不允许数据重复\n 2. set虽然和hash的存储结构相同 , 但无法使用存储值的空间\n\n\n# 常用指令\n\n\n# key\n\n * del key 删除指定key\n * exists key 判断key是否存在\n * type key 获取key的类型\n * sort key 排序 默认对数字排序 如果要按自然排序则 需要指定 alpha 查询出来是排序 原数据不做改变 并且可以根据 ASC|DESC 指定升降序\n * rename key new key 重命名\n * renamenx key newkey 存在则修改 否则不做任何操作\n\n----------------------------------------\n\n * expire key seconds 为指定key设置有效期 秒\n * pexpire key milliseconds 为指定key设置有效期 毫秒\n * expireat key timestamp 时间戳\n * pexpireat key milliseconds-timestamp 毫秒级时间戳\n * ttl key 获取key的有效时间 秒 -1为永久 -2为不存在\n * pttl key 获取key的有效时间 毫秒\n * persist key 将此key转为永久性\n * keys pattern 表达式查询\n   * \n\n\n# db\n\nrdis服务器默认提供16个数据库 从0到15 默认为0 每个数据库之间的数据相互独立\n\n * select index 切换数据库\n * ping 测试与服务器是否连同 发送消息\n * move key db 数据移动\n * dbsize 当前数据库key总量\n * flushdb 当前数据库清除\n * flushall 全部数据库清除',normalizedContent:'# redis\n\nredis 是用c语言开发的一个开源的高性能键值对数据库\n\n特征:\n\n 1. 数据间没有必然的关联关系\n 2. 内部采用单线程机制进行工作\n 3. 高性能\n 4. 多数据类型支持\n    * 字符串类型 string\n    * 列表类型 list\n    * 双列类型 hash\n    * 集合类型 set\n    * 有序集合类型 zset/sorted_set\n 5. 支持持久化, 可以进行数据灾难恢复\n\n\n# 安装\n\nwget https://download.redis.io/releases/redis-6.2.5.tar.gz\ntar -xvf redis-6.2.5.tar.gz \ncd redis-6.2.5\nmake\nmake install\n\nredis-server  \n# redis-server --port 6380  指定端口 默认为6379\n\n#客户端\nredis-cli \n# redis-cli -p 6380\n\n#创建存放配置和数据目录\nmkdir conf\nmkdir data\ncp redis.conf ./conf\n\nfirewall-cmd --zone=public --add-port=6379/tcp --permanent\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 配置文件\n\nredis.conf\n\nbind 192.168.130.128 #绑定对外访问的ip  建议为本机ip  也可注释掉任何主机都可访问\nprotected-mode no  #关闭主机保护模式 yes为开启\nrequirepass 123456  #设置密码\nport 6379\ntimeout 0 #客户端闲置等待时间 到时间关闭\ndaemonize yes #后台运行\nlogfile "log-6379.log"  #日志文件名\ndir root/redis-6.2.5/data\nmaxclients 0  #服务器允许最大连接数 默认为0无限\nloglevel verbose  #debug verbose notice waring 四个级别 开发期间为verbose  生产环境配置为notice 降低写日志io的频度\n\nredis-server /root/redis-6.2.5/conf/redis.conf \nauth 123456\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n#连接\nredis-cli \nauth 12\n\n\n1\n2\n3\n\n\n\n# 数据类型指令\n\n\n\nredis 自身是一个 map 所有的数据类型都是 键值对形式存储\n\n而数据类型指的value部分的类型 key永远是字符串\n\n\n# string\n\n一个存储空间保存一个数据\n\n如果是数字也可以作为数字操作\n\n * set key value 添加/修改数据\n * get key 获取数据\n * del key 删除数据\n * setnx key value 判断性添加数据\n * mset key1 value1 key2 value2 ... m是multiple缩写\n * mget key1 key2 ... 获取多个数据\n * strlen key 获取数据字符长度\n * append key value 追加信息到原始信息后面 (如存在则追加 否则新建)\n\n----------------------------------------\n\n * incr key 设置数值+1 如果key不存在 则从0开始并新建 只有数字字符串可以\n * incrby key increment 设置数值+n\n * incrbyfloat key increment 设置小数增加+n\n * decr key 设置数值-1\n * decrby key increment 设置数值减少指定范围\n * setex key seconds value 设置数值具有指定的生命周期秒 如:setex next 10 hello\n * psetex key milliseconds value 设置数值具有指定的生命周期毫秒\n\n----------------------------------------\n\n注意事项:\n\n 1. (integer)0 为false 1为成功 也可能为运行的结果值\n 2. 数据为获取到是 对应为(nil) === null\n 3. 数据最大的存储量为 512mb\n 4. string在内部存储就是一个字符串 当遇到增减类操作 incr,decr时会转成数值类型进行计算\n 5. 在操作数值时,如果原始数据不能转成数值或超出了redis数值的范围 将报错 java.long的max_value\n 6. redis所有操作都是原子性,采用单线程处理所有业务,命令都是一个一个执行的,因此无需考虑并发带来的数据影响\n\n\n\n\n# hash\n\n一个存储空间保存多个键值对数据 hash空间里面的key我们通常称为field\n\n如果field数比较少,存储结构会优化为数组,较多时为hashmap结构\n\n * hset key field value 添加/修改数据\n * hget key field 获取数据\n * hgetall key 获取整个hash\n * hdel key field1 [field] 删除hash指定的field\n * hsetnx key field value 设置field, 如果field存在则不做任何操作\n * hmset key field1 value1 field2 value2 设置多个filed\n * hmget key field1 field2 获取多个数据\n * hlen key 获取哈希表中field的数量\n * hexists key field 判断哈希表是否存在指定的field\n\n----------------------------------------\n\n * hkeys key 获取哈希所有的字段名\n * hvals key 获取哈希所有的字段值\n * hincrby key field increment 设置指定字段的数值增加指定范围 可以为负数\n * hincrbyfloat key field increment 设置指定字段的数值增加指定小数范围\n\n----------------------------------------\n\n注意事项:\n\n 1. hash类型中的value只能存储字符串 不允许其他数据类型\n 2. 每个hash可以存储 2的32次方-1个键值对\n 3. hash设计初衷不是为了存储大量对象而设计的 不可以将hash转为对象列表使用\n 4. hgetall 操作可以获取全部属性 如果男人field过多 遍历整体数效率就会很低 造成数据访问瓶颈\n\n\n# list\n\n一个存储空间保存多个数据 并且根据存储顺序一致 底层使用了双向链表存储结构实现\n\n * lpush key value1 [value2] ... 从左添加/删除数据 可以存储多个值\n * rpush key value1 [value2] ... 从右添加/删除数据\n * lrange key start stop 获取指定范围的数据 下标从0开 -1为最后一个元素或list长度-1\n * lindex key index 获取索引的数据 如索引越界则查询为nil\n * llen key 获取list长度\n * lpop key 从左获取并移除数据\n * rpop key 从右获取并移除数据\n\n----------------------------------------\n\n * lrem key count value 移除指定个数的指定数据 如删除个数超出总个数 则删除全部指定数据\n * blpop key1 [key2] timeout 规定时间内获取并移除数据 秒单位 如果有多key则按顺序获取并移除 只有前面为nil后面的key才会执行 每次执行一次 是一个阻塞队列\n * brpop key1 [key2] timeout 规定时间内获取并移除数据\n * brpoppush source destination timeout 将a列表push到b列表中\n\n----------------------------------------\n\n注意事项:\n\n 1. list数据都是string类型 最多2的32次方-1个元素\n 2. list有索引的概念, 但操作数据通常以队列形式入队出队 或以栈的形式\n 3. 获取全部数据的操作结束索引为-1\n 4. list可以对数据进行分页操作 第一页数据来源list 而第二页后的通常以数据库形式加载\n\n\n# set\n\nset与hash存储结构完全相同,但set只存储键,不存储值 值为nil 并且值是不允许重复的\n\n * sadd key member1 [member2] 添加数据\n * smembers key 获取全部数据\n * srem key member1 [member2] 删除指定数据\n * scard key 获取集合数据总量\n * sismember key member 判断集合中是否包含指定数据\n * srandmember key [count] 随机获取集合中指定数量的数据\n * spop key [count] 随机获取集合中的某个数据并移除出集合\n\n----------------------------------------\n\n * sinter key1 [key2 ...] 求两个集合的交集\n * sunion key1 [key2 ...] 求两个集合的并集\n * sdiff key1 [key2 ...] 求两个集合的差集\n * sinterstore destination key [key2 ...] 求两个集合的交集并存储到指定集合中\n * sunionstore destination key [key2 ...] 求两个集合的并集并存储到指定集合中\n * sdiffstore destination key [key2 ...] 求两个集合的差集并存储到指定集合中\n * smove source destination member 将指定数据从a集合移动到b集合\n\n----------------------------------------\n\n注意事项:\n\n 1. set不允许数据重复\n 2. set虽然和hash的存储结构相同 , 但无法使用存储值的空间\n\n\n# 常用指令\n\n\n# key\n\n * del key 删除指定key\n * exists key 判断key是否存在\n * type key 获取key的类型\n * sort key 排序 默认对数字排序 如果要按自然排序则 需要指定 alpha 查询出来是排序 原数据不做改变 并且可以根据 asc|desc 指定升降序\n * rename key new key 重命名\n * renamenx key newkey 存在则修改 否则不做任何操作\n\n----------------------------------------\n\n * expire key seconds 为指定key设置有效期 秒\n * pexpire key milliseconds 为指定key设置有效期 毫秒\n * expireat key timestamp 时间戳\n * pexpireat key milliseconds-timestamp 毫秒级时间戳\n * ttl key 获取key的有效时间 秒 -1为永久 -2为不存在\n * pttl key 获取key的有效时间 毫秒\n * persist key 将此key转为永久性\n * keys pattern 表达式查询\n   * \n\n\n# db\n\nrdis服务器默认提供16个数据库 从0到15 默认为0 每个数据库之间的数据相互独立\n\n * select index 切换数据库\n * ping 测试与服务器是否连同 发送消息\n * move key db 数据移动\n * dbsize 当前数据库key总量\n * flushdb 当前数据库清除\n * flushall 全部数据库清除',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"注释",frontmatter:{title:"注释",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/d37106/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/04.%E6%B3%A8%E9%87%8A.html",relativePath:"后端/05.Python/04.注释.md",key:"v-702958dc",path:"/pages/d37106/",headersStr:null,content:"# 注释\n\n> # 表示单行注释，不执行，对代码进行说明\n> \n> ctrl+/ 取消注释快捷键\n\n'''\n多行注释\n多行注释2\n'''\n\n\n1\n2\n3\n4\n",normalizedContent:"# 注释\n\n> # 表示单行注释，不执行，对代码进行说明\n> \n> ctrl+/ 取消注释快捷键\n\n'''\n多行注释\n多行注释2\n'''\n\n\n1\n2\n3\n4\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"close project",frontmatter:{title:"close project",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/0c7ca5/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/02.close%20project.html",relativePath:"后端/05.Python/02.close project.md",key:"v-16a2d958",path:"/pages/0c7ca5/",headersStr:null,content:"# close project\n\n> 菜单中的 关闭项目/工程",normalizedContent:"# close project\n\n> 菜单中的 关闭项目/工程",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"变量以及数据类型",frontmatter:{title:"变量以及数据类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/f3c5b0/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/05.%E5%8F%98%E9%87%8F%E4%BB%A5%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html",relativePath:"后端/05.Python/05.变量以及数据类型.md",key:"v-9c066f90",path:"/pages/f3c5b0/",headers:[{level:2,title:"数字类型",slug:"数字类型",normalizedTitle:"数字类型",charIndex:15},{level:2,title:"字符串类型",slug:"字符串类型",normalizedTitle:"字符串类型",charIndex:128},{level:2,title:"布尔类型",slug:"布尔类型",normalizedTitle:"布尔类型",charIndex:209}],headersStr:"数字类型 字符串类型 布尔类型",content:"# 变量以及数据类型\n\n\n# 数字类型\n\n> 数字 int整数类型\n> \n> 小数 float浮点类型\n> \n> 复数 Complex复数类型 由实数部分和虚数部分所组成的数,形如a＋bi 形如: -1 ** 0.5（一个虚数与另一个实数所组成）\n\n\n# 字符串类型\n\n> 其实就是一段普通的文字，要求使用成对的单引号或双引号引用，不能混用单双引号\n> \n> 数字类型加了引号后也变成字符串，不能与数字类运算\n\n\n# 布尔类型\n\n> 用来表示真假/对错，分真和假2种形态 注意开头的T和F要大写小写会报错\n> \n> True 真\n> \n> False 假",normalizedContent:"# 变量以及数据类型\n\n\n# 数字类型\n\n> 数字 int整数类型\n> \n> 小数 float浮点类型\n> \n> 复数 complex复数类型 由实数部分和虚数部分所组成的数,形如a＋bi 形如: -1 ** 0.5（一个虚数与另一个实数所组成）\n\n\n# 字符串类型\n\n> 其实就是一段普通的文字，要求使用成对的单引号或双引号引用，不能混用单双引号\n> \n> 数字类型加了引号后也变成字符串，不能与数字类运算\n\n\n# 布尔类型\n\n> 用来表示真假/对错，分真和假2种形态 注意开头的t和f要大写小写会报错\n> \n> true 真\n> \n> false 假",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"IDE",frontmatter:{title:"IDE",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/35c9ab/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/01.IDE.html",relativePath:"后端/05.Python/01.IDE.md",key:"v-0680b9de",path:"/pages/35c9ab/",headersStr:null,content:"# IDE\n\n> IDE的概念(Integrated Development Environment)称为集成开发环境。是一款有图形化界面的软件，它集成了编辑，编译，分析，执行以及调试等功能。python中最常用的IDE是Pycharm",normalizedContent:"# ide\n\n> ide的概念(integrated development environment)称为集成开发环境。是一款有图形化界面的软件，它集成了编辑，编译，分析，执行以及调试等功能。python中最常用的ide是pycharm",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"交互性编程",frontmatter:{title:"交互性编程",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/ec9ba3/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/03.%E4%BA%A4%E4%BA%92%E6%80%A7%E7%BC%96%E7%A8%8B.html",relativePath:"后端/05.Python/03.交互性编程.md",key:"v-410d528c",path:"/pages/ec9ba3/",headersStr:null,content:"# 交互性编程\n\n> cmd命令符和pycharm中的python console中支持交互性编程\n> \n> exit()退出\n> \n> **是次方",normalizedContent:"# 交互性编程\n\n> cmd命令符和pycharm中的python console中支持交互性编程\n> \n> exit()退出\n> \n> **是次方",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"MongoDB",frontmatter:{title:"MongoDB",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/e5cb41/",categories:["后端","SQL"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/04.SQL/24.MongoDB.html",relativePath:"后端/04.SQL/24.MongoDB.md",key:"v-1003aec2",path:"/pages/e5cb41/",headers:[{level:2,title:"体系结构",slug:"体系结构",normalizedTitle:"体系结构",charIndex:138},{level:2,title:"数据模型",slug:"数据模型",normalizedTitle:"数据模型",charIndex:149},{level:2,title:"单机部署",slug:"单机部署",normalizedTitle:"单机部署",charIndex:260},{level:3,title:"windows启动",slug:"windows启动",normalizedTitle:"windows启动",charIndex:421},{level:3,title:"Linux启动",slug:"linux启动",normalizedTitle:"linux启动",charIndex:1884},{level:2,title:"数据库",slug:"数据库",normalizedTitle:"数据库",charIndex:206},{level:3,title:"选择和创建数据库",slug:"选择和创建数据库",normalizedTitle:"选择和创建数据库",charIndex:4481},{level:3,title:"查询当前有权限查看的数据",slug:"查询当前有权限查看的数据",normalizedTitle:"查询当前有权限查看的数据",charIndex:4606},{level:3,title:"查看当前正在使用的数据库名称",slug:"查看当前正在使用的数据库名称",normalizedTitle:"查看当前正在使用的数据库名称",charIndex:4661},{level:3,title:"数据库命名规范",slug:"数据库命名规范",normalizedTitle:"数据库命名规范",charIndex:4689},{level:3,title:"默认三个数据库作用",slug:"默认三个数据库作用",normalizedTitle:"默认三个数据库作用",charIndex:4834},{level:3,title:"数据库删除",slug:"数据库删除",normalizedTitle:"数据库删除",charIndex:5053},{level:2,title:"集合",slug:"集合",normalizedTitle:"集合",charIndex:4992},{level:3,title:"显式创建",slug:"显式创建",normalizedTitle:"显式创建",charIndex:5121},{level:3,title:"隐式创建",slug:"隐式创建",normalizedTitle:"隐式创建",charIndex:5198},{level:3,title:"查询集合",slug:"查询集合",normalizedTitle:"查询集合",charIndex:5253},{level:3,title:"集合删除",slug:"集合删除",normalizedTitle:"集合删除",charIndex:5297},{level:2,title:"文档CRUD",slug:"文档crud",normalizedTitle:"文档crud",charIndex:5354},{level:3,title:"插入",slug:"插入",normalizedTitle:"插入",charIndex:4546},{level:3,title:"查询",slug:"查询",normalizedTitle:"查询",charIndex:1564},{level:3,title:"更新",slug:"更新",normalizedTitle:"更新",charIndex:9119},{level:3,title:"删除",slug:"删除",normalizedTitle:"删除",charIndex:3355},{level:2,title:"文档的分页查询",slug:"文档的分页查询",normalizedTitle:"文档的分页查询",charIndex:10077},{level:3,title:"统计查询",slug:"统计查询",normalizedTitle:"统计查询",charIndex:10089},{level:3,title:"分页列表查询",slug:"分页列表查询",normalizedTitle:"分页列表查询",charIndex:10255},{level:3,title:"排序查询",slug:"排序查询",normalizedTitle:"排序查询",charIndex:10531},{level:3,title:"分页和排序查询的顺序",slug:"分页和排序查询的顺序",normalizedTitle:"分页和排序查询的顺序",charIndex:10679},{level:2,title:"高级查询",slug:"高级查询",normalizedTitle:"高级查询",charIndex:10784},{level:3,title:"正则条件查询",slug:"正则条件查询",normalizedTitle:"正则条件查询",charIndex:10793},{level:3,title:"比较查询",slug:"比较查询",normalizedTitle:"比较查询",charIndex:10870},{level:3,title:"包含查询",slug:"包含查询",normalizedTitle:"包含查询",charIndex:8898},{level:3,title:"不包含查询",slug:"不包含查询",normalizedTitle:"不包含查询",charIndex:11353},{level:3,title:"条件连接查询",slug:"条件连接查询",normalizedTitle:"条件连接查询",charIndex:11438},{level:2,title:"索引",slug:"索引",normalizedTitle:"索引",charIndex:5380},{level:3,title:"单字段索引",slug:"单字段索引",normalizedTitle:"单字段索引",charIndex:11858},{level:3,title:"复合索引",slug:"复合索引",normalizedTitle:"复合索引",charIndex:11971},{level:3,title:"其他索引",slug:"其他索引",normalizedTitle:"其他索引",charIndex:12088},{level:3,title:"索引管理",slug:"索引管理",normalizedTitle:"索引管理",charIndex:12421},{level:3,title:"索引的使用",slug:"索引的使用",normalizedTitle:"索引的使用",charIndex:12901},{level:3,title:"涵盖查询",slug:"涵盖查询",normalizedTitle:"涵盖查询",charIndex:13210},{level:2,title:"Java连接MongoDB",slug:"java连接mongodb",normalizedTitle:"java连接mongodb",charIndex:13328},{level:2,title:"评论案例",slug:"评论案例",normalizedTitle:"评论案例",charIndex:13721},{level:3,title:"模块搭建",slug:"模块搭建",normalizedTitle:"模块搭建",charIndex:13730},{level:3,title:"实体类",slug:"实体类",normalizedTitle:"实体类",charIndex:14806},{level:3,title:"增删改查方法",slug:"增删改查方法",normalizedTitle:"增删改查方法",charIndex:19348},{level:3,title:"创建测试方法",slug:"创建测试方法",normalizedTitle:"创建测试方法",charIndex:21835},{level:3,title:"分页查询测试",slug:"分页查询测试",normalizedTitle:"分页查询测试",charIndex:23880},{level:3,title:"评论点赞",slug:"评论点赞",normalizedTitle:"评论点赞",charIndex:24991},{level:2,title:"副本集",slug:"副本集",normalizedTitle:"副本集",charIndex:25979},{level:3,title:"副本集创建",slug:"副本集创建",normalizedTitle:"副本集创建",charIndex:26506},{level:3,title:"初始化配置副本集和主节点",slug:"初始化配置副本集和主节点",normalizedTitle:"初始化配置副本集和主节点",charIndex:29879},{level:3,title:"添加副本从节点",slug:"添加副本从节点",normalizedTitle:"添加副本从节点",charIndex:30122},{level:3,title:"添加仲裁节点",slug:"添加仲裁节点",normalizedTitle:"添加仲裁节点",charIndex:30217},{level:3,title:"副本集的数据读写操作",slug:"副本集的数据读写操作",normalizedTitle:"副本集的数据读写操作",charIndex:30317},{level:3,title:"主节点的选举原则",slug:"主节点的选举原则",normalizedTitle:"主节点的选举原则",charIndex:30615},{level:3,title:"提升优先级",slug:"提升优先级",normalizedTitle:"提升优先级",charIndex:31135},{level:3,title:"Compass连接副本集",slug:"compass连接副本集",normalizedTitle:"compass连接副本集",charIndex:31257},{level:3,title:"SpringDataMongoDB连接副本集",slug:"springdatamongodb连接副本集",normalizedTitle:"springdatamongodb连接副本集",charIndex:31429},{level:2,title:"分片集群",slug:"分片集群",normalizedTitle:"分片集群",charIndex:32080},{level:3,title:"搭建分片集群架构",slug:"搭建分片集群架构",normalizedTitle:"搭建分片集群架构",charIndex:32354},{level:3,title:"Compass连接分片集群",slug:"compass连接分片集群",normalizedTitle:"compass连接分片集群",charIndex:47354},{level:3,title:"SpringDataMongDB 连接分片集群",slug:"springdatamongdb-连接分片集群",normalizedTitle:"springdatamongdb 连接分片集群",charIndex:47384},{level:2,title:"安全认证",slug:"安全认证",normalizedTitle:"安全认证",charIndex:47948},{level:3,title:"角色",slug:"角色",normalizedTitle:"角色",charIndex:26200},{level:3,title:"单实例安全认证",slug:"单实例安全认证",normalizedTitle:"单实例安全认证",charIndex:48591},{level:3,title:"副本集环境",slug:"副本集环境",normalizedTitle:"副本集环境",charIndex:50364},{level:3,title:"分片集群环境",slug:"分片集群环境",normalizedTitle:"分片集群环境",charIndex:52084},{level:2,title:"4.0新特性",slug:"_4-0新特性",normalizedTitle:"4.0新特性",charIndex:53351},{level:3,title:"加载外部js文件",slug:"加载外部js文件",normalizedTitle:"加载外部js文件",charIndex:53362},{level:3,title:"事务性",slug:"事务性",normalizedTitle:"事务性",charIndex:53445},{level:3,title:"聚合数据类型转换",slug:"聚合数据类型转换",normalizedTitle:"聚合数据类型转换",charIndex:57153}],headersStr:"体系结构 数据模型 单机部署 windows启动 Linux启动 数据库 选择和创建数据库 查询当前有权限查看的数据 查看当前正在使用的数据库名称 数据库命名规范 默认三个数据库作用 数据库删除 集合 显式创建 隐式创建 查询集合 集合删除 文档CRUD 插入 查询 更新 删除 文档的分页查询 统计查询 分页列表查询 排序查询 分页和排序查询的顺序 高级查询 正则条件查询 比较查询 包含查询 不包含查询 条件连接查询 索引 单字段索引 复合索引 其他索引 索引管理 索引的使用 涵盖查询 Java连接MongoDB 评论案例 模块搭建 实体类 增删改查方法 创建测试方法 分页查询测试 评论点赞 副本集 副本集创建 初始化配置副本集和主节点 添加副本从节点 添加仲裁节点 副本集的数据读写操作 主节点的选举原则 提升优先级 Compass连接副本集 SpringDataMongoDB连接副本集 分片集群 搭建分片集群架构 Compass连接分片集群 SpringDataMongDB 连接分片集群 安全认证 角色 单实例安全认证 副本集环境 分片集群环境 4.0新特性 加载外部js文件 事务性 聚合数据类型转换",content:'# MongoDB\n\nMongoDB它支持的数据结构非常松散,是一种类JSON的格式 BSON 它即可以存储比较复杂的数据类型 又相当的灵活\n\nMongoDB中的记录是一个文档 它是一个键值对数据结构 MongoDB文档类似于JSON对象 即一个文档认为就是一个对象\n\n\n# 体系结构\n\n\n\n\n# 数据模型\n\nMongoDB的最小存储单位就是文档(document)对象。文档(document)对象对应于关系型数据库的行。数据在MongoDB中以BSON（Binary-JSON）文档的格式存储在磁盘上。\n\n\n\n\n# 单机部署\n\nhttps://www.mongodb.com/try/download/community\n\nMongoDB的版本命名规范如：x.y.z； y为奇数时表示当前版本为开发版，如：1.5.2、4.1.13； y为偶数时表示当前版本为稳定版，如：1.6.3、4.0.10； z是修正版本号，数字越大越好。\n\n\n# windows启动\n\n 1. 解压到本地\n\n 2. 在MongoDB目录下创建 data\n\n 3. 在data文件夹下 分别创建 db 和 logs 文件夹\n\n 4. 进入bin目录下命令行启动\n    \n    mongod --dbpath=D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db --logpath=D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log --logappend\n    \n    \n    1\n    \n\nMongoDB默认端口为27017 如果要指定端口可以通过--port来设置\n\nmongod --dbpath=D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db --logpath=D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log --logappend --port 27018\n\n\n1\n\n\n# 使用配置文件启动\n\n 1. 在MongoDB目录下conf目录\n\n 2. 进入conf目录创建mongod.conf\n    \n    storage:\n     dbPath: D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db\n    systemLog:\n     destination: file\n     path: D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log\n     logAppend: true\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 在bin目录下命令行启动\n    \n    mongod -f ../config/mongod.conf\n    #或者\n    mongod --config ../config/mongod.conf\n    \n    \n    1\n    2\n    3\n    \n\n# 连接数据库\n\n 1. 在bin目录下命令行启动\n    \n    mongo\n    #或者\n    mongo --host=127.0.0.1 --port=27017\n    \n    \n    1\n    2\n    3\n    \n    \n    show dbs; #查询数据库\n    exit; #退出\n    \n    \n    1\n    2\n    \n\n# 安装为服务项\n\n在bin目录下执行\n\nmongod --dbpath "D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db" --logpath "D:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log" --serviceName "mongodb" --serviceDisplayName "mongodb" --install\nnet start mongodb\n\n\n1\n2\n\n\n\n# Linux启动\n\n解压安装\n\nwget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.2.15.tgz\ntar -zxvf mongodb-linux-x86_64-rhel70-4.2.15.tgz\nmv mongodb-linux-x86_64-rhel70-4.2.15 /usr/local/mongodb \nmkdir -p /mongodb/single/data/db  #数据目录\nmkdir -p /mongodb/single/log  #日志文件\nvim /mongodb/single/mongod.conf\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 配置内容\nsystemLog:\n # MongoDB发送所有日志输出的目标指定为文件\n # The path of the log file to which mongod or mongos should send all diagnostic logging information\n destination: file\n # mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/single/log/mongod.log"\n # 当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n # mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n # The directory where the mongod instance stores its data.Default Value is "/data/db".\n dbPath: "/mongodb/single/data/db"\n journal:\n  # 启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n # 启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\nnet:\n # 服务实例绑定的IP，默认是localhost\n bindIp: localhost,192.168.130.212    #ip为当前服务器局域网ip地址 不设置外部无法通过ip访问\n # bindIp\n # 绑定的端口，默认是27017\n port: 27017\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n启动\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n#/usr/local/mongodb/bin/mongod --config /mongodb/single/mongod.conf\n#查看防火墙状态\nsystemctl status firewalld\n#临时关闭防火墙\nsystemctl stop firewalld\n#开机禁止启动防火墙\nsystemctl disable firewalld\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果一旦数据损坏 导致表死锁\n\nrm -f /mongodb/single/data/db/*.lock #删除锁文件\n/usr/local/mongdb/bin/mongod --repair\n\n\n1\n2\n\n\n# 配置环境变量\n\nvi /etc/profile\n#配置内容\nexport MONGODB_HOME=/usr/local/mongodb\nexport PATH=$PATH:$MONGODB_HOME/bin\n\n\n1\n2\n3\n4\n\n\n# 配置为服务加入启动\n\ncd /lib/systemd/system  \nvi mongodb.service  \n#配置内容\n[Unit]\nDescription=mongodb\nAfter=network.target remote-fs.target nss-lookup.target\n\n[Service]\nType=forking\nExecStart=/usr/local/mongodb/bin/mongod --config /mongodb/single/mongodb.conf\nExecReload=/bin/kill -s HUP $MAINPID\nExecStop=/usr/local/mongodb/bin/mongod --shutdown --config /mongodb/single/mongodb.conf\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n#echo "/usr/local/mongodb/bin/mongod --dbpath=/data/db --fork --bind_ip=0.0.0.0 --port 27017 --logpath=/data/db/log --logappend --auth" >> /etc/rc.local\n\nchmod 754 mongodb.service\n\n\n1\n2\n3\n\n\n#启动服务\nsystemctl start mongodb.service\n#关闭服务\nsystemctl stop mongodb.service\n#开机启动\nsystemctl enable mongodb.service\n#关闭开启启动\nsystemctl disable mongodb.service\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 标准的关闭方法\n\nmongo\nuse admin\ndb.shutdownServer()\n\n\n1\n2\n3\n\n\n进程id关闭法\n\nps -ef | grep mongo\nkill -2 pid\n\n\n1\n2\n\n\n\n# 数据库\n\n\n# 选择和创建数据库\n\n#use 数据库名称   \nuse articledb  #如果没有此数据库则自动创建 \n#如果新的数据库没有插入数据 showdbs是无法查看到新创建的数据库 因为此时数据库存储在内存当中 并未持久化\n\n\n1\n2\n3\n\n\n\n# 查询当前有权限查看的数据\n\nshow dbs\n#或者\nshow databases\n\n\n1\n2\n3\n\n\n\n# 查看当前正在使用的数据库名称\n\ndb\n\n\n1\n\n\n\n# 数据库命名规范\n\n数据库名可以是满足以下条件的任意UTF-8字符串。\n\n * 不能是空字符串（ "")。\n * 不得含有 \' \'（空格)、.、$、/、\\和\\0 (空字符)。\n * 应全部小写。\n * 最多 64字节。\n\n有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。\n\n\n# 默认三个数据库作用\n\n * admin ： 从权限的角度来看，这是"root"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。\n * local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合\n * config : 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。\n\n\n# 数据库删除\n\ndb.dropDatabase()  #删除当前使用的库\n\n\n1\n\n\n\n# 集合\n\n集合,类似于关系型数据中的表\n\n\n# 显式创建\n\n#db.createCollection(集合名)\ndb.createCollection("my")  #成功返回1\n\n\n1\n2\n\n\n\n# 隐式创建\n\n当向一个集合中插入文档时 如果集合不存在 则自动创建集合\n\n通常我们使用隐式创建文档即可\n\n\n# 查询集合\n\nshow collections #查询当前库的所有集合\n\n\n1\n\n\n\n# 集合删除\n\n#db.集合名.drop()\ndb.my.drop()  #成功则返回true\n\n\n1\n2\n\n\n\n# 文档CRUD\n\n文档的数据结构和JSON基本一样\n\n索引存储在集合中数据都是 BSON 格式\n\n\n# 插入\n\n通过insert() 或者 save() 方法向集合中插入文档\n\n# 单问插入\n\n#单文档插入\ndb.collection.insert(\n <document or array of documents>,\n {\n  writeConcern: <document>,\n  ordered: <boolean>\n }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n#db.集合名.inset({""})\n#单文档插入\ndb.comment.insert({"articleid":"100000","content":"今天天气真好，阳光明媚","userid":"1001","nickname":"Rose","createdatetime":new Date(),"likenum":NumberInt(10),"state":null})  #返回1则插入成功\n\n\n1\n2\n3\n\n\n# 批量插入\n\n#批量插入\ndb.collection.insertMany(\n [ <document 1> , <document 2>, ... ],\n {\n   writeConcern: <document>,\n   ordered: <boolean>\n }\n)\n\n#db.集合名.insertMany([{""},{""}])\ndb.comment.insertMany([{"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new Date("2019-08-05T22:08:15.522Z"),"likenum":NumberInt(1000),"state":"1"},{"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new Date("2019-08-05T23:58:51.485Z"),"likenum":NumberInt(888),"state":"1"},{"_id":"3","articleid":"100001","content":"我一直喝凉开水，冬天夏天都喝。","userid":"1004","nickname":"杰克船长","createdatetime":new Date("2019-08-06T01:05:06.321Z"),"likenum":NumberInt(666),"state":"1"},{"_id":"4","articleid":"100001","content":"专家说不能空腹吃饭，影响健康。","userid":"1003","nickname":"凯撒","createdatetime":new Date("2019-08-06T08:18:35.288Z"),"likenum":NumberInt(2000),"state":"1"},{"_id":"5","articleid":"100001","content":"研究表明，刚烧开的水千万不能喝，因为烫嘴。","userid":"1003","nickname":"凯撒","createdatetime":new Date("2019-08-06T11:01:02.521Z"),"likenum":NumberInt(3000),"state":"1"}]);  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# try catch\n\n我们通过批量插入时由于数据较多可能会出现失败 我们可以通过try catch进行异常处理\n\ntry {\n  db.comment.insertMany([\n    {\n      _id: \'1\',\n      articleid: \'100001\',\n      content: \'我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。\',\n      userid: \'1002\',\n      nickname: \'相忘于江湖\',\n      createdatetime: new Date(\'2019-08-05T22:08:15.522Z\'),\n      likenum: NumberInt(1000),\n      state: \'1\',\n    },\n    {\n      _id: \'2\',\n      articleid: \'100001\',\n      content: \'我夏天空腹喝凉开水，冬天喝温开水\',\n      userid: \'1005\',\n      nickname: \'伊人憔悴\',\n      createdatetime: new Date(\'2019-08-05T23:58:51.485Z\'),\n      likenum: NumberInt(888),\n      state: \'1\',\n    },\n    {\n      _id: \'3\',\n      articleid: \'100001\',\n      content: \'我一直喝凉开水，冬天夏天都喝。\',\n      userid: \'1004\',\n      nickname: \'杰克船长\',\n      createdatetime: new Date(\'2019-08-06T01:05:06.321Z\'),\n      likenum: NumberInt(666),\n      state: \'1\',\n    },\n    {\n      _id: \'4\',\n      articleid: \'100001\',\n      content: \'专家说不能空腹吃饭，影响健康。\',\n      userid: \'1003\',\n      nickname: \'凯撒\',\n      createdatetime: new Date(\'2019-08-06T08:18:35.288Z\'),\n      likenum: NumberInt(2000),\n      state: \'1\',\n    },\n    {\n      _id: \'5\',\n      articleid: \'100001\',\n      content: \'研究表明，刚烧开的水千万不能喝，因为烫嘴。\',\n      userid: \'1003\',\n      nickname: \'凯撒\',\n      createdatetime: new Date(\'2019-08-06T11:01:02.521Z\'),\n      likenum: NumberInt(3000),\n      state: \'1\',\n    },\n  ])\n} catch (e) {\n  print(e)\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n\n\n\n# 查询\n\n查询当前数据库中的文档\n\n# 查询所有文档\n\n#db.collection.find(<query>, [projection])\ndb.comment.find()\n\n\n1\n2\n\n\n\n\n# 条件查询\n\n查询文档中包含此文档内容的所有文档\n\n#db.collection.find({键:"内容"})\ndb.comment.find({articleid:"100001"})\n\n\n1\n2\n\n\n# 只返回第一个结果\n\n#db.collection.findOne({键:"内容"})\ndb.comment.findOne({articleid:"100001"})\n\n\n1\n2\n\n\n# 投影查询\n\n查询出来的文档 只显示需要的键值对 相对应sql中的查询结果只显示指定列\n\nfind传参时给出指定的键字段 1为查询此键 0为过滤此键 默认自动包含查询_id键\n\n#db.collection.find({键:"内容"},{键:1,键:0})  \ndb.comment.find({articleid:"100001"},{content:1})  #查询包含100001文档中的 content键内容\ndb.comment.find({articleid:"100001"},{content:1,_id:0}) #默认查询包含_id 可以通过0过滤\n\n\n\n1\n2\n3\n4\n\n\n\n# 更新\n\n//语法\ndb.collection.update(query, update, options)\n//或\ndb.collection.update(\n\t<query>,\n\t<update>,\n\t{\n\t\tupsert: <boolean>,\n\t\tmulti: <boolean>,\n\t\twriteConcern: <document>,\n\t\tcollation: <document>,\n\t\tarrayFilters: [ <filterdocument1>, ... ],\n\t\thint: \t<document|string> \t\t\t\t// Available starting in MongoDB 4.2\n\t}\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n# 覆盖修改\n\n修改指定文档 并覆盖更新整个文档 即修改为当前语句中的字段 原先字段不保留\n\ndb.comment.update({_id:"1"},{likenum:NumberInt(1001)})  #可以理解为删除原文档并插入此文档\n\n\n1\n\n\n# 局部修改\n\n我们可以通过修改器$set来修改指定键的内容 并保留原文档其他内容\n\n#默认只修改符合条件的第一条数据\ndb.comment.update({_id:"2"},{$set:{likenum:NumberInt(778)}})\n#修改所有符合条件的数据需要加上参数{multi:true}\ndb.comment.update({userid:"1003"},{$set:{nickname:"张三"}},{multi:true})\n\n\n1\n2\n3\n4\n\n\n# 列值增长修改\n\n通过$inc运算符实现 可以对该键的值进行递增 递减\n\ndb.comment.update({_id:"2"},{$inc:{likenum:NumberInt(1)}})\n\n\n1\n\n\n\n# 删除\n\n# 条件删除\n\n删除符合条件的文档\n\n#db.集合.remove({条件})\ndb.comment.remove({_id:"1"})  #返回值为删除多少条数据\n\n\n1\n2\n\n\n# 删除全部\n\ndb.comment.remove({})\n\n\n1\n\n\n\n# 文档的分页查询\n\n\n# 统计查询\n\n使用count()方法\n\ndb.collection.count(query, options)\n\n\n1\n\n\n\n\n# 统计当前集合所有文档数\n\ndb.comment.count()  #返回值是文档数\n\n\n1\n\n\n# 按条件查询\n\ndb.comment.count({userid:"1003"}) \n\n\n1\n\n\n\n# 分页列表查询\n\n通过limit()方法 读取指定数量的数据 使用skip()方法跳过指定数量的数据\n\n#db.collection.find().limit(number).skip(number)\ndb.comment.find().limit(3)  #查询前3条的数据\ndb.comment.find().skip(3) #跳过前3条的数据\n\ndb.comment.find().skip(0).limit(2)  #第一页\ndb.comment.find().skip(2).limit(2)  #第二页\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 排序查询\n\nsort()方法对数据进行排序 使用1和-1来指定升序和降序 sort({键:1}) 默认以id进行升序\n\ndb.comment.find().sort({userid:-1,likenum:1}) #对userid的内容进行降序 再对likenum的内容进行升序\n\n\n1\n\n\n\n# 分页和排序查询的顺序\n\nskip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()，和命令编写顺序无关。\n\n\n# 高级查询\n\n\n# 正则条件查询\n\n#db.集合.find({键:/正则表达式/})\ndb.comment.find({content:/^专家/})\n\n\n1\n2\n\n\n\n# 比较查询\n\ndb.集合名称.find({ "field" : { $gt: value }}) # 大于: field > value\ndb.集合名称.find({ "field" : { $lt: value }}) # 小于: field < value\ndb.集合名称.find({ "field" : { $gte: value }}) # 大于等于: field >= value\ndb.集合名称.find({ "field" : { $lte: value }}) # 小于等于: field <= value\ndb.集合名称.find({ "field" : { $ne: value }}) # 不等于: field != value\n\n#例子\ndb.comment.find({likenum:{$gt:NumberInt(700)}})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 包含查询\n\n使用$in 查询包含指定内容的文档\n\ndb.comment.find({userid:{$in:["1003","1004"]})\n\n\n1\n\n\n\n# 不包含查询\n\n使用$nin 查询不包含指定内容的文档\n\ndb.comment.find({userid:{$nin:["1003","1004"]})\n\n\n1\n\n\n\n# 条件连接查询\n\n前面我们通过$set查询单条件的文档 如果为多条件则需要使用 $and进行查询\n\ndb.comment.find({$and:[{likenum:{$gte:NumberInt(700)}},{likenum:{$lt:NumberInt(2000)}}]})   # 查询likenum 大于等于700 并且小于2000的文档\n\n\n\n1\n2\n\n\n# 或者连接\n\n使用$or进行查询\n\ndb.comment.find({$or:[{likenum:{$gte:NumberInt(700)}},{likenum:{$lt:NumberInt(2000)}}]})   # 查询likenum 大于等于700 并且小于2000的文档\n\n\n1\n\n\n\n# 索引\n\nMongoDB没有下索引下 是进行全集合扫描 如果数据量较大时查询效率非常低\n\nMongoDB索引使用B树数据结构(B-Tree,MySQL是B+Tree)\n\n\n# 单字段索引\n\nMongoDB支持在文档的单个字段上创建用户定义的升序/降序索引 称为单字段索引\n\n对于单个字段索引和排序操作，索引键的排序顺序（即升序或降序）并不重要，因为MongoDB可以在任何方向上遍历索引。\n\n\n\n\n# 复合索引\n\n复合索引中列出的字段顺序具有重要意义。例如，如果复合索引由 { userid: 1, score: -1 } 组成，则索引首先按userid正序排序，然后 在每个userid的值内，再在按score倒序排序。\n\n\n\n\n# 其他索引\n\n * 地理空间索引（Geospatial Index） 为了支持对地理空间坐标数据的有效查询，MongoDB提供了两种特殊的索引：返回结果时使用平面几何的二维索引和返回结果时使用球面 几何的二维球面索引。\n\n * 文本索引（Text Indexes） MongoDB提供了一种文本索引类型，支持在集合中搜索字符串内容。这些文本索引不存储特定于语言的停止词（例如“the”、“a”、“or”）， 而将集合中的词作为词干，只存储根词。\n\n * 哈希索引（Hashed Indexes） 为了支持基于散列的分片，MongoDB提供了散列索引类型，它对字段值的散列进行索引。这些索引在其范围内的值分布更加随机，但只支 持相等匹配，不支持基于范围的查询。\n\n\n# 索引管理\n\n# 查看索引\n\n返回一个集合中的所有索引的数组 _id主键自带索引为升序\n\ndb.comment.getIndexes()\n\n\n1\n\n\n# 创建索引\n\n\n\n#db.collection.createIndex(keys, options)\ndb.comment.createIndex({userid:1})  # 给userid创建索引 为升序 索引名默认为键名后加上_(1或者-1)\ndb.comment.createIndex({userid:1,nickname:-1}) #复合索引\n\n\n1\n2\n3\n\n\n# 删除索引\n\n如果要删除文本索引只能通过索引名删除\n\n_id索引无法删除\n\n#db.collection.dropIndex(索引名)\ndb.comment.dropIndex()  #删除所有索引\ndb.comment.dropIndex("userid_1_nickname_-1") #删除指定索引\ndb.comment.dropIndex({userid:1})  #根据创建索引条件删除索引\n\n\n1\n2\n3\n4\n\n\n\n# 索引的使用\n\n分析查询性能（Analyze Query Performance）通常使用执行计划（解释计划、Explain Plan）来查看查询的情况，如查询耗费的时间、是否基于索引查询等。 那么，通常，我们想知道，建立的索引是否有效，效果如何，都需要通过执行计划查看。\n\n#db.collection.find(query,options).explain(options)\ndb.comment.find({uderid:"1003"}).explain()\n\n\n1\n2\n\n\n关键点看： "stage" : "COLLSCAN", 表示全集合扫描\n\n"stage" : "IXSCAN" ,基于索引的扫描\n\n\n# 涵盖查询\n\nCovered Queries 当查询条件和查询的投影仅包含索引字段时，MongoDB直接从索引返回结果，而不扫描任何文档或将文档带入内存。 这些覆盖的查询可以非常有效。 其实就是查询符合查询条件 索引字段的值\n\n\n\n\n# Java连接MongoDB\n\n 1. mongodb-driver 是mongo官方推出的java连接mongoDB的驱动包，相当于JDBC驱动。我们通过一个入门的案例来了解mongodb-driver 的基本使用。 官方驱动说明和下载： http://mongodb.github.io/mongo-java-driver/ 官方驱动示例文档：http://mongodb.github.io/mongo-java-driver/3.8/driver/getting-started/quick-start/\n 2. SpringDataMongoDB SpringData家族成员之一，用于操作MongoDB的持久层框架，封装了底层的mongodb-driver。 官网主页： https://projects.spring.io/spring-data-mongodb/\n\n\n# 评论案例\n\n\n# 模块搭建\n\n 1. 继承spring-boot项目 导入springdataMongoDB坐标\n\n <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.1.6.RELEASE</version>\n        <relativePath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-mongodb</artifactId>\n        </dependency>\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n 2. 创建application.yml\n    \n    spring:\n      data:\n        mongodb:\n          host: 192.168.130.212  # 主机地址\n          database: articledb  # 数据库\n          port: 27017 # 默认端口是27017\n          #也可以使用uri连接\n          #uri: mongodb://192.168.40.134:27017/articledb\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    \n\n\n# 实体类\n\npackage com.itcatst.article.po;\n\nimport org.springframework.data.annotation.Id;\nimport org.springframework.data.mongodb.core.index.CompoundIndex;\nimport org.springframework.data.mongodb.core.index.Indexed;\nimport org.springframework.data.mongodb.core.mapping.Document;\nimport org.springframework.data.mongodb.core.mapping.Field;\n\nimport java.io.Serializable;\nimport java.time.LocalDateTime;\nimport java.util.Date;\n\n/**\n * 文章评论实体类\n */\n//把一个java类声明为mongodb的文档，可以通过collection参数指定这个类对应的文档。\n//@Document(collection="mongodb 对应 collection 名")\n// 若未加 @Document ，该 bean save 到 mongo 的 comment collection\n// 若添加 @Document ，则 save 到 comment collection\n@Document(collection = "comment")//可以省略，如果省略，则默认使用类名小写映射集合 会映射到与类名一致的集合当中\n//复合索引\n// @CompoundIndex( def = "{\'userid\': 1, \'nickname\': -1}")\npublic class Comment implements Serializable {\n    //主键标识，该属性的值会自动对应mongodb的主键字段"_id"，如果该属性名就叫“id”,则该注解可以省略，否则必须写\n    @Id\n    private String id;//主键\n    //该属性对应mongodb的字段的名字，如果一致，则无需该注解\n    @Field("content")\n    private String content;//吐槽内容\n    private Date publishtime;//发布日期\n    //添加了一个单字段的索引\n    @Indexed\n    private String userid;//发布人ID\n    private String nickname;//昵称\n    private LocalDateTime createdatetime;//评论的日期时间\n    private Integer likenum;//点赞数\n    private Integer replynum;//回复数\n    private String state;//状态\n    private String parentid;//上级ID\n    private String articleid;\n\n    //getter and setter.....\n    public String getId() {\n        return id;\n    }\n\n    public void setId(String id) {\n        this.id = id;\n    }\n\n    public String getContent() {\n        return content;\n    }\n\n    public void setContent(String content) {\n        this.content = content;\n    }\n\n    public Date getPublishtime() {\n        return publishtime;\n    }\n\n    public void setPublishtime(Date publishtime) {\n        this.publishtime = publishtime;\n    }\n\n    public String getUserid() {\n        return userid;\n    }\n\n    public void setUserid(String userid) {\n        this.userid = userid;\n    }\n\n    public String getNickname() {\n        return nickname;\n    }\n\n    public void setNickname(String nickname) {\n        this.nickname = nickname;\n    }\n\n    public LocalDateTime getCreatedatetime() {\n        return createdatetime;\n    }\n\n    public void setCreatedatetime(LocalDateTime createdatetime) {\n        this.createdatetime = createdatetime;\n    }\n\n    public Integer getLikenum() {\n        return likenum;\n    }\n\n    public void setLikenum(Integer likenum) {\n        this.likenum = likenum;\n    }\n\n    public Integer getReplynum() {\n        return replynum;\n    }\n\n    public void setReplynum(Integer replynum) {\n        this.replynum = replynum;\n    }\n\n    public String getState() {\n        return state;\n    }\n\n    public void setState(String state) {\n        this.state = state;\n    }\n\n    public String getParentid() {\n        return parentid;\n    }\n\n    public void setParentid(String parentid) {\n        this.parentid = parentid;\n    }\n\n    public String getArticleid() {\n        return articleid;\n    }\n\n    public void setArticleid(String articleid) {\n        this.articleid = articleid;\n    }\n\n    @Override\n    public String toString() {\n        return "Comment{" +\n                "id=\'" + id + \'\\\'\' +\n                ", content=\'" + content + \'\\\'\' +\n                ", publishtime=" + publishtime +\n                ", userid=\'" + userid + \'\\\'\' +\n                ", nickname=\'" + nickname + \'\\\'\' +\n                ", createdatetime=" + createdatetime +\n                ", likenum=" + likenum +\n                ", replynum=" + replynum +\n                ", state=\'" + state + \'\\\'\' +\n                ", parentid=\'" + parentid + \'\\\'\' +\n                ", articleid=\'" + articleid + \'\\\'\' +\n                \'}\';\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n\n * @Document(collection = "comment") 声明为mongodb文档 并且映射到comment集合中 如果当前实体类名与集合一致则collection可以忽略(并且只能是小写名称的实体类)\n * @CompoundIndex( def = "{\'userid\': 1, \'nickname\': -1}") 复合索引 def属性与设置复活索引条件一致 为键和排序方式\n * @Id 声明此属性为主键 如果属性名为id则该注解可以省略不加\n * @Field("content") 当成员属性名称与集合中的键字段不一致时 可以使用该注解映射为集合中指定的键字段 此时映射为content键字段\n * @Indexed 添加为单字段索引\n\n\n# 增删改查方法\n\n 1. 创建dao层接口 并继承MongoRepository 泛型为 实体类 和 id的类型\n    \n    package com.itcatst.article.dao;\n    \n    import com.itcatst.article.po.Comment;\n    import org.springframework.data.mongodb.repository.MongoRepository;\n    \n    public interface CommentRepository extends MongoRepository<Comment,String> {\n    }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n 2. 创建Service层 注入dao层 并调用动态代理里的方法\n    \n    package com.itcatst.article.service;\n    \n    import com.itcatst.article.dao.CommentRepository;\n    import com.itcatst.article.po.Comment;\n    import org.springframework.beans.factory.annotation.Autowired;\n    import org.springframework.stereotype.Service;\n    \n    import java.util.List;\n    \n    @Service\n    public class CommentService {\n    \n        @Autowired\n        private CommentRepository commentRepository;\n    \n        /**\n         * 保存一个评论\n         *\n         * @param comment\n         */\n        public void saveComment(Comment comment) {\n            //如果需要自定义主键，可以在这里指定主键；如果不指定主键，MongoDB会自动生成主键\n            //设置一些默认初始值。。。\n            //调用dao\n            commentRepository.save(comment);\n        }\n    \n        /**\n         * 更新评论\n         *\n         * @param comment\n         */\n        public void updateComment(Comment comment) {\n            //调用dao\n            commentRepository.save(comment);\n        }\n    \n        /**\n         * 根据id删除评论\n         *\n         * @param id\n         */\n        public void deleteCommentById(String id) {\n            //调用dao\n            commentRepository.deleteById(id);\n        }\n    \n        /**\n         * 查询所有评论\n         *\n         * @return\n         */\n        public List<Comment> findCommentList() {\n            //调用dao\n            return commentRepository.findAll();\n        }\n    \n        /**\n         * 根据id查询评论\n         *\n         * @param id\n         * @return\n         */\n        public Comment findCommentById(String id) {\n            //调用dao\n            return commentRepository.findById(id).get();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    68\n    \n\n\n# 创建测试方法\n\n 1. 新建junit测试方法 类路径要与主程序一致\n    \n    package com.itcatst.article.service;\n    \n    import com.itcatst.article.po.Comment;\n    import com.itcatst.article.service.CommentService;\n    import org.junit.Test;\n    import org.junit.runner.RunWith;\n    import org.springframework.beans.factory.annotation.Autowired;\n    import org.springframework.boot.test.context.SpringBootTest;\n    import org.springframework.test.context.junit4.SpringRunner;\n    \n    import java.time.LocalDateTime;\n    import java.util.List;\n    \n    @RunWith(SpringRunner.class)\n    @SpringBootTest\n    public class CommentServiceTest {\n    \n        @Autowired\n        private CommentService commentService;\n    \n        @Test\n        public void testFindCommentList() {\n            List<Comment> commentList = commentService.findCommentList();\n            System.out.println(commentList);\n        }\n    \n        @Test\n        public void testFndCommentById() {\n            Comment commentById = commentService.findCommentById("1");\n            System.out.println(commentById);\n        }\n    \n        /**\n         * 保存一个评论\n         */\n        @Test\n        public void testSaveComment() {\n            Comment comment = new Comment();\n            //此处没有指定id 但MongoDB默认自动生成 我们推荐自动生成 不推荐自己书写id\n            comment.setArticleid("100000");\n            comment.setContent("测试添加的数据");\n            comment.setCreatedatetime(LocalDateTime.now());\n            comment.setUserid("1003");\n            comment.setNickname("凯撒大帝");\n            comment.setState("1");\n            comment.setLikenum(0);\n            comment.setReplynum(0);\n    \n            commentService.saveComment(comment);\n        }\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    \n\n\n# 分页查询测试\n\n 1. 在dao层接口创建新的方法 必须要指定对应的属性名为方法名否则会报错\n    \n    格式: Page<实体类> findBy根据哪个属性来查询(传递属性类型 属性名,Pageable pageable);\n    \n    ////根据父id，查询子评论的分页列表\n    Page<Comment> findByParentid(String parentid, Pageable pageable);\n    \n    \n    1\n    2\n    \n\n 2. 在service层调用dao层的方法\n    \n    /**\n         * 分页查询\n         *\n         * @param parentid 根据哪个属性来查询\n         * @param page     页码\n         * @param size     每页个数\n         * @return\n         */\n        public Page<Comment> findCommentListByParentid(String parentid, int page, int size) {\n            return commentRepository.findByParentid(parentid, PageRequest.of(page-1, size));\n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n 3. 测试用例\n    \n    @Test\n    public void testFindCommentListByParentid(){\n        Page<Comment> page = commentService.findCommentListByParentid("3", 1, 2);\n        System.out.println(page.getTotalElements());//返回总条数\n        List<Comment> content = page.getContent();  //返回结果的集合\n        System.out.println(content);\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n\n# 评论点赞\n\n最简单的方法就是根据id查询当前对应的文档 并将点赞数+1 但执行效率不高 因为只需要修改点赞数 没有必要把所有的字段都查询出来\n\n我们可以使用 MongoTemplate 类实现对某列进行操作\n\n 1. 在service层注入 MongoTemplate 并编修改方法\n    \n       @Autowired\n        private MongoTemplate mongoTemplate;\n    \n        /**\n         * 点赞数+1\n         *\n         * @param id\n         */\n        public void updateCommentLikenum(String id) {\n            //查询条件\n            //Query query = Query.query(Criteria.where("_id").is(id)).addCriteria(Criteria.where("nickname").is("凯撒大帝"));  //使用Criteria.when(键字段).is(条件)  判断条件 addCriteria可以无限扩展条件\n            Query query = Query.query(Criteria.where("_id").is(id));\n            //更新条件\n            Update update = new Update();\n            update.inc("likenum",1); //将likenum数+1  如单传键字段则自动加1\n    //        update.set()\n    \n            mongoTemplate.updateFirst(query, update, Comment.class);\n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n\n# 副本集\n\nMongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高 可用性，是所有生产部署的基础。\n\n主从复制和副本集区别 主从集群和副本集最大的区别就是副本集没有固定的“主节点”；整个集群会选出一个“主节点”，当其挂 掉后，又在剩下的从节点中选中其他节点为“主节点”，副本集总有一个活跃点(主、primary)和一个或多 个备份节点(从、secondary)。\n\n副本集有两种类型三种角色:\n\n两种类型：\n\n * 主节点（ Primary）类型：数据操作的主要连接点，可读写。\n * 次要（辅助、从）节点（ Secondaries）类型：数据冗余备份节点，可以读或选举。\n\n三种角色：\n\n * 主要成员（Primary）：主要接收所有写操作。就是主节点。\n * 副本成员（Replicate）：从主节点通过复制操作以维护相同的数据集，即备份数据，不可写操作，但可 以读操作（但需要配置）。是默认的一种从节点类型。\n * 仲裁者（ Arbiter）：不保留任何数据的副本，只具有投票选举作用。当然也可以将仲裁服务器维护为副 本集的一部分，即副本成员同时也可以是仲裁者。也是一种从节点类型。\n\n\n# 副本集创建\n\n# 主节点\n\n#主节点\nmkdir -p /mongodb/replica_sets/myrs_27017/log \\ & #日志文件\nmkdir -p /mongodb/replica_sets/myrs_27017/data/db #数据文件\n\nvim /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n2\n3\n4\n5\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/replica_sets/myrs_27017/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/replica_sets/myrs_27017/data/db"\n journal:\n  #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/replica_sets/myrs_27017/log/mongod.pid"\nnet:\n #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #bindIp\n #绑定的端口\n port: 27017\nreplication:\n #副本集的名称  同一个副本集中集群必须一致\n replSetName: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动主节点\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n\n\n# 从节点\n\n#副本节点\nmkdir -p /mongodb/replica_sets/myrs_27018/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27018/data/db\nvim /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n2\n3\n4\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/replica_sets/myrs_27018/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/replica_sets/myrs_27018/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/replica_sets/myrs_27018/log/mongod.pid"\nnet:\n #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #bindIp\n #绑定的端口\n port: 27018\nreplication:\n #副本集的名称\n replSetName: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n\n\n# 仲裁节点\n\n#仲裁节点\nmkdir -p /mongodb/replica_sets/myrs_27019/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27019/data/db\nvim /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n4\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/replica_sets/myrs_27019/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/replica_sets/myrs_27019/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/replica_sets/myrs_27019/log/mongod.pid"\nnet:\n #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #bindIp\n #绑定的端口\n port: 27019\nreplication:\n #副本集的名称\n replSetName: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n\n\n\n# 初始化配置副本集和主节点\n\n#使用mongo客户端任意节点都可以 但尽量连上主节点\n/usr/local/mongodb/bin/mongo --host=192.168.130.212 --port=27017\n\n#初始化命令\nrs.initiate() #使用默认的配置来初始化副本集  “ok”的值为1，说明创建成功  并且此节点变为主节点\n\nrs.conf() #查看默认节点配置\n\nrs.status() # 查询副本集状态\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 添加副本从节点\n\n#rs.add(host:post)\nrs.add(192.168.130.212:27018)\n\nrs.status() #查看副本集状态\n\n\n1\n2\n3\n4\n\n\n\n# 添加仲裁节点\n\n#rs.addArb(host:post)\nrs.addarB(192.168.130.212:27019)\n\nrs.status() #查看副本集状态\n\n\n1\n2\n3\n4\n\n\n\n# 副本集的数据读写操作\n\n 1. 只有主节点写入数据和读取数据\n\n 2. 默认情况从节点无法读取数据 也无法写入数据 可以进行设置增加读的权限\n    \n    #连接从节点的客户端\n    rs.slaveOk() #该命令是 db.getMongo().setSlaveOk() 的简化命令\n    #或者\n    rs.slaveOk(true) \n    #此时实现了读写分离 让主插入数据 让从来读取数据\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 3. 仲裁者节点，不存放任何业务数据的，可以登录查看 只存放副本集配置等数据。\n\n\n# 主节点的选举原则\n\nMongoDB在副本集中，会自动进行主节点的选举，主节点选举的触发条件：\n\n 1. 主节点故障\n 2. 主节点网络不可达（默认心跳信息为10秒）\n 3. 人工干预（rs.stepDown(600)）\n\n选举规则是根据票数来决定谁获胜：\n\n * 票数最高，且获得了 “大多数”成员的投票支持的节点获胜。 “大多数”的定义为：假设复制集内投票成员数量为N，则大多数为 N/2 + 1。例如：3个投票成员， 则大多数的值是2。当复制集内存活成员数量不足大多数时，整个复制集将无法选举出Primary， 复制集将无法提供写服务，处于只读状态。\n * 若票数相同，且都获得了 “大多数”成员的投票支持的，数据新的节点获胜。 数据的新旧是通过操作日志oplog来对比的。\n\n在获得票数的时候，优先级（priority）参数影响重大。 可以通过设置优先级（priority）来设置额外票数。优先级即权重，取值为0-1000，相当于可额外增加 0-1000的票数，优先级的值越大，就越可能获得多数成员的投票（votes）数。指定较高的值可使成员 更有资格成为主要成员，更低的值可使成员更不符合条件。 默认情况下，优先级的值是1\n\n\n# 提升优先级\n\n#先导出配置到变量中\ncfg=rs.conf()\n#修改指定优先级  ID号默认从0开始\ncfg.members[1].priority=2\n#重新加载配置\nrs.reconfig(cfg)\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Compass连接副本集\n\n#将host修改为当前主节点的ip\nvar config = rs.config();\n config.members[0].host="192.168.130.212:27017";rs.reconfig(config) \n\n\n1\n2\n3\n\n\n\n\n选择Primary 为主节点 Secondary为副本集\n\n\n# SpringDataMongoDB连接副本集\n\nmongodb://host1,host2,host3/articledb?connect=replicaSet&slaveOk=true&replicaSet=副本集名字\n\n * slaveOk=true ：开启副本节点读的功能，可实现读写分离。\n * connect=replicaSet ：自动到副本集中选择读写的主机。如果slaveOK是打开的，则实现了读写分 离\n\n主机必须是副本集中所有的主机，包括主节点、副本节点、仲裁节点。\n\nspring:\n  data:\n    mongodb:\n      # host: 192.168.130.212  # 主机地址\n      # database: articledb  # 数据库\n      # port: 27017 # 默认端口是27017\n      #也可以使用uri连接\n      #uri: mongodb://192.168.40.134:27017/articledb\n        # 副本集的连接字符串\n      uri: mongodb://192.168.42.212:27017,192.168.42.212:27018,192.168.42.212:27019/article\n      db?connect=replicaSet&slaveOk=true&replicaSet=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 分片集群\n\n分片（sharding）是一种跨多台机器分布数据的方法， MongoDB使用分片来支持具有非常大的数据集 和高吞吐量操作的部署。\n\nMongoDB分片群集包含以下组件：\n\n * 分片（存储）：每个分片包含分片数据的子集。 每个分片都可以部署为副本集。\n * mongos （路由）：mongos充当查询路由器，在客户端应用程序和分片集群之间提供接口。\n * config servers （“调度”的配置）：配置服务器存储群集的元数据和配置设置。 从MongoDB 3.4开 始，必须将配置服务器部署为副本集（CSRS）。\n\n\n\n\n# 搭建分片集群架构\n\n\n\n# 第一个副本集\n\n#-----------myshardrs01\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/data/db\n\n#myshardrs01_27018\nvim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myshardrs01_27018/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.pid"\nnet:\n #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #bindIp\n #绑定的端口\n port: 27018\nreplication:\n #副本集的名称\n replSetName: myshardrs01\nsharding:\n #分片角色\n clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nclusterRole 设置分片角色:\n\n * shardsvr 分片角色\n * configsvr 配置角色\n\n#myshardrs01_27118\nvim /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myshardrs01_27118/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.pid"\nnet:\n #服务实例绑定所有IP\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #绑定的端口\n port: 27118\nreplication:\n replSetName: myshardrs01\nsharding:\n clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n#myshardrs01_27218\nvim /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myshardrs01_27218/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.pid"\nnet:\n #服务实例绑定的IP\n bindIp: localhost,192.168.42.130.212\n #绑定的端口\n port: 27218\nreplication:\n replSetName: myshardrs01\nsharding:\n clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n启动第一套副本集：一主一副本一仲裁\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n2\n3\n\n * 初始化副本集和创建主节点\n\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27018 #尽量连接主节点\n\n\n1\n\n\nrs.initiate()  #初始化主节点\nrs.status() #状态查看\nrs.add("192.168.130.212:27118")  #添加副节点\nrs.addArb("192.168.130.212:27218") #添加仲裁节点\nrs.conf()\n\n\n1\n2\n3\n4\n5\n\n\n# 第二套副本集\n\n#-----------myshardrs02\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n#myshardrs02_27318\nvim /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myshardrs02_27318/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.pid"\nnet:\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #绑定的端口\n port: 27318\nreplication:\n replSetName: myshardrs02\nsharding:\n clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n#myshardrs02_27418\nvim /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myshardrs02_27418/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.pid"\nnet:\n #服务实例绑定所有IP\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #绑定的端口\n port: 27418\nreplication:\n replSetName: myshardrs02\nsharding:\n clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n#myshardrs02_27518\nvim /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myshardrs02_27518/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.pid"\nnet:\n #服务实例绑定所有IP\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #绑定的端口\n port: 27518\nreplication:\n replSetName: myshardrs02\nsharding:\n clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动第二套副本集：一主一副本一仲裁\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\nps -ef |grep mongod #一共6个服务\n\n\n1\n2\n3\n4\n\n * 初始化副本集和创建主节点\n\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27318 #尽量连接主节点\n\n\n1\n\n\nrs.initiate()  #初始化主节点\nrs.status() #状态查看\nrs.add("192.168.130.212:27418")  #添加副节点\nrs.addArb("192.168.130.212:27518") #添加仲裁节点\nrs.conf()\n\n\n1\n2\n3\n4\n5\n\n\n# 配置节点副本集\n\n#-----------configrs\n#建立数据节点data和日志目录\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n#myconfigrs_27019\nvim /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n\n\n1\n2\n\n\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myconfigrs_27019/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.pid"\nnet:\n #服务实例绑定所有IP\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #绑定的端口\n port: 27019\nreplication:\n replSetName: myconfigrs\nsharding:\n clusterRole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n此时分片角色为配置角色\n\n#myconfigrs_27119\nvim /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myconfigrs_27119/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.pid"\nnet:\n #服务实例绑定所有IP\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #绑定的端口\n port: 27119\nreplication:\n replSetName: myconfigrs\nsharding:\n clusterRole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n#myconfigrs_27219\nvim /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n dbPath: "/mongodb/sharded_cluster/myconfigrs_27219/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.pid"\nnet:\n #服务实例绑定所有IP\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.42.130.212\n #绑定的端口\n port: 27219\nreplication:\n replSetName: myconfigrs\nsharding:\n clusterRole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动配置副本集：一主两副本\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\nps -ef |grep mongod #一共9个服务\n\n\n1\n2\n3\n4\n\n * 初始化副本集和创建主节点\n\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27019 #尽量连接主节点\n\n\n1\n\n\nrs.initiate()  #初始化主节点\nrs.status() #状态查看\nrs.add("192.168.130.212:27119")  #添加副节点\nrs.add("192.168.130.212:27119") #添加副节点\nrs.conf()\n\n\n1\n2\n3\n4\n5\n\n\n# 路由节点创建和连接\n\n#-----------mongos01\nmkdir -p /mongodb/sharded_cluster/mymongos_27017/log\n\n\n1\n2\n\n\n#mymongos_27017节点\nvi /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\n\n1\n2\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/mymongos_27017/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: /mongodb/sharded_cluster/mymongos_27017/log/mongod.pid"\nnet:\n #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.130.212\n #bindIp\n #绑定的端口\n port: 27017\nsharding:\n #指定配置节点副本集\n configDB: myconfigrs/192.168.42.130.212:27019,192.168.42.130.212:27119,192.168.42.130.212:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\nconfigDB 地址为 配置副本集名称/配置节点1,配置节点2,配置节点3\n\n#启动\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\n\n1\n2\n\n\n#连接 此时为mongos\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27017\n\n\n1\n2\n\n\n通过路由节点操作，现在只是连接了配置节点，还没有连接分片数据节点，因此写不进去数据，如果写数据会报错。\n\n# 路由节点添加分片\n\n#sh.addShard(副本集名称/主节点,副本节点,仲裁节点)  三种角色节点必须都添加到路由中 \nsh.addShard("myshardrs01/192.168.42.130.212:27018,192.168.42.130.212:27118,192.168.42.130.212:27218")  #第一套\nsh.addShard("myshardrs02/192.168.42.130.212:27318,192.168.42.130.212:27418,192.168.42.130.212:27518")  #第二套\nsh.status() #查询分片状态\n#sh.enableSharding("库名")\nsh.enableSharding("articledb") #开启分片功能\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 集合分片和分片规则\n\nsh.shardCollection(namespace, key, unique)\n\n\n\n 1. 分片规则一：哈希策略 对指定字段的值 进行哈希计算出存放的位置 每个副本集对应则对应的范围 通过哈希计算出存放的副本集中\n    \n    sh.shardCollection("articledb.comment",{"nickname":"hashed"})\n    \n    \n    1\n    \n\n 2. 分片规则二：范围策略 对于 基于范围的分片 ,MongoDB按键的范围把数据分成不同部分.\n    \n    sh.shardCollection("articledb.author",{"age":1})\n    \n    \n    1\n    \n\n注意事项:\n\n * 一个集合只能指定一个片键，否则报错。\n * 一旦对一个集合分片，分片键和分片值就不可改变。 如：不能给集合选择不同的分片键、不能更新 分片键的值。\n * 根据age索引进行分配数据\n\n如果添加分片失败，需要先手动移除分片，检查添加分片的信息的正确性后，再次添加分片。\n\nuse admin\ndb.runCommand( { removeShard: "myshardrs02" } )\n\n\n1\n2\n\n\ndb.printShardingStatus() #显示集群的详细信息\nsh.isBalancerRunning()  #查询交换器是否工作\nsh.getBalancerState() #查看Balancer状态 \n\n\n1\n2\n3\n\n\n# 范围规则配置数据块大小\n\n范围规则默认存放在数据块中 如果数据块（chunk）没有填满，默认的数据块尺寸（chunksize）是64M，填满后才会考虑向其他片的数据块填充数据\n\nuse config\ndb.settings.save({_id:"chunksize", value: 64})  #默认为64m\n\n\n1\n2\n\n\n# 追加路由节点\n\n#-----------mongos02\nmkdir -p /mongodb/sharded_cluster/mymongos_27117/log\n\n\n1\n2\n\n\nvi /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n\n\nsystemLog:\n #MongoDB发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/mymongos_27117/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logAppend: true\nprocessManagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n pidFilePath: /mongodb/sharded_cluster/mymongos_27117/log/mongod.pid"\nnet:\n #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindIpAll: true\n #服务实例绑定的IP\n bindIp: localhost,192.168.0.2\n #bindIp\n #绑定的端口\n port: 27117\nsharding:\nconfigDB:\nmyconfigrs/192.168.42.130.212:27019,192.168.42.130.212:27119,192.168.42.130.212:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#启动\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n2\n\n\n第二个路由无需配置，因为分片配置都保存到了配置服务器中了。\n\n\n# Compass连接分片集群\n\n连接路由节点即可\n\n\n\n\n# SpringDataMongDB 连接分片集群\n\nspring:\n  data:\n    mongodb:\n      # host: 192.168.130.212  # 主机地址\n      # database: articledb  # 数据库\n      # port: 27017 # 默认端口是27017\n        #也可以使用uri连接\n        #uri: mongodb://192.168.40.134:27017/articledb\n      # 副本集的连接字符串\n            uri: mongodb://192.168.40.134:27017,192.168.40.134:27117/articledb\n\n#      uri: mongodb://192.168.42.212:27017,192.168.42.212:27018,192.168.42.212:27019/article\n#      db?connect=replicaSet&slaveOk=true&replicaSet=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n如果有多个节点可以使用逗号隔开 SpringDataMongDB默认有负载均衡策略\n\n\n# 安全认证\n\n默认情况下，MongoDB实例启动运行时是没有启用用户访问权限控制的\n\n\n# 角色\n\n常用的内置角色：\n\n * 数据库用户角色： read、readWrite;\n * 所有数据库用户角色： readAnyDatabase、readWriteAnyDatabase、\n * userAdminAnyDatabase、dbAdminAnyDatabase\n * 数据库管理角色： dbAdmin、dbOwner、userAdmin；\n * 集群管理角色： clusterAdmin、clusterManager、clusterMonitor、hostManager；\n * 备份恢复角色： backup、restore；\n * 超级用户角色： root\n * 内部角色： system\n\n\n\ndb.runCommand({ rolesInfo: 1 })  #查询所有角色权限(仅用户自定义角色)\ndb.runCommand({ rolesInfo: 1, showBuiltinRoles: true })  #查询所有角色权限(包含内置角色)\ndb.runCommand({ rolesInfo: "<rolename>" })  #查询当前数据库中的某角色的权限\ndb.runCommand({ rolesInfo: { role: "<rolename>", db: "<database>" } }   #查询其它数据库中指定的角色权限\n\n\n1\n2\n3\n4\n\n\n\n# 单实例安全认证\n\n * 添加用户和权限\n\nuse admin\ndb.createUser({user:"myroot",pwd:"123456",roles:["root"]})  #创建myroot用户 密码123456 权限为root  如果不指定db名称则默认为当前所在库\ndb.createUser({user:"myadmin",pwd:"123456",roles:[{role:"userAdminAnyDatabase",db:"admin"}]}) #创建一个在指定库中管理用户的用户\ndb.system.users.find()  #查询已经创建的用户情况\ndb.dropUser("myadmin") #删除指定用户\ndb.changeUserPassword("myroot", "123456")  #修改指定用户密码\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 1. 本案例创建了两个用户，分别对应超管和专门用来管理用户的角色，事实上，你只需要一个用户即 可。如果你对安全要求很高，防止超管泄漏，则不要创建超管用户。\n 2. 和其它数据库（MySQL）一样，权限的管理都差不多一样，也是将用户和权限信息保存到数据库对 应的表中。Mongodb存储所有的用户信息在admin 数据库的集合system.users中，保存用户名、密码 和数据库信息。\n 3. 如果不指定数据库，则创建的指定的权限的用户在所有的数据库上有效，如 {role:"userAdminAnyDatabase", db:""}\n\n * 认证测试\n\nuse admin\ndb.auth("myroot","123456")\n\n\n1\n2\n\n\n如果开启了认证后，登录的客户端的用户必须使用admin库的角色，如拥有root角色的myadmin用 户，再通过myadmin用户去创建其他角色的用户\n\n# 服务端开启认证和客户端连接登陆\n\n 1. 参数方式 启动服务端时加锁--auth参数\n    \n    /usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf --auth\n    \n    \n    1\n    \n\n 2. 配置方式\n    \n    vim /mongodb/single/mongod.conf\n    \n    #配置文件追加\n    security:\n     authorization: enabled  #开启授权认证\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n#此时连接可以进入客户端 但无法执行任何操作 只有认证成功后能使用\nuse admin  #此用户能查看什么库就切换到什么库中再认证  否则报错\ndb.auth("myroot","123456")\n\n\n1\n2\n3\n\n\n连接客户端时直接认证 同样查看什么库就切换到什么库中再认证 否则报错\n\nmongo --host 192.168.42.130.212 --port 27017 --authenticationDatabase admin -u myroot -p 123456\n\n\n1\n\n * -u ：用户名\n * -p ：密码\n * -- authenticationDatabase ：指定连接到哪个库。当登录是指定用户名密码时，必须指定对应的 数据库！\n\n# SpringDataMongoDB连接认证\n\nspring:\n #数据源配置\ndata:\n mongodb:\n   # 主机地址\n#   host: 192.168.42.130.212\n   # 数据库\n#   database: articledb\n   # 默认端口是27017\n#   port: 27017\n   #帐号\n#   username: bobo\n   #密码\n#   password: 123456\n   #单机有认证的情况下，也使用字符串连接\n  uri: mongodb://bobo:123456@192.168.130.212/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 副本集环境\n\n只需要在主节点上添加用户，副本集会自动同步\n\nuse admin\ndb.createUser({user:"myroot",pwd:"123456",roles:["root"]})\n\n\n1\n2\n\n\n生成副本集认证的key文件 在centos中\n\nopenssl rand -base64 90 -out ./mongo.keyfile\nchmod 400 ./mongo.keyfile\nll mongo.keyfile\n\n\n1\n2\n3\n\n\n所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须 有读的权限，否则将来会报错\n\n#此时应该通过网络传输此key文件给集群机器\ncp mongo.keyfile /mongodb/replica_sets/myrs_27017\ncp mongo.keyfile /mongodb/replica_sets/myrs_27018\ncp mongo.keyfile /mongodb/replica_sets/myrs_27019\n\n\n1\n2\n3\n4\n\n\n修改各个节点的配置文件\n\nvim /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n\n\nsecurity:\n #KeyFile鉴权文件\n keyFile: /mongodb/replica_sets/myrs_27017/mongo.keyfile\n #开启认证方式运行\n authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\nvim /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n\n\nsecurity:\n #KeyFile鉴权文件\n keyFile: /mongodb/replica_sets/myrs_27018/mongo.keyfile\n #开启认证方式运行\n authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\nvim /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n\n\nsecurity:\n #KeyFile鉴权文件\n keyFile: /mongodb/replica_sets/myrs_27019/mongo.keyfile\n #开启认证方式运行\n authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n重新启动\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n\n\n创建新的用户读和写\n\nmongo\nuse admin\ndb.auth("myroot","123456") #先登陆认证管理员账号\nuse articledb\ndb.createUser({user: "bobo", pwd: "123456", roles: ["readWrite"]})\n\n\n1\n2\n3\n4\n5\n\n\n# SpringDataMongoDB连接副本集\n\nspring:\n #数据源配置\ndata:\n mongodb:\n   #副本集有认证的情况下，字符串连接\n  uri:\nmongodb://bobo:123456@192.168.42.130.212:27017,192.168.42.130.212:27018,192.168.42.130.212:27019/articledb?connect=replicaSet&slaveOk=true&replicaSet=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 分片集群环境\n\n关闭配置服务器副本集的服务，建议依次关闭副本节点、主节点再关闭路由服务器的服务\n\nrs.stepDown()  #告知副本集说本机要下线\nuse admin \ndb.shutdownServer()\n\n\n1\n2\n3\n\n 1. 生成key文件\n\nopenssl rand -base64 90 -out ./mongo.keyfile\nchmod 400 ./mongo.keyfile\n\n\n1\n2\n\n\n 2. 拷贝key文件到各个服务上\n\n 3. 修改配置文件\n    \n    #这里是各个服务上的配置文件\n    vim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n    \n    \n    1\n    2\n    \n    \n    security:\n     #KeyFile鉴权文件 注意要修改路径\n     keyFile: /mongodb/replica_sets/myrs_27019/mongo.keyfile\n     #开启认证方式运行\n     authorization: enabled\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 4. 重新启动各个节点\n    \n    先启动配置节点，再启动分片节点，最后启动路由节点。如果先启动分片节点，会卡住\n    \n    /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n    \n    \n    1\n    \n\n 5. 客户端mongo，通过localhost登录任意一个mongos路由\n    \n    /usr/local/mongodb/bin/mongo --port 27017\n    \n    \n    1\n    \n\n 6. 创建账号\n    \n    use admin\n    db.createUser({user:"myroot",pwd:"123456",roles:["root"]}) #管理员账号\n    db.createUser({user: "bobo", pwd: "123456", roles: [{ role: "readWrite",db: "articledb" }]}) #指定库读写账号\n    \n    \n    1\n    2\n    3\n    \n\n# SpringDataMongoDB连接认证\n\nspring:\n #数据源配置\ndata:\n mongodb:\n   # 分片集群有认证的情况下，字符串连接\n  uri:\nmongodb://bobo:123456@192.168.42.130.212:27017,192.168.42.130.212:27117/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 4.0新特性\n\n\n# 加载外部js文件\n\n加载当前路径下的js文件 启动shell命令时的路径和js所在路径要一致\n\nload("aaa.js") #返回true则成功\n\n\n1\n\n\n\n# 事务性\n\npackage com.itheima.sh.demo_01;\n\nimport com.mongodb.MongoClient;\nimport com.mongodb.ServerAddress;\nimport com.mongodb.client.*;\nimport com.mongodb.client.model.Filters;\nimport org.bson.Document;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static com.mongodb.client.model.Filters.eq;\nimport static com.mongodb.client.model.Updates.inc;\n\npublic class Demo02 {\n    static MongoClient mongoClient;\n    static MongoDatabase mongoDatabase;\n    static MongoCollection<Document> collection;\n\n    public static void main(String[] args) throws Exception{\n        /////////////////////////////////////////////////////////////////////////////////////////////////////////////\n        //副本集\n        final List<ServerAddress> servers=new ArrayList<ServerAddress>();\n        servers.add(new ServerAddress("127.0.0.1", 27000));\n        servers.add(new ServerAddress("127.0.0.1", 27001));\n        servers.add(new ServerAddress("127.0.0.1", 27002));\n\n        mongoClient = new MongoClient(servers);\n\n        // 连接到数据库 itcast表示数据库名\n        mongoDatabase = mongoClient.getDatabase("itcast");\n        //打印数据库最开始的状态\n        printDataState();\n        //转账 带事务的\n        transerTransacFunds("a", "b", 100);\n    }\n\n    //带事务的转账\n    private static void transerTransacFunds(String a, String b, int money) {\n        System.out.println("------------使用事务------------");\n        System.out.println("a向b转账100元，有可能会发生异常，回到之前的状态。两个人的操作在同一个事务中");\n        System.out.println("-------------------------------");\n\n        //获取session\n        ClientSession session = mongoClient.startSession();\n        try {\n            //开启事务\n            session.startTransaction();\n            //a减100\n            minusTransacFromA(session, a, money);\n            //模拟异常\n            int x = 1 / 0;\n            //b加100\n            addTransacToB(session, b, money);\n            //提交事务\n            session.commitTransaction();\n        } catch (Exception e) {\n            System.out.println("带事务，转账失败，回到开启事务之前的状态");\n            //回滚事务\n            session.abortTransaction();\n        } finally {\n            //关闭session\n            session.close();\n            //输出账户状态\n            printDataState();\n        }\n    }\n\n    //带事务b加100\n    private static void addTransacToB(ClientSession session, String b, int money) {\n        System.out.println("b账户增加100");\n        //更新文档   将文档中likes=100的文档修改为likes=200\n        collection.updateMany(session, Filters.eq("name", b), inc("money", money));\n    }\n\n    //带事务a减100\n    private static void minusTransacFromA(ClientSession session, String a, int money) {\n        System.out.println("a账户减少100");\n        //inc 表示累加函数\n        collection.updateMany(session, Filters.eq("name", a), inc("money", -money));\n    }\n\n    private static void printDataState() {\n        //persons表示itcast数据库中的集合名\n        collection = mongoDatabase.getCollection("account");\n        System.out.println("数据库中的起始状态：");\n\n        //检索所有文档\n        /**\n         * 1. 获取迭代器FindIterable<Document>\n         * 2. 获取游标MongoCursor<Document>\n         * 3. 通过游标遍历检索出的文档集合\n         * */\n        //1. 获取迭代器FindIterable<Document>\n        FindIterable<Document> findIterable = collection.find();\n        //2. 获取游标MongoCursor<Document>\n        MongoCursor<Document> mongoCursor = findIterable.iterator();\n        //3. 通过游标遍历检索出的文档集合\n        while (mongoCursor.hasNext()) {\n            System.out.println(mongoCursor.next());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# 聚合数据类型转换\n\nhttps://www.runoob.com/mongodb/mongodb-replication.html\n\nMongoDB4.0增加了一个新的聚合操作符：$convert,用来进行数据类型的转换。这个类型转换操作符简化了数据的抽取、转换和加载的过程。同时将客户端的处理数据的压力转移到了服务器端。从而减轻了客户端处理数据的压力。\n\n#from -- 转账发起人\n#to -- 转账接收人\n#time -- 转账时间\n#money -- 转账金额\nuse itcast;\nvar one = {"from":"a","to":"b","money":100};\nvar two = {"from":"a","to":"b","money":200,"time":ISODate("2018-05-11T13:58:51.122Z")};\nvar thr = {"from":"a","to":"b","money":300,"time":"2018-07-10 14:38:50"};\nvar four = {"from":"a","to":"b","money":100,"time":"2017-04-16 14:38:50"};\nvar five = {"from":"a","to":"b","money":500,"time":1569569092514};\nvar six = {"from":"a","to":"b","money":500,"time":"Last Friday"};\ndb.transfer.insertMany([one,two,thr,four,five,six]);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n我们发现上述结果的时间每条转账记录都不一致。非常杂乱。有的转账记录是标准的例如ISODate,有的时间是字符串，有的是使用整数表示，而还有的根本没有转账记录。还有的时间是无效的。\n\n那么这个时候我们就可以使用MongoDB4.0引入的数据类型转换的操作符来将转账时间统一为一致的数据类型。\n\nconversionStage={\n    //聚合通道\n    $project:{\n        //在数据映射通道中保留原来的数据项，设置为1\n        from:1,\n        to:1,\n        money:1,\n        time:{//转账时间需要做一些类型的转换\n            //转换操作符\n            $convert:{\n                //必须书写的 原来的转账时间\n                input:"$time",\n                //必须书写的 希望把这一项数据转换哪种类型，date就是mongo的标准时间类型ISODate\n                to:"date",\n                //onError这一项是可选的。对于存在的属性，但是属性值是完全没有办法转换为标准的日期格式，可以对其显示。\n                onError:{\n                \t//$concat表示字段拼接操作符\n                    $concat:["can not convert ",{$toString:"$time"}," to date type"]\n                },\n                //缺失转账时间这一项，可以对其提示\n                onNull:"missing time"\n            }\n        }\n    }\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n使用聚合函数aggregate()处理上述数据。执行转换操作\n\n#db.集合名.aggregate([聚合操作内容])\ndb.transfer.aggregate([conversionStage]);\n\n\n1\n2\n',normalizedContent:'# mongodb\n\nmongodb它支持的数据结构非常松散,是一种类json的格式 bson 它即可以存储比较复杂的数据类型 又相当的灵活\n\nmongodb中的记录是一个文档 它是一个键值对数据结构 mongodb文档类似于json对象 即一个文档认为就是一个对象\n\n\n# 体系结构\n\n\n\n\n# 数据模型\n\nmongodb的最小存储单位就是文档(document)对象。文档(document)对象对应于关系型数据库的行。数据在mongodb中以bson（binary-json）文档的格式存储在磁盘上。\n\n\n\n\n# 单机部署\n\nhttps://www.mongodb.com/try/download/community\n\nmongodb的版本命名规范如：x.y.z； y为奇数时表示当前版本为开发版，如：1.5.2、4.1.13； y为偶数时表示当前版本为稳定版，如：1.6.3、4.0.10； z是修正版本号，数字越大越好。\n\n\n# windows启动\n\n 1. 解压到本地\n\n 2. 在mongodb目录下创建 data\n\n 3. 在data文件夹下 分别创建 db 和 logs 文件夹\n\n 4. 进入bin目录下命令行启动\n    \n    mongod --dbpath=d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db --logpath=d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log --logappend\n    \n    \n    1\n    \n\nmongodb默认端口为27017 如果要指定端口可以通过--port来设置\n\nmongod --dbpath=d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db --logpath=d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log --logappend --port 27018\n\n\n1\n\n\n# 使用配置文件启动\n\n 1. 在mongodb目录下conf目录\n\n 2. 进入conf目录创建mongod.conf\n    \n    storage:\n     dbpath: d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db\n    systemlog:\n     destination: file\n     path: d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log\n     logappend: true\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 在bin目录下命令行启动\n    \n    mongod -f ../config/mongod.conf\n    #或者\n    mongod --config ../config/mongod.conf\n    \n    \n    1\n    2\n    3\n    \n\n# 连接数据库\n\n 1. 在bin目录下命令行启动\n    \n    mongo\n    #或者\n    mongo --host=127.0.0.1 --port=27017\n    \n    \n    1\n    2\n    3\n    \n    \n    show dbs; #查询数据库\n    exit; #退出\n    \n    \n    1\n    2\n    \n\n# 安装为服务项\n\n在bin目录下执行\n\nmongod --dbpath "d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\db" --logpath "d:\\compile\\mongodb-win32-x86_64-2012plus-4.2.16\\data\\logs\\mongolog.log" --servicename "mongodb" --servicedisplayname "mongodb" --install\nnet start mongodb\n\n\n1\n2\n\n\n\n# linux启动\n\n解压安装\n\nwget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.2.15.tgz\ntar -zxvf mongodb-linux-x86_64-rhel70-4.2.15.tgz\nmv mongodb-linux-x86_64-rhel70-4.2.15 /usr/local/mongodb \nmkdir -p /mongodb/single/data/db  #数据目录\nmkdir -p /mongodb/single/log  #日志文件\nvim /mongodb/single/mongod.conf\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 配置内容\nsystemlog:\n # mongodb发送所有日志输出的目标指定为文件\n # the path of the log file to which mongod or mongos should send all diagnostic logging information\n destination: file\n # mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/single/log/mongod.log"\n # 当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n # mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n # the directory where the mongod instance stores its data.default value is "/data/db".\n dbpath: "/mongodb/single/data/db"\n journal:\n  # 启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n # 启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\nnet:\n # 服务实例绑定的ip，默认是localhost\n bindip: localhost,192.168.130.212    #ip为当前服务器局域网ip地址 不设置外部无法通过ip访问\n # bindip\n # 绑定的端口，默认是27017\n port: 27017\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n启动\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n#/usr/local/mongodb/bin/mongod --config /mongodb/single/mongod.conf\n#查看防火墙状态\nsystemctl status firewalld\n#临时关闭防火墙\nsystemctl stop firewalld\n#开机禁止启动防火墙\nsystemctl disable firewalld\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果一旦数据损坏 导致表死锁\n\nrm -f /mongodb/single/data/db/*.lock #删除锁文件\n/usr/local/mongdb/bin/mongod --repair\n\n\n1\n2\n\n\n# 配置环境变量\n\nvi /etc/profile\n#配置内容\nexport mongodb_home=/usr/local/mongodb\nexport path=$path:$mongodb_home/bin\n\n\n1\n2\n3\n4\n\n\n# 配置为服务加入启动\n\ncd /lib/systemd/system  \nvi mongodb.service  \n#配置内容\n[unit]\ndescription=mongodb\nafter=network.target remote-fs.target nss-lookup.target\n\n[service]\ntype=forking\nexecstart=/usr/local/mongodb/bin/mongod --config /mongodb/single/mongodb.conf\nexecreload=/bin/kill -s hup $mainpid\nexecstop=/usr/local/mongodb/bin/mongod --shutdown --config /mongodb/single/mongodb.conf\nprivatetmp=true\n\n[install]\nwantedby=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n#echo "/usr/local/mongodb/bin/mongod --dbpath=/data/db --fork --bind_ip=0.0.0.0 --port 27017 --logpath=/data/db/log --logappend --auth" >> /etc/rc.local\n\nchmod 754 mongodb.service\n\n\n1\n2\n3\n\n\n#启动服务\nsystemctl start mongodb.service\n#关闭服务\nsystemctl stop mongodb.service\n#开机启动\nsystemctl enable mongodb.service\n#关闭开启启动\nsystemctl disable mongodb.service\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 标准的关闭方法\n\nmongo\nuse admin\ndb.shutdownserver()\n\n\n1\n2\n3\n\n\n进程id关闭法\n\nps -ef | grep mongo\nkill -2 pid\n\n\n1\n2\n\n\n\n# 数据库\n\n\n# 选择和创建数据库\n\n#use 数据库名称   \nuse articledb  #如果没有此数据库则自动创建 \n#如果新的数据库没有插入数据 showdbs是无法查看到新创建的数据库 因为此时数据库存储在内存当中 并未持久化\n\n\n1\n2\n3\n\n\n\n# 查询当前有权限查看的数据\n\nshow dbs\n#或者\nshow databases\n\n\n1\n2\n3\n\n\n\n# 查看当前正在使用的数据库名称\n\ndb\n\n\n1\n\n\n\n# 数据库命名规范\n\n数据库名可以是满足以下条件的任意utf-8字符串。\n\n * 不能是空字符串（ "")。\n * 不得含有 \' \'（空格)、.、$、/、\\和\\0 (空字符)。\n * 应全部小写。\n * 最多 64字节。\n\n有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。\n\n\n# 默认三个数据库作用\n\n * admin ： 从权限的角度来看，这是"root"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。\n * local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合\n * config : 当mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。\n\n\n# 数据库删除\n\ndb.dropdatabase()  #删除当前使用的库\n\n\n1\n\n\n\n# 集合\n\n集合,类似于关系型数据中的表\n\n\n# 显式创建\n\n#db.createcollection(集合名)\ndb.createcollection("my")  #成功返回1\n\n\n1\n2\n\n\n\n# 隐式创建\n\n当向一个集合中插入文档时 如果集合不存在 则自动创建集合\n\n通常我们使用隐式创建文档即可\n\n\n# 查询集合\n\nshow collections #查询当前库的所有集合\n\n\n1\n\n\n\n# 集合删除\n\n#db.集合名.drop()\ndb.my.drop()  #成功则返回true\n\n\n1\n2\n\n\n\n# 文档crud\n\n文档的数据结构和json基本一样\n\n索引存储在集合中数据都是 bson 格式\n\n\n# 插入\n\n通过insert() 或者 save() 方法向集合中插入文档\n\n# 单问插入\n\n#单文档插入\ndb.collection.insert(\n <document or array of documents>,\n {\n  writeconcern: <document>,\n  ordered: <boolean>\n }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n#db.集合名.inset({""})\n#单文档插入\ndb.comment.insert({"articleid":"100000","content":"今天天气真好，阳光明媚","userid":"1001","nickname":"rose","createdatetime":new date(),"likenum":numberint(10),"state":null})  #返回1则插入成功\n\n\n1\n2\n3\n\n\n# 批量插入\n\n#批量插入\ndb.collection.insertmany(\n [ <document 1> , <document 2>, ... ],\n {\n   writeconcern: <document>,\n   ordered: <boolean>\n }\n)\n\n#db.集合名.insertmany([{""},{""}])\ndb.comment.insertmany([{"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new date("2019-08-05t22:08:15.522z"),"likenum":numberint(1000),"state":"1"},{"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new date("2019-08-05t23:58:51.485z"),"likenum":numberint(888),"state":"1"},{"_id":"3","articleid":"100001","content":"我一直喝凉开水，冬天夏天都喝。","userid":"1004","nickname":"杰克船长","createdatetime":new date("2019-08-06t01:05:06.321z"),"likenum":numberint(666),"state":"1"},{"_id":"4","articleid":"100001","content":"专家说不能空腹吃饭，影响健康。","userid":"1003","nickname":"凯撒","createdatetime":new date("2019-08-06t08:18:35.288z"),"likenum":numberint(2000),"state":"1"},{"_id":"5","articleid":"100001","content":"研究表明，刚烧开的水千万不能喝，因为烫嘴。","userid":"1003","nickname":"凯撒","createdatetime":new date("2019-08-06t11:01:02.521z"),"likenum":numberint(3000),"state":"1"}]);  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# try catch\n\n我们通过批量插入时由于数据较多可能会出现失败 我们可以通过try catch进行异常处理\n\ntry {\n  db.comment.insertmany([\n    {\n      _id: \'1\',\n      articleid: \'100001\',\n      content: \'我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。\',\n      userid: \'1002\',\n      nickname: \'相忘于江湖\',\n      createdatetime: new date(\'2019-08-05t22:08:15.522z\'),\n      likenum: numberint(1000),\n      state: \'1\',\n    },\n    {\n      _id: \'2\',\n      articleid: \'100001\',\n      content: \'我夏天空腹喝凉开水，冬天喝温开水\',\n      userid: \'1005\',\n      nickname: \'伊人憔悴\',\n      createdatetime: new date(\'2019-08-05t23:58:51.485z\'),\n      likenum: numberint(888),\n      state: \'1\',\n    },\n    {\n      _id: \'3\',\n      articleid: \'100001\',\n      content: \'我一直喝凉开水，冬天夏天都喝。\',\n      userid: \'1004\',\n      nickname: \'杰克船长\',\n      createdatetime: new date(\'2019-08-06t01:05:06.321z\'),\n      likenum: numberint(666),\n      state: \'1\',\n    },\n    {\n      _id: \'4\',\n      articleid: \'100001\',\n      content: \'专家说不能空腹吃饭，影响健康。\',\n      userid: \'1003\',\n      nickname: \'凯撒\',\n      createdatetime: new date(\'2019-08-06t08:18:35.288z\'),\n      likenum: numberint(2000),\n      state: \'1\',\n    },\n    {\n      _id: \'5\',\n      articleid: \'100001\',\n      content: \'研究表明，刚烧开的水千万不能喝，因为烫嘴。\',\n      userid: \'1003\',\n      nickname: \'凯撒\',\n      createdatetime: new date(\'2019-08-06t11:01:02.521z\'),\n      likenum: numberint(3000),\n      state: \'1\',\n    },\n  ])\n} catch (e) {\n  print(e)\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n\n\n\n# 查询\n\n查询当前数据库中的文档\n\n# 查询所有文档\n\n#db.collection.find(<query>, [projection])\ndb.comment.find()\n\n\n1\n2\n\n\n\n\n# 条件查询\n\n查询文档中包含此文档内容的所有文档\n\n#db.collection.find({键:"内容"})\ndb.comment.find({articleid:"100001"})\n\n\n1\n2\n\n\n# 只返回第一个结果\n\n#db.collection.findone({键:"内容"})\ndb.comment.findone({articleid:"100001"})\n\n\n1\n2\n\n\n# 投影查询\n\n查询出来的文档 只显示需要的键值对 相对应sql中的查询结果只显示指定列\n\nfind传参时给出指定的键字段 1为查询此键 0为过滤此键 默认自动包含查询_id键\n\n#db.collection.find({键:"内容"},{键:1,键:0})  \ndb.comment.find({articleid:"100001"},{content:1})  #查询包含100001文档中的 content键内容\ndb.comment.find({articleid:"100001"},{content:1,_id:0}) #默认查询包含_id 可以通过0过滤\n\n\n\n1\n2\n3\n4\n\n\n\n# 更新\n\n//语法\ndb.collection.update(query, update, options)\n//或\ndb.collection.update(\n\t<query>,\n\t<update>,\n\t{\n\t\tupsert: <boolean>,\n\t\tmulti: <boolean>,\n\t\twriteconcern: <document>,\n\t\tcollation: <document>,\n\t\tarrayfilters: [ <filterdocument1>, ... ],\n\t\thint: \t<document|string> \t\t\t\t// available starting in mongodb 4.2\n\t}\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n# 覆盖修改\n\n修改指定文档 并覆盖更新整个文档 即修改为当前语句中的字段 原先字段不保留\n\ndb.comment.update({_id:"1"},{likenum:numberint(1001)})  #可以理解为删除原文档并插入此文档\n\n\n1\n\n\n# 局部修改\n\n我们可以通过修改器$set来修改指定键的内容 并保留原文档其他内容\n\n#默认只修改符合条件的第一条数据\ndb.comment.update({_id:"2"},{$set:{likenum:numberint(778)}})\n#修改所有符合条件的数据需要加上参数{multi:true}\ndb.comment.update({userid:"1003"},{$set:{nickname:"张三"}},{multi:true})\n\n\n1\n2\n3\n4\n\n\n# 列值增长修改\n\n通过$inc运算符实现 可以对该键的值进行递增 递减\n\ndb.comment.update({_id:"2"},{$inc:{likenum:numberint(1)}})\n\n\n1\n\n\n\n# 删除\n\n# 条件删除\n\n删除符合条件的文档\n\n#db.集合.remove({条件})\ndb.comment.remove({_id:"1"})  #返回值为删除多少条数据\n\n\n1\n2\n\n\n# 删除全部\n\ndb.comment.remove({})\n\n\n1\n\n\n\n# 文档的分页查询\n\n\n# 统计查询\n\n使用count()方法\n\ndb.collection.count(query, options)\n\n\n1\n\n\n\n\n# 统计当前集合所有文档数\n\ndb.comment.count()  #返回值是文档数\n\n\n1\n\n\n# 按条件查询\n\ndb.comment.count({userid:"1003"}) \n\n\n1\n\n\n\n# 分页列表查询\n\n通过limit()方法 读取指定数量的数据 使用skip()方法跳过指定数量的数据\n\n#db.collection.find().limit(number).skip(number)\ndb.comment.find().limit(3)  #查询前3条的数据\ndb.comment.find().skip(3) #跳过前3条的数据\n\ndb.comment.find().skip(0).limit(2)  #第一页\ndb.comment.find().skip(2).limit(2)  #第二页\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 排序查询\n\nsort()方法对数据进行排序 使用1和-1来指定升序和降序 sort({键:1}) 默认以id进行升序\n\ndb.comment.find().sort({userid:-1,likenum:1}) #对userid的内容进行降序 再对likenum的内容进行升序\n\n\n1\n\n\n\n# 分页和排序查询的顺序\n\nskip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()，和命令编写顺序无关。\n\n\n# 高级查询\n\n\n# 正则条件查询\n\n#db.集合.find({键:/正则表达式/})\ndb.comment.find({content:/^专家/})\n\n\n1\n2\n\n\n\n# 比较查询\n\ndb.集合名称.find({ "field" : { $gt: value }}) # 大于: field > value\ndb.集合名称.find({ "field" : { $lt: value }}) # 小于: field < value\ndb.集合名称.find({ "field" : { $gte: value }}) # 大于等于: field >= value\ndb.集合名称.find({ "field" : { $lte: value }}) # 小于等于: field <= value\ndb.集合名称.find({ "field" : { $ne: value }}) # 不等于: field != value\n\n#例子\ndb.comment.find({likenum:{$gt:numberint(700)}})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 包含查询\n\n使用$in 查询包含指定内容的文档\n\ndb.comment.find({userid:{$in:["1003","1004"]})\n\n\n1\n\n\n\n# 不包含查询\n\n使用$nin 查询不包含指定内容的文档\n\ndb.comment.find({userid:{$nin:["1003","1004"]})\n\n\n1\n\n\n\n# 条件连接查询\n\n前面我们通过$set查询单条件的文档 如果为多条件则需要使用 $and进行查询\n\ndb.comment.find({$and:[{likenum:{$gte:numberint(700)}},{likenum:{$lt:numberint(2000)}}]})   # 查询likenum 大于等于700 并且小于2000的文档\n\n\n\n1\n2\n\n\n# 或者连接\n\n使用$or进行查询\n\ndb.comment.find({$or:[{likenum:{$gte:numberint(700)}},{likenum:{$lt:numberint(2000)}}]})   # 查询likenum 大于等于700 并且小于2000的文档\n\n\n1\n\n\n\n# 索引\n\nmongodb没有下索引下 是进行全集合扫描 如果数据量较大时查询效率非常低\n\nmongodb索引使用b树数据结构(b-tree,mysql是b+tree)\n\n\n# 单字段索引\n\nmongodb支持在文档的单个字段上创建用户定义的升序/降序索引 称为单字段索引\n\n对于单个字段索引和排序操作，索引键的排序顺序（即升序或降序）并不重要，因为mongodb可以在任何方向上遍历索引。\n\n\n\n\n# 复合索引\n\n复合索引中列出的字段顺序具有重要意义。例如，如果复合索引由 { userid: 1, score: -1 } 组成，则索引首先按userid正序排序，然后 在每个userid的值内，再在按score倒序排序。\n\n\n\n\n# 其他索引\n\n * 地理空间索引（geospatial index） 为了支持对地理空间坐标数据的有效查询，mongodb提供了两种特殊的索引：返回结果时使用平面几何的二维索引和返回结果时使用球面 几何的二维球面索引。\n\n * 文本索引（text indexes） mongodb提供了一种文本索引类型，支持在集合中搜索字符串内容。这些文本索引不存储特定于语言的停止词（例如“the”、“a”、“or”）， 而将集合中的词作为词干，只存储根词。\n\n * 哈希索引（hashed indexes） 为了支持基于散列的分片，mongodb提供了散列索引类型，它对字段值的散列进行索引。这些索引在其范围内的值分布更加随机，但只支 持相等匹配，不支持基于范围的查询。\n\n\n# 索引管理\n\n# 查看索引\n\n返回一个集合中的所有索引的数组 _id主键自带索引为升序\n\ndb.comment.getindexes()\n\n\n1\n\n\n# 创建索引\n\n\n\n#db.collection.createindex(keys, options)\ndb.comment.createindex({userid:1})  # 给userid创建索引 为升序 索引名默认为键名后加上_(1或者-1)\ndb.comment.createindex({userid:1,nickname:-1}) #复合索引\n\n\n1\n2\n3\n\n\n# 删除索引\n\n如果要删除文本索引只能通过索引名删除\n\n_id索引无法删除\n\n#db.collection.dropindex(索引名)\ndb.comment.dropindex()  #删除所有索引\ndb.comment.dropindex("userid_1_nickname_-1") #删除指定索引\ndb.comment.dropindex({userid:1})  #根据创建索引条件删除索引\n\n\n1\n2\n3\n4\n\n\n\n# 索引的使用\n\n分析查询性能（analyze query performance）通常使用执行计划（解释计划、explain plan）来查看查询的情况，如查询耗费的时间、是否基于索引查询等。 那么，通常，我们想知道，建立的索引是否有效，效果如何，都需要通过执行计划查看。\n\n#db.collection.find(query,options).explain(options)\ndb.comment.find({uderid:"1003"}).explain()\n\n\n1\n2\n\n\n关键点看： "stage" : "collscan", 表示全集合扫描\n\n"stage" : "ixscan" ,基于索引的扫描\n\n\n# 涵盖查询\n\ncovered queries 当查询条件和查询的投影仅包含索引字段时，mongodb直接从索引返回结果，而不扫描任何文档或将文档带入内存。 这些覆盖的查询可以非常有效。 其实就是查询符合查询条件 索引字段的值\n\n\n\n\n# java连接mongodb\n\n 1. mongodb-driver 是mongo官方推出的java连接mongodb的驱动包，相当于jdbc驱动。我们通过一个入门的案例来了解mongodb-driver 的基本使用。 官方驱动说明和下载： http://mongodb.github.io/mongo-java-driver/ 官方驱动示例文档：http://mongodb.github.io/mongo-java-driver/3.8/driver/getting-started/quick-start/\n 2. springdatamongodb springdata家族成员之一，用于操作mongodb的持久层框架，封装了底层的mongodb-driver。 官网主页： https://projects.spring.io/spring-data-mongodb/\n\n\n# 评论案例\n\n\n# 模块搭建\n\n 1. 继承spring-boot项目 导入springdatamongodb坐标\n\n <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.1.6.release</version>\n        <relativepath/> \x3c!-- lookup parent from repository --\x3e\n    </parent>\n\n    <dependencies>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-data-mongodb</artifactid>\n        </dependency>\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n 2. 创建application.yml\n    \n    spring:\n      data:\n        mongodb:\n          host: 192.168.130.212  # 主机地址\n          database: articledb  # 数据库\n          port: 27017 # 默认端口是27017\n          #也可以使用uri连接\n          #uri: mongodb://192.168.40.134:27017/articledb\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    \n\n\n# 实体类\n\npackage com.itcatst.article.po;\n\nimport org.springframework.data.annotation.id;\nimport org.springframework.data.mongodb.core.index.compoundindex;\nimport org.springframework.data.mongodb.core.index.indexed;\nimport org.springframework.data.mongodb.core.mapping.document;\nimport org.springframework.data.mongodb.core.mapping.field;\n\nimport java.io.serializable;\nimport java.time.localdatetime;\nimport java.util.date;\n\n/**\n * 文章评论实体类\n */\n//把一个java类声明为mongodb的文档，可以通过collection参数指定这个类对应的文档。\n//@document(collection="mongodb 对应 collection 名")\n// 若未加 @document ，该 bean save 到 mongo 的 comment collection\n// 若添加 @document ，则 save 到 comment collection\n@document(collection = "comment")//可以省略，如果省略，则默认使用类名小写映射集合 会映射到与类名一致的集合当中\n//复合索引\n// @compoundindex( def = "{\'userid\': 1, \'nickname\': -1}")\npublic class comment implements serializable {\n    //主键标识，该属性的值会自动对应mongodb的主键字段"_id"，如果该属性名就叫“id”,则该注解可以省略，否则必须写\n    @id\n    private string id;//主键\n    //该属性对应mongodb的字段的名字，如果一致，则无需该注解\n    @field("content")\n    private string content;//吐槽内容\n    private date publishtime;//发布日期\n    //添加了一个单字段的索引\n    @indexed\n    private string userid;//发布人id\n    private string nickname;//昵称\n    private localdatetime createdatetime;//评论的日期时间\n    private integer likenum;//点赞数\n    private integer replynum;//回复数\n    private string state;//状态\n    private string parentid;//上级id\n    private string articleid;\n\n    //getter and setter.....\n    public string getid() {\n        return id;\n    }\n\n    public void setid(string id) {\n        this.id = id;\n    }\n\n    public string getcontent() {\n        return content;\n    }\n\n    public void setcontent(string content) {\n        this.content = content;\n    }\n\n    public date getpublishtime() {\n        return publishtime;\n    }\n\n    public void setpublishtime(date publishtime) {\n        this.publishtime = publishtime;\n    }\n\n    public string getuserid() {\n        return userid;\n    }\n\n    public void setuserid(string userid) {\n        this.userid = userid;\n    }\n\n    public string getnickname() {\n        return nickname;\n    }\n\n    public void setnickname(string nickname) {\n        this.nickname = nickname;\n    }\n\n    public localdatetime getcreatedatetime() {\n        return createdatetime;\n    }\n\n    public void setcreatedatetime(localdatetime createdatetime) {\n        this.createdatetime = createdatetime;\n    }\n\n    public integer getlikenum() {\n        return likenum;\n    }\n\n    public void setlikenum(integer likenum) {\n        this.likenum = likenum;\n    }\n\n    public integer getreplynum() {\n        return replynum;\n    }\n\n    public void setreplynum(integer replynum) {\n        this.replynum = replynum;\n    }\n\n    public string getstate() {\n        return state;\n    }\n\n    public void setstate(string state) {\n        this.state = state;\n    }\n\n    public string getparentid() {\n        return parentid;\n    }\n\n    public void setparentid(string parentid) {\n        this.parentid = parentid;\n    }\n\n    public string getarticleid() {\n        return articleid;\n    }\n\n    public void setarticleid(string articleid) {\n        this.articleid = articleid;\n    }\n\n    @override\n    public string tostring() {\n        return "comment{" +\n                "id=\'" + id + \'\\\'\' +\n                ", content=\'" + content + \'\\\'\' +\n                ", publishtime=" + publishtime +\n                ", userid=\'" + userid + \'\\\'\' +\n                ", nickname=\'" + nickname + \'\\\'\' +\n                ", createdatetime=" + createdatetime +\n                ", likenum=" + likenum +\n                ", replynum=" + replynum +\n                ", state=\'" + state + \'\\\'\' +\n                ", parentid=\'" + parentid + \'\\\'\' +\n                ", articleid=\'" + articleid + \'\\\'\' +\n                \'}\';\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n\n * @document(collection = "comment") 声明为mongodb文档 并且映射到comment集合中 如果当前实体类名与集合一致则collection可以忽略(并且只能是小写名称的实体类)\n * @compoundindex( def = "{\'userid\': 1, \'nickname\': -1}") 复合索引 def属性与设置复活索引条件一致 为键和排序方式\n * @id 声明此属性为主键 如果属性名为id则该注解可以省略不加\n * @field("content") 当成员属性名称与集合中的键字段不一致时 可以使用该注解映射为集合中指定的键字段 此时映射为content键字段\n * @indexed 添加为单字段索引\n\n\n# 增删改查方法\n\n 1. 创建dao层接口 并继承mongorepository 泛型为 实体类 和 id的类型\n    \n    package com.itcatst.article.dao;\n    \n    import com.itcatst.article.po.comment;\n    import org.springframework.data.mongodb.repository.mongorepository;\n    \n    public interface commentrepository extends mongorepository<comment,string> {\n    }\n    \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n 2. 创建service层 注入dao层 并调用动态代理里的方法\n    \n    package com.itcatst.article.service;\n    \n    import com.itcatst.article.dao.commentrepository;\n    import com.itcatst.article.po.comment;\n    import org.springframework.beans.factory.annotation.autowired;\n    import org.springframework.stereotype.service;\n    \n    import java.util.list;\n    \n    @service\n    public class commentservice {\n    \n        @autowired\n        private commentrepository commentrepository;\n    \n        /**\n         * 保存一个评论\n         *\n         * @param comment\n         */\n        public void savecomment(comment comment) {\n            //如果需要自定义主键，可以在这里指定主键；如果不指定主键，mongodb会自动生成主键\n            //设置一些默认初始值。。。\n            //调用dao\n            commentrepository.save(comment);\n        }\n    \n        /**\n         * 更新评论\n         *\n         * @param comment\n         */\n        public void updatecomment(comment comment) {\n            //调用dao\n            commentrepository.save(comment);\n        }\n    \n        /**\n         * 根据id删除评论\n         *\n         * @param id\n         */\n        public void deletecommentbyid(string id) {\n            //调用dao\n            commentrepository.deletebyid(id);\n        }\n    \n        /**\n         * 查询所有评论\n         *\n         * @return\n         */\n        public list<comment> findcommentlist() {\n            //调用dao\n            return commentrepository.findall();\n        }\n    \n        /**\n         * 根据id查询评论\n         *\n         * @param id\n         * @return\n         */\n        public comment findcommentbyid(string id) {\n            //调用dao\n            return commentrepository.findbyid(id).get();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    68\n    \n\n\n# 创建测试方法\n\n 1. 新建junit测试方法 类路径要与主程序一致\n    \n    package com.itcatst.article.service;\n    \n    import com.itcatst.article.po.comment;\n    import com.itcatst.article.service.commentservice;\n    import org.junit.test;\n    import org.junit.runner.runwith;\n    import org.springframework.beans.factory.annotation.autowired;\n    import org.springframework.boot.test.context.springboottest;\n    import org.springframework.test.context.junit4.springrunner;\n    \n    import java.time.localdatetime;\n    import java.util.list;\n    \n    @runwith(springrunner.class)\n    @springboottest\n    public class commentservicetest {\n    \n        @autowired\n        private commentservice commentservice;\n    \n        @test\n        public void testfindcommentlist() {\n            list<comment> commentlist = commentservice.findcommentlist();\n            system.out.println(commentlist);\n        }\n    \n        @test\n        public void testfndcommentbyid() {\n            comment commentbyid = commentservice.findcommentbyid("1");\n            system.out.println(commentbyid);\n        }\n    \n        /**\n         * 保存一个评论\n         */\n        @test\n        public void testsavecomment() {\n            comment comment = new comment();\n            //此处没有指定id 但mongodb默认自动生成 我们推荐自动生成 不推荐自己书写id\n            comment.setarticleid("100000");\n            comment.setcontent("测试添加的数据");\n            comment.setcreatedatetime(localdatetime.now());\n            comment.setuserid("1003");\n            comment.setnickname("凯撒大帝");\n            comment.setstate("1");\n            comment.setlikenum(0);\n            comment.setreplynum(0);\n    \n            commentservice.savecomment(comment);\n        }\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    \n\n\n# 分页查询测试\n\n 1. 在dao层接口创建新的方法 必须要指定对应的属性名为方法名否则会报错\n    \n    格式: page<实体类> findby根据哪个属性来查询(传递属性类型 属性名,pageable pageable);\n    \n    ////根据父id，查询子评论的分页列表\n    page<comment> findbyparentid(string parentid, pageable pageable);\n    \n    \n    1\n    2\n    \n\n 2. 在service层调用dao层的方法\n    \n    /**\n         * 分页查询\n         *\n         * @param parentid 根据哪个属性来查询\n         * @param page     页码\n         * @param size     每页个数\n         * @return\n         */\n        public page<comment> findcommentlistbyparentid(string parentid, int page, int size) {\n            return commentrepository.findbyparentid(parentid, pagerequest.of(page-1, size));\n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n 3. 测试用例\n    \n    @test\n    public void testfindcommentlistbyparentid(){\n        page<comment> page = commentservice.findcommentlistbyparentid("3", 1, 2);\n        system.out.println(page.gettotalelements());//返回总条数\n        list<comment> content = page.getcontent();  //返回结果的集合\n        system.out.println(content);\n    \n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n\n# 评论点赞\n\n最简单的方法就是根据id查询当前对应的文档 并将点赞数+1 但执行效率不高 因为只需要修改点赞数 没有必要把所有的字段都查询出来\n\n我们可以使用 mongotemplate 类实现对某列进行操作\n\n 1. 在service层注入 mongotemplate 并编修改方法\n    \n       @autowired\n        private mongotemplate mongotemplate;\n    \n        /**\n         * 点赞数+1\n         *\n         * @param id\n         */\n        public void updatecommentlikenum(string id) {\n            //查询条件\n            //query query = query.query(criteria.where("_id").is(id)).addcriteria(criteria.where("nickname").is("凯撒大帝"));  //使用criteria.when(键字段).is(条件)  判断条件 addcriteria可以无限扩展条件\n            query query = query.query(criteria.where("_id").is(id));\n            //更新条件\n            update update = new update();\n            update.inc("likenum",1); //将likenum数+1  如单传键字段则自动加1\n    //        update.set()\n    \n            mongotemplate.updatefirst(query, update, comment.class);\n        }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n\n# 副本集\n\nmongodb中的副本集（replica set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高 可用性，是所有生产部署的基础。\n\n主从复制和副本集区别 主从集群和副本集最大的区别就是副本集没有固定的“主节点”；整个集群会选出一个“主节点”，当其挂 掉后，又在剩下的从节点中选中其他节点为“主节点”，副本集总有一个活跃点(主、primary)和一个或多 个备份节点(从、secondary)。\n\n副本集有两种类型三种角色:\n\n两种类型：\n\n * 主节点（ primary）类型：数据操作的主要连接点，可读写。\n * 次要（辅助、从）节点（ secondaries）类型：数据冗余备份节点，可以读或选举。\n\n三种角色：\n\n * 主要成员（primary）：主要接收所有写操作。就是主节点。\n * 副本成员（replicate）：从主节点通过复制操作以维护相同的数据集，即备份数据，不可写操作，但可 以读操作（但需要配置）。是默认的一种从节点类型。\n * 仲裁者（ arbiter）：不保留任何数据的副本，只具有投票选举作用。当然也可以将仲裁服务器维护为副 本集的一部分，即副本成员同时也可以是仲裁者。也是一种从节点类型。\n\n\n# 副本集创建\n\n# 主节点\n\n#主节点\nmkdir -p /mongodb/replica_sets/myrs_27017/log \\ & #日志文件\nmkdir -p /mongodb/replica_sets/myrs_27017/data/db #数据文件\n\nvim /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n2\n3\n4\n5\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/replica_sets/myrs_27017/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/replica_sets/myrs_27017/data/db"\n journal:\n  #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/replica_sets/myrs_27017/log/mongod.pid"\nnet:\n #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #bindip\n #绑定的端口\n port: 27017\nreplication:\n #副本集的名称  同一个副本集中集群必须一致\n replsetname: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动主节点\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n\n\n# 从节点\n\n#副本节点\nmkdir -p /mongodb/replica_sets/myrs_27018/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27018/data/db\nvim /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n2\n3\n4\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/replica_sets/myrs_27018/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/replica_sets/myrs_27018/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/replica_sets/myrs_27018/log/mongod.pid"\nnet:\n #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #bindip\n #绑定的端口\n port: 27018\nreplication:\n #副本集的名称\n replsetname: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n\n\n# 仲裁节点\n\n#仲裁节点\nmkdir -p /mongodb/replica_sets/myrs_27019/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27019/data/db\nvim /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n4\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/replica_sets/myrs_27019/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/replica_sets/myrs_27019/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/replica_sets/myrs_27019/log/mongod.pid"\nnet:\n #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #bindip\n #绑定的端口\n port: 27019\nreplication:\n #副本集的名称\n replsetname: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n\n\n\n# 初始化配置副本集和主节点\n\n#使用mongo客户端任意节点都可以 但尽量连上主节点\n/usr/local/mongodb/bin/mongo --host=192.168.130.212 --port=27017\n\n#初始化命令\nrs.initiate() #使用默认的配置来初始化副本集  “ok”的值为1，说明创建成功  并且此节点变为主节点\n\nrs.conf() #查看默认节点配置\n\nrs.status() # 查询副本集状态\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 添加副本从节点\n\n#rs.add(host:post)\nrs.add(192.168.130.212:27018)\n\nrs.status() #查看副本集状态\n\n\n1\n2\n3\n4\n\n\n\n# 添加仲裁节点\n\n#rs.addarb(host:post)\nrs.addarb(192.168.130.212:27019)\n\nrs.status() #查看副本集状态\n\n\n1\n2\n3\n4\n\n\n\n# 副本集的数据读写操作\n\n 1. 只有主节点写入数据和读取数据\n\n 2. 默认情况从节点无法读取数据 也无法写入数据 可以进行设置增加读的权限\n    \n    #连接从节点的客户端\n    rs.slaveok() #该命令是 db.getmongo().setslaveok() 的简化命令\n    #或者\n    rs.slaveok(true) \n    #此时实现了读写分离 让主插入数据 让从来读取数据\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 3. 仲裁者节点，不存放任何业务数据的，可以登录查看 只存放副本集配置等数据。\n\n\n# 主节点的选举原则\n\nmongodb在副本集中，会自动进行主节点的选举，主节点选举的触发条件：\n\n 1. 主节点故障\n 2. 主节点网络不可达（默认心跳信息为10秒）\n 3. 人工干预（rs.stepdown(600)）\n\n选举规则是根据票数来决定谁获胜：\n\n * 票数最高，且获得了 “大多数”成员的投票支持的节点获胜。 “大多数”的定义为：假设复制集内投票成员数量为n，则大多数为 n/2 + 1。例如：3个投票成员， 则大多数的值是2。当复制集内存活成员数量不足大多数时，整个复制集将无法选举出primary， 复制集将无法提供写服务，处于只读状态。\n * 若票数相同，且都获得了 “大多数”成员的投票支持的，数据新的节点获胜。 数据的新旧是通过操作日志oplog来对比的。\n\n在获得票数的时候，优先级（priority）参数影响重大。 可以通过设置优先级（priority）来设置额外票数。优先级即权重，取值为0-1000，相当于可额外增加 0-1000的票数，优先级的值越大，就越可能获得多数成员的投票（votes）数。指定较高的值可使成员 更有资格成为主要成员，更低的值可使成员更不符合条件。 默认情况下，优先级的值是1\n\n\n# 提升优先级\n\n#先导出配置到变量中\ncfg=rs.conf()\n#修改指定优先级  id号默认从0开始\ncfg.members[1].priority=2\n#重新加载配置\nrs.reconfig(cfg)\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# compass连接副本集\n\n#将host修改为当前主节点的ip\nvar config = rs.config();\n config.members[0].host="192.168.130.212:27017";rs.reconfig(config) \n\n\n1\n2\n3\n\n\n\n\n选择primary 为主节点 secondary为副本集\n\n\n# springdatamongodb连接副本集\n\nmongodb://host1,host2,host3/articledb?connect=replicaset&slaveok=true&replicaset=副本集名字\n\n * slaveok=true ：开启副本节点读的功能，可实现读写分离。\n * connect=replicaset ：自动到副本集中选择读写的主机。如果slaveok是打开的，则实现了读写分 离\n\n主机必须是副本集中所有的主机，包括主节点、副本节点、仲裁节点。\n\nspring:\n  data:\n    mongodb:\n      # host: 192.168.130.212  # 主机地址\n      # database: articledb  # 数据库\n      # port: 27017 # 默认端口是27017\n      #也可以使用uri连接\n      #uri: mongodb://192.168.40.134:27017/articledb\n        # 副本集的连接字符串\n      uri: mongodb://192.168.42.212:27017,192.168.42.212:27018,192.168.42.212:27019/article\n      db?connect=replicaset&slaveok=true&replicaset=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 分片集群\n\n分片（sharding）是一种跨多台机器分布数据的方法， mongodb使用分片来支持具有非常大的数据集 和高吞吐量操作的部署。\n\nmongodb分片群集包含以下组件：\n\n * 分片（存储）：每个分片包含分片数据的子集。 每个分片都可以部署为副本集。\n * mongos （路由）：mongos充当查询路由器，在客户端应用程序和分片集群之间提供接口。\n * config servers （“调度”的配置）：配置服务器存储群集的元数据和配置设置。 从mongodb 3.4开 始，必须将配置服务器部署为副本集（csrs）。\n\n\n\n\n# 搭建分片集群架构\n\n\n\n# 第一个副本集\n\n#-----------myshardrs01\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/data/db\n\n#myshardrs01_27018\nvim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myshardrs01_27018/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.pid"\nnet:\n #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #bindip\n #绑定的端口\n port: 27018\nreplication:\n #副本集的名称\n replsetname: myshardrs01\nsharding:\n #分片角色\n clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nclusterrole 设置分片角色:\n\n * shardsvr 分片角色\n * configsvr 配置角色\n\n#myshardrs01_27118\nvim /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myshardrs01_27118/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.pid"\nnet:\n #服务实例绑定所有ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #绑定的端口\n port: 27118\nreplication:\n replsetname: myshardrs01\nsharding:\n clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n#myshardrs01_27218\nvim /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myshardrs01_27218/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.pid"\nnet:\n #服务实例绑定的ip\n bindip: localhost,192.168.42.130.212\n #绑定的端口\n port: 27218\nreplication:\n replsetname: myshardrs01\nsharding:\n clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n启动第一套副本集：一主一副本一仲裁\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n2\n3\n\n * 初始化副本集和创建主节点\n\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27018 #尽量连接主节点\n\n\n1\n\n\nrs.initiate()  #初始化主节点\nrs.status() #状态查看\nrs.add("192.168.130.212:27118")  #添加副节点\nrs.addarb("192.168.130.212:27218") #添加仲裁节点\nrs.conf()\n\n\n1\n2\n3\n4\n5\n\n\n# 第二套副本集\n\n#-----------myshardrs02\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n#myshardrs02_27318\nvim /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myshardrs02_27318/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.pid"\nnet:\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #绑定的端口\n port: 27318\nreplication:\n replsetname: myshardrs02\nsharding:\n clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n#myshardrs02_27418\nvim /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myshardrs02_27418/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.pid"\nnet:\n #服务实例绑定所有ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #绑定的端口\n port: 27418\nreplication:\n replsetname: myshardrs02\nsharding:\n clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n#myshardrs02_27518\nvim /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myshardrs02_27518/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.pid"\nnet:\n #服务实例绑定所有ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #绑定的端口\n port: 27518\nreplication:\n replsetname: myshardrs02\nsharding:\n clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动第二套副本集：一主一副本一仲裁\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\nps -ef |grep mongod #一共6个服务\n\n\n1\n2\n3\n4\n\n * 初始化副本集和创建主节点\n\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27318 #尽量连接主节点\n\n\n1\n\n\nrs.initiate()  #初始化主节点\nrs.status() #状态查看\nrs.add("192.168.130.212:27418")  #添加副节点\nrs.addarb("192.168.130.212:27518") #添加仲裁节点\nrs.conf()\n\n\n1\n2\n3\n4\n5\n\n\n# 配置节点副本集\n\n#-----------configrs\n#建立数据节点data和日志目录\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n#myconfigrs_27019\nvim /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n\n\n1\n2\n\n\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myconfigrs_27019/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.pid"\nnet:\n #服务实例绑定所有ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #绑定的端口\n port: 27019\nreplication:\n replsetname: myconfigrs\nsharding:\n clusterrole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n此时分片角色为配置角色\n\n#myconfigrs_27119\nvim /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myconfigrs_27119/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.pid"\nnet:\n #服务实例绑定所有ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #绑定的端口\n port: 27119\nreplication:\n replsetname: myconfigrs\nsharding:\n clusterrole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n#myconfigrs_27219\nvim /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nstorage:\n #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n dbpath: "/mongodb/sharded_cluster/myconfigrs_27219/data/db"\n journal:\n   #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n  enabled: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.pid"\nnet:\n #服务实例绑定所有ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.42.130.212\n #绑定的端口\n port: 27219\nreplication:\n replsetname: myconfigrs\nsharding:\n clusterrole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动配置副本集：一主两副本\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\nps -ef |grep mongod #一共9个服务\n\n\n1\n2\n3\n4\n\n * 初始化副本集和创建主节点\n\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27019 #尽量连接主节点\n\n\n1\n\n\nrs.initiate()  #初始化主节点\nrs.status() #状态查看\nrs.add("192.168.130.212:27119")  #添加副节点\nrs.add("192.168.130.212:27119") #添加副节点\nrs.conf()\n\n\n1\n2\n3\n4\n5\n\n\n# 路由节点创建和连接\n\n#-----------mongos01\nmkdir -p /mongodb/sharded_cluster/mymongos_27017/log\n\n\n1\n2\n\n\n#mymongos_27017节点\nvi /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\n\n1\n2\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/mymongos_27017/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: /mongodb/sharded_cluster/mymongos_27017/log/mongod.pid"\nnet:\n #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.130.212\n #bindip\n #绑定的端口\n port: 27017\nsharding:\n #指定配置节点副本集\n configdb: myconfigrs/192.168.42.130.212:27019,192.168.42.130.212:27119,192.168.42.130.212:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\nconfigdb 地址为 配置副本集名称/配置节点1,配置节点2,配置节点3\n\n#启动\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\n\n1\n2\n\n\n#连接 此时为mongos\n/usr/local/mongodb/bin/mongo --host 192.168.130.212 --port 27017\n\n\n1\n2\n\n\n通过路由节点操作，现在只是连接了配置节点，还没有连接分片数据节点，因此写不进去数据，如果写数据会报错。\n\n# 路由节点添加分片\n\n#sh.addshard(副本集名称/主节点,副本节点,仲裁节点)  三种角色节点必须都添加到路由中 \nsh.addshard("myshardrs01/192.168.42.130.212:27018,192.168.42.130.212:27118,192.168.42.130.212:27218")  #第一套\nsh.addshard("myshardrs02/192.168.42.130.212:27318,192.168.42.130.212:27418,192.168.42.130.212:27518")  #第二套\nsh.status() #查询分片状态\n#sh.enablesharding("库名")\nsh.enablesharding("articledb") #开启分片功能\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 集合分片和分片规则\n\nsh.shardcollection(namespace, key, unique)\n\n\n\n 1. 分片规则一：哈希策略 对指定字段的值 进行哈希计算出存放的位置 每个副本集对应则对应的范围 通过哈希计算出存放的副本集中\n    \n    sh.shardcollection("articledb.comment",{"nickname":"hashed"})\n    \n    \n    1\n    \n\n 2. 分片规则二：范围策略 对于 基于范围的分片 ,mongodb按键的范围把数据分成不同部分.\n    \n    sh.shardcollection("articledb.author",{"age":1})\n    \n    \n    1\n    \n\n注意事项:\n\n * 一个集合只能指定一个片键，否则报错。\n * 一旦对一个集合分片，分片键和分片值就不可改变。 如：不能给集合选择不同的分片键、不能更新 分片键的值。\n * 根据age索引进行分配数据\n\n如果添加分片失败，需要先手动移除分片，检查添加分片的信息的正确性后，再次添加分片。\n\nuse admin\ndb.runcommand( { removeshard: "myshardrs02" } )\n\n\n1\n2\n\n\ndb.printshardingstatus() #显示集群的详细信息\nsh.isbalancerrunning()  #查询交换器是否工作\nsh.getbalancerstate() #查看balancer状态 \n\n\n1\n2\n3\n\n\n# 范围规则配置数据块大小\n\n范围规则默认存放在数据块中 如果数据块（chunk）没有填满，默认的数据块尺寸（chunksize）是64m，填满后才会考虑向其他片的数据块填充数据\n\nuse config\ndb.settings.save({_id:"chunksize", value: 64})  #默认为64m\n\n\n1\n2\n\n\n# 追加路由节点\n\n#-----------mongos02\nmkdir -p /mongodb/sharded_cluster/mymongos_27117/log\n\n\n1\n2\n\n\nvi /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n\n\nsystemlog:\n #mongodb发送所有日志输出的目标指定为文件\n destination: file\n #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n path: "/mongodb/sharded_cluster/mymongos_27117/log/mongod.log"\n #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n logappend: true\nprocessmanagement:\n #启用在后台运行mongos或mongod进程的守护进程模式。\n fork: true\n #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n pidfilepath: /mongodb/sharded_cluster/mymongos_27117/log/mongod.pid"\nnet:\n #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n #bindipall: true\n #服务实例绑定的ip\n bindip: localhost,192.168.0.2\n #bindip\n #绑定的端口\n port: 27117\nsharding:\nconfigdb:\nmyconfigrs/192.168.42.130.212:27019,192.168.42.130.212:27119,192.168.42.130.212:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#启动\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n2\n\n\n第二个路由无需配置，因为分片配置都保存到了配置服务器中了。\n\n\n# compass连接分片集群\n\n连接路由节点即可\n\n\n\n\n# springdatamongdb 连接分片集群\n\nspring:\n  data:\n    mongodb:\n      # host: 192.168.130.212  # 主机地址\n      # database: articledb  # 数据库\n      # port: 27017 # 默认端口是27017\n        #也可以使用uri连接\n        #uri: mongodb://192.168.40.134:27017/articledb\n      # 副本集的连接字符串\n            uri: mongodb://192.168.40.134:27017,192.168.40.134:27117/articledb\n\n#      uri: mongodb://192.168.42.212:27017,192.168.42.212:27018,192.168.42.212:27019/article\n#      db?connect=replicaset&slaveok=true&replicaset=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n如果有多个节点可以使用逗号隔开 springdatamongdb默认有负载均衡策略\n\n\n# 安全认证\n\n默认情况下，mongodb实例启动运行时是没有启用用户访问权限控制的\n\n\n# 角色\n\n常用的内置角色：\n\n * 数据库用户角色： read、readwrite;\n * 所有数据库用户角色： readanydatabase、readwriteanydatabase、\n * useradminanydatabase、dbadminanydatabase\n * 数据库管理角色： dbadmin、dbowner、useradmin；\n * 集群管理角色： clusteradmin、clustermanager、clustermonitor、hostmanager；\n * 备份恢复角色： backup、restore；\n * 超级用户角色： root\n * 内部角色： system\n\n\n\ndb.runcommand({ rolesinfo: 1 })  #查询所有角色权限(仅用户自定义角色)\ndb.runcommand({ rolesinfo: 1, showbuiltinroles: true })  #查询所有角色权限(包含内置角色)\ndb.runcommand({ rolesinfo: "<rolename>" })  #查询当前数据库中的某角色的权限\ndb.runcommand({ rolesinfo: { role: "<rolename>", db: "<database>" } }   #查询其它数据库中指定的角色权限\n\n\n1\n2\n3\n4\n\n\n\n# 单实例安全认证\n\n * 添加用户和权限\n\nuse admin\ndb.createuser({user:"myroot",pwd:"123456",roles:["root"]})  #创建myroot用户 密码123456 权限为root  如果不指定db名称则默认为当前所在库\ndb.createuser({user:"myadmin",pwd:"123456",roles:[{role:"useradminanydatabase",db:"admin"}]}) #创建一个在指定库中管理用户的用户\ndb.system.users.find()  #查询已经创建的用户情况\ndb.dropuser("myadmin") #删除指定用户\ndb.changeuserpassword("myroot", "123456")  #修改指定用户密码\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 1. 本案例创建了两个用户，分别对应超管和专门用来管理用户的角色，事实上，你只需要一个用户即 可。如果你对安全要求很高，防止超管泄漏，则不要创建超管用户。\n 2. 和其它数据库（mysql）一样，权限的管理都差不多一样，也是将用户和权限信息保存到数据库对 应的表中。mongodb存储所有的用户信息在admin 数据库的集合system.users中，保存用户名、密码 和数据库信息。\n 3. 如果不指定数据库，则创建的指定的权限的用户在所有的数据库上有效，如 {role:"useradminanydatabase", db:""}\n\n * 认证测试\n\nuse admin\ndb.auth("myroot","123456")\n\n\n1\n2\n\n\n如果开启了认证后，登录的客户端的用户必须使用admin库的角色，如拥有root角色的myadmin用 户，再通过myadmin用户去创建其他角色的用户\n\n# 服务端开启认证和客户端连接登陆\n\n 1. 参数方式 启动服务端时加锁--auth参数\n    \n    /usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf --auth\n    \n    \n    1\n    \n\n 2. 配置方式\n    \n    vim /mongodb/single/mongod.conf\n    \n    #配置文件追加\n    security:\n     authorization: enabled  #开启授权认证\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n#此时连接可以进入客户端 但无法执行任何操作 只有认证成功后能使用\nuse admin  #此用户能查看什么库就切换到什么库中再认证  否则报错\ndb.auth("myroot","123456")\n\n\n1\n2\n3\n\n\n连接客户端时直接认证 同样查看什么库就切换到什么库中再认证 否则报错\n\nmongo --host 192.168.42.130.212 --port 27017 --authenticationdatabase admin -u myroot -p 123456\n\n\n1\n\n * -u ：用户名\n * -p ：密码\n * -- authenticationdatabase ：指定连接到哪个库。当登录是指定用户名密码时，必须指定对应的 数据库！\n\n# springdatamongodb连接认证\n\nspring:\n #数据源配置\ndata:\n mongodb:\n   # 主机地址\n#   host: 192.168.42.130.212\n   # 数据库\n#   database: articledb\n   # 默认端口是27017\n#   port: 27017\n   #帐号\n#   username: bobo\n   #密码\n#   password: 123456\n   #单机有认证的情况下，也使用字符串连接\n  uri: mongodb://bobo:123456@192.168.130.212/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 副本集环境\n\n只需要在主节点上添加用户，副本集会自动同步\n\nuse admin\ndb.createuser({user:"myroot",pwd:"123456",roles:["root"]})\n\n\n1\n2\n\n\n生成副本集认证的key文件 在centos中\n\nopenssl rand -base64 90 -out ./mongo.keyfile\nchmod 400 ./mongo.keyfile\nll mongo.keyfile\n\n\n1\n2\n3\n\n\n所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须 有读的权限，否则将来会报错\n\n#此时应该通过网络传输此key文件给集群机器\ncp mongo.keyfile /mongodb/replica_sets/myrs_27017\ncp mongo.keyfile /mongodb/replica_sets/myrs_27018\ncp mongo.keyfile /mongodb/replica_sets/myrs_27019\n\n\n1\n2\n3\n4\n\n\n修改各个节点的配置文件\n\nvim /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n\n\nsecurity:\n #keyfile鉴权文件\n keyfile: /mongodb/replica_sets/myrs_27017/mongo.keyfile\n #开启认证方式运行\n authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\nvim /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n\n\nsecurity:\n #keyfile鉴权文件\n keyfile: /mongodb/replica_sets/myrs_27018/mongo.keyfile\n #开启认证方式运行\n authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\nvim /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n\n\nsecurity:\n #keyfile鉴权文件\n keyfile: /mongodb/replica_sets/myrs_27019/mongo.keyfile\n #开启认证方式运行\n authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n重新启动\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n\n\n创建新的用户读和写\n\nmongo\nuse admin\ndb.auth("myroot","123456") #先登陆认证管理员账号\nuse articledb\ndb.createuser({user: "bobo", pwd: "123456", roles: ["readwrite"]})\n\n\n1\n2\n3\n4\n5\n\n\n# springdatamongodb连接副本集\n\nspring:\n #数据源配置\ndata:\n mongodb:\n   #副本集有认证的情况下，字符串连接\n  uri:\nmongodb://bobo:123456@192.168.42.130.212:27017,192.168.42.130.212:27018,192.168.42.130.212:27019/articledb?connect=replicaset&slaveok=true&replicaset=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 分片集群环境\n\n关闭配置服务器副本集的服务，建议依次关闭副本节点、主节点再关闭路由服务器的服务\n\nrs.stepdown()  #告知副本集说本机要下线\nuse admin \ndb.shutdownserver()\n\n\n1\n2\n3\n\n 1. 生成key文件\n\nopenssl rand -base64 90 -out ./mongo.keyfile\nchmod 400 ./mongo.keyfile\n\n\n1\n2\n\n\n 2. 拷贝key文件到各个服务上\n\n 3. 修改配置文件\n    \n    #这里是各个服务上的配置文件\n    vim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n    \n    \n    1\n    2\n    \n    \n    security:\n     #keyfile鉴权文件 注意要修改路径\n     keyfile: /mongodb/replica_sets/myrs_27019/mongo.keyfile\n     #开启认证方式运行\n     authorization: enabled\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 4. 重新启动各个节点\n    \n    先启动配置节点，再启动分片节点，最后启动路由节点。如果先启动分片节点，会卡住\n    \n    /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n    \n    \n    1\n    \n\n 5. 客户端mongo，通过localhost登录任意一个mongos路由\n    \n    /usr/local/mongodb/bin/mongo --port 27017\n    \n    \n    1\n    \n\n 6. 创建账号\n    \n    use admin\n    db.createuser({user:"myroot",pwd:"123456",roles:["root"]}) #管理员账号\n    db.createuser({user: "bobo", pwd: "123456", roles: [{ role: "readwrite",db: "articledb" }]}) #指定库读写账号\n    \n    \n    1\n    2\n    3\n    \n\n# springdatamongodb连接认证\n\nspring:\n #数据源配置\ndata:\n mongodb:\n   # 分片集群有认证的情况下，字符串连接\n  uri:\nmongodb://bobo:123456@192.168.42.130.212:27017,192.168.42.130.212:27117/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 4.0新特性\n\n\n# 加载外部js文件\n\n加载当前路径下的js文件 启动shell命令时的路径和js所在路径要一致\n\nload("aaa.js") #返回true则成功\n\n\n1\n\n\n\n# 事务性\n\npackage com.itheima.sh.demo_01;\n\nimport com.mongodb.mongoclient;\nimport com.mongodb.serveraddress;\nimport com.mongodb.client.*;\nimport com.mongodb.client.model.filters;\nimport org.bson.document;\n\nimport java.util.arraylist;\nimport java.util.list;\n\nimport static com.mongodb.client.model.filters.eq;\nimport static com.mongodb.client.model.updates.inc;\n\npublic class demo02 {\n    static mongoclient mongoclient;\n    static mongodatabase mongodatabase;\n    static mongocollection<document> collection;\n\n    public static void main(string[] args) throws exception{\n        /////////////////////////////////////////////////////////////////////////////////////////////////////////////\n        //副本集\n        final list<serveraddress> servers=new arraylist<serveraddress>();\n        servers.add(new serveraddress("127.0.0.1", 27000));\n        servers.add(new serveraddress("127.0.0.1", 27001));\n        servers.add(new serveraddress("127.0.0.1", 27002));\n\n        mongoclient = new mongoclient(servers);\n\n        // 连接到数据库 itcast表示数据库名\n        mongodatabase = mongoclient.getdatabase("itcast");\n        //打印数据库最开始的状态\n        printdatastate();\n        //转账 带事务的\n        transertransacfunds("a", "b", 100);\n    }\n\n    //带事务的转账\n    private static void transertransacfunds(string a, string b, int money) {\n        system.out.println("------------使用事务------------");\n        system.out.println("a向b转账100元，有可能会发生异常，回到之前的状态。两个人的操作在同一个事务中");\n        system.out.println("-------------------------------");\n\n        //获取session\n        clientsession session = mongoclient.startsession();\n        try {\n            //开启事务\n            session.starttransaction();\n            //a减100\n            minustransacfroma(session, a, money);\n            //模拟异常\n            int x = 1 / 0;\n            //b加100\n            addtransactob(session, b, money);\n            //提交事务\n            session.committransaction();\n        } catch (exception e) {\n            system.out.println("带事务，转账失败，回到开启事务之前的状态");\n            //回滚事务\n            session.aborttransaction();\n        } finally {\n            //关闭session\n            session.close();\n            //输出账户状态\n            printdatastate();\n        }\n    }\n\n    //带事务b加100\n    private static void addtransactob(clientsession session, string b, int money) {\n        system.out.println("b账户增加100");\n        //更新文档   将文档中likes=100的文档修改为likes=200\n        collection.updatemany(session, filters.eq("name", b), inc("money", money));\n    }\n\n    //带事务a减100\n    private static void minustransacfroma(clientsession session, string a, int money) {\n        system.out.println("a账户减少100");\n        //inc 表示累加函数\n        collection.updatemany(session, filters.eq("name", a), inc("money", -money));\n    }\n\n    private static void printdatastate() {\n        //persons表示itcast数据库中的集合名\n        collection = mongodatabase.getcollection("account");\n        system.out.println("数据库中的起始状态：");\n\n        //检索所有文档\n        /**\n         * 1. 获取迭代器finditerable<document>\n         * 2. 获取游标mongocursor<document>\n         * 3. 通过游标遍历检索出的文档集合\n         * */\n        //1. 获取迭代器finditerable<document>\n        finditerable<document> finditerable = collection.find();\n        //2. 获取游标mongocursor<document>\n        mongocursor<document> mongocursor = finditerable.iterator();\n        //3. 通过游标遍历检索出的文档集合\n        while (mongocursor.hasnext()) {\n            system.out.println(mongocursor.next());\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# 聚合数据类型转换\n\nhttps://www.runoob.com/mongodb/mongodb-replication.html\n\nmongodb4.0增加了一个新的聚合操作符：$convert,用来进行数据类型的转换。这个类型转换操作符简化了数据的抽取、转换和加载的过程。同时将客户端的处理数据的压力转移到了服务器端。从而减轻了客户端处理数据的压力。\n\n#from -- 转账发起人\n#to -- 转账接收人\n#time -- 转账时间\n#money -- 转账金额\nuse itcast;\nvar one = {"from":"a","to":"b","money":100};\nvar two = {"from":"a","to":"b","money":200,"time":isodate("2018-05-11t13:58:51.122z")};\nvar thr = {"from":"a","to":"b","money":300,"time":"2018-07-10 14:38:50"};\nvar four = {"from":"a","to":"b","money":100,"time":"2017-04-16 14:38:50"};\nvar five = {"from":"a","to":"b","money":500,"time":1569569092514};\nvar six = {"from":"a","to":"b","money":500,"time":"last friday"};\ndb.transfer.insertmany([one,two,thr,four,five,six]);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n我们发现上述结果的时间每条转账记录都不一致。非常杂乱。有的转账记录是标准的例如isodate,有的时间是字符串，有的是使用整数表示，而还有的根本没有转账记录。还有的时间是无效的。\n\n那么这个时候我们就可以使用mongodb4.0引入的数据类型转换的操作符来将转账时间统一为一致的数据类型。\n\nconversionstage={\n    //聚合通道\n    $project:{\n        //在数据映射通道中保留原来的数据项，设置为1\n        from:1,\n        to:1,\n        money:1,\n        time:{//转账时间需要做一些类型的转换\n            //转换操作符\n            $convert:{\n                //必须书写的 原来的转账时间\n                input:"$time",\n                //必须书写的 希望把这一项数据转换哪种类型，date就是mongo的标准时间类型isodate\n                to:"date",\n                //onerror这一项是可选的。对于存在的属性，但是属性值是完全没有办法转换为标准的日期格式，可以对其显示。\n                onerror:{\n                \t//$concat表示字段拼接操作符\n                    $concat:["can not convert ",{$tostring:"$time"}," to date type"]\n                },\n                //缺失转账时间这一项，可以对其提示\n                onnull:"missing time"\n            }\n        }\n    }\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n使用聚合函数aggregate()处理上述数据。执行转换操作\n\n#db.集合名.aggregate([聚合操作内容])\ndb.transfer.aggregate([conversionstage]);\n\n\n1\n2\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"集合类型",frontmatter:{title:"集合类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/11c241/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/08.%E9%9B%86%E5%90%88%E7%B1%BB%E5%9E%8B.html",relativePath:"后端/05.Python/08.集合类型.md",key:"v-75630836",path:"/pages/11c241/",headersStr:null,content:"# 集合类型\n\n> X = {7，y，x，1}\n> Y = set() #空集合\n> \n> \n> 1\n> 2",normalizedContent:"# 集合类型\n\n> x = {7，y，x，1}\n> y = set() #空集合\n> \n> \n> 1\n> 2",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"列表类型",frontmatter:{title:"列表类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/d7e80b/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/06.%E5%88%97%E8%A1%A8%E7%B1%BB%E5%9E%8B.html",relativePath:"后端/05.Python/06.列表类型.md",key:"v-8b08f0b2",path:"/pages/d7e80b/",headersStr:null,content:"# 列表类型\n\n> names = ['1','2','3']\n> \n> \n> 1",normalizedContent:"# 列表类型\n\n> names = ['1','2','3']\n> \n> \n> 1",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"查看数据类型",frontmatter:{title:"查看数据类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/1f8618/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/10.%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html",relativePath:"后端/05.Python/10.查看数据类型.md",key:"v-aa36ff9c",path:"/pages/1f8618/",headersStr:null,content:"# 查看数据类型\n\n> 使用type()可以查看一个变量对应的数据类型\n> \n> print(type(123))\n> \n> \n> 1\n> \n> \n> type(a).print 可以补全\n> \n> python变量是没有数据类型的，这里的类型是指的是变量对应数值的数据类型",normalizedContent:"# 查看数据类型\n\n> 使用type()可以查看一个变量对应的数据类型\n> \n> print(type(123))\n> \n> \n> 1\n> \n> \n> type(a).print 可以补全\n> \n> python变量是没有数据类型的，这里的类型是指的是变量对应数值的数据类型",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"字典类型",frontmatter:{title:"字典类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/bbada7/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/07.%E5%AD%97%E5%85%B8%E7%B1%BB%E5%9E%8B.html",relativePath:"后端/05.Python/07.字典类型.md",key:"v-51da2d27",path:"/pages/bbada7/",headersStr:null,content:"# 字典类型\n\n> person ={'name':'4','age':17,'addr':'某某'}\n> a = {} #空字典\n> #字典有tag标签，集合只有数据虽然都是用中文括号括起\n> \n> \n> 1\n> 2\n> 3",normalizedContent:"# 字典类型\n\n> person ={'name':'4','age':17,'addr':'某某'}\n> a = {} #空字典\n> #字典有tag标签，集合只有数据虽然都是用中文括号括起\n> \n> \n> 1\n> 2\n> 3",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"元组类型",frontmatter:{title:"元组类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/684f07/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/09.%E5%85%83%E7%BB%84%E7%B1%BB%E5%9E%8B.html",relativePath:"后端/05.Python/09.元组类型.md",key:"v-55473949",path:"/pages/684f07/",headersStr:null,content:"# 元组类型\n\n> nums = (1,2,3,4,5,6)\n> \n> \n> 1",normalizedContent:"# 元组类型\n\n> nums = (1,2,3,4,5,6)\n> \n> \n> 1",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"输出",frontmatter:{title:"输出",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/1d58f1/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/12.%E8%BE%93%E5%87%BA.html",relativePath:"后端/05.Python/12.输出.md",key:"v-7b8967f0",path:"/pages/1d58f1/",headersStr:null,content:"# 输出\n\n> # print(self, *args, sep=' ', end='\\n', file=None): # known special case of print\n> # sep 参数用来表示输出时，每个值之间的连接符，默认为空格为连接符\n> # end 当执行完print后，接下来输入的字符。默认\\n 换行键\n> # file 输出到指定文件中\n> # 打印一个数字，默认使用十进制输出\n> print(123,'对对对',sep=\"+\"，end='\\n')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6",normalizedContent:"# 输出\n\n> # print(self, *args, sep=' ', end='\\n', file=none): # known special case of print\n> # sep 参数用来表示输出时，每个值之间的连接符，默认为空格为连接符\n> # end 当执行完print后，接下来输入的字符。默认\\n 换行键\n> # file 输出到指定文件中\n> # 打印一个数字，默认使用十进制输出\n> print(123,'对对对',sep=\"+\"，end='\\n')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"标识符",frontmatter:{title:"标识符",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/d9c568/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/11.%E6%A0%87%E8%AF%86%E7%AC%A6.html",relativePath:"后端/05.Python/11.标识符.md",key:"v-f2fe618e",path:"/pages/d9c568/",headers:[{level:2,title:"标识符的命名规则和规范",slug:"标识符的命名规则和规范",normalizedTitle:"标识符的命名规则和规范",charIndex:33}],headersStr:"标识符的命名规则和规范",content:"# 标识符\n\n> 变量、模块名、函数名、类名\n> \n> \n> # 标识符的命名规则和规范\n> \n> 规则\n> \n>  1. 由数字、字母和_组成，不能以数字开头\n> \n>  2. 区分大小写\n> \n>  3. 不能使用关键字\n>     \n>     AND     AS     ASSERT   BREAK      CLASS    CONTINUE\n>     def     del    elif     else       except   finally\n>     for     from   False    global     if       import\n>     in      is     lambda   nonlocal   not      None\n>     or      pass   raise    return     try      True\n>     while   with   yield                        \n> \n> 规范\n> \n>  1. 顾名思义\n> \n>  2. 遵守一定的命名规范\n>     \n>     1. 小驼峰命名法:第一个单词的首字母小写，然后每个字母的首字母都大写 userNameAndPassword\n>     2. 大驼峰命名法:每个个单词的首字母都大写 PersonModel\n>     3. 使用下划线连接: user_name_and_password\n>     4. 在python里的变量、函数和模块名使用下划线连接；python里的类名用大驼峰命名法",normalizedContent:"# 标识符\n\n> 变量、模块名、函数名、类名\n> \n> \n> # 标识符的命名规则和规范\n> \n> 规则\n> \n>  1. 由数字、字母和_组成，不能以数字开头\n> \n>  2. 区分大小写\n> \n>  3. 不能使用关键字\n>     \n>     and     as     assert   break      class    continue\n>     def     del    elif     else       except   finally\n>     for     from   false    global     if       import\n>     in      is     lambda   nonlocal   not      none\n>     or      pass   raise    return     try      true\n>     while   with   yield                        \n> \n> 规范\n> \n>  1. 顾名思义\n> \n>  2. 遵守一定的命名规范\n>     \n>     1. 小驼峰命名法:第一个单词的首字母小写，然后每个字母的首字母都大写 usernameandpassword\n>     2. 大驼峰命名法:每个个单词的首字母都大写 personmodel\n>     3. 使用下划线连接: user_name_and_password\n>     4. 在python里的变量、函数和模块名使用下划线连接；python里的类名用大驼峰命名法",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"输入",frontmatter:{title:"输入",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/c9ca02/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/13.%E8%BE%93%E5%85%A5.html",relativePath:"后端/05.Python/13.输入.md",key:"v-47dc7e48",path:"/pages/c9ca02/",headersStr:null,content:'# 输入\n\n> #input() 括号里写入提示的信息\n> #定于一个变量可以保存input中被用户输入的内容\n> #input保存的结果是字符串\n> #int()可以把字符串转变成int整数型\n> \n> a = input("hello")\n> print(a)\n> print(type(a))\n> print(int(a)+1)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9',normalizedContent:'# 输入\n\n> #input() 括号里写入提示的信息\n> #定于一个变量可以保存input中被用户输入的内容\n> #input保存的结果是字符串\n> #int()可以把字符串转变成int整数型\n> \n> a = input("hello")\n> print(a)\n> print(type(a))\n> print(int(a)+1)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"bin、oct、hex内置函数",frontmatter:{title:"bin、oct、hex内置函数",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/1e0ec6/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/16.bin%E3%80%81oct%E3%80%81hex%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0.html",relativePath:"后端/05.Python/16.bin、oct、hex内置函数.md",key:"v-38a8ef5e",path:"/pages/1e0ec6/",headersStr:null,content:"# bin、oct、hex内置函数\n\n> 使用bin()函数可以将数字/十进制转变成二进制\n> \n> 使用oct()函数可以将数字/十进制转变成八进制\n> \n> 使用hex()函数可以将数字/十进制转变成十六进制",normalizedContent:"# bin、oct、hex内置函数\n\n> 使用bin()函数可以将数字/十进制转变成二进制\n> \n> 使用oct()函数可以将数字/十进制转变成八进制\n> \n> 使用hex()函数可以将数字/十进制转变成十六进制",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"进制转换",frontmatter:{title:"进制转换",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/e533d0/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/15.%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2.html",relativePath:"后端/05.Python/15.进制转换.md",key:"v-4601280a",path:"/pages/e533d0/",headers:[{level:2,title:"二进制转八进制",slug:"二进制转八进制",normalizedTitle:"二进制转八进制",charIndex:11},{level:2,title:"二进制转十六进制",slug:"二进制转十六进制",normalizedTitle:"二进制转十六进制",charIndex:190},{level:2,title:"二进制转十进制",slug:"二进制转十进制",normalizedTitle:"二进制转十进制",charIndex:377},{level:2,title:"十进制转二进制",slug:"十进制转二进制",normalizedTitle:"十进制转二进制",charIndex:482}],headersStr:"二进制转八进制 二进制转十六进制 二进制转十进制 十进制转二进制",content:"# 进制转换\n\n\n# 二进制转八进制\n\n> 3个为一组如开头不够3位则用0填充\n> \n> 101 010\n> \n> 各组3位数倒着各自乘以2的0到X次方 1-2-4-8-16-32-64\n> \n> 1*2的零次方+0*2的一次方+1*2的二次方=1+0+4=5\n> \n> 0*2的零次方+1*2的一次方+0*2的二次方=0+2+0=2\n> \n> 则42的八进制为 5 2\n\n\n# 二进制转十六进制\n\n> 4位为一组开头不够4位则用0填充\n> \n> 各组4位数倒着各自乘以2的0到X次方\n> \n> 0010 1010\n> \n> 0*2的零次方+1*2的一次方+0*2的二次方+0*2的三次方=0+2+0+0=2\n> \n> 0*2的零次方+1*2的一次方+0*2的二次方+1*2的三次方=0+2+0+8=10=A\n> \n> 则42的十六进制为 2A\n\n\n# 二进制转十进制\n\n> 不用分组，直接倒序各自乘以2^0到2^X\n> \n> 101010\n> \n> 0*2^0+1*2^1+0*2^2+1*2^3+0*2^4+1*2^5=0+2+0+8+0+32=42\n\n\n# 十进制转二进制\n\n> 将十进制的数除以要转换的进制数，二进制除以2，不断的除以2直到结果为0，除以结果的余数为转换符，最后倒序排列完成转换\n> \n> 42/2=21 余0\n> \n> 21/2=10 余1\n> \n> 10/2=5 余0\n> \n> 5/2=2 余1\n> \n> 2/2=1 余0\n> \n> 1/2=0 余1\n> \n> 再倒序排列则为 101010",normalizedContent:"# 进制转换\n\n\n# 二进制转八进制\n\n> 3个为一组如开头不够3位则用0填充\n> \n> 101 010\n> \n> 各组3位数倒着各自乘以2的0到x次方 1-2-4-8-16-32-64\n> \n> 1*2的零次方+0*2的一次方+1*2的二次方=1+0+4=5\n> \n> 0*2的零次方+1*2的一次方+0*2的二次方=0+2+0=2\n> \n> 则42的八进制为 5 2\n\n\n# 二进制转十六进制\n\n> 4位为一组开头不够4位则用0填充\n> \n> 各组4位数倒着各自乘以2的0到x次方\n> \n> 0010 1010\n> \n> 0*2的零次方+1*2的一次方+0*2的二次方+0*2的三次方=0+2+0+0=2\n> \n> 0*2的零次方+1*2的一次方+0*2的二次方+1*2的三次方=0+2+0+8=10=a\n> \n> 则42的十六进制为 2a\n\n\n# 二进制转十进制\n\n> 不用分组，直接倒序各自乘以2^0到2^x\n> \n> 101010\n> \n> 0*2^0+1*2^1+0*2^2+1*2^3+0*2^4+1*2^5=0+2+0+8+0+32=42\n\n\n# 十进制转二进制\n\n> 将十进制的数除以要转换的进制数，二进制除以2，不断的除以2直到结果为0，除以结果的余数为转换符，最后倒序排列完成转换\n> \n> 42/2=21 余0\n> \n> 21/2=10 余1\n> \n> 10/2=5 余0\n> \n> 5/2=2 余1\n> \n> 2/2=1 余0\n> \n> 1/2=0 余1\n> \n> 再倒序排列则为 101010",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"数据类型的转换",frontmatter:{title:"数据类型的转换",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/ebc10c/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/17.%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%BD%AC%E6%8D%A2.html",relativePath:"后端/05.Python/17.数据类型的转换.md",key:"v-457b85c0",path:"/pages/ebc10c/",headers:[{level:2,title:"大小写转换",slug:"大小写转换",normalizedTitle:"大小写转换",charIndex:199},{level:2,title:"布尔值的转换",slug:"布尔值的转换",normalizedTitle:"布尔值的转换",charIndex:518},{level:2,title:"隐形类型转换",slug:"隐形类型转换",normalizedTitle:"隐形类型转换",charIndex:773}],headersStr:"大小写转换 布尔值的转换 隐形类型转换",content:'# 数据类型的转换\n\n> 不同类型的数据不能进行运算否则会报错\n> \n> int()使用可以将其他类型的数据转换为int整数，如果字符串不是一段合法的数字转换会报错，默认base(进制)是十进制，如要转换其他进制的数据需更改后面的参数改成16/8/2即可。\n> \n> str()使用可以将其他类型的数据转为str字符串\n> \n> float()使用可以将其他类型数据转为float浮点小数\n\n\n# 大小写转换\n\n> (str.upper())          # 把所有字符中的小写字母转换成大写字母\n> print(str.lower())          # 把所有字符中的大写字母转换成小写字母\n> print(str.capitalize())     # 把第一个字母转化为大写字母，其余小写\n> print(str.title())          # 把每个单词的第一个字母转化为大写，其余小写 \n> a.lower()\n> a.upper()\n> a.title()首字母大写\n> a.swapcase()首字大写以及大小写互换\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n\n\n# 布尔值的转换\n\n> bool()使用可以将其他类型数据转为bool布尔值，只有数字0转换成布尔值是Flase，字符串转换成布尔值都是Truem,只有空字符串转换成布尔值才是Flase\n> \n> 在python里面只有空字符‘’，数字0，空字典{}，空列表[]，空集合set(),空元组()和空数据None会被转换成为Flase，其他都是转换成True\n> \n> #在计算机中布尔值 True 和 Flase 都是用 1 和 0 来保存的\n> print(True + 1)\n> \n> \n> 1\n> 2\n\n\n# 隐形类型转换\n\n> if 0:      #因为0对应的布尔值为Flase所以隐形转换了不成立输出打印\n> print("hello123")\n> \n> if 3:\n> print("hello")\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n\n',normalizedContent:'# 数据类型的转换\n\n> 不同类型的数据不能进行运算否则会报错\n> \n> int()使用可以将其他类型的数据转换为int整数，如果字符串不是一段合法的数字转换会报错，默认base(进制)是十进制，如要转换其他进制的数据需更改后面的参数改成16/8/2即可。\n> \n> str()使用可以将其他类型的数据转为str字符串\n> \n> float()使用可以将其他类型数据转为float浮点小数\n\n\n# 大小写转换\n\n> (str.upper())          # 把所有字符中的小写字母转换成大写字母\n> print(str.lower())          # 把所有字符中的大写字母转换成小写字母\n> print(str.capitalize())     # 把第一个字母转化为大写字母，其余小写\n> print(str.title())          # 把每个单词的第一个字母转化为大写，其余小写 \n> a.lower()\n> a.upper()\n> a.title()首字母大写\n> a.swapcase()首字大写以及大小写互换\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n\n\n# 布尔值的转换\n\n> bool()使用可以将其他类型数据转为bool布尔值，只有数字0转换成布尔值是flase，字符串转换成布尔值都是truem,只有空字符串转换成布尔值才是flase\n> \n> 在python里面只有空字符‘’，数字0，空字典{}，空列表[]，空集合set(),空元组()和空数据none会被转换成为flase，其他都是转换成true\n> \n> #在计算机中布尔值 true 和 flase 都是用 1 和 0 来保存的\n> print(true + 1)\n> \n> \n> 1\n> 2\n\n\n# 隐形类型转换\n\n> if 0:      #因为0对应的布尔值为flase所以隐形转换了不成立输出打印\n> print("hello123")\n> \n> if 3:\n> print("hello")\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"二进制 八进制 十进制 十六进制",frontmatter:{title:"二进制 八进制 十进制 十六进制",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/6019e1/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/14.%E4%BA%8C%E8%BF%9B%E5%88%B6%20%E5%85%AB%E8%BF%9B%E5%88%B6%20%E5%8D%81%E8%BF%9B%E5%88%B6%20%E5%8D%81%E5%85%AD%E8%BF%9B%E5%88%B6.html",relativePath:"后端/05.Python/14.二进制 八进制 十进制 十六进制.md",key:"v-ceac6f68",path:"/pages/6019e1/",headersStr:null,content:"# 二进制 八进制 十进制 十六进制\n\n> 计算机只保存二进制0和1，同时支持八进制和十六进制\n> \n> 二进制(Bin) 八进制(Oct) 十六进制(Hex) 十进制(Dec) python中都能识别\n> \n> 以 0b 开头的数字是二进制 0-1 bin（）\n> \n> 以 0o 开头的数字是八进制 0-7 oct()\n> \n> 以 0x 开头的数字是十六进制 0-9 A-F hex()",normalizedContent:"# 二进制 八进制 十进制 十六进制\n\n> 计算机只保存二进制0和1，同时支持八进制和十六进制\n> \n> 二进制(bin) 八进制(oct) 十六进制(hex) 十进制(dec) python中都能识别\n> \n> 以 0b 开头的数字是二进制 0-1 bin（）\n> \n> 以 0o 开头的数字是八进制 0-7 oct()\n> \n> 以 0x 开头的数字是十六进制 0-9 a-f hex()",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"算数运算符",frontmatter:{title:"算数运算符",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/acb0af/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/18.%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/05.Python/18.算数运算符.md",key:"v-43b7262a",path:"/pages/acb0af/",headers:[{level:2,title:"算数运算符在字符串中的使用",slug:"算数运算符在字符串中的使用",normalizedTitle:"算数运算符在字符串中的使用",charIndex:210},{level:2,title:"赋值运算符",slug:"赋值运算符",normalizedTitle:"赋值运算符",charIndex:314},{level:2,title:"特殊用法",slug:"特殊用法",normalizedTitle:"特殊用法",charIndex:496},{level:2,title:"变量元组拆包",slug:"变量元组拆包",normalizedTitle:"变量元组拆包",charIndex:850},{level:2,title:"可变长度变量",slug:"可变长度变量",normalizedTitle:"可变长度变量",charIndex:1013}],headersStr:"算数运算符在字符串中的使用 赋值运算符 特殊用法 变量元组拆包 可变长度变量",content:"# 算数运算符\n\n> + - * / **幂运算 //整除 %取余(取模) ()小括号 提高运算优先级\n> \n> 注意运算符的优先顺序\n> \n> 在python3，2个整数相除，得到的结果会是一个浮点数\n> \n> 除以会返回一个浮点小数\n> \n> 整除只取整数部分，整除向下取整，如为负数则向更小的数值取整\n> \n> 取余只取余的部分，如10/3=3 余1 则整除结果为1，被除数如果小于除数则整除结果为被除数\n\n\n# 算数运算符在字符串中的使用\n\n> 加法运算符：只能用于2个字符串数据，用来拼接2个字符串。 数字和字符串之间不能进行加法运算\n> \n> 乘法运算符：可以用于数字和字符串之间，用于将一个字符串重复多次\n\n\n# 赋值运算符\n\n> = 等号在计算机编程里，我们称为赋值运算符，和数字中的等号有一定的区别\n> \n> 计算机编程里，等号(赋值运算符)的作用是将等号右边的值赋值给左边\n> \n> 等号的左边不能是常量或表达式 如 10 = x ，1 + 1 =y\n> \n> X = 1\n> \n> X +=2 #与X = X + 2相同 + - * / // ** % 都可以\n\n\n# 特殊用法\n\n> 等号连接的变量可以传递赋值，如 a = b = c =42，则3个变量的赋值都为42，因为等号的作用把右边的值赋值给左边\n> \n> 不能 a = 1 = b = c 这样写会报错 如提示code 1 则还是报错\n> \n> #特殊用法\n> a = b = c = d = e =42  #等号连接的变量可以传递赋值，等号的作用是把右边的值赋值给左边\n> #a = b = 'heloo' = c = d       这样的写法是不正确的\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> x = 'a','b','c'   #其实本质是元组只不过把括号省略了\n> print(x)            #('a', 'b', 'c')\n> \n> \n> 1\n> 2\n> \n> \n> \n> # 变量元组拆包\n> \n> x , y = 3 , 5   #元组拆包\n> #x, y, c =3 , 4   #这样写不行\n> #x, y = 3 , 4 , 5 , 6 , 7    拆包值与被赋值值必须相同数量,否则将报错\n> print(x , y)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> \n> \n> \n> # 可变长度变量\n> \n> o , *p , q = 1 , 2 ,3 ,4 ,5 ,6 ,7 ,8            #在变量前面加*星号可以使变量为可变长度，中间剩下的直接打包赋值给加星号的变量(按顺序从等号右边赋值给变量剩下的打包给加星号的变量)\n> print(o,p,q)  #1 [2, 3, 4, 5, 6, 7] 8\n> \n> \n> 1\n> 2",normalizedContent:"# 算数运算符\n\n> + - * / **幂运算 //整除 %取余(取模) ()小括号 提高运算优先级\n> \n> 注意运算符的优先顺序\n> \n> 在python3，2个整数相除，得到的结果会是一个浮点数\n> \n> 除以会返回一个浮点小数\n> \n> 整除只取整数部分，整除向下取整，如为负数则向更小的数值取整\n> \n> 取余只取余的部分，如10/3=3 余1 则整除结果为1，被除数如果小于除数则整除结果为被除数\n\n\n# 算数运算符在字符串中的使用\n\n> 加法运算符：只能用于2个字符串数据，用来拼接2个字符串。 数字和字符串之间不能进行加法运算\n> \n> 乘法运算符：可以用于数字和字符串之间，用于将一个字符串重复多次\n\n\n# 赋值运算符\n\n> = 等号在计算机编程里，我们称为赋值运算符，和数字中的等号有一定的区别\n> \n> 计算机编程里，等号(赋值运算符)的作用是将等号右边的值赋值给左边\n> \n> 等号的左边不能是常量或表达式 如 10 = x ，1 + 1 =y\n> \n> x = 1\n> \n> x +=2 #与x = x + 2相同 + - * / // ** % 都可以\n\n\n# 特殊用法\n\n> 等号连接的变量可以传递赋值，如 a = b = c =42，则3个变量的赋值都为42，因为等号的作用把右边的值赋值给左边\n> \n> 不能 a = 1 = b = c 这样写会报错 如提示code 1 则还是报错\n> \n> #特殊用法\n> a = b = c = d = e =42  #等号连接的变量可以传递赋值，等号的作用是把右边的值赋值给左边\n> #a = b = 'heloo' = c = d       这样的写法是不正确的\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> x = 'a','b','c'   #其实本质是元组只不过把括号省略了\n> print(x)            #('a', 'b', 'c')\n> \n> \n> 1\n> 2\n> \n> \n> \n> # 变量元组拆包\n> \n> x , y = 3 , 5   #元组拆包\n> #x, y, c =3 , 4   #这样写不行\n> #x, y = 3 , 4 , 5 , 6 , 7    拆包值与被赋值值必须相同数量,否则将报错\n> print(x , y)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> \n> \n> \n> # 可变长度变量\n> \n> o , *p , q = 1 , 2 ,3 ,4 ,5 ,6 ,7 ,8            #在变量前面加*星号可以使变量为可变长度，中间剩下的直接打包赋值给加星号的变量(按顺序从等号右边赋值给变量剩下的打包给加星号的变量)\n> print(o,p,q)  #1 [2, 3, 4, 5, 6, 7] 8\n> \n> \n> 1\n> 2",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"分割",frontmatter:{title:"分割",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/687726/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/19.%E5%88%86%E5%89%B2.html",relativePath:"后端/05.Python/19.分割.md",key:"v-314e9546",path:"/pages/687726/",headers:[{level:2,title:"参数",slug:"参数",normalizedTitle:"参数",charIndex:34}],headersStr:"参数",content:'# 分割\n\n> split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串\n> \n> str.split(str="", num=string.count(str)).\n> \n> \n> 1\n> \n> \n> \n> # 参数\n> \n>  * str -- 分隔符，默认为所有的空字符，包括空格、换行(\\n)、制表符(\\t)等。\n>  * num -- 分割次数。默认为 -1, 即分隔所有。',normalizedContent:'# 分割\n\n> split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串\n> \n> str.split(str="", num=string.count(str)).\n> \n> \n> 1\n> \n> \n> \n> # 参数\n> \n>  * str -- 分隔符，默认为所有的空字符，包括空格、换行(\\n)、制表符(\\t)等。\n>  * num -- 分割次数。默认为 -1, 即分隔所有。',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"提取4位数的各位数",frontmatter:{title:"提取4位数的各位数",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/a15165/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/21.%E6%8F%90%E5%8F%964%E4%BD%8D%E6%95%B0%E7%9A%84%E5%90%84%E4%BD%8D%E6%95%B0.html",relativePath:"后端/05.Python/21.提取4位数的各位数.md",key:"v-2e61c2f3",path:"/pages/a15165/",headersStr:null,content:"# 提取4位数的各位数\n\n> 1234//1000=1.234整除=1 提取第一位 #整除1 1234 //100=12.34=12%10=2 提取第二位 #余2 1234 //10=123.4=123%10=120 提取第三位 #余3 1234%10 =1234-1230= 余4 提取第四位 #余4",normalizedContent:"# 提取4位数的各位数\n\n> 1234//1000=1.234整除=1 提取第一位 #整除1 1234 //100=12.34=12%10=2 提取第二位 #余2 1234 //10=123.4=123%10=120 提取第三位 #余3 1234%10 =1234-1230= 余4 提取第四位 #余4",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"鸡兔同笼",frontmatter:{title:"鸡兔同笼",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/4d8423/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/22.%E9%B8%A1%E5%85%94%E5%90%8C%E7%AC%BC.html",relativePath:"后端/05.Python/22.鸡兔同笼.md",key:"v-05d97d43",path:"/pages/4d8423/",headersStr:null,content:"# 鸡兔同笼\n\n> 二元一次方 2X+4Y=94 X+Y=35\n> \n> 假设法 假设全部动物是2条腿 则 35*2=70 70根脚\n> 总共94则 94-70=24 则24根是兔子的脚 24/2=12 再除以被去掉兔子的2根脚可以得出兔子的数量 35-12=23 总数减去兔子就可以得出鸡的数量\n> \n> 假如兔子也两条腿 则总脚数 94/2=47 假设兔子两条腿的总数减去头数 则47-35=12 可以求出兔子的数量 35-12=23 鸡的数量",normalizedContent:"# 鸡兔同笼\n\n> 二元一次方 2x+4y=94 x+y=35\n> \n> 假设法 假设全部动物是2条腿 则 35*2=70 70根脚\n> 总共94则 94-70=24 则24根是兔子的脚 24/2=12 再除以被去掉兔子的2根脚可以得出兔子的数量 35-12=23 总数减去兔子就可以得出鸡的数量\n> \n> 假如兔子也两条腿 则总脚数 94/2=47 假设兔子两条腿的总数减去头数 则47-35=12 可以求出兔子的数量 35-12=23 鸡的数量",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"交换变量的值",frontmatter:{title:"交换变量的值",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/c13570/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/20.%E4%BA%A4%E6%8D%A2%E5%8F%98%E9%87%8F%E7%9A%84%E5%80%BC.html",relativePath:"后端/05.Python/20.交换变量的值.md",key:"v-54d2601a",path:"/pages/c13570/",headers:[{level:2,title:"加减法",slug:"加减法",normalizedTitle:"加减法",charIndex:13},{level:2,title:"乘除法",slug:"乘除法",normalizedTitle:"乘除法",charIndex:122}],headersStr:"加减法 乘除法",content:"# 交换变量的值\n\n\n# 加减法\n\n> a=2 b=7\n> \n> a = a + b= 2 + 7 =9\n> \n> b = a - b= 9 - 7 =2\n> \n> a = a - b= 9 - 2 =7\n> \n> #a=b ,b=a\n\n\n# 乘除法\n\n> a=2 b=7 a = a * b =2*7=14 b = a // b =14 // 7 =2 a = a // b =14 // 2 =7",normalizedContent:"# 交换变量的值\n\n\n# 加减法\n\n> a=2 b=7\n> \n> a = a + b= 2 + 7 =9\n> \n> b = a - b= 9 - 7 =2\n> \n> a = a - b= 9 - 2 =7\n> \n> #a=b ,b=a\n\n\n# 乘除法\n\n> a=2 b=7 a = a * b =2*7=14 b = a // b =14 // 7 =2 a = a // b =14 // 2 =7",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"逻辑运算符",frontmatter:{title:"逻辑运算符",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/70c3fd/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/25.%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/05.Python/25.逻辑运算符.md",key:"v-36fd303c",path:"/pages/70c3fd/",headers:[{level:2,title:"逻辑运算符的短路",slug:"逻辑运算符的短路",normalizedTitle:"逻辑运算符的短路",charIndex:497}],headersStr:"逻辑运算符的短路",content:"# 逻辑运算符\n\n> 逻辑与 and（并且） 逻辑或or(或者) 逻辑非not\n> \n> 逻辑与规则:只要有一个运算数是False，结果就是False；只有所有的运算数都是True，结果才是True（找第一个False）\n> \n> print(5 > 3 and 6 < 8 and 1 < 7)  #True\n> print(3 > 1 and 9 < 5 and 2 < 3)  #False\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑或规则:只要有一个运算是True，结果就是True;只有所有的运算数都是False，结果才是False（找第一个True）\n> \n> print(5 > 3 or 6 < 1 or 1 > 3) #True\n> print(1 > 3 or 3 > 6) #False\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑非运算:Ture ==> False; False ==>Ture(取相反)\n> \n> print(not(5 < 1))  #True\n> print(not(6 > 1))  #False\n> \n> \n> 1\n> 2\n\n\n# 逻辑运算符的短路\n\n> 逻辑与的短路，只要检索到有False就输出False，后面的不会继续检索，取第一个False的值\n> \n> 5 > 3 and print('hello')  #True 继续检索\n> 8 < 2 and print('weiwei')  #False 不会继续执行下一个\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑或的短路，只要检索到有True就输出True，后面的不会继续检索，取第一个为True\n> \n> 6 > 2 or print('123')  #True，不会继续执行下一个\n> 9 < 1 or print('456')  #False 继续检索\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑与运算做取值时，取第一个为False；如果所以运算数都是True，取最后一个值\n> \n> print(1 and 4 and 'hello' and 'test')   #test\n> print(4 and 7 and 0 and '' and '123')   #0\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑或运算做取值时，取第一个为True；如果所以运算数都是False，取最后一个值\n> \n> print(0 or 'hello' or '' or [])  #hello\n> print(0 or [] or '' or None or ()) # ()\n> \n> \n> 1\n> 2",normalizedContent:"# 逻辑运算符\n\n> 逻辑与 and（并且） 逻辑或or(或者) 逻辑非not\n> \n> 逻辑与规则:只要有一个运算数是false，结果就是false；只有所有的运算数都是true，结果才是true（找第一个false）\n> \n> print(5 > 3 and 6 < 8 and 1 < 7)  #true\n> print(3 > 1 and 9 < 5 and 2 < 3)  #false\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑或规则:只要有一个运算是true，结果就是true;只有所有的运算数都是false，结果才是false（找第一个true）\n> \n> print(5 > 3 or 6 < 1 or 1 > 3) #true\n> print(1 > 3 or 3 > 6) #false\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑非运算:ture ==> false; false ==>ture(取相反)\n> \n> print(not(5 < 1))  #true\n> print(not(6 > 1))  #false\n> \n> \n> 1\n> 2\n\n\n# 逻辑运算符的短路\n\n> 逻辑与的短路，只要检索到有false就输出false，后面的不会继续检索，取第一个false的值\n> \n> 5 > 3 and print('hello')  #true 继续检索\n> 8 < 2 and print('weiwei')  #false 不会继续执行下一个\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑或的短路，只要检索到有true就输出true，后面的不会继续检索，取第一个为true\n> \n> 6 > 2 or print('123')  #true，不会继续执行下一个\n> 9 < 1 or print('456')  #false 继续检索\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑与运算做取值时，取第一个为false；如果所以运算数都是true，取最后一个值\n> \n> print(1 and 4 and 'hello' and 'test')   #test\n> print(4 and 7 and 0 and '' and '123')   #0\n> \n> \n> 1\n> 2\n> \n> \n> 逻辑或运算做取值时，取第一个为true；如果所以运算数都是false，取最后一个值\n> \n> print(0 or 'hello' or '' or [])  #hello\n> print(0 or [] or '' or none or ()) # ()\n> \n> \n> 1\n> 2",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"比较运算符",frontmatter:{title:"比较运算符",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/661f38/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/24.%E6%AF%94%E8%BE%83%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/05.Python/24.比较运算符.md",key:"v-74d7ff21",path:"/pages/661f38/",headersStr:null,content:"# 比较运算符\n\n> 大于 > 小于 < 大于等于 >= 小于等于 <= 不等于 != <>（也是不等于但python3.7不支持了） == 等于\n> \n> 判断结果输出是布尔值\n> \n> print(2 > 1) #True\n> print(3 < 2) #False\n> print(2 >= 2) #True\n> print(2 <= 1) #False\n> print(1 != 2) #True\n> print('hello' == 'hello') #True\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> \n> \n> #比较符在字符串中使用\n> #字符串之间使用比较运算符，会根据各个字符逐一编码值(ASCII码表)比较\n> print('a' > 'b')   #False 97 > 98\n> print('abc' > 'a')  #False   97 > 98 逐一对比False就不往下\n> \n> #字符串和数字不能进行比较，与等于做运算结果为False，与不等于做运算为True，不支持其他的比较运算\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> \n> \n> # print('a' > 90 )\n> \n> print('a' == 90) #False\n> print('a' != 90) #True\n> \n> \n> 1\n> 2\n> 3\n> 4",normalizedContent:"# 比较运算符\n\n> 大于 > 小于 < 大于等于 >= 小于等于 <= 不等于 != <>（也是不等于但python3.7不支持了） == 等于\n> \n> 判断结果输出是布尔值\n> \n> print(2 > 1) #true\n> print(3 < 2) #false\n> print(2 >= 2) #true\n> print(2 <= 1) #false\n> print(1 != 2) #true\n> print('hello' == 'hello') #true\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> \n> \n> #比较符在字符串中使用\n> #字符串之间使用比较运算符，会根据各个字符逐一编码值(ascii码表)比较\n> print('a' > 'b')   #false 97 > 98\n> print('abc' > 'a')  #false   97 > 98 逐一对比false就不往下\n> \n> #字符串和数字不能进行比较，与等于做运算结果为false，与不等于做运算为true，不支持其他的比较运算\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> \n> \n> # print('a' > 90 )\n> \n> print('a' == 90) #false\n> print('a' != 90) #true\n> \n> \n> 1\n> 2\n> 3\n> 4",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"位运算符",frontmatter:{title:"位运算符",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/8f779d/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/26.%E4%BD%8D%E8%BF%90%E7%AE%97%E7%AC%A6.html",relativePath:"后端/05.Python/26.位运算符.md",key:"v-635d2d7a",path:"/pages/8f779d/",headersStr:null,content:"# 位运算符\n\n> 按位与 & 按位或| 按位异或^ 按位左移<< 按位右移>> 按位取反~\n> \n> a = 42\n> b = 13\n> # 按位与&  是把整型转换成二进制进行运算，两个二进制同为1则为1，否则为0,进行运算的结果\n> #  42 = 0b101010   0010 1010    ==>\n> #  13 = 0b1101     0000 1101    ==>0000 1000   ==> 0b1000 = 8\n> print(a & b)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> \n> \n> 按位或| 只有一个为1就为1\n> \n> #  00101111 ==> 47\n> print(a | b)\n> \n> \n> 1\n> 2\n> \n> \n> 按位异或^ 相同为0，不同为1\n> \n> # 00100111  ==> 39\n> print(a ^ b)\n> \n> \n> 1\n> 2\n> \n> \n> 按位左移 << 转换成二进制在尾部加0，即头部向前移n位\n> \n> #   101 ==>  101000\n> x = 5\n> print(x << 3)   # x << n ==> a * 2 ** n ==> a*2的n次方\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> 按位右移 >> 转换成二进制在头部加0，即尾部向后移n位，溢出的数要丢弃(也就是丢弃尾部二进制的n位数)\n> \n> # 10001 ==> 100\n> x = 17\n> print(x >> 2) # x >> n ==> a 除以2的n次方\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> 按位取反~ 把数据每个二进制位进行取反，即把1变成0,0变成1,涉及补码和反码有点绕，以后补习 #https://www.cnblogs.com/jniantic/p/12189648.html\n> \n> # 0101 ==> 010\n> x = 6\n> print(~6)  #~x = -(x+1)\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> 用位运算符拆分十六进制各两位出来\n> \n> x = 0xF0384E\n> R = x >> 16              #右移16位把后面16位溢出\n> G = x >> 8 & 255       # 255 为 1111 1111，然后与运算把前面的8位去除\n> B = x & 255              # 同上，把前面16位去除\n> print(hex(R),hex(G),hex(B))\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5",normalizedContent:"# 位运算符\n\n> 按位与 & 按位或| 按位异或^ 按位左移<< 按位右移>> 按位取反~\n> \n> a = 42\n> b = 13\n> # 按位与&  是把整型转换成二进制进行运算，两个二进制同为1则为1，否则为0,进行运算的结果\n> #  42 = 0b101010   0010 1010    ==>\n> #  13 = 0b1101     0000 1101    ==>0000 1000   ==> 0b1000 = 8\n> print(a & b)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> \n> \n> 按位或| 只有一个为1就为1\n> \n> #  00101111 ==> 47\n> print(a | b)\n> \n> \n> 1\n> 2\n> \n> \n> 按位异或^ 相同为0，不同为1\n> \n> # 00100111  ==> 39\n> print(a ^ b)\n> \n> \n> 1\n> 2\n> \n> \n> 按位左移 << 转换成二进制在尾部加0，即头部向前移n位\n> \n> #   101 ==>  101000\n> x = 5\n> print(x << 3)   # x << n ==> a * 2 ** n ==> a*2的n次方\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> 按位右移 >> 转换成二进制在头部加0，即尾部向后移n位，溢出的数要丢弃(也就是丢弃尾部二进制的n位数)\n> \n> # 10001 ==> 100\n> x = 17\n> print(x >> 2) # x >> n ==> a 除以2的n次方\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> 按位取反~ 把数据每个二进制位进行取反，即把1变成0,0变成1,涉及补码和反码有点绕，以后补习 #https://www.cnblogs.com/jniantic/p/12189648.html\n> \n> # 0101 ==> 010\n> x = 6\n> print(~6)  #~x = -(x+1)\n> \n> \n> 1\n> 2\n> 3\n> \n> \n> 用位运算符拆分十六进制各两位出来\n> \n> x = 0xf0384e\n> r = x >> 16              #右移16位把后面16位溢出\n> g = x >> 8 & 255       # 255 为 1111 1111，然后与运算把前面的8位去除\n> b = x & 255              # 同上，把前面16位去除\n> print(hex(r),hex(g),hex(b))\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"圆的公式",frontmatter:{title:"圆的公式",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/bc04ac/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/23.%E5%9C%86%E7%9A%84%E5%85%AC%E5%BC%8F.html",relativePath:"后端/05.Python/23.圆的公式.md",key:"v-1bd5106a",path:"/pages/bc04ac/",headersStr:null,content:"# 圆的公式\n\n> 圆周长 C=2πr\n> \n> 圆面积 S=πr²; S=π(d/2）^2\n> \n> 球体表面积 S=4πr²=πD²\n> \n> 球体体积 V=(4/3)πr^3\n> \n> 圆柱体积 V=πr² h",normalizedContent:"# 圆的公式\n\n> 圆周长 c=2πr\n> \n> 圆面积 s=πr²; s=π(d/2）^2\n> \n> 球体表面积 s=4πr²=πd²\n> \n> 球体体积 v=(4/3)πr^3\n> \n> 圆柱体积 v=πr² h",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"if …elif…elif的使用",frontmatter:{title:"if …elif…elif的使用",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/9cbf41/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/29.if%20%E2%80%A6elif%E2%80%A6elif%E7%9A%84%E4%BD%BF%E7%94%A8.html",relativePath:"后端/05.Python/29.if …elif…elif的使用.md",key:"v-534da396",path:"/pages/9cbf41/",headersStr:null,content:"# if …elif…elif的使用\n\nnub = int(input())\n# #多个if语句，语句和语句之间，不存在关联，各走各的\n# if 1 < nub < 3:\n#     print('小于3')\n#\n# if 3 < nub < 7:\n#     print('小于7')\n#\n# if 7 < nub < 9:\n#     print('小于9')\n\n\nif 1 <= nub <= 3:\n    print('小于或等于3')\nelif 3 <= nub <= 7:         #不满足第一if的条件走一下一直走到满足if条件的语句\n    print('小于或等于7')\nelif 7 <= nub <= 9:\n    print('小于或等于9')\nelse:print('请输入1-9的数字')  #否则语句，判断不在if条件中的然后出此结果\n\n\n#if语句的嵌套\n#在python中，使用强制缩进来表示语句之间的结构\na =input('是否同意请输入Y/N:')\nif a == 'Y':\n    print('你已同意')\n    next = input('下一步Y/N')\n    if next == 'Y':\n        print('恭喜你')\n    else:\n        print('你已取消')\nelse:\n    print('你已经取消同意')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n",normalizedContent:"# if …elif…elif的使用\n\nnub = int(input())\n# #多个if语句，语句和语句之间，不存在关联，各走各的\n# if 1 < nub < 3:\n#     print('小于3')\n#\n# if 3 < nub < 7:\n#     print('小于7')\n#\n# if 7 < nub < 9:\n#     print('小于9')\n\n\nif 1 <= nub <= 3:\n    print('小于或等于3')\nelif 3 <= nub <= 7:         #不满足第一if的条件走一下一直走到满足if条件的语句\n    print('小于或等于7')\nelif 7 <= nub <= 9:\n    print('小于或等于9')\nelse:print('请输入1-9的数字')  #否则语句，判断不在if条件中的然后出此结果\n\n\n#if语句的嵌套\n#在python中，使用强制缩进来表示语句之间的结构\na =input('是否同意请输入y/n:')\nif a == 'y':\n    print('你已同意')\n    next = input('下一步y/n')\n    if next == 'y':\n        print('恭喜你')\n    else:\n        print('你已取消')\nelse:\n    print('你已经取消同意')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"运算符的优先级",frontmatter:{title:"运算符的优先级",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/689d0c/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/27.%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7.html",relativePath:"后端/05.Python/27.运算符的优先级.md",key:"v-87e6b8fc",path:"/pages/689d0c/",headersStr:null,content:"# 运算符的优先级\n\n#逻辑运算符的优先级   not > and > or\n#先加减后乘除\n#（）括号内是最高优先级的\n# ** 比乘除高\n# ~ + - （正负号） 高\nprint(True or False and True)  #True\nprint(True and not False or False)  #True\n\n\n1\n2\n3\n4\n5\n6\n7\n",normalizedContent:"# 运算符的优先级\n\n#逻辑运算符的优先级   not > and > or\n#先加减后乘除\n#（）括号内是最高优先级的\n# ** 比乘除高\n# ~ + - （正负号） 高\nprint(true or false and true)  #true\nprint(true and not false or false)  #true\n\n\n1\n2\n3\n4\n5\n6\n7\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"if else  分支 条件判断语句",frontmatter:{title:"if else  分支 条件判断语句",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/0f98fb/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/28.if%20else%20%20%E5%88%86%E6%94%AF%20%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD%E8%AF%AD%E5%8F%A5.html",relativePath:"后端/05.Python/28.if else  分支 条件判断语句.md",key:"v-07ac2091",path:"/pages/0f98fb/",headersStr:null,content:"# if else 分支 条件判断语句\n\n> 判断语句\n> \n> 条件判断语句 if / if else / if elif elif else python中不支持 switch...case 条件语句\n> \n> if 条件判断 条件成立则向下执行if的子命令，不成立则忽略 数字跟字符串做比较 ==等于的结果为 False， !=的结果为True，其他运算则报错\n> \n> if中只能用==作相等比较，不能使用等号=赋值运算符作相等比较\n> \n> if 3 > 2:\n>  print(123)\n> \n> if 1 > 3:\n>  print(456)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> \n> \n> if...else 判断条件 if 判断条件: 条件成立的代码 else: 条件不成立的代码（否则）\n> \n> #判断一个数 是否能整除4和8\n> numb =int(input('请输入一个数字:'))\n> if numb % 4 == 0 and numb % 8 == 0:\n>  print('这个数能被4和8整除')\n> else:\n>  print('这个数不能被4和8整除')\n> \n> #判断一个数 是否能被5和8整除，但是不能同时被5和8整除\n> num =int(input('请输入一个数字2:'))\n> if (num % 5 == 0 or num % 8 == 0) and (num % 40 != 0):\n>  print('这个数能被5或8整除，但不能被5和8整除')\n> \n> \n> #判断一个年份是否是闰年，（条件: 年份能被4整除（普通闰年），但不能被100整除，但可以被400整除（称为世纪闰年））\n> year = int(input('请输入一个年份'))\n> if (year % 4 == 0 and year % 100 != 0 ) or (year % 400 == 0):\n>  print('这个年份是闰年')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17",normalizedContent:"# if else 分支 条件判断语句\n\n> 判断语句\n> \n> 条件判断语句 if / if else / if elif elif else python中不支持 switch...case 条件语句\n> \n> if 条件判断 条件成立则向下执行if的子命令，不成立则忽略 数字跟字符串做比较 ==等于的结果为 false， !=的结果为true，其他运算则报错\n> \n> if中只能用==作相等比较，不能使用等号=赋值运算符作相等比较\n> \n> if 3 > 2:\n>  print(123)\n> \n> if 1 > 3:\n>  print(456)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> \n> \n> if...else 判断条件 if 判断条件: 条件成立的代码 else: 条件不成立的代码（否则）\n> \n> #判断一个数 是否能整除4和8\n> numb =int(input('请输入一个数字:'))\n> if numb % 4 == 0 and numb % 8 == 0:\n>  print('这个数能被4和8整除')\n> else:\n>  print('这个数不能被4和8整除')\n> \n> #判断一个数 是否能被5和8整除，但是不能同时被5和8整除\n> num =int(input('请输入一个数字2:'))\n> if (num % 5 == 0 or num % 8 == 0) and (num % 40 != 0):\n>  print('这个数能被5或8整除，但不能被5和8整除')\n> \n> \n> #判断一个年份是否是闰年，（条件: 年份能被4整除（普通闰年），但不能被100整除，但可以被400整除（称为世纪闰年））\n> year = int(input('请输入一个年份'))\n> if (year % 4 == 0 and year % 100 != 0 ) or (year % 400 == 0):\n>  print('这个年份是闰年')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"调试代码(Debug)",frontmatter:{title:"调试代码(Debug)",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/921d6f/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/32.%E8%B0%83%E8%AF%95%E4%BB%A3%E7%A0%81(Debug).html",relativePath:"后端/05.Python/32.调试代码(Debug).md",key:"v-0c44b074",path:"/pages/921d6f/",headersStr:null,content:"# 调试代码(Debug)\n\n在pycharm中鼠标点击代码序号中间有个红色圆圈，点击之后加了个断点意思是代码运行到此处会停顿等待管理调试用户点击继续执行（Debug）\n\n> F8 Step Over下一步",normalizedContent:"# 调试代码(debug)\n\n在pycharm中鼠标点击代码序号中间有个红色圆圈，点击之后加了个断点意思是代码运行到此处会停顿等待管理调试用户点击继续执行（debug）\n\n> f8 step over下一步",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"if中的隐性转化",frontmatter:{title:"if中的隐性转化",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/a08b12/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/30.if%E4%B8%AD%E7%9A%84%E9%9A%90%E6%80%A7%E8%BD%AC%E5%8C%96.html",relativePath:"后端/05.Python/30.if中的隐性转化.md",key:"v-50b2e256",path:"/pages/a08b12/",headersStr:null,content:"# if中的隐性转化\n\n> if 4:  #if 后面需要是一个布尔值，如果不是布尔值会自动转换成布尔值,大于或者等于1的数为True，只有0转换为False\n>     print('hello')\n> \n> \n> 1\n> 2",normalizedContent:"# if中的隐性转化\n\n> if 4:  #if 后面需要是一个布尔值，如果不是布尔值会自动转换成布尔值,大于或者等于1的数为true，只有0转换为false\n>     print('hello')\n> \n> \n> 1\n> 2",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"三元表达式",frontmatter:{title:"三元表达式",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/6be9fd/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/31.%E4%B8%89%E5%85%83%E8%A1%A8%E8%BE%BE%E5%BC%8F.html",relativePath:"后端/05.Python/31.三元表达式.md",key:"v-947b9e0a",path:"/pages/6be9fd/",headersStr:null,content:"# 三元表达式\n\n> #3. 三元表达式(if...else语句的简写),将if...else转变成一行代码\n> num1 = 12\n> num2 = 42\n> # if num1 < num2:\n> #     x = num2\n> # else:\n> #     x = num1\n> x = num1 if num1 > num2 else num2   # a if 1 > 2 else b,意思是如果1>2成立 则输出a否则输出b\n> print(x,'这数大')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9",normalizedContent:"# 三元表达式\n\n> #3. 三元表达式(if...else语句的简写),将if...else转变成一行代码\n> num1 = 12\n> num2 = 42\n> # if num1 < num2:\n> #     x = num2\n> # else:\n> #     x = num1\n> x = num1 if num1 > num2 else num2   # a if 1 > 2 else b,意思是如果1>2成立 则输出a否则输出b\n> print(x,'这数大')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"猜拳游戏",frontmatter:{title:"猜拳游戏",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/5f7ab5/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/34.%E7%8C%9C%E6%8B%B3%E6%B8%B8%E6%88%8F.html",relativePath:"后端/05.Python/34.猜拳游戏.md",key:"v-6f5fde1c",path:"/pages/5f7ab5/",headersStr:null,content:"# 猜拳游戏\n\n> #if中只能用==作相等比较，不能使用等号=赋值运算符作相等比较\n> #input是用来接收用户的输入数据\n> #随机出一个数\n> #需要用的随机数模块 random\n> player = int(input('猜拳游戏请输入     0.石头     1.剪刀     2.布：'))\n> import random   #导入模块/加载模块\n> computer = random.randint(0,2)  #random.randint(a,b)==> 能够生成[a,b]的随机整数\n> print(computer)\n> if player == computer:\n>     print('平局')\n> elif player - computer == -1 or  player - computer == 2:\n>     print('你赢了')\n> else:print('你输了')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13",normalizedContent:"# 猜拳游戏\n\n> #if中只能用==作相等比较，不能使用等号=赋值运算符作相等比较\n> #input是用来接收用户的输入数据\n> #随机出一个数\n> #需要用的随机数模块 random\n> player = int(input('猜拳游戏请输入     0.石头     1.剪刀     2.布：'))\n> import random   #导入模块/加载模块\n> computer = random.randint(0,2)  #random.randint(a,b)==> 能够生成[a,b]的随机整数\n> print(computer)\n> if player == computer:\n>     print('平局')\n> elif player - computer == -1 or  player - computer == 2:\n>     print('你赢了')\n> else:print('你输了')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"pass语句",frontmatter:{title:"pass语句",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/1f3eec/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/33.pass%E8%AF%AD%E5%8F%A5.html",relativePath:"后端/05.Python/33.pass语句.md",key:"v-41675d98",path:"/pages/1f3eec/",headersStr:null,content:"# pass语句\n\n> #pass 语句 ，没有实际意义，单纯用来占位，保证语句的完整性\n> age = int(input('请输入你的年龄'))\n> if age > 18:  #if后面一定要接下一步的执行代码否则会报错，可以使用pass来占位保证完整性\n>     pass\n> print('123')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5",normalizedContent:"# pass语句\n\n> #pass 语句 ，没有实际意义，单纯用来占位，保证语句的完整性\n> age = int(input('请输入你的年龄'))\n> if age > 18:  #if后面一定要接下一步的执行代码否则会报错，可以使用pass来占位保证完整性\n>     pass\n> print('123')\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"随机数",frontmatter:{title:"随机数",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/559847/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/35.%E9%9A%8F%E6%9C%BA%E6%95%B0.html",relativePath:"后端/05.Python/35.随机数.md",key:"v-f624614c",path:"/pages/559847/",headersStr:null,content:"# 随机数\n\n> import random   #导入模块/加载模块\n> a = random.randint(0,2)  #random.randint(a,b)==> 能够生成[a,b]的随机整数，只输出一个随机数\n> \n> \n> 1\n> 2",normalizedContent:"# 随机数\n\n> import random   #导入模块/加载模块\n> a = random.randint(0,2)  #random.randint(a,b)==> 能够生成[a,b]的随机整数，只输出一个随机数\n> \n> \n> 1\n> 2",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"循环",frontmatter:{title:"循环",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/aed4d8/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/36.%E5%BE%AA%E7%8E%AF.html",relativePath:"后端/05.Python/36.循环.md",key:"v-3cd48312",path:"/pages/aed4d8/",headers:[{level:2,title:"循环的基本使用",slug:"循环的基本使用",normalizedTitle:"循环的基本使用",charIndex:9},{level:2,title:"for…in循环",slug:"for-in循环",normalizedTitle:"for…in循环",charIndex:724},{level:2,title:"for…else",slug:"for-else",normalizedTitle:"for…else",charIndex:1402},{level:2,title:"break和continue 关键字",slug:"break和continue-关键字",normalizedTitle:"break和continue 关键字",charIndex:1709},{level:2,title:"嵌套循环",slug:"嵌套循环",normalizedTitle:"嵌套循环",charIndex:2209}],headersStr:"循环的基本使用 for…in循环 for…else break和continue 关键字 嵌套循环",content:"# 循环\n\n\n# 循环的基本使用\n\n> python中分while 循环和 for 循环 python中不支持do...while循环\n> \n> while 循环的基本使用 while 判断条件：\n> \n> 条件成立时执行的代码\n> \n> \n> i = 0\n> while i < 10:\n>     print('hello world')\n>     i += 1         #i++ python中没有自增自减运算符\n> \n> \n> #求1-100所有整数的和\n> i = 0\n> sum = 0\n> while i < 100 :\n>     i += 1\n>     sum = sum +i\n> print(sum)\n> \n> \n> #求1-100所有偶数的和\n> i = 0\n> sum = 0\n> while i < 100:\n>     i += 1\n>     if i % 2 == 0:  #判断i是偶数才加到sum里面去\n>         sum = sum + i\n> print(sum)\n> \n> #求 35-987所有整数的和\n> i = 34\n> sum = 0\n> while i < 987:\n>     i += 1\n>     sum +=  i\n> print(sum)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17\n> 18\n> 19\n> 20\n> 21\n> 22\n> 23\n> 24\n> 25\n> 26\n> 27\n> 28\n> 29\n> 30\n> 31\n> 32\n\n\n# for…in循环\n\n> #python中for循环指的for...in循环，和c语言中的for不一样\n> #for 语句格式： for ele（变量） in iterable（迭代，可以理解为连续的一组数据，可以遍历的数据，包含内置的string、list、dict、tuple、set()）\n> #in的后面必须是一个可迭代的对象如字符串、列表、字典、元组、集合、range\n> \n> # for i in [1,2,3,4,5,6,7,8,9]:\n> #      print(i)\n> \n> #range 内置类用来生成指定区间的整数序列（列表）\n> #for循环打印1-10\n> for i in range(1,11):   #range不包含输出最后一位\n>     print(i)\n> \n> #用for循环求出1-100整数之和\n> sum = 0   #用来保存数字之和的变量\n> for i in range(1,101):\n>     sum +=i\n> print(sum)\n> \n> #for循环如果迭代类型为字符串则会把字符串一个一个拆分为一个个字符输出\n> for i in \"hello\":   #会把字符串一个一个拆分为一个个字符输出\n>     print(i) #h   e    l    l    o\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17\n> 18\n> 19\n> 20\n> 21\n\n\n# for…else\n\n#统计101-200中素数的个数，并输出所有的素数。（素数又你叫质数，就是只能被1和它本身整除的数）\na =0\nfor i in range(101,201):\n    for j in range(2,i):  #除以比当前数字小的数\n        if i % j ==0:\n            break    #判断成立结束for j循环\n    else:       #当for里面的break没有执行的时候else就执行\n            a +=1\n            print(i)\nprint(a)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# break和continue 关键字\n\n> break和continue在python中只能用在循环语句里 break：用来结束循环 continue：用来结束本来循环，开启下一轮循环\n> \n> \n> \n> \n> i = 0\n> while i < 5:\n>     i +=1\n>     if i ==3:        #判断i==3 continue结束本次循环，重新回到while判断\n>         continue   #continue 用于结束执行时本次循环，重新回到循环条件进行下一步循环，不继续执行下面的代码\n>     print(i)\n> \n> \n> i = 0\n> while i < 5:\n>     if i ==3:\n>         i +=1\n>         break    #break 用于直接结束整个循环\n>     i += 1\n>     print(i)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17\n> 18\n\n\n# 嵌套循环",normalizedContent:"# 循环\n\n\n# 循环的基本使用\n\n> python中分while 循环和 for 循环 python中不支持do...while循环\n> \n> while 循环的基本使用 while 判断条件：\n> \n> 条件成立时执行的代码\n> \n> \n> i = 0\n> while i < 10:\n>     print('hello world')\n>     i += 1         #i++ python中没有自增自减运算符\n> \n> \n> #求1-100所有整数的和\n> i = 0\n> sum = 0\n> while i < 100 :\n>     i += 1\n>     sum = sum +i\n> print(sum)\n> \n> \n> #求1-100所有偶数的和\n> i = 0\n> sum = 0\n> while i < 100:\n>     i += 1\n>     if i % 2 == 0:  #判断i是偶数才加到sum里面去\n>         sum = sum + i\n> print(sum)\n> \n> #求 35-987所有整数的和\n> i = 34\n> sum = 0\n> while i < 987:\n>     i += 1\n>     sum +=  i\n> print(sum)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17\n> 18\n> 19\n> 20\n> 21\n> 22\n> 23\n> 24\n> 25\n> 26\n> 27\n> 28\n> 29\n> 30\n> 31\n> 32\n\n\n# for…in循环\n\n> #python中for循环指的for...in循环，和c语言中的for不一样\n> #for 语句格式： for ele（变量） in iterable（迭代，可以理解为连续的一组数据，可以遍历的数据，包含内置的string、list、dict、tuple、set()）\n> #in的后面必须是一个可迭代的对象如字符串、列表、字典、元组、集合、range\n> \n> # for i in [1,2,3,4,5,6,7,8,9]:\n> #      print(i)\n> \n> #range 内置类用来生成指定区间的整数序列（列表）\n> #for循环打印1-10\n> for i in range(1,11):   #range不包含输出最后一位\n>     print(i)\n> \n> #用for循环求出1-100整数之和\n> sum = 0   #用来保存数字之和的变量\n> for i in range(1,101):\n>     sum +=i\n> print(sum)\n> \n> #for循环如果迭代类型为字符串则会把字符串一个一个拆分为一个个字符输出\n> for i in \"hello\":   #会把字符串一个一个拆分为一个个字符输出\n>     print(i) #h   e    l    l    o\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17\n> 18\n> 19\n> 20\n> 21\n\n\n# for…else\n\n#统计101-200中素数的个数，并输出所有的素数。（素数又你叫质数，就是只能被1和它本身整除的数）\na =0\nfor i in range(101,201):\n    for j in range(2,i):  #除以比当前数字小的数\n        if i % j ==0:\n            break    #判断成立结束for j循环\n    else:       #当for里面的break没有执行的时候else就执行\n            a +=1\n            print(i)\nprint(a)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# break和continue 关键字\n\n> break和continue在python中只能用在循环语句里 break：用来结束循环 continue：用来结束本来循环，开启下一轮循环\n> \n> \n> \n> \n> i = 0\n> while i < 5:\n>     i +=1\n>     if i ==3:        #判断i==3 continue结束本次循环，重新回到while判断\n>         continue   #continue 用于结束执行时本次循环，重新回到循环条件进行下一步循环，不继续执行下面的代码\n>     print(i)\n> \n> \n> i = 0\n> while i < 5:\n>     if i ==3:\n>         i +=1\n>         break    #break 用于直接结束整个循环\n>     i += 1\n>     print(i)\n> \n> \n> 1\n> 2\n> 3\n> 4\n> 5\n> 6\n> 7\n> 8\n> 9\n> 10\n> 11\n> 12\n> 13\n> 14\n> 15\n> 16\n> 17\n> 18\n\n\n# 嵌套循环",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"快捷键",frontmatter:{title:"快捷键",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/d3bc75/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/38.%E5%BF%AB%E6%8D%B7%E9%94%AE.html",relativePath:"后端/05.Python/38.快捷键.md",key:"v-2075c59e",path:"/pages/d3bc75/",headersStr:null,content:"# 快捷键\n\n双击shift 全局搜索功能 （registry）\n\n（reformat code）快速格式化代码 ctrl+alt+l\n\n快速复制选中的代码 ctrl+d\n\n（move line up）移动一行代码 alt+shift+上下箭头\n\ndelete line)删除一行代码 ctrl+y",normalizedContent:"# 快捷键\n\n双击shift 全局搜索功能 （registry）\n\n（reformat code）快速格式化代码 ctrl+alt+l\n\n快速复制选中的代码 ctrl+d\n\n（move line up）移动一行代码 alt+shift+上下箭头\n\ndelete line)删除一行代码 ctrl+y",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"更改某个变量全部代码",frontmatter:{title:"更改某个变量全部代码",date:"2022-03-18T00:52:50.000Z",permalink:"/pages/2d9020/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/39.%E6%9B%B4%E6%94%B9%E6%9F%90%E4%B8%AA%E5%8F%98%E9%87%8F%E5%85%A8%E9%83%A8%E4%BB%A3%E7%A0%81.html",relativePath:"后端/05.Python/39.更改某个变量全部代码.md",key:"v-9421b7d2",path:"/pages/2d9020/",headersStr:null,content:"# 更改某个变量全部代码\n\n右键变量 refactor -rename\n\n快捷键shift+F6",normalizedContent:"# 更改某个变量全部代码\n\n右键变量 refactor -rename\n\n快捷键shift+f6",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"range 的使用",frontmatter:{title:"range 的使用",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/6bc87f/",categories:["后端","Python"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/05.Python/37.range%20%E7%9A%84%E4%BD%BF%E7%94%A8.html",relativePath:"后端/05.Python/37.range 的使用.md",key:"v-88a93a4e",path:"/pages/6bc87f/",headersStr:null,content:"# range 的使用\n\nrange 内置类用来生成指定区间的整数序列（列表）\n\n#for循环打印1-10\nfor i in range(1,11):   #range不包含输出最后一位\n    print(i)\n#输出 1,2,3,4,5,6,7,8,9,10  \n#本质上输出是一个列表 [1,2,3,4,5,6,7,8,9,10]  遍历一遍数据一个一个输出\n\n\n1\n2\n3\n4\n5\n",normalizedContent:"# range 的使用\n\nrange 内置类用来生成指定区间的整数序列（列表）\n\n#for循环打印1-10\nfor i in range(1,11):   #range不包含输出最后一位\n    print(i)\n#输出 1,2,3,4,5,6,7,8,9,10  \n#本质上输出是一个列表 [1,2,3,4,5,6,7,8,9,10]  遍历一遍数据一个一个输出\n\n\n1\n2\n3\n4\n5\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Beautifulsoup4",frontmatter:{title:"Beautifulsoup4",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/f14379/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/02.Beautifulsoup4.html",relativePath:"后端/07.Python模块/02.Beautifulsoup4.md",key:"v-c0ebeed8",path:"/pages/f14379/",headers:[{level:2,title:"构造方法",slug:"构造方法",normalizedTitle:"构造方法",charIndex:791},{level:2,title:"四大对象种类",slug:"四大对象种类",normalizedTitle:"四大对象种类",charIndex:1052},{level:2,title:"遍历文档树",slug:"遍历文档树",normalizedTitle:"遍历文档树",charIndex:1349},{level:2,title:"搜索文档树",slug:"搜索文档树",normalizedTitle:"搜索文档树",charIndex:4079},{level:3,title:"字符串",slug:"字符串",normalizedTitle:"字符串",charIndex:2172},{level:3,title:"正则表达式",slug:"正则表达式",normalizedTitle:"正则表达式",charIndex:4377},{level:3,title:"列表",slug:"列表",normalizedTitle:"列表",charIndex:1215},{level:3,title:"keyword   属性值",slug:"keyword-属性值",normalizedTitle:"keyword   属性值",charIndex:null},{level:3,title:"按CSS搜索",slug:"按css搜索",normalizedTitle:"按css搜索",charIndex:5077},{level:3,title:"String 参数",slug:"string-参数",normalizedTitle:"string 参数",charIndex:6183},{level:3,title:"limit 参数",slug:"limit-参数",normalizedTitle:"limit 参数",charIndex:6784},{level:3,title:"recursive 参数",slug:"recursive-参数",normalizedTitle:"recursive 参数",charIndex:7112},{level:3,title:"父辈节点",slug:"父辈节点",normalizedTitle:"父辈节点",charIndex:3648},{level:2,title:"修改文档树",slug:"修改文档树",normalizedTitle:"修改文档树",charIndex:7396},{level:3,title:"修改tag的名称和属性",slug:"修改tag的名称和属性",normalizedTitle:"修改tag的名称和属性",charIndex:7447},{level:3,title:"修改 .string",slug:"修改-string",normalizedTitle:"修改 .string",charIndex:7810},{level:3,title:"append()",slug:"append",normalizedTitle:"append()",charIndex:8140},{level:3,title:"clear()",slug:"clear",normalizedTitle:"clear()",charIndex:8349},{level:3,title:"extract()",slug:"extract",normalizedTitle:"extract()",charIndex:8577},{level:3,title:"decompose()",slug:"decompose",normalizedTitle:"decompose()",charIndex:8930},{level:3,title:"replace_with()",slug:"replace-with",normalizedTitle:"replace_with()",charIndex:9196},{level:2,title:"bs4的简单使用",slug:"bs4的简单使用",normalizedTitle:"bs4的简单使用",charIndex:9582}],headersStr:"构造方法 四大对象种类 遍历文档树 搜索文档树 字符串 正则表达式 列表 keyword   属性值 按CSS搜索 String 参数 limit 参数 recursive 参数 父辈节点 修改文档树 修改tag的名称和属性 修改 .string append() clear() extract() decompose() replace_with() bs4的简单使用",content:'# Beautifulsoup4\n\nBeautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.\n\nBeautifulsoup 四种解析器\n\n解析器             使用方法                                                         优势                              劣势\nPython标准库       BeautifulSoup(markup, "html.parser")                         Python的内置标准库执行速度适中文档容错能力强       Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差\nlxml HTML 解析器   BeautifulSoup(markup, "lxml")                                速度快文档容错能力强                      需要安装C语言库\nlxml XML 解析器    BeautifulSoup(markup, ["lxml-xml"])``BeautifulSoup(markup,   速度快唯一支持XML的解析器                  需要安装C语言库\n                "xml")\nhtml5lib        BeautifulSoup(markup, "html5lib")                            最好的容错性以浏览器的方式解析文档生成HTML5格式的文档   速度慢不依赖外部扩展\n\n\n# 构造方法\n\n * BeautifulSoup(html,"html.parser")\n\nurl="http://wsjkw.sc.gov.cn/scwsjkw/gzbd/fyzt.shtml"\nres=requests.get(url)\nres.encoding="utf-8"  #转为utf-u解码\nhtml=res.text  #将源码转为文本赋值给html\nsoup=BeautifulSoup(html,\'html.parser\')   #对象化html     "lxml"\n\n\n1\n2\n3\n4\n5\n\n\n\n# 四大对象种类\n\n 1. Tag HTML中的一个个标签\n    1. Name（标签名） 通过 .name获取\n    2. Attributes（属性） 字典获取 tag["class"] tag.attrs 可以获取整个字典\n    3. Multi-valued attributes（多值属性） 也是字典获取 返回一个列表\n 2. NavigableString 获取标签内部的文字用 .string 即可\n 3. BeautifulSoup 表示的是一个文档的内容\n 4. Comment 是一个特殊类型的 NavigableString 对象，其输出的内容不包括注释符号。\n\n\n# 遍历文档树\n\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html_doc, \'html.parser\')\n\n\n1\n2\n\n\n * soup.tag 使用标签名进行遍历 soup.head soup.h2\n\n * Tag.contents 获取该标签中所有元素\n   \n   * head_tag = soup.head\n     head_tag\n     # <head><title>The Dormouse\'s story</title></head>\n     \n     head_tag.contents\n     [<title>The Dormouse\'s story</title>]\n     \n     title_tag = head_tag.contents[0]\n     title_tag\n     # <title>The Dormouse\'s story</title>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     \n\n * Tag.children 对tag的子节点进行循环\n   \n   * for child in title_tag.children:\n         print(child)\n     \n     \n     \n     1\n     2\n     3\n     \n\n * Tag.descendants 可以对所有tag的子孙节点进行递归循环\n   \n   * for child in head_tag.descendants:\n         print(child)\n     \n     \n     1\n     2\n     \n\n * soup.strings 获取tag中的字符串包含子标签\n   \n   * for string in soup.strings:\n         print(repr(string))\n         # u"The Dormouse\'s story"\n         # u\'\\n\\n\'\n         # u"The Dormouse\'s story"\n         # u\'\\n\\n\'\n         # u\'Once upon a time there were three little sisters; and their names were\\n\'\n         # u\'Elsie\'\n         # u\',\\n\'\n         # u\'Lacie\'\n         # u\' and\\n\'\n         # u\'Tillie\'\n         # u\';\\nand they lived at the bottom of a well.\'\n         # u\'\\n\\n\'\n         # u\'...\'\n         # u\'\\n\'\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     \n\n * soup.stripped_strings 输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容:\n   \n   * for string in soup.stripped_strings:\n         print(repr(string))\n         # u"The Dormouse\'s story"\n         # u"The Dormouse\'s story"\n         # u\'Once upon a time there were three little sisters; and their names were\'\n         # u\'Elsie\'\n         # u\',\'\n         # u\'Lacie\'\n         # u\'and\'\n         # u\'Tillie\'\n         # u\';\\nand they lived at the bottom of a well.\'\n         # u\'...\'\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     \n\n * Tag.parent 获取某个元素的父节点\n   \n   * title_tag = soup.title\n     title_tag\n     # <title>The Dormouse\'s story</title>\n     title_tag.parent\n     # <head><title>The Dormouse\'s story</title></head>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * Tag.parents 可以递归得到元素的所有父辈节点\n   \n   * link = soup.a\n     link\n     # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>\n     for parent in link.parents:\n         if parent is None:\n             print(parent)\n         else:\n             print(parent.name)\n     # p\n     # body\n     # html\n     # [document]\n     # None\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     \n\n\n# 搜索文档树\n\n * soud.find() 搜索第一个符合条件的标签\n * soud.find_all( name , attrs , recursive , string , **kwargs ) 搜索所有符合条件的标签\n\n唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.\n\n\n# 字符串\n\n最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容\n\nsoup.find_all(\'b\')\n# [<b>The Dormouse\'s story</b>]\n\n\n1\n2\n\n\n\n# 正则表达式\n\n如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.\n\nimport re\nfor tag in soup.find_all(re.compile("^b")):\n    print(tag.name)\n# body\n# b\n\n\n1\n2\n3\n4\n5\n\n\n\n# 列表\n\n如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回\n\nsoup.find_all(["a", "b"])\n# [<b>The Dormouse\'s story</b>,\n#  <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,\n#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]\n\n\n1\n2\n3\n4\n5\n\n\n\n# keyword 属性值\n\n如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索\n\nsoup.find_all(id=\'link2\') #搜索所有id为link2的标签\n# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]\n\n\n1\n2\n\n\n\n# 按CSS搜索\n\nclass 在Python中是保留字,使用 class 做参数会导致语法错误.从Beautiful Soup的4.1.1版本开始,可以通过 class_ 参数搜索有指定CSS类名的tag\n\nsoup.find_all("a", class_="sister")\n# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,\n#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]\n\n\n1\n2\n3\n4\n\n\nclass 属性是 多值属性 .按照CSS类名搜索tag时,可以分别搜索tag中的每个CSS类名\n\ncss_soup = BeautifulSoup(\'<p class="body strikeout"></p>\')\ncss_soup.find_all-0-0-("p", class_="strikeout")\n# [<p class="body strikeout"></p>]\n\ncss_soup.find_all("p", class_="body")\n# [<p class="body strikeout"></p>]\n\n\n1\n2\n3\n4\n5\n6\n\n\n搜索 class 属性时也可以通过CSS值完全匹配\n\ncss_soup.find_all("p", class_="body strikeout")\n# [<p class="body strikeout"></p>]\n\n\n1\n2\n\n\n完全匹配 class 的值时,如果CSS类名的顺序与实际不符,将搜索不到结果\n\nsoup.find_all("a", attrs={"class": "sister"})\n# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,\n#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]\n\n\n1\n2\n3\n4\n\n\n\n# String 参数\n\n通过 string 参数可以搜搜文档中的字符串内容\n\nsoup.find_all(string="Elsie")\n# [u\'Elsie\']\n\nsoup.find_all(string=["Tillie", "Elsie", "Lacie"])\n# [u\'Elsie\', u\'Lacie\', u\'Tillie\']\n\nsoup.find_all(string=re.compile("Dormouse"))\n[u"The Dormouse\'s story", u"The Dormouse\'s story"]\n\ndef is_the_only_string_within_a_tag(s):\n    ""Return True if this string is the only child of its parent tag.""\n    return (s == s.parent.string)\n\nsoup.find_all(string=is_the_only_string_within_a_tag)\n# [u"The Dormouse\'s story", u"The Dormouse\'s story", u\'Elsie\', u\'Lacie\', u\'Tillie\', u\'...\']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# limit 参数\n\nfind_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果.\n\nsoup.find_all("a", limit=2)\n# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]\n\n\n1\n2\n3\n\n\n\n# recursive 参数\n\n调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False\n\n\n# 父辈节点\n\nfind_all() 和 find() 只搜索当前节点的所有子节点,孙子节点等. find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同,搜索文档搜索文档包含的内容\n\n * soud.find_parents()\n * soud.find_parents()\n\n\n# 修改文档树\n\nBeautiful Soup的强项是文档树的搜索,但同时也可以方便的修改文档树\n\n\n# 修改tag的名称和属性\n\n重命名一个tag,改变属性的值,添加或删除属性\n\nsoup = BeautifulSoup(\'<b class="boldest">Extremely bold</b>\')\ntag = soup.b\n\ntag.name = "blockquote"\ntag[\'class\'] = \'verybold\'\ntag[\'id\'] = 1\ntag\n# <blockquote class="verybold" id="1">Extremely bold</blockquote>\n\ndel tag[\'class\']\ndel tag[\'id\']\ntag\n# <blockquote>Extremely bold</blockquote>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 修改 .string\n\n给tag的 .string 属性赋值,就相当于用当前的内容替代了原来的内容\n\n如果当前的tag包含了其它tag,那么给它的 .string 属性赋值会覆盖掉原有的所有内容包括子tag\n\nmarkup = \'<a href="http://example.com/">I linked to <i>example.com</i></a>\'\nsoup = BeautifulSoup(markup)\n\ntag = soup.a\ntag.string = "New link text."\ntag\n# <a href="http://example.com/">New link text.</a>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# append()\n\nTag.append() 方法想tag中添加内容\n\nsoup = BeautifulSoup("<a>Foo</a>")\nsoup.a.append("Bar")\n\nsoup\n# <html><head></head><body><a>FooBar</a></body></html>\nsoup.a.contents\n# [u\'Foo\', u\'Bar\']\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# clear()\n\nTag.clear() 方法移除当前tag的内容\n\nmarkup = \'<a href="http://example.com/">I linked to <i>example.com</i></a>\'\nsoup = BeautifulSoup(markup)\ntag = soup.a\n\ntag.clear()\ntag\n# <a href="http://example.com/"></a>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# extract()\n\nPageElement.extract() 方法将当前tag移除文档树,并作为方法结果返回\n\nmarkup = \'<a href="http://example.com/">I linked to <i>example.com</i></a>\'\nsoup = BeautifulSoup(markup)\na_tag = soup.a\n\ni_tag = soup.i.extract()\n\na_tag\n# <a href="http://example.com/">I linked to</a>\n\ni_tag\n# <i>example.com</i>\n\nprint(i_tag.parent)\nNone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# decompose()\n\nTag.decompose() 方法将当前节点移除文档树并完全销毁\n\nmarkup = \'<a href="http://example.com/">I linked to <i>example.com</i></a>\'\nsoup = BeautifulSoup(markup)\na_tag = soup.a\n\nsoup.i.decompose()\n\na_tag\n# <a href="http://example.com/">I linked to</a>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# replace_with()\n\nPageElement.replace_with() 方法移除文档树中的某段内容,并用新tag或文本节点替代它:\n\nmarkup = \'<a href="http://example.com/">I linked to <i>example.com</i></a>\'\nsoup = BeautifulSoup(markup)\na_tag = soup.a\n\nnew_tag = soup.new_tag("b")\nnew_tag.string = "example.net"\na_tag.i.replace_with(new_tag)\n\na_tag\n# <a href="http://example.com/">I linked to <b>example.net</b></a>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# bs4的简单使用\n\nfrom bs4 import BeautifulSoup\nimport requests\n\nurl="http://wsjkw.sc.gov.cn/scwsjkw/gzbd/fyzt.shtml"\nres=requests.get(url)\nres.encoding="utf-8"  #转为utf-u解码\nhtml=res.text  #将源码转为文本赋值给html\nsoup=BeautifulSoup(html,\'html.parser\')   #对象化html     "lxml"\nh =soup.find(\'h2\')   #寻找文本中 h2标签的内容\nprint(h)\nh = soup.find(\'h2\').text    #只获取h2标签中的内容\nprint(h)\n\na = soup.find(\'a\') #会寻找文本中第一个a标签\nprint(a)\nprint(type(a))\na = soup.find(\'a\').attrs   #获取a标签中  href以及 target  存放在字典中\nprint(a)\nprint(a[\'href\'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',normalizedContent:'# beautifulsoup4\n\nbeautiful soup 是一个可以从html或xml文件中提取数据的python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.beautiful soup会帮你节省数小时甚至数天的工作时间.\n\nbeautifulsoup 四种解析器\n\n解析器             使用方法                                                         优势                              劣势\npython标准库       beautifulsoup(markup, "html.parser")                         python的内置标准库执行速度适中文档容错能力强       python 2.7.3 or 3.2.2)前 的版本中文档容错能力差\nlxml html 解析器   beautifulsoup(markup, "lxml")                                速度快文档容错能力强                      需要安装c语言库\nlxml xml 解析器    beautifulsoup(markup, ["lxml-xml"])``beautifulsoup(markup,   速度快唯一支持xml的解析器                  需要安装c语言库\n                "xml")\nhtml5lib        beautifulsoup(markup, "html5lib")                            最好的容错性以浏览器的方式解析文档生成html5格式的文档   速度慢不依赖外部扩展\n\n\n# 构造方法\n\n * beautifulsoup(html,"html.parser")\n\nurl="http://wsjkw.sc.gov.cn/scwsjkw/gzbd/fyzt.shtml"\nres=requests.get(url)\nres.encoding="utf-8"  #转为utf-u解码\nhtml=res.text  #将源码转为文本赋值给html\nsoup=beautifulsoup(html,\'html.parser\')   #对象化html     "lxml"\n\n\n1\n2\n3\n4\n5\n\n\n\n# 四大对象种类\n\n 1. tag html中的一个个标签\n    1. name（标签名） 通过 .name获取\n    2. attributes（属性） 字典获取 tag["class"] tag.attrs 可以获取整个字典\n    3. multi-valued attributes（多值属性） 也是字典获取 返回一个列表\n 2. navigablestring 获取标签内部的文字用 .string 即可\n 3. beautifulsoup 表示的是一个文档的内容\n 4. comment 是一个特殊类型的 navigablestring 对象，其输出的内容不包括注释符号。\n\n\n# 遍历文档树\n\nfrom bs4 import beautifulsoup\nsoup = beautifulsoup(html_doc, \'html.parser\')\n\n\n1\n2\n\n\n * soup.tag 使用标签名进行遍历 soup.head soup.h2\n\n * tag.contents 获取该标签中所有元素\n   \n   * head_tag = soup.head\n     head_tag\n     # <head><title>the dormouse\'s story</title></head>\n     \n     head_tag.contents\n     [<title>the dormouse\'s story</title>]\n     \n     title_tag = head_tag.contents[0]\n     title_tag\n     # <title>the dormouse\'s story</title>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     \n\n * tag.children 对tag的子节点进行循环\n   \n   * for child in title_tag.children:\n         print(child)\n     \n     \n     \n     1\n     2\n     3\n     \n\n * tag.descendants 可以对所有tag的子孙节点进行递归循环\n   \n   * for child in head_tag.descendants:\n         print(child)\n     \n     \n     1\n     2\n     \n\n * soup.strings 获取tag中的字符串包含子标签\n   \n   * for string in soup.strings:\n         print(repr(string))\n         # u"the dormouse\'s story"\n         # u\'\\n\\n\'\n         # u"the dormouse\'s story"\n         # u\'\\n\\n\'\n         # u\'once upon a time there were three little sisters; and their names were\\n\'\n         # u\'elsie\'\n         # u\',\\n\'\n         # u\'lacie\'\n         # u\' and\\n\'\n         # u\'tillie\'\n         # u\';\\nand they lived at the bottom of a well.\'\n         # u\'\\n\\n\'\n         # u\'...\'\n         # u\'\\n\'\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     \n\n * soup.stripped_strings 输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容:\n   \n   * for string in soup.stripped_strings:\n         print(repr(string))\n         # u"the dormouse\'s story"\n         # u"the dormouse\'s story"\n         # u\'once upon a time there were three little sisters; and their names were\'\n         # u\'elsie\'\n         # u\',\'\n         # u\'lacie\'\n         # u\'and\'\n         # u\'tillie\'\n         # u\';\\nand they lived at the bottom of a well.\'\n         # u\'...\'\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     \n\n * tag.parent 获取某个元素的父节点\n   \n   * title_tag = soup.title\n     title_tag\n     # <title>the dormouse\'s story</title>\n     title_tag.parent\n     # <head><title>the dormouse\'s story</title></head>\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * tag.parents 可以递归得到元素的所有父辈节点\n   \n   * link = soup.a\n     link\n     # <a class="sister" href="http://example.com/elsie" id="link1">elsie</a>\n     for parent in link.parents:\n         if parent is none:\n             print(parent)\n         else:\n             print(parent.name)\n     # p\n     # body\n     # html\n     # [document]\n     # none\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     \n\n\n# 搜索文档树\n\n * soud.find() 搜索第一个符合条件的标签\n * soud.find_all( name , attrs , recursive , string , **kwargs ) 搜索所有符合条件的标签\n\n唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.\n\n\n# 字符串\n\n最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,beautiful soup会查找与字符串完整匹配的内容\n\nsoup.find_all(\'b\')\n# [<b>the dormouse\'s story</b>]\n\n\n1\n2\n\n\n\n# 正则表达式\n\n如果传入正则表达式作为参数,beautiful soup会通过正则表达式的 match() 来匹配内容.\n\nimport re\nfor tag in soup.find_all(re.compile("^b")):\n    print(tag.name)\n# body\n# b\n\n\n1\n2\n3\n4\n5\n\n\n\n# 列表\n\n如果传入列表参数,beautiful soup会将与列表中任一元素匹配的内容返回\n\nsoup.find_all(["a", "b"])\n# [<b>the dormouse\'s story</b>,\n#  <a class="sister" href="http://example.com/elsie" id="link1">elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">lacie</a>,\n#  <a class="sister" href="http://example.com/tillie" id="link3">tillie</a>]\n\n\n1\n2\n3\n4\n5\n\n\n\n# keyword 属性值\n\n如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索\n\nsoup.find_all(id=\'link2\') #搜索所有id为link2的标签\n# [<a class="sister" href="http://example.com/lacie" id="link2">lacie</a>]\n\n\n1\n2\n\n\n\n# 按css搜索\n\nclass 在python中是保留字,使用 class 做参数会导致语法错误.从beautiful soup的4.1.1版本开始,可以通过 class_ 参数搜索有指定css类名的tag\n\nsoup.find_all("a", class_="sister")\n# [<a class="sister" href="http://example.com/elsie" id="link1">elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">lacie</a>,\n#  <a class="sister" href="http://example.com/tillie" id="link3">tillie</a>]\n\n\n1\n2\n3\n4\n\n\nclass 属性是 多值属性 .按照css类名搜索tag时,可以分别搜索tag中的每个css类名\n\ncss_soup = beautifulsoup(\'<p class="body strikeout"></p>\')\ncss_soup.find_all-0-0-("p", class_="strikeout")\n# [<p class="body strikeout"></p>]\n\ncss_soup.find_all("p", class_="body")\n# [<p class="body strikeout"></p>]\n\n\n1\n2\n3\n4\n5\n6\n\n\n搜索 class 属性时也可以通过css值完全匹配\n\ncss_soup.find_all("p", class_="body strikeout")\n# [<p class="body strikeout"></p>]\n\n\n1\n2\n\n\n完全匹配 class 的值时,如果css类名的顺序与实际不符,将搜索不到结果\n\nsoup.find_all("a", attrs={"class": "sister"})\n# [<a class="sister" href="http://example.com/elsie" id="link1">elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">lacie</a>,\n#  <a class="sister" href="http://example.com/tillie" id="link3">tillie</a>]\n\n\n1\n2\n3\n4\n\n\n\n# string 参数\n\n通过 string 参数可以搜搜文档中的字符串内容\n\nsoup.find_all(string="elsie")\n# [u\'elsie\']\n\nsoup.find_all(string=["tillie", "elsie", "lacie"])\n# [u\'elsie\', u\'lacie\', u\'tillie\']\n\nsoup.find_all(string=re.compile("dormouse"))\n[u"the dormouse\'s story", u"the dormouse\'s story"]\n\ndef is_the_only_string_within_a_tag(s):\n    ""return true if this string is the only child of its parent tag.""\n    return (s == s.parent.string)\n\nsoup.find_all(string=is_the_only_string_within_a_tag)\n# [u"the dormouse\'s story", u"the dormouse\'s story", u\'elsie\', u\'lacie\', u\'tillie\', u\'...\']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# limit 参数\n\nfind_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与sql中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果.\n\nsoup.find_all("a", limit=2)\n# [<a class="sister" href="http://example.com/elsie" id="link1">elsie</a>,\n#  <a class="sister" href="http://example.com/lacie" id="link2">lacie</a>]\n\n\n1\n2\n3\n\n\n\n# recursive 参数\n\n调用tag的 find_all() 方法时,beautiful soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=false\n\n\n# 父辈节点\n\nfind_all() 和 find() 只搜索当前节点的所有子节点,孙子节点等. find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同,搜索文档搜索文档包含的内容\n\n * soud.find_parents()\n * soud.find_parents()\n\n\n# 修改文档树\n\nbeautiful soup的强项是文档树的搜索,但同时也可以方便的修改文档树\n\n\n# 修改tag的名称和属性\n\n重命名一个tag,改变属性的值,添加或删除属性\n\nsoup = beautifulsoup(\'<b class="boldest">extremely bold</b>\')\ntag = soup.b\n\ntag.name = "blockquote"\ntag[\'class\'] = \'verybold\'\ntag[\'id\'] = 1\ntag\n# <blockquote class="verybold" id="1">extremely bold</blockquote>\n\ndel tag[\'class\']\ndel tag[\'id\']\ntag\n# <blockquote>extremely bold</blockquote>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 修改 .string\n\n给tag的 .string 属性赋值,就相当于用当前的内容替代了原来的内容\n\n如果当前的tag包含了其它tag,那么给它的 .string 属性赋值会覆盖掉原有的所有内容包括子tag\n\nmarkup = \'<a href="http://example.com/">i linked to <i>example.com</i></a>\'\nsoup = beautifulsoup(markup)\n\ntag = soup.a\ntag.string = "new link text."\ntag\n# <a href="http://example.com/">new link text.</a>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# append()\n\ntag.append() 方法想tag中添加内容\n\nsoup = beautifulsoup("<a>foo</a>")\nsoup.a.append("bar")\n\nsoup\n# <html><head></head><body><a>foobar</a></body></html>\nsoup.a.contents\n# [u\'foo\', u\'bar\']\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# clear()\n\ntag.clear() 方法移除当前tag的内容\n\nmarkup = \'<a href="http://example.com/">i linked to <i>example.com</i></a>\'\nsoup = beautifulsoup(markup)\ntag = soup.a\n\ntag.clear()\ntag\n# <a href="http://example.com/"></a>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# extract()\n\npageelement.extract() 方法将当前tag移除文档树,并作为方法结果返回\n\nmarkup = \'<a href="http://example.com/">i linked to <i>example.com</i></a>\'\nsoup = beautifulsoup(markup)\na_tag = soup.a\n\ni_tag = soup.i.extract()\n\na_tag\n# <a href="http://example.com/">i linked to</a>\n\ni_tag\n# <i>example.com</i>\n\nprint(i_tag.parent)\nnone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# decompose()\n\ntag.decompose() 方法将当前节点移除文档树并完全销毁\n\nmarkup = \'<a href="http://example.com/">i linked to <i>example.com</i></a>\'\nsoup = beautifulsoup(markup)\na_tag = soup.a\n\nsoup.i.decompose()\n\na_tag\n# <a href="http://example.com/">i linked to</a>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# replace_with()\n\npageelement.replace_with() 方法移除文档树中的某段内容,并用新tag或文本节点替代它:\n\nmarkup = \'<a href="http://example.com/">i linked to <i>example.com</i></a>\'\nsoup = beautifulsoup(markup)\na_tag = soup.a\n\nnew_tag = soup.new_tag("b")\nnew_tag.string = "example.net"\na_tag.i.replace_with(new_tag)\n\na_tag\n# <a href="http://example.com/">i linked to <b>example.net</b></a>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# bs4的简单使用\n\nfrom bs4 import beautifulsoup\nimport requests\n\nurl="http://wsjkw.sc.gov.cn/scwsjkw/gzbd/fyzt.shtml"\nres=requests.get(url)\nres.encoding="utf-8"  #转为utf-u解码\nhtml=res.text  #将源码转为文本赋值给html\nsoup=beautifulsoup(html,\'html.parser\')   #对象化html     "lxml"\nh =soup.find(\'h2\')   #寻找文本中 h2标签的内容\nprint(h)\nh = soup.find(\'h2\').text    #只获取h2标签中的内容\nprint(h)\n\na = soup.find(\'a\') #会寻找文本中第一个a标签\nprint(a)\nprint(type(a))\na = soup.find(\'a\').attrs   #获取a标签中  href以及 target  存放在字典中\nprint(a)\nprint(a[\'href\'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"request",frontmatter:{title:"request",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/cf7131/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/01.request.html",relativePath:"后端/07.Python模块/01.request.md",key:"v-689f8518",path:"/pages/cf7131/",headers:[{level:2,title:"发送请求",slug:"发送请求",normalizedTitle:"发送请求",charIndex:343},{level:2,title:"代理",slug:"代理",normalizedTitle:"代理",charIndex:904},{level:2,title:"定制请求头 header",slug:"定制请求头-header",normalizedTitle:"定制请求头 header",charIndex:1085},{level:2,title:"Cookie",slug:"cookie",normalizedTitle:"cookie",charIndex:1992},{level:2,title:"使用Corntab定时调度爬虫",slug:"使用corntab定时调度爬虫",normalizedTitle:"使用corntab定时调度爬虫",charIndex:2359}],headersStr:"发送请求 代理 定制请求头 header Cookie 使用Corntab定时调度爬虫",content:"# request\n\n * request.urlopen(url) 打开一个url获取Response 对象\n\n * res.getirl() 获取主机地址\n\n * res.getcode() 获取状态码\n   \n   * 200为成功\n   * 3xx发生了重定向\n   * 4xx访问资源有问题\n   * 5xx内部错误\n\n * res.info() 获取响应头\n\n * res.read() 获取的是字节形式的内容 返回一文本对象\n   \n   * textl.decode(\"utf-8\") 需要指定编码才能正确显示\n\n * res.json() 如果返回为json数据直接解码\n\n * res.encoding = \"utf-8\" 将Response 以此编码读取\n\n\n# 发送请求\n\n * res.get(url) 发送get请求\n   \n   * #地址传递值\n     requests.get(http://httpbin.org/get?name=gemey&age=22)\n     \n     #字典传递\n     data = {\n         'name': 'tom',\n         'age': 20\n     }\n     \n     response = requests.get('http://httpbin.org/get', params=data)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     \n\n * res.post(url) 发送post请求\n   \n   * #post请求通过字典或者 json字符串传递参数\n     data = {'name':'tom','age':'22'}\n     response = requests.post('http://httpbin.org/post', data=data)\n     \n     \n     1\n     2\n     3\n     \n\n\n# 代理\n\n同添加headers方法，代理参数也要是一个dict 属性名为proxies\n\nproxy = {\n    'http': '120.25.253.234:812',\n    'https' '163.125.222.244:8123'\n}\nreq = requests.get(url, proxies=proxy)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 定制请求头 header\n\n添加header信息,这是最基本的反爬的措施 有一些网站拥有反爬技术,我们需要模拟真实浏览器的包头进行访问发生\n\n为请求添加 HTTP 头部，只要简单地传递一个 dict 给 headers 参数就可以了。\n\n * request.Request(url,headers=header)\n   \n   * #添加header信息,这是最基本的反爬的措施\n     url =\"http://www.dianping.com/\"  #有一些网站拥有反爬技术,我们需要模拟真实浏览器的包头进行访问发生\n     header={\n        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\"\n     }   #需要一个字典存放包头\n     req=request.Request(url,headers=header)  #requests需要一个网站和包头\n     res=request.urlopen(req)\n     \n     print(res.geturl())  #获取主机地址\n     print(res.getcode())  #获取请求状态码   200为成功  3xx发生了重定向  4xx访问资源有问题   5xx内部错误\n     print(res.info())  #获取响应头\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     \n\n * 获取响应头\n   \n   * r.headers['Content-Type']\n     r.headers.get('content-type')  #根据key获取响应头\n     \n     \n     1\n     2\n     \n\n\n# Cookie\n\n如果某个响应中包含一些 cookie，你可以快速访问它们\n\n * res.cookies[\"键名\"] 获取cookies\n\nurl ='http://example.com/some/cookie/setting/url'\nr = requests.get(url)\nr.cookies['example_cookie_name']  #获取指定key的cookies\n\n\n1\n2\n3\n\n * 发送cookies 通过请求时传递cookies属性\n\nurl = 'http://httpbin.org/cookies'\ncookies = dict(cookies_are='working')\nr = requests.get(url, cookies=cookies)\nr.text\n\n\n1\n2\n3\n4\n\n\n\n# 使用Corntab定时调度爬虫\n\n在linux上安装chrome\n\ncd /etc/yum.repos.d/\ntouch google-chrome.repo\n\n\n1\n2\n\n\n添加chrome源\n\n[google-chrome]\nname=google-chrome\nbaseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch\nenabled=1\ngpgcheck=1\ngpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub\n\n\n1\n2\n3\n4\n5\n6\n\n\nyum -y install google-chrome-stable --nogpgcheck\n\n\n1\n\n\ncrontab -e 编写python执行",normalizedContent:"# request\n\n * request.urlopen(url) 打开一个url获取response 对象\n\n * res.getirl() 获取主机地址\n\n * res.getcode() 获取状态码\n   \n   * 200为成功\n   * 3xx发生了重定向\n   * 4xx访问资源有问题\n   * 5xx内部错误\n\n * res.info() 获取响应头\n\n * res.read() 获取的是字节形式的内容 返回一文本对象\n   \n   * textl.decode(\"utf-8\") 需要指定编码才能正确显示\n\n * res.json() 如果返回为json数据直接解码\n\n * res.encoding = \"utf-8\" 将response 以此编码读取\n\n\n# 发送请求\n\n * res.get(url) 发送get请求\n   \n   * #地址传递值\n     requests.get(http://httpbin.org/get?name=gemey&age=22)\n     \n     #字典传递\n     data = {\n         'name': 'tom',\n         'age': 20\n     }\n     \n     response = requests.get('http://httpbin.org/get', params=data)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     \n\n * res.post(url) 发送post请求\n   \n   * #post请求通过字典或者 json字符串传递参数\n     data = {'name':'tom','age':'22'}\n     response = requests.post('http://httpbin.org/post', data=data)\n     \n     \n     1\n     2\n     3\n     \n\n\n# 代理\n\n同添加headers方法，代理参数也要是一个dict 属性名为proxies\n\nproxy = {\n    'http': '120.25.253.234:812',\n    'https' '163.125.222.244:8123'\n}\nreq = requests.get(url, proxies=proxy)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 定制请求头 header\n\n添加header信息,这是最基本的反爬的措施 有一些网站拥有反爬技术,我们需要模拟真实浏览器的包头进行访问发生\n\n为请求添加 http 头部，只要简单地传递一个 dict 给 headers 参数就可以了。\n\n * request.request(url,headers=header)\n   \n   * #添加header信息,这是最基本的反爬的措施\n     url =\"http://www.dianping.com/\"  #有一些网站拥有反爬技术,我们需要模拟真实浏览器的包头进行访问发生\n     header={\n        \"user-agent\":\"mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/89.0.4389.114 safari/537.36\"\n     }   #需要一个字典存放包头\n     req=request.request(url,headers=header)  #requests需要一个网站和包头\n     res=request.urlopen(req)\n     \n     print(res.geturl())  #获取主机地址\n     print(res.getcode())  #获取请求状态码   200为成功  3xx发生了重定向  4xx访问资源有问题   5xx内部错误\n     print(res.info())  #获取响应头\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     \n\n * 获取响应头\n   \n   * r.headers['content-type']\n     r.headers.get('content-type')  #根据key获取响应头\n     \n     \n     1\n     2\n     \n\n\n# cookie\n\n如果某个响应中包含一些 cookie，你可以快速访问它们\n\n * res.cookies[\"键名\"] 获取cookies\n\nurl ='http://example.com/some/cookie/setting/url'\nr = requests.get(url)\nr.cookies['example_cookie_name']  #获取指定key的cookies\n\n\n1\n2\n3\n\n * 发送cookies 通过请求时传递cookies属性\n\nurl = 'http://httpbin.org/cookies'\ncookies = dict(cookies_are='working')\nr = requests.get(url, cookies=cookies)\nr.text\n\n\n1\n2\n3\n4\n\n\n\n# 使用corntab定时调度爬虫\n\n在linux上安装chrome\n\ncd /etc/yum.repos.d/\ntouch google-chrome.repo\n\n\n1\n2\n\n\n添加chrome源\n\n[google-chrome]\nname=google-chrome\nbaseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch\nenabled=1\ngpgcheck=1\ngpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub\n\n\n1\n2\n3\n4\n5\n6\n\n\nyum -y install google-chrome-stable --nogpgcheck\n\n\n1\n\n\ncrontab -e 编写python执行",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"jieba",frontmatter:{title:"jieba",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/4ab7a0/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/04.jieba.html",relativePath:"后端/07.Python模块/04.jieba.md",key:"v-94fc8dd8",path:"/pages/4ab7a0/",headers:[{level:2,title:"基于 TF-IDF 算法的关键词抽取",slug:"基于-tf-idf-算法的关键词抽取",normalizedTitle:"基于 tf-idf 算法的关键词抽取",charIndex:888},{level:2,title:"基于 TextRank 算法的关键词抽取",slug:"基于-textrank-算法的关键词抽取",normalizedTitle:"基于 textrank 算法的关键词抽取",charIndex:1694}],headersStr:"基于 TF-IDF 算法的关键词抽取 基于 TextRank 算法的关键词抽取",content:'# jieba\n\n“结巴”中文分词\n\n * jieba.cut(str) cut_all 参数用来控制是否采用全模式 use_paddle 参数用来控制是否使用paddle模式下的分词模式，paddle模式采用延迟加载方式\n * jieba.cut_for_search(str) 搜索引擎模式 构建倒排索引的分词，粒度比较细\n * jieba.lcut(str) 直接返回 list 结果\n\n# encoding=utf-8\nimport jieba\n\njieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\nstrs=["我来到北京清华大学","乒乓球拍卖完了","中国科学技术大学"]\nfor str in strs:\n    seg_list = jieba.cut(str,use_paddle=True) # 使用paddle模式\n    print("Paddle Mode: " + \'/\'.join(list(seg_list)))\n\nseg_list = jieba.cut("我来到北京清华大学", cut_all=True)\nprint("Full Mode: " + "/ ".join(seg_list))  # 全模式\n\nseg_list = jieba.cut("我来到北京清华大学", cut_all=False)\nprint("Default Mode: " + "/ ".join(seg_list))  # 精确模式\n\nseg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式\nprint(", ".join(seg_list))\n\nseg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式\nprint(", ".join(seg_list))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 基于 TF-IDF 算法的关键词抽取\n\n标签    含义     标签    含义     标签    含义     标签     含义\nn     普通名词   f     方位名词   s     处所名词   t      时间\nnr    人名     ns    地名     nt    机构名    nw     作品名\nnz    其他专名   v     普通动词   vd    动副词    vn     名动词\na     形容词    ad    副形词    an    名形词    d      副词\nm     数量词    q     量词     r     代词     p      介词\nc     连词     u     助词     xc    其他虚词   w      标点符号\nPER   人名     LOC   地名     ORG   机构名    TIME   时间\n\n * jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n   * sentence 为待提取的文本\n   * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n   * withWeight 为是否一并返回关键词权重值，默认值为 False\n   * allowPOS 仅包括指定词性的词，默认值为空，即不筛\n * jieba.analyse.TFIDF(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件\n\nimport jieba.analyse\njieba.analyse.extract_tags(res.text,topK=5,allowPOS=(\'n\',\'nr\',\'ns\'))\n\n\n1\n2\n\n\n\n# 基于 TextRank 算法的关键词抽取\n\n * jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(\'ns\', \'n\', \'vn\', \'v\')) 直接使用，接口相同，注意默认过滤词性。\n   * sentence 为待提取的文本\n   * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n   * withWeight 为是否一并返回关键词权重值，默认值为 False\n   * allowPOS 仅包括指定词性的词，默认值为(\'ns\', \'n\', \'vn\', \'v\')\n\njieba.analyse.textrank(res.text,topK=5,allowPOS=("n","v"))\n\n\n1\n',normalizedContent:'# jieba\n\n“结巴”中文分词\n\n * jieba.cut(str) cut_all 参数用来控制是否采用全模式 use_paddle 参数用来控制是否使用paddle模式下的分词模式，paddle模式采用延迟加载方式\n * jieba.cut_for_search(str) 搜索引擎模式 构建倒排索引的分词，粒度比较细\n * jieba.lcut(str) 直接返回 list 结果\n\n# encoding=utf-8\nimport jieba\n\njieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\nstrs=["我来到北京清华大学","乒乓球拍卖完了","中国科学技术大学"]\nfor str in strs:\n    seg_list = jieba.cut(str,use_paddle=true) # 使用paddle模式\n    print("paddle mode: " + \'/\'.join(list(seg_list)))\n\nseg_list = jieba.cut("我来到北京清华大学", cut_all=true)\nprint("full mode: " + "/ ".join(seg_list))  # 全模式\n\nseg_list = jieba.cut("我来到北京清华大学", cut_all=false)\nprint("default mode: " + "/ ".join(seg_list))  # 精确模式\n\nseg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式\nprint(", ".join(seg_list))\n\nseg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式\nprint(", ".join(seg_list))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 基于 tf-idf 算法的关键词抽取\n\n标签    含义     标签    含义     标签    含义     标签     含义\nn     普通名词   f     方位名词   s     处所名词   t      时间\nnr    人名     ns    地名     nt    机构名    nw     作品名\nnz    其他专名   v     普通动词   vd    动副词    vn     名动词\na     形容词    ad    副形词    an    名形词    d      副词\nm     数量词    q     量词     r     代词     p      介词\nc     连词     u     助词     xc    其他虚词   w      标点符号\nper   人名     loc   地名     org   机构名    time   时间\n\n * jieba.analyse.extract_tags(sentence, topk=20, withweight=false, allowpos=())\n   * sentence 为待提取的文本\n   * topk 为返回几个 tf/idf 权重最大的关键词，默认值为 20\n   * withweight 为是否一并返回关键词权重值，默认值为 false\n   * allowpos 仅包括指定词性的词，默认值为空，即不筛\n * jieba.analyse.tfidf(idf_path=none) 新建 tfidf 实例，idf_path 为 idf 频率文件\n\nimport jieba.analyse\njieba.analyse.extract_tags(res.text,topk=5,allowpos=(\'n\',\'nr\',\'ns\'))\n\n\n1\n2\n\n\n\n# 基于 textrank 算法的关键词抽取\n\n * jieba.analyse.textrank(sentence, topk=20, withweight=false, allowpos=(\'ns\', \'n\', \'vn\', \'v\')) 直接使用，接口相同，注意默认过滤词性。\n   * sentence 为待提取的文本\n   * topk 为返回几个 tf/idf 权重最大的关键词，默认值为 20\n   * withweight 为是否一并返回关键词权重值，默认值为 false\n   * allowpos 仅包括指定词性的词，默认值为(\'ns\', \'n\', \'vn\', \'v\')\n\njieba.analyse.textrank(res.text,topk=5,allowpos=("n","v"))\n\n\n1\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"re 正则表达式",frontmatter:{title:"re 正则表达式",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/a3d900/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/03.re%20%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html",relativePath:"后端/07.Python模块/03.re 正则表达式.md",key:"v-30e5d4a2",path:"/pages/a3d900/",headers:[{level:3,title:"贪婪模式",slug:"贪婪模式",normalizedTitle:"贪婪模式",charIndex:658}],headersStr:"贪婪模式",content:'# re 正则表达式\n\n * re.search(pattern, string) 扫描整个字符串并返回第一个成功的匹配\n\n * re.match(pattern, string) match 只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回 None 而 re.search 匹配整个字符串，直到找到一个匹配。\n\n * 匹配对象.groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号\n\n * 匹配对象.group(n) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。\n\n * re.sub(patten,repl,string,count=0) re.sub用于替换字符串中的匹配项\n   \n   * repl : 替换的字符串，也可为一个函数。\n   \n   * count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。\n   \n   * #将不以A-Za-z1-9\' 的替换成 空格\n     re.sub(\'[^A-Za-z1-9\\\']\',\' \',rep.text)\n     \n     \n     1\n     2\n     \n\nimport re\nhtml ="4月5日0-24时，四川无新增新型冠状病毒肺炎确诊病例，新增治愈出院病例1例，无新增疑似病例，无新增死亡病例。"\nc="新增治愈出院病例(\\d+)例"\n# \\d匹配0-9任意数字    .* 匹配最多任意字符贪婪模式只匹配找到的第一个字符   .*?匹配最少任意字符非贪婪模式  并返回所以符合的条件的全部字符 保存到列表\nc=re.search(c,html)  #re.search(regx,str)  在str中查找满足条件的字符串,匹配不上则返回None\nprint(c)\nprint(c.groups())  #使用groups可以获取小括号里所有内容\nprint(c.group(0))  #对返回结果可以分组,可在字符串内添加小括号分离数据\nprint(c.group(1))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 贪婪模式\n\n模式            描述\n^             匹配字符串的开头\n$             匹配字符串的末尾。\n.             匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。\n[...]         用来表示一组字符,单独列出：[amk] 匹配 \'a\'，\'m\'或\'k\'\n[^...]        不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。\nre*           匹配0个或多个的表达式。\nre+           匹配1个或多个的表达式。\nre?           匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式\nre{ n}        匹配n个前面表达式。例如，"o{2}"不能匹配"Bob"中的"o"，但是能匹配"food"中的两个o。\nre{ n,}       精确匹配n个前面表达式。例如，"o{2,}"不能匹配"Bob"中的"o"，但能匹配"foooood"中的所有o。"o{1,}"等价于"o+"。"o{0,}"则等价于"o*"。\nre{ n, m}     匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式\na| b          匹配a或b\n(re)          匹配括号内的表达式，也表示一个组\n(?imx)        正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。\n(?-imx)       正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。\n(?: re)       类似 (...), 但是不表示一个组\n(?imx: re)    在括号中使用i, m, 或 x 可选标志\n(?-imx: re)   在括号中不使用i, m, 或 x 可选标志\n(?#...)       注释.\n(?= re)       前向肯定界定符。如果所含正则表达式，以 ...\n              表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。\n(?! re)       前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功。\n(?> re)       匹配的独立模式，省去回溯。\n\\w            匹配数字字母下划线\n\\W            匹配非数字字母下划线\n\\s            匹配任意空白字符，等价于 [\\t\\n\\r\\f]。\n\\S            匹配任意非空字符\n\\d            匹配任意数字，等价于 [0-9]。\n\\D            匹配任意非数字\n\\A            匹配字符串开始\n\\Z            匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。\n\\z            匹配字符串结束\n\\G            匹配最后匹配完成的位置。\n\\b            匹配一个单词边界，也就是指单词和空格间的位置。例如， \'er\\b\' 可以匹配"never" 中的 \'er\'，但不能匹配\n              "verb" 中的 \'er\'。\n\\B            匹配非单词边界。\'er\\B\' 能匹配 "verb" 中的 \'er\'，但不能匹配 "never" 中的 \'er\'。\n\\n, \\t, 等。    匹配一个换行符。匹配一个制表符, 等\n\\1...\\9       匹配第n个分组的内容。\n\\10           匹配第n个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式。\n\n.* 匹配最多任意字符 .*?匹配任意字符贪婪模式 重复运行当前匹配条件\n\nimport re\nhtml =" 当日转为确诊病例0例，当日解除集中隔离医学观察0例。尚在集中隔离医学观察28例（均为境外输入），比前一日增加4例。"\nc="确诊病例(\\d+).*?医学观察(\\d+).*?隔离医学观察(\\d+).*前一日增加(\\d+)"\n# \\d匹配0-9任意数字    .* 匹配最多任意字符贪心算法    .*?匹配最少任意字符非贪心算法\nc=re.search(c,html)  #re.search(regx,str)  在str中查找满足条件的字符串,匹配不上则返回None\nprint(c)\nprint(c.groups())  #使用groups可以获取小括号里所有内容\nprint(c.group(0))  #对返回结果可以分组,可在字符串内添加小括号分离数据\nprint(c.group(1),c.group(2),c.group(3),c.group(4))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',normalizedContent:'# re 正则表达式\n\n * re.search(pattern, string) 扫描整个字符串并返回第一个成功的匹配\n\n * re.match(pattern, string) match 只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回 none 而 re.search 匹配整个字符串，直到找到一个匹配。\n\n * 匹配对象.groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号\n\n * 匹配对象.group(n) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。\n\n * re.sub(patten,repl,string,count=0) re.sub用于替换字符串中的匹配项\n   \n   * repl : 替换的字符串，也可为一个函数。\n   \n   * count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。\n   \n   * #将不以a-za-z1-9\' 的替换成 空格\n     re.sub(\'[^a-za-z1-9\\\']\',\' \',rep.text)\n     \n     \n     1\n     2\n     \n\nimport re\nhtml ="4月5日0-24时，四川无新增新型冠状病毒肺炎确诊病例，新增治愈出院病例1例，无新增疑似病例，无新增死亡病例。"\nc="新增治愈出院病例(\\d+)例"\n# \\d匹配0-9任意数字    .* 匹配最多任意字符贪婪模式只匹配找到的第一个字符   .*?匹配最少任意字符非贪婪模式  并返回所以符合的条件的全部字符 保存到列表\nc=re.search(c,html)  #re.search(regx,str)  在str中查找满足条件的字符串,匹配不上则返回none\nprint(c)\nprint(c.groups())  #使用groups可以获取小括号里所有内容\nprint(c.group(0))  #对返回结果可以分组,可在字符串内添加小括号分离数据\nprint(c.group(1))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 贪婪模式\n\n模式            描述\n^             匹配字符串的开头\n$             匹配字符串的末尾。\n.             匹配任意字符，除了换行符，当re.dotall标记被指定时，则可以匹配包括换行符的任意字符。\n[...]         用来表示一组字符,单独列出：[amk] 匹配 \'a\'，\'m\'或\'k\'\n[^...]        不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。\nre*           匹配0个或多个的表达式。\nre+           匹配1个或多个的表达式。\nre?           匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式\nre{ n}        匹配n个前面表达式。例如，"o{2}"不能匹配"bob"中的"o"，但是能匹配"food"中的两个o。\nre{ n,}       精确匹配n个前面表达式。例如，"o{2,}"不能匹配"bob"中的"o"，但能匹配"foooood"中的所有o。"o{1,}"等价于"o+"。"o{0,}"则等价于"o*"。\nre{ n, m}     匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式\na| b          匹配a或b\n(re)          匹配括号内的表达式，也表示一个组\n(?imx)        正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。\n(?-imx)       正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。\n(?: re)       类似 (...), 但是不表示一个组\n(?imx: re)    在括号中使用i, m, 或 x 可选标志\n(?-imx: re)   在括号中不使用i, m, 或 x 可选标志\n(?#...)       注释.\n(?= re)       前向肯定界定符。如果所含正则表达式，以 ...\n              表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。\n(?! re)       前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功。\n(?> re)       匹配的独立模式，省去回溯。\n\\w            匹配数字字母下划线\n\\w            匹配非数字字母下划线\n\\s            匹配任意空白字符，等价于 [\\t\\n\\r\\f]。\n\\s            匹配任意非空字符\n\\d            匹配任意数字，等价于 [0-9]。\n\\d            匹配任意非数字\n\\a            匹配字符串开始\n\\z            匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。\n\\z            匹配字符串结束\n\\g            匹配最后匹配完成的位置。\n\\b            匹配一个单词边界，也就是指单词和空格间的位置。例如， \'er\\b\' 可以匹配"never" 中的 \'er\'，但不能匹配\n              "verb" 中的 \'er\'。\n\\b            匹配非单词边界。\'er\\b\' 能匹配 "verb" 中的 \'er\'，但不能匹配 "never" 中的 \'er\'。\n\\n, \\t, 等。    匹配一个换行符。匹配一个制表符, 等\n\\1...\\9       匹配第n个分组的内容。\n\\10           匹配第n个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式。\n\n.* 匹配最多任意字符 .*?匹配任意字符贪婪模式 重复运行当前匹配条件\n\nimport re\nhtml =" 当日转为确诊病例0例，当日解除集中隔离医学观察0例。尚在集中隔离医学观察28例（均为境外输入），比前一日增加4例。"\nc="确诊病例(\\d+).*?医学观察(\\d+).*?隔离医学观察(\\d+).*前一日增加(\\d+)"\n# \\d匹配0-9任意数字    .* 匹配最多任意字符贪心算法    .*?匹配最少任意字符非贪心算法\nc=re.search(c,html)  #re.search(regx,str)  在str中查找满足条件的字符串,匹配不上则返回none\nprint(c)\nprint(c.groups())  #使用groups可以获取小括号里所有内容\nprint(c.group(0))  #对返回结果可以分组,可在字符串内添加小括号分离数据\nprint(c.group(1),c.group(2),c.group(3),c.group(4))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"selenium",frontmatter:{title:"selenium",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/34405e/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/06.selenium.html",relativePath:"后端/07.Python模块/06.selenium.md",key:"v-4a91f9f8",path:"/pages/34405e/",headersStr:null,content:'# selenium\n\n * webdriver.Chrome() 调用什么测试浏览器 返回browser对象\n\n * browser.get(url) 打开指定网页\n   \n   * from selenium import webdriver\n     import time\n     \n     browser = webdriver.Chrome()\n     browser.get(\'http://top.baidu.com/buzz?b=396&c=12&fr=topbuzz_b2_c12\')\n     print(browser.page_source)  #获取页面内容\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * browser.find_elements_by_xpath(xpath) 获取对指定xpath路径的标签对象\n   \n   * browser.find_elements_by_xpath(\'//*[@id="main"]/div[2]/div\')#找到xpath对应的标签\n     \n     \n     1\n     \n\n * webdriver.ChromeOptions() 无头模式,隐藏窗口操作 返回option对象\n   \n   * option.add_argument("--headless") 隐藏浏览器\n   \n   * option.add_argument("--no-sandbox") Linux中禁用沙箱\n   \n   * from selenium import webdriver\n     \n     option =webdriver.ChromeOptions() #无头模式,隐藏窗口操作\n     option.add_argument("--headless")   #隐藏浏览器\n     option.add_argument("--no-sandbox")  #Linux中禁用沙箱\n     browser = webdriver.Chrome(options=option)\n     browser.get("https://www.bilibili.com/v/popular/all?spm_id_from=333.851.b_7072696d61727950616765546162.3")\n     print(browser.page_source)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     ',normalizedContent:'# selenium\n\n * webdriver.chrome() 调用什么测试浏览器 返回browser对象\n\n * browser.get(url) 打开指定网页\n   \n   * from selenium import webdriver\n     import time\n     \n     browser = webdriver.chrome()\n     browser.get(\'http://top.baidu.com/buzz?b=396&c=12&fr=topbuzz_b2_c12\')\n     print(browser.page_source)  #获取页面内容\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * browser.find_elements_by_xpath(xpath) 获取对指定xpath路径的标签对象\n   \n   * browser.find_elements_by_xpath(\'//*[@id="main"]/div[2]/div\')#找到xpath对应的标签\n     \n     \n     1\n     \n\n * webdriver.chromeoptions() 无头模式,隐藏窗口操作 返回option对象\n   \n   * option.add_argument("--headless") 隐藏浏览器\n   \n   * option.add_argument("--no-sandbox") linux中禁用沙箱\n   \n   * from selenium import webdriver\n     \n     option =webdriver.chromeoptions() #无头模式,隐藏窗口操作\n     option.add_argument("--headless")   #隐藏浏览器\n     option.add_argument("--no-sandbox")  #linux中禁用沙箱\n     browser = webdriver.chrome(options=option)\n     browser.get("https://www.bilibili.com/v/popular/all?spm_id_from=333.851.b_7072696d61727950616765546162.3")\n     print(browser.page_source)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"机器学习",frontmatter:{title:"机器学习",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/ddf9fb/",categories:["后端","机器学习"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/08.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html",relativePath:"后端/08.机器学习/01.机器学习.md",key:"v-5c159d57",path:"/pages/ddf9fb/",headers:[{level:2,title:"工作流程",slug:"工作流程",normalizedTitle:"工作流程",charIndex:11},{level:2,title:"特征工程",slug:"特征工程",normalizedTitle:"特征工程",charIndex:41},{level:2,title:"机器学习算法分类",slug:"机器学习算法分类",normalizedTitle:"机器学习算法分类",charIndex:159},{level:3,title:"监督学习",slug:"监督学习",normalizedTitle:"监督学习",charIndex:172},{level:3,title:"无监督学习",slug:"无监督学习",normalizedTitle:"无监督学习",charIndex:269},{level:3,title:"半监督学习",slug:"半监督学习",normalizedTitle:"半监督学习",charIndex:379},{level:3,title:"强化学习",slug:"强化学习",normalizedTitle:"强化学习",charIndex:413},{level:2,title:"分类模型评估",slug:"分类模型评估",normalizedTitle:"分类模型评估",charIndex:466},{level:2,title:"拟合",slug:"拟合",normalizedTitle:"拟合",charIndex:652},{level:2,title:"安装库",slug:"安装库",normalizedTitle:"安装库",charIndex:816},{level:2,title:"快捷键",slug:"快捷键",normalizedTitle:"快捷键",charIndex:1207}],headersStr:"工作流程 特征工程 机器学习算法分类 监督学习 无监督学习 半监督学习 强化学习 分类模型评估 拟合 安装库 快捷键",content:"# 机器学习\n\n\n# 工作流程\n\n 1. 获取数据\n 2. 数据基本除了\n 3. 特征工程\n 4. 机器学习(模型训练)\n 5. 模型评估\n\n\n# 特征工程\n\n * 把数据转换为机器更容易识别的数据\n * 数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已\n * 特征提取 特征预处理 特征降维\n\n\n# 机器学习算法分类\n\n\n# 监督学习\n\n * 输入数据是由输入特征值和目标值所组成\n   * 函数的输出可以是一个连续的值(称为回归)\n     * \n   * 或是输出是有限个离散值(称为分类)\n     * \n\n\n# 无监督学习\n\n输入数据是由输入特征值组成\n\n输入数据没有被标记,也没有确定的结果. 样板数据类别未知,需要根据样本间的相似性对样本进行分类(聚类,clustering) 试图使类内差距最小化,类间差距最大化\n\n\n\n\n# 半监督学习\n\n训练集同时包含有标记样板数据和未标记样板数据\n\n\n# 强化学习\n\n自动进行决策 并且可以做连续决策\n\n动态过程,上一步数据的输出是下一步数据的输入\n\n\n\n\n# 分类模型评估\n\n * 准确率\n * 精确率\n * 召回率\n * F1-score\n * AUC指标\n\n----------------------------------------\n\n * 均方根误差\n   \n   * \n\n * 相对平方误差\n   \n   * \n\n * 平方绝对误差\n   \n   * \n\n * 决定系数\n   \n   * \n   \n   \n   # 拟合\n\n * 欠拟合 机器学习到的特征太少 导致区分标准太粗糙 不能准确识别出输入数据\n\n * 过拟合 机器学习到的特征太多 导致验证数据集以及测试数据集中表现不佳\n\nhttps://archive.ics.uci.edu/ml/index.php\n\nhttps://archive-beta.ics.uci.edu/\n\n\n# 安装库\n\nrequirements.txt\n\nmatplotlib\nnumpy\npandas\ntables\njupyter\n\n\n1\n2\n3\n4\n5\n\n\n使用pip安装 在当前目录安装\n\n#升级一下\npython -m pip install -U pip\npython -m pip install -U --force-reinstall pip\npip install -r requirements.txt\n\n\n1\n2\n3\n4\n\n\n安装jupyter插件\n\npip install jupyter-contrib-nbextensions\n#执行\njupyter contrib nbextension install --user --skip-running-check\n\n\n1\n2\n3\n\n\n\n\n安装自动整理\n\npip install autopep8\n\n\n1\n\n\n\n# 快捷键\n\n * 添加cell: a 或者 b\n * 删除: x\n * 修改cell的模式: m 或 y\n * 执行: shift+enter\n * 打开帮助文档 shift+tab",normalizedContent:"# 机器学习\n\n\n# 工作流程\n\n 1. 获取数据\n 2. 数据基本除了\n 3. 特征工程\n 4. 机器学习(模型训练)\n 5. 模型评估\n\n\n# 特征工程\n\n * 把数据转换为机器更容易识别的数据\n * 数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已\n * 特征提取 特征预处理 特征降维\n\n\n# 机器学习算法分类\n\n\n# 监督学习\n\n * 输入数据是由输入特征值和目标值所组成\n   * 函数的输出可以是一个连续的值(称为回归)\n     * \n   * 或是输出是有限个离散值(称为分类)\n     * \n\n\n# 无监督学习\n\n输入数据是由输入特征值组成\n\n输入数据没有被标记,也没有确定的结果. 样板数据类别未知,需要根据样本间的相似性对样本进行分类(聚类,clustering) 试图使类内差距最小化,类间差距最大化\n\n\n\n\n# 半监督学习\n\n训练集同时包含有标记样板数据和未标记样板数据\n\n\n# 强化学习\n\n自动进行决策 并且可以做连续决策\n\n动态过程,上一步数据的输出是下一步数据的输入\n\n\n\n\n# 分类模型评估\n\n * 准确率\n * 精确率\n * 召回率\n * f1-score\n * auc指标\n\n----------------------------------------\n\n * 均方根误差\n   \n   * \n\n * 相对平方误差\n   \n   * \n\n * 平方绝对误差\n   \n   * \n\n * 决定系数\n   \n   * \n   \n   \n   # 拟合\n\n * 欠拟合 机器学习到的特征太少 导致区分标准太粗糙 不能准确识别出输入数据\n\n * 过拟合 机器学习到的特征太多 导致验证数据集以及测试数据集中表现不佳\n\nhttps://archive.ics.uci.edu/ml/index.php\n\nhttps://archive-beta.ics.uci.edu/\n\n\n# 安装库\n\nrequirements.txt\n\nmatplotlib\nnumpy\npandas\ntables\njupyter\n\n\n1\n2\n3\n4\n5\n\n\n使用pip安装 在当前目录安装\n\n#升级一下\npython -m pip install -u pip\npython -m pip install -u --force-reinstall pip\npip install -r requirements.txt\n\n\n1\n2\n3\n4\n\n\n安装jupyter插件\n\npip install jupyter-contrib-nbextensions\n#执行\njupyter contrib nbextension install --user --skip-running-check\n\n\n1\n2\n3\n\n\n\n\n安装自动整理\n\npip install autopep8\n\n\n1\n\n\n\n# 快捷键\n\n * 添加cell: a 或者 b\n * 删除: x\n * 修改cell的模式: m 或 y\n * 执行: shift+enter\n * 打开帮助文档 shift+tab",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"time",frontmatter:{title:"time",date:"2022-03-18T00:53:26.000Z",permalink:"/pages/fd5355/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/07.time.html",relativePath:"后端/07.Python模块/07.time.md",key:"v-6748a4dc",path:"/pages/fd5355/",headers:[{level:3,title:"时间戳转换",slug:"时间戳转换",normalizedTitle:"时间戳转换",charIndex:160}],headersStr:"时间戳转换",content:'# time\n\n * time.time() 获取当前时间戳\n * time.localtime(time) 将时间戳转为time对象\n * time.strftime(str,longtime) 将time对象转为字符串\n * time.strptime(longtime,str) 将time对象转为字符串\n\n\n# 时间戳转换\n\nimport time\nq=time.time()  #获取当前时间戳\n\na=time.localtime(time.time())  #将时间戳转为time对象\n#time.struct_time(tm_year=2021, tm_mon=4, tm_mday=29, tm_hour=21, tm_min=38, tm_sec=11, tm_wday=3, tm_yday=119, tm_isdst=0)\n\na= time.strftime("%Y-%m-%d %H:%M:%S", a)  #将time对象转为字符串\n#2021-04-29 21:38:11\n\na = time.strptime(a,"%Y-%m-%d %H:%M:%S")  #将字符串时间转为 time对象     插入字符串格式化必须与字符串一一对应\n#time.struct_time(tm_year=2021, tm_mon=4, tm_mday=29, tm_hour=21, tm_min=38, tm_sec=11, tm_wday=3, tm_yday=119, tm_isdst=-1)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n',normalizedContent:'# time\n\n * time.time() 获取当前时间戳\n * time.localtime(time) 将时间戳转为time对象\n * time.strftime(str,longtime) 将time对象转为字符串\n * time.strptime(longtime,str) 将time对象转为字符串\n\n\n# 时间戳转换\n\nimport time\nq=time.time()  #获取当前时间戳\n\na=time.localtime(time.time())  #将时间戳转为time对象\n#time.struct_time(tm_year=2021, tm_mon=4, tm_mday=29, tm_hour=21, tm_min=38, tm_sec=11, tm_wday=3, tm_yday=119, tm_isdst=0)\n\na= time.strftime("%y-%m-%d %h:%m:%s", a)  #将time对象转为字符串\n#2021-04-29 21:38:11\n\na = time.strptime(a,"%y-%m-%d %h:%m:%s")  #将字符串时间转为 time对象     插入字符串格式化必须与字符串一一对应\n#time.struct_time(tm_year=2021, tm_mon=4, tm_mday=29, tm_hour=21, tm_min=38, tm_sec=11, tm_wday=3, tm_yday=119, tm_isdst=-1)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Falsk",frontmatter:{title:"Falsk",date:"2022-03-18T00:53:26.000Z",permalink:"/pages/a6ad03/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/08.Falsk.html",relativePath:"后端/07.Python模块/08.Falsk.md",key:"v-16076614",path:"/pages/a6ad03/",headers:[{level:2,title:"生产模式部署",slug:"生产模式部署",normalizedTitle:"生产模式部署",charIndex:12}],headersStr:"生产模式部署",content:"# Falsk\n\n\n# 生产模式部署\n\n安装Gunicorn\n\npip install gunicorn\n\n\n1\n\n\n启动Gunicorn\n\ngunicorn -b 127.0.0.1:8080 -D app:app # 第一个app为app.py 第二个app为flask中实例化的名称\n\n\n1\n",normalizedContent:"# falsk\n\n\n# 生产模式部署\n\n安装gunicorn\n\npip install gunicorn\n\n\n1\n\n\n启动gunicorn\n\ngunicorn -b 127.0.0.1:8080 -d app:app # 第一个app为app.py 第二个app为flask中实例化的名称\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"pymysql  数据库调用",frontmatter:{title:"pymysql  数据库调用",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/f8134b/",categories:["后端","Python模块"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/07.Python%E6%A8%A1%E5%9D%97/05.pymysql%20%20%E6%95%B0%E6%8D%AE%E5%BA%93%E8%B0%83%E7%94%A8.html",relativePath:"后端/07.Python模块/05.pymysql  数据库调用.md",key:"v-3e534332",path:"/pages/f8134b/",headersStr:null,content:'# pymysql 数据库调用\n\n * pymysql.connect(host,user,passwd,db,charset) 创建数据连接对象 返回db对象\n * db.cursor() 创建游标 返回cursor对象\n * db.commit() 提交事务\n * cursor.execute(sql) 执行指定sql语句\n * cursor.fetchall() 如果cursor有返回结果则使用此方法 读取返回结果\n * close() 关闭\n\nimport pymysql\n#创建连接\ndb = pymysql.connect(host=\'127.0.0.1\',user=\'root\',passwd=\'373213257\',db=\'cov\',charset=\'utf8\')    #charset是utf8而不是utf-8\n\n\n#创建游标\ncursor=db.cursor()\n\n\ncursor.execute("DROP TABLE IF EXISTS EMPLOYEE")   ## 使用 execute() 方法执行 SQL，如果表存在则删除\n\n\nsql = """CREATE TABLE EMPLOYEE (\n         FIRST_NAME  CHAR(20) NOT NULL,\n         LAST_NAME  CHAR(20),\n         AGE INT,  \n         SEX CHAR(1),\n         INCOME FLOAT )"""\n\ncursor.execute(sql)  #使用cursor.execute传入sql语句  sql存放是字符串\n\ncursor.close()  #关闭游标\ndb.close()  #关闭数据库连接\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n读取结果\n\nsql ="""INSERT INTO history VALUES(\'2020-01-02\',2,3,4,5,6,7)"""\ncursor.execute(sql)\ndb.commit() #提交事务\ncursor.execute("select * from history")\nres = cursor.fetchall()\nprint(res)\n\n\n1\n2\n3\n4\n5\n6\n',normalizedContent:'# pymysql 数据库调用\n\n * pymysql.connect(host,user,passwd,db,charset) 创建数据连接对象 返回db对象\n * db.cursor() 创建游标 返回cursor对象\n * db.commit() 提交事务\n * cursor.execute(sql) 执行指定sql语句\n * cursor.fetchall() 如果cursor有返回结果则使用此方法 读取返回结果\n * close() 关闭\n\nimport pymysql\n#创建连接\ndb = pymysql.connect(host=\'127.0.0.1\',user=\'root\',passwd=\'373213257\',db=\'cov\',charset=\'utf8\')    #charset是utf8而不是utf-8\n\n\n#创建游标\ncursor=db.cursor()\n\n\ncursor.execute("drop table if exists employee")   ## 使用 execute() 方法执行 sql，如果表存在则删除\n\n\nsql = """create table employee (\n         first_name  char(20) not null,\n         last_name  char(20),\n         age int,  \n         sex char(1),\n         income float )"""\n\ncursor.execute(sql)  #使用cursor.execute传入sql语句  sql存放是字符串\n\ncursor.close()  #关闭游标\ndb.close()  #关闭数据库连接\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n读取结果\n\nsql ="""insert into history values(\'2020-01-02\',2,3,4,5,6,7)"""\ncursor.execute(sql)\ndb.commit() #提交事务\ncursor.execute("select * from history")\nres = cursor.fetchall()\nprint(res)\n\n\n1\n2\n3\n4\n5\n6\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"matplotlib",frontmatter:{title:"matplotlib",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/9e1ba1/",categories:["后端","机器学习"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/08.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02.matplotlib.html",relativePath:"后端/08.机器学习/02.matplotlib.md",key:"v-56a1eebf",path:"/pages/9e1ba1/",headers:[{level:2,title:"三层结构",slug:"三层结构",normalizedTitle:"三层结构",charIndex:72},{level:2,title:"绘制",slug:"绘制",normalizedTitle:"绘制",charIndex:54},{level:2,title:"保存为图片",slug:"保存为图片",normalizedTitle:"保存为图片",charIndex:505},{level:2,title:"折线图绘画",slug:"折线图绘画",normalizedTitle:"折线图绘画",charIndex:760},{level:2,title:"中文显示问题解决",slug:"中文显示问题解决",normalizedTitle:"中文显示问题解决",charIndex:1498},{level:2,title:"多次plot和图例",slug:"多次plot和图例",normalizedTitle:"多次plot和图例",charIndex:1607},{level:3,title:"图例显示位置",slug:"图例显示位置",normalizedTitle:"图例显示位置",charIndex:2421},{level:2,title:"多个坐标系显示",slug:"多个坐标系显示",normalizedTitle:"多个坐标系显示",charIndex:2604},{level:2,title:"数学折线图 figure",slug:"数学折线图-figure",normalizedTitle:"数学折线图 figure",charIndex:4046},{level:2,title:"散点图  scatter",slug:"散点图-scatter",normalizedTitle:"散点图  scatter",charIndex:null},{level:2,title:"柱状图  bar",slug:"柱状图-bar",normalizedTitle:"柱状图  bar",charIndex:null},{level:2,title:"饼图   pie",slug:"饼图-pie",normalizedTitle:"饼图   pie",charIndex:null},{level:2,title:"直方图 hist",slug:"直方图-hist",normalizedTitle:"直方图 hist",charIndex:5126}],headersStr:"三层结构 绘制 保存为图片 折线图绘画 中文显示问题解决 多次plot和图例 图例显示位置 多个坐标系显示 数学折线图 figure 散点图  scatter 柱状图  bar 饼图   pie 直方图 hist",content:"# matplotlib\n\n主要用于开发2d图表 3d也可以绘画\n\n绘图流程:\n\n 1. 创建画布\n 2. 绘制图像\n 3. 显示图像\n\n\n# 三层结构\n\n 1. 容器层\n    * canvas\n    * figure\n    * axes\n 2. 辅助显示层\n    * 添加x轴 y轴描述 标题等等\n 3. 图像层\n    * 绘制什么图像的声明\n\n\n# 绘制\n\n 1. plt.figure(figsize=(), dpi=) figsize:指定图的长宽 dpi:图像的清晰度 返回fig对象 创建画布\n 2. plt.plot(x, y) 绘制图像\n 3. plt.show() 显示图像\n\nimport matplotlib.pyplot as plt\n\n#创建画布  figesize为画布尺寸  dpi是像素点\nplt.figure(figsize=(20, 8), dpi=100)\n\n#图像绘制\nx = [1, 2, 3]\ny = [4, 5, 6]\nplt.plot(x, y)\n\n#图像展示\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 保存为图片\n\n#创建画布  figesize为画布尺寸  dpi是像素点\nplt.figure(figsize=(20, 8), dpi=100)\n\n#图像绘制\nx = [1, 2, 3]\ny = [4, 5, 6]\nplt.plot(x, y)\n\n#保存图片\nplt.savefig(\"test-mat.png\")\n\n#图像展示 show会释放资源 当show调用后plt对象对应的资源也释放了 无法保存为图片\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 折线图绘画\n\nimport random\n\nx = range(60)\ny = [random.uniform(10, 15) for i in x]\n\nplt.rcParams['font.sans-serif'] = ['SimHei'] # 步骤一（替换sans-serif字体）\nplt.rcParams['axes.unicode_minus'] = False   # 步骤二（解决坐标轴负数的负号显示问题）\n    \n# 创建画布\nplt.figure(figsize=(15, 8), dpi=300)\n\n# 绘制\nplt.plot(x, y)\n\n\n# 添加 x,y轴刻度\ny_ticks = range(40)  # 需要提供一个列表\nplt.yticks(y_ticks[::5])  # 通过yticks方法赋予属性\n\nx_ticks_label = [\"11点{}分\".format(i) for i in x]\nplt.xticks(x[::5], x_ticks_label[::5])   #第一个参数必须为数字不能为字符串数组\n\n#添加网格\nplt.grid(True, linestyle=\"-\",alpha=1)  #linestyle为线样式 -为折线 --为虚线   alpha为线的透明度\n\n#添加描述\nplt.xlabel(\"时间\")\nplt.ylabel(\"温度\")\nplt.title(\"一小时温度变化图\")\n\n\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n\n\n# 中文显示问题解决\n\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n\n1\n2\n\n\n\n# 多次plot和图例\n\nx = range(60)\ny_beijing = [random.uniform(10, 15) for i in x]\ny_shanghai = [random.uniform(10, 25) for i in x]\n\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n\nplt.figure(figsize=(15, 8), dpi=300)\n\n\nplt.plot(x, y_beijing, label=\"北京\")\nplt.plot(x, y_shanghai, label=\"上海\",color='r' , linestyle='-')  # 第二次绘制  如果想显示图例必须赋予laber图例名  color 为线条颜色  linestyle为线条样式 \n#具体值参照以下图\n\n\ny_ticks = range(40)\nplt.yticks(y_ticks[::5])\n\nx_ticks_label = [\"11点{}分\".format(i) for i in x]\nplt.xticks(x[::5], x_ticks_label[::5])\n\n\nplt.grid(True, linestyle=\"-\", alpha=1)\n\nplt.xlabel(\"时间\")\nplt.ylabel(\"温度\")\nplt.title(\"一小时温度变化图\")\n\n# 显示图例\nplt.legend(loc=3)  #loc为图例位置 值参照以下图   \n\n\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n\n\n# 图例显示位置\n\n# 绘制折线图\nplt.plot(x, y_shanghai, label=\"上海\")\n# 使用多次plot可以画多个折线\nplt.plot(x, y_beijing, color='r', linestyle='--', label=\"北京\")\n# 显示图例\nplt.legend(loc=\"best\")\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n# 多个坐标系显示\n\nx = range(60)\ny_beijing = [random.uniform(10, 15) for i in x]\ny_shanghai = [random.uniform(10, 25) for i in x]\n\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n\n# 使用subplot绘制多个坐标系\n# plt.figure(figsize=(15, 8), dpi=300)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=300)   #nrows 几行    ncols几列  并且大部分需要转换为set_xxxx方法\n\n\n# plt.plot(x, y_beijing, label=\"北京\")\n# plt.plot(x, y_shanghai, label=\"上海\",color='r' , linestyle='-')\naxes[0].plot(x, y_beijing, label=\"北京\")\naxes[1].plot(x, y_shanghai, label=\"上海\", color='r', linestyle='-')\n\n\ny_ticks = range(40)\n# plt.yticks(y_ticks[::5])\naxes[0].set_yticks(y_ticks[::5])\naxes[1].set_yticks(y_ticks[::5])\n\nx_ticks_label = [\"11点{}分\".format(i) for i in x]\n# plt.xticks(x[::5], x_ticks_label[::5])\naxes[0].set_xticks(x[::5])\naxes[0].set_xticklabels(x_ticks_label[::5])\naxes[1].set_xticks(x[::5])\naxes[1].set_xticklabels(x_ticks_label[::5])\n\n\n\n\nplt.grid(True, linestyle=\"-\", alpha=1)\n\n# plt.xlabel(\"时间\")\n# plt.ylabel(\"温度\")\n# plt.title(\"一小时温度变化图\")\naxes[0].set_xlabel(\"时间\")\naxes[0].set_ylabel(\"温度\")\naxes[0].set_title(\"北京一小时温度变化图\")\naxes[1].set_xlabel(\"时间\")\naxes[1].set_ylabel(\"温度\")\naxes[1].set_title(\"上海一小时温度变化图\")\n\n\n# plt.legend(loc=3)\naxes[0].legend(loc=3)\naxes[1].legend(loc=3)\n\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n\n\n# 数学折线图 figure\n\nimport numpy as np\n\n# 从-10 到 10 的1000个数据\nx = np.linspace(-10,10,1000)\ny = np.sin(x)\n\nplt.figure(figsize=(20,8),dpi=100)\n\nplt.plot(x,y)\n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n\n# 散点图 scatter\n\nx = [10.0, 8.07, 13.0, 9.05, 11.0, 14.0, 13.4, 10.0, 14.0, 12.5, 9.15,\n     11.5, 3.03, 12.2, 2.02, 1.05, 4.05, 6.03, 12.0, 12.0, 7.08, 5.02]\ny = [8.04, 6.95, 7.58, 8.81, 8.33, 7.66, 6.81, 6.33, 8.96, 6.82, 7.20,\n     7.20, 4.23, 7.83, 4.47, 3.33, 4.96, 7.24, 6.26, 8.84, 5.82, 5.68]\n\nplt.figure(figsize=(20, 8), dpi=100)\n\n#散点图\nplt.scatter(x, y)\n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n\n# 柱状图 bar\n\nx =['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\ny =[120, 200, 150, 80, 70, 110, 130]\n\nplt.figure(figsize=(20, 8), dpi=100)\n\n#柱状图\nplt.bar(x, y,width=0.4)  #width为柱状图宽度\n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n\n# 饼图 pie\n\nx =[2,5,12,70,2,9]\nlabels = ['娱乐','育儿','饮食','房贷','交通','其它']\nplt.figure(figsize=(20, 8), dpi=100)\n\n#饼图\nplt.pie(x,labels=labels)  #x为数据   labels为lengt的名称  \n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n# 直方图 hist\n\nplt.rcParams['font.family']='SimHei'\nplt.rcParams['font.size']=20\n\n# 直方图\nmu = 100\nsigma = 20\nx = np.random.normal(100,20,100) # 均值和标准差\n\nplt.hist(x,bins=20,color='red',histtype='stepfilled',alpha=0.75)\nplt.title('直方图数据分析与展示')\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n",normalizedContent:"# matplotlib\n\n主要用于开发2d图表 3d也可以绘画\n\n绘图流程:\n\n 1. 创建画布\n 2. 绘制图像\n 3. 显示图像\n\n\n# 三层结构\n\n 1. 容器层\n    * canvas\n    * figure\n    * axes\n 2. 辅助显示层\n    * 添加x轴 y轴描述 标题等等\n 3. 图像层\n    * 绘制什么图像的声明\n\n\n# 绘制\n\n 1. plt.figure(figsize=(), dpi=) figsize:指定图的长宽 dpi:图像的清晰度 返回fig对象 创建画布\n 2. plt.plot(x, y) 绘制图像\n 3. plt.show() 显示图像\n\nimport matplotlib.pyplot as plt\n\n#创建画布  figesize为画布尺寸  dpi是像素点\nplt.figure(figsize=(20, 8), dpi=100)\n\n#图像绘制\nx = [1, 2, 3]\ny = [4, 5, 6]\nplt.plot(x, y)\n\n#图像展示\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 保存为图片\n\n#创建画布  figesize为画布尺寸  dpi是像素点\nplt.figure(figsize=(20, 8), dpi=100)\n\n#图像绘制\nx = [1, 2, 3]\ny = [4, 5, 6]\nplt.plot(x, y)\n\n#保存图片\nplt.savefig(\"test-mat.png\")\n\n#图像展示 show会释放资源 当show调用后plt对象对应的资源也释放了 无法保存为图片\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 折线图绘画\n\nimport random\n\nx = range(60)\ny = [random.uniform(10, 15) for i in x]\n\nplt.rcparams['font.sans-serif'] = ['simhei'] # 步骤一（替换sans-serif字体）\nplt.rcparams['axes.unicode_minus'] = false   # 步骤二（解决坐标轴负数的负号显示问题）\n    \n# 创建画布\nplt.figure(figsize=(15, 8), dpi=300)\n\n# 绘制\nplt.plot(x, y)\n\n\n# 添加 x,y轴刻度\ny_ticks = range(40)  # 需要提供一个列表\nplt.yticks(y_ticks[::5])  # 通过yticks方法赋予属性\n\nx_ticks_label = [\"11点{}分\".format(i) for i in x]\nplt.xticks(x[::5], x_ticks_label[::5])   #第一个参数必须为数字不能为字符串数组\n\n#添加网格\nplt.grid(true, linestyle=\"-\",alpha=1)  #linestyle为线样式 -为折线 --为虚线   alpha为线的透明度\n\n#添加描述\nplt.xlabel(\"时间\")\nplt.ylabel(\"温度\")\nplt.title(\"一小时温度变化图\")\n\n\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n\n\n# 中文显示问题解决\n\nplt.rcparams['font.sans-serif'] = ['simhei']\nplt.rcparams['axes.unicode_minus'] = false\n\n\n1\n2\n\n\n\n# 多次plot和图例\n\nx = range(60)\ny_beijing = [random.uniform(10, 15) for i in x]\ny_shanghai = [random.uniform(10, 25) for i in x]\n\nplt.rcparams['font.sans-serif'] = ['simhei']\nplt.rcparams['axes.unicode_minus'] = false\n\n\nplt.figure(figsize=(15, 8), dpi=300)\n\n\nplt.plot(x, y_beijing, label=\"北京\")\nplt.plot(x, y_shanghai, label=\"上海\",color='r' , linestyle='-')  # 第二次绘制  如果想显示图例必须赋予laber图例名  color 为线条颜色  linestyle为线条样式 \n#具体值参照以下图\n\n\ny_ticks = range(40)\nplt.yticks(y_ticks[::5])\n\nx_ticks_label = [\"11点{}分\".format(i) for i in x]\nplt.xticks(x[::5], x_ticks_label[::5])\n\n\nplt.grid(true, linestyle=\"-\", alpha=1)\n\nplt.xlabel(\"时间\")\nplt.ylabel(\"温度\")\nplt.title(\"一小时温度变化图\")\n\n# 显示图例\nplt.legend(loc=3)  #loc为图例位置 值参照以下图   \n\n\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n\n\n# 图例显示位置\n\n# 绘制折线图\nplt.plot(x, y_shanghai, label=\"上海\")\n# 使用多次plot可以画多个折线\nplt.plot(x, y_beijing, color='r', linestyle='--', label=\"北京\")\n# 显示图例\nplt.legend(loc=\"best\")\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n# 多个坐标系显示\n\nx = range(60)\ny_beijing = [random.uniform(10, 15) for i in x]\ny_shanghai = [random.uniform(10, 25) for i in x]\n\nplt.rcparams['font.sans-serif'] = ['simhei']\nplt.rcparams['axes.unicode_minus'] = false\n\n\n# 使用subplot绘制多个坐标系\n# plt.figure(figsize=(15, 8), dpi=300)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=300)   #nrows 几行    ncols几列  并且大部分需要转换为set_xxxx方法\n\n\n# plt.plot(x, y_beijing, label=\"北京\")\n# plt.plot(x, y_shanghai, label=\"上海\",color='r' , linestyle='-')\naxes[0].plot(x, y_beijing, label=\"北京\")\naxes[1].plot(x, y_shanghai, label=\"上海\", color='r', linestyle='-')\n\n\ny_ticks = range(40)\n# plt.yticks(y_ticks[::5])\naxes[0].set_yticks(y_ticks[::5])\naxes[1].set_yticks(y_ticks[::5])\n\nx_ticks_label = [\"11点{}分\".format(i) for i in x]\n# plt.xticks(x[::5], x_ticks_label[::5])\naxes[0].set_xticks(x[::5])\naxes[0].set_xticklabels(x_ticks_label[::5])\naxes[1].set_xticks(x[::5])\naxes[1].set_xticklabels(x_ticks_label[::5])\n\n\n\n\nplt.grid(true, linestyle=\"-\", alpha=1)\n\n# plt.xlabel(\"时间\")\n# plt.ylabel(\"温度\")\n# plt.title(\"一小时温度变化图\")\naxes[0].set_xlabel(\"时间\")\naxes[0].set_ylabel(\"温度\")\naxes[0].set_title(\"北京一小时温度变化图\")\naxes[1].set_xlabel(\"时间\")\naxes[1].set_ylabel(\"温度\")\naxes[1].set_title(\"上海一小时温度变化图\")\n\n\n# plt.legend(loc=3)\naxes[0].legend(loc=3)\naxes[1].legend(loc=3)\n\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n\n\n# 数学折线图 figure\n\nimport numpy as np\n\n# 从-10 到 10 的1000个数据\nx = np.linspace(-10,10,1000)\ny = np.sin(x)\n\nplt.figure(figsize=(20,8),dpi=100)\n\nplt.plot(x,y)\n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n\n# 散点图 scatter\n\nx = [10.0, 8.07, 13.0, 9.05, 11.0, 14.0, 13.4, 10.0, 14.0, 12.5, 9.15,\n     11.5, 3.03, 12.2, 2.02, 1.05, 4.05, 6.03, 12.0, 12.0, 7.08, 5.02]\ny = [8.04, 6.95, 7.58, 8.81, 8.33, 7.66, 6.81, 6.33, 8.96, 6.82, 7.20,\n     7.20, 4.23, 7.83, 4.47, 3.33, 4.96, 7.24, 6.26, 8.84, 5.82, 5.68]\n\nplt.figure(figsize=(20, 8), dpi=100)\n\n#散点图\nplt.scatter(x, y)\n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n\n# 柱状图 bar\n\nx =['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\ny =[120, 200, 150, 80, 70, 110, 130]\n\nplt.figure(figsize=(20, 8), dpi=100)\n\n#柱状图\nplt.bar(x, y,width=0.4)  #width为柱状图宽度\n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n\n# 饼图 pie\n\nx =[2,5,12,70,2,9]\nlabels = ['娱乐','育儿','饮食','房贷','交通','其它']\nplt.figure(figsize=(20, 8), dpi=100)\n\n#饼图\nplt.pie(x,labels=labels)  #x为数据   labels为lengt的名称  \n\nplt.grid()\n\nplt.show\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\n# 直方图 hist\n\nplt.rcparams['font.family']='simhei'\nplt.rcparams['font.size']=20\n\n# 直方图\nmu = 100\nsigma = 20\nx = np.random.normal(100,20,100) # 均值和标准差\n\nplt.hist(x,bins=20,color='red',histtype='stepfilled',alpha=0.75)\nplt.title('直方图数据分析与展示')\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Numpy",frontmatter:{title:"Numpy",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/a58615/",categories:["后端","机器学习"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/08.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03.Numpy.html",relativePath:"后端/08.机器学习/03.Numpy.md",key:"v-6c324d6f",path:"/pages/a58615/",headers:[{level:2,title:"读取数据",slug:"读取数据",normalizedTitle:"读取数据",charIndex:161},{level:2,title:"数组转ndarray",slug:"数组转ndarray",normalizedTitle:"数组转ndarray",charIndex:237},{level:2,title:"ndarray数组方法",slug:"ndarray数组方法",normalizedTitle:"ndarray数组方法",charIndex:393},{level:2,title:"ndarray数组生成方法",slug:"ndarray数组生成方法",normalizedTitle:"ndarray数组生成方法",charIndex:585},{level:2,title:"数组的索引和切片",slug:"数组的索引和切片",normalizedTitle:"数组的索引和切片",charIndex:1076},{level:2,title:"形状修改",slug:"形状修改",normalizedTitle:"形状修改",charIndex:1138},{level:2,title:"数组拼接",slug:"数组拼接",normalizedTitle:"数组拼接",charIndex:1611},{level:2,title:"数组切分",slug:"数组切分",normalizedTitle:"数组切分",charIndex:1678},{level:2,title:"类型修改",slug:"类型修改",normalizedTitle:"类型修改",charIndex:1858},{level:2,title:"复制",slug:"复制",normalizedTitle:"复制",charIndex:817},{level:2,title:"数组的去重",slug:"数组的去重",normalizedTitle:"数组的去重",charIndex:2183},{level:2,title:"数组运算",slug:"数组运算",normalizedTitle:"数组运算",charIndex:2219},{level:3,title:"判断函数",slug:"判断函数",normalizedTitle:"判断函数",charIndex:2977},{level:2,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:3144},{level:3,title:"统计运算",slug:"统计运算",normalizedTitle:"统计运算",charIndex:3225},{level:2,title:"正态分布",slug:"正态分布",normalizedTitle:"正态分布",charIndex:3473},{level:2,title:"矩阵",slug:"矩阵",normalizedTitle:"矩阵",charIndex:3577},{level:3,title:"向量",slug:"向量",normalizedTitle:"向量",charIndex:3737},{level:3,title:"加法和标量乘法",slug:"加法和标量乘法",normalizedTitle:"加法和标量乘法",charIndex:3787},{level:3,title:"矩阵乘法的性质",slug:"矩阵乘法的性质",normalizedTitle:"矩阵乘法的性质",charIndex:3988},{level:3,title:"逆、转置",slug:"逆、转置",normalizedTitle:"逆、转置",charIndex:4165},{level:3,title:"np方法",slug:"np方法",normalizedTitle:"np方法",charIndex:4977}],headersStr:"读取数据 数组转ndarray ndarray数组方法 ndarray数组生成方法 数组的索引和切片 形状修改 数组拼接 数组切分 类型修改 复制 数组的去重 数组运算 判断函数 排序 统计运算 正态分布 矩阵 向量 加法和标量乘法 矩阵乘法的性质 逆、转置 np方法",content:'# Numpy\n\n开源的python科学计算库 用于快速处理任意维度的数组 npmpy中存储对象是ndarray\n\n优势:\n\n 1. 内容块风格\n 2. 支持并行化运算\n 3. 效率高于纯python\n 4. 底层使用了C,内部释放了GIL\n\n * np.array([]) 创建np数组 返回ndarray对象\n\n\n# 读取数据\n\nnp.genfromtxt("xxx.txt",delimiter=",",dtype=str) # 从txt中读取数据\n\n\n1\n\n\n\n# 数组转ndarray\n\n将数组或者多维数组转为np数组\n\nnp.arryay([5,10,15,1])\nndarray = np.arryay([[5,10,15,1],[15,2,5,3]])\n\n\n1\n2\n\n * 列表中元素会自动转换为同一类 一般全部统一为一个类型\n * 每个元素内 元素数必须相同\n\n\n# ndarray数组方法\n\n * ndarray.shape 数组维度和维度元素个数的元组\n * ndarray.ndim 数组维数\n * ndarray.size 数组中全部的元素数量\n * ndarray.itemsize 一个数组元素的长度(字节)\n * ndarray.floor() 将数组中元素向上取整\n * ndarray.dtype 数组元素的类型\n   * \n\n\n# ndarray数组生成方法\n\n * np.arange(15,30,5) 创建等差数组 生成一个np数组从15到30 步进为5个 默认为1维\n * p.logspace(start,stop, num) 创建等比数列 默认生成50个num p.logspace(0,2, 3) 从10的0次方到10的2次方 生成3个元素 默认为50个\n * np.zeros((3,4)) 生成一个全为0的np数组 3行4列\n * np.zeros_like(a, dtype) 复制当前数组的维度和行列数 生成一个全为0的数组\n * np.ones((2,3,4), dtype=np.int32) 生成全为一的数组 二维三行4列 默认为float64类型 zeros也是\n * np.ones_like(a, dtype) 复制当前数组的维度和行列数 生成一个全为1的数组\n * np.random.random((2,3)) 多少个random就是多维 生成一个两行三列的数组 取值为(-1,1)\n * np.linspace(0,5,100) 生成一个0-5 之间平均取100个值\n\n\n# 数组的索引和切片\n\n * ndarray[s,r] 根据维度和索引 获取值\n * ndarray[0:] 支持切片\n\n\n# 形状修改\n\n * ndarray.reshape(shape, order) 将原数组 转为为指定的行列\n   \n   * stock_change = np.random.normal(0, 1, (8, 10))\n     stock_change.reshape([10,8])  #将行列互换了  reshape方法并不会修改元素的个数和产生新的元素\n     stock_change.reshape([-1,2])  #如果不知道具体的行或列可以使用-1代替 自动计算   如这里的 不知道多少行 每个行2个数据\n     #一定要整除 否则报错\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * ndarray.resize(new_shape) reshape并不会对原数组进行修改 而产生新数组 我们使用resize可以对原数组进行修改\n\n * ndarray.revel() 将数组变成一维 将全部元素存放为数组中\n\n * ndarray.T 将数组的行、列进行互换\n\n\n# 数组拼接\n\n * np.hstack((a,b)) 将两数组相连接\n * np.vstack((a,b)) 将两数组纵向拼接\n\n\n# 数组切分\n\n * np.hshplit(a,3) 将指定数组平均拆分成3个np数组 行的拆分 列数不变\n * np.hshplit(a,(3,4)) 如果传递元组则在指定索引下 拆分 在第3列拆分1个 在第4列拆分1个 其他拆分为1个\n * np.vshplit(a,3) 列拆分 行不变\n * np.hshplit(a,(3,4)) 在指定列数拆分\n\n\n# 类型修改\n\n * ndarray.astype(flaot) 转换元素类型\n   \n   * df[\'金牌数\'] = df[\'金牌数\'].fillna("0").astype(int) #先替换缺失值\n     \n     \n     1\n     \n\n * ndarray.tostring([order]) 转为字符串输出\n\n * ndarray.tobytes([order]) 转为字节数组\n\n\n# 复制\n\n * np.array(object, dtype) 将数组转为np数组 深拷贝\n * np.asarray(a, dtype) 浅拷贝\n * ndarray.copy() 浅拷贝\n * ndarray.view() 深拷贝\n\n\n# 数组的去重\n\n * np.unique(nparray) 去重\n\n\n# 数组运算\n\n * np数组支持两个np数组 之间运算 同时支持与常数运算\n   \n   * a = np.array([20,30,40,50])\n     b = np.array([1,2,3])\n     #数组在进行矢量化运算时，要求数组的形状是相等的。当形状不相等的数组执行算术运算的时候，就会出现广播机制，该机制会对数组进行扩展，使数组的shape属性值一样，这样，就可以进行矢量化运算了。\n     a + b # [21,32,43,50]\n     a - 1 # [19,29,39,49]\n     a * 2 # [40,60,80,100]\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * ndarray == 10 支持 > = <比较运算符 返回为同样维度的布尔数组\n   \n   * na = np.arryay([5,10,15,1])\n     t = na == 10\n     print (na[t])  #因为返回是布尔数组 所以可以根据索引获取具体值\n     print (na[na == 10])\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * 支持 逻辑运算\n   \n   * na = np.arryay([5,10,15,1])\n     t = (na >= 10) && (na <= 5)\n     \n     \n     1\n     2\n     \n\n * np.exp() 平方\n\n * np.sqrt() 开根号\n\n * np.tile(a,(2,2)) 将行数和列数扩展为指定倍数 传递数组和一个元组\n\n\n# 判断函数\n\n * np.all() 所有元素都符合条件则返回true np.all(arry >60 )\n * np.any() 有一个符合则返回true\n * np.where(temp > 60, 1, 0) 三元运算符 成立返回参数2 否则返回参数3\n * ndarray.argmax(axis=) 返回最大值的索引\n\n\n# 排序\n\n * ndarray.sort(axis=) 排序默认为升序\n * ndarray.argsort(axis=) 按元素排序 返回元素之前的索引\n\n\n# 统计运算\n\n * ndarray.min(axis) 获取最小值\n\n * ndarray.max(axis) 获取最大值\n\n * ndarray.sum(axis=1) 求和 axis=1则为行求和 axis=0为列切换\n\n * ndarray.median(axis) 返回中位值\n\n * ndarray.mean(axis,dtype) 返回平均值\n\n * ndarray.std(axis,dtype) 返回标准差\n\n * ndarray.var(axis,dtype) 返回方差\n\n\n# 正态分布\n\n正态分布是一种概率分布。正态分布是具有两个参数μ和σ的连续型随机变量的分布，第一参数μ是服从正态分布的随机变量的均值，第二个参 数σ是此随机变量的标准差，所以正态分布记作N(μ，σ )。\n\n\n# 矩阵\n\n矩阵必须是二维的 但array可以是多维的\n\n$$ A= \\left{ \\begin{matrix} a & b & c & d & e\\ f & g & h & i & j \\ k & l & m & n & o \\ p & q & r & s & t \\end{matrix} \\right} $$\n\n\n# 向量\n\n向量是一种特殊的矩阵 向量一般都是列向量\n\n如: (3x1)的矩阵\n\n向量是一维的\n\n\n# 加法和标量乘法\n\n * 加法:行列数相等时可以相加\n\n * 乘法: 标(常)量 乘以 矩阵 直接相乘 按原来位置\n\n * 矩阵向量乘法: (M行,N列) X (N行,L列) = (M行,L列) 并且 N列和N行相等\n\n\n\n * 矩阵乘法:\n\n即A的第一行 各个数 都乘以B的第一列各个数 相加 得到 C的第一行第一个数\n\n第一行 各个数 都乘以B的第二列各个数 相加 得到 C的第一行第二个数\n\n\n# 矩阵乘法的性质\n\n * 矩阵乘法不满足交换律 A*B != B*A\n * 矩阵的乘法满足结合律: A*(B*C)=(A*B)*C\n * 单位矩阵:在矩阵乘法中 有一种特殊矩阵 称为单位矩阵 它是个方针 一般用 I 或者 E表示 从左上角到右下角的对角线(称为主对角线) 上的元素均为为1 其他全为0 如果A*B=E 那么矩阵A和B互为逆矩阵\n\n\n\n\n# 逆、转置\n\n 1. 待定系数法\n\n给予一个2X2的矩阵 我们假设一个同样为2X2的矩阵 [a b] [b c]\n\nA B矩阵相乘 转为单位矩阵\n\n\n\n\n\n求出 a b c d 的值 ,就可以得出 A的逆矩阵\n\n\n\n 2. 伴随矩阵求逆矩阵\n\n * 伴随矩阵是矩阵元素所对应的代数余子式，所构成的矩阵，转置后得到的新矩阵。\n\n\n\n原矩阵为 [1 2]\n\n[-1 -3]\n\nA11 为 第一行和第一列的元素 都去掉剩下 -3 又因为A11为 1+1=2 为偶数是整数 1*-3=-3 A11=-3\n\nA12 为 第一行和第二列的元素 都去掉剩下 -1 又因为A12为 1+2=3 为奇数 所以为负号 A12=-(-1)=1\n\nA21 为 第二行和第一列的元素 都去掉剩下 2 2+1为奇数 A21=-(2)=-2\n\nA22 为 第二行和第二列的元素 都去掉剩下 1 为偶数 A22=1\n\n得出[-3 1] 的矩阵 进行转置即行列变换第一行内容变成第一列内容 变成 [-3 -2]\n\n[-2 1] 第二行内容变成第二列内容 [1 1]\n\n接下来求出矩阵A的行列式\n\n * 行列式\n\n二阶行列式的计算方法是“对角线法则” 主对角线元素积与副对角线元素积的差\n\n\n\n二阶行列式并不适合三阶使用\n\n正对角线为正，反对角线为负。\n\n\n\nxsc + yt*a + ra* z - zsa - yr*c - tb* x\n\n由行列号性质得出\n\n|A|1*(-1)-(-1)*2=-1\n\nA⁻¹=A*/|A| = A*/(-1)=-A* = [3 2]\n\n[-1 -1]\n\n 3. 初等变换求逆矩阵\n\n首先我们得出A的增广矩阵\n\n\n\n然后进行初等行变换。依次进行\n\n第1行加到第2行，得到\n\n第2行×2加到第1行，得到\n\n第2行×(-1)，得到\n\n\n\n因此逆矩阵A⁻¹=\n\n3 2\n\n-1 -1\n\n\n\n# 转置\n\n即将矩阵的行列互换\n\n\n\n\n# np方法\n\n * np.matmul()\n   \n   * a = np.array([[80, 86], [82, 80], [85, 78], [90, 90],\n                  [86, 82], [82, 90]])  # 是的多行2列的矩阵\n     b = np.array([[0.7], [0.3]])  # 两行一列的矩阵\n     np.matmul(a, b)  # 转为M行L列 即多行一列\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * np.dot()\n   \n   * #np.matmul(a, 10) #只支持矩阵相乘\n     np.dot(a, 10)  # matmul和dot功能一致 但dot支持点乘  标量运算 matmul不支持点乘\n     \n     \n     1\n     2\n     ',normalizedContent:'# numpy\n\n开源的python科学计算库 用于快速处理任意维度的数组 npmpy中存储对象是ndarray\n\n优势:\n\n 1. 内容块风格\n 2. 支持并行化运算\n 3. 效率高于纯python\n 4. 底层使用了c,内部释放了gil\n\n * np.array([]) 创建np数组 返回ndarray对象\n\n\n# 读取数据\n\nnp.genfromtxt("xxx.txt",delimiter=",",dtype=str) # 从txt中读取数据\n\n\n1\n\n\n\n# 数组转ndarray\n\n将数组或者多维数组转为np数组\n\nnp.arryay([5,10,15,1])\nndarray = np.arryay([[5,10,15,1],[15,2,5,3]])\n\n\n1\n2\n\n * 列表中元素会自动转换为同一类 一般全部统一为一个类型\n * 每个元素内 元素数必须相同\n\n\n# ndarray数组方法\n\n * ndarray.shape 数组维度和维度元素个数的元组\n * ndarray.ndim 数组维数\n * ndarray.size 数组中全部的元素数量\n * ndarray.itemsize 一个数组元素的长度(字节)\n * ndarray.floor() 将数组中元素向上取整\n * ndarray.dtype 数组元素的类型\n   * \n\n\n# ndarray数组生成方法\n\n * np.arange(15,30,5) 创建等差数组 生成一个np数组从15到30 步进为5个 默认为1维\n * p.logspace(start,stop, num) 创建等比数列 默认生成50个num p.logspace(0,2, 3) 从10的0次方到10的2次方 生成3个元素 默认为50个\n * np.zeros((3,4)) 生成一个全为0的np数组 3行4列\n * np.zeros_like(a, dtype) 复制当前数组的维度和行列数 生成一个全为0的数组\n * np.ones((2,3,4), dtype=np.int32) 生成全为一的数组 二维三行4列 默认为float64类型 zeros也是\n * np.ones_like(a, dtype) 复制当前数组的维度和行列数 生成一个全为1的数组\n * np.random.random((2,3)) 多少个random就是多维 生成一个两行三列的数组 取值为(-1,1)\n * np.linspace(0,5,100) 生成一个0-5 之间平均取100个值\n\n\n# 数组的索引和切片\n\n * ndarray[s,r] 根据维度和索引 获取值\n * ndarray[0:] 支持切片\n\n\n# 形状修改\n\n * ndarray.reshape(shape, order) 将原数组 转为为指定的行列\n   \n   * stock_change = np.random.normal(0, 1, (8, 10))\n     stock_change.reshape([10,8])  #将行列互换了  reshape方法并不会修改元素的个数和产生新的元素\n     stock_change.reshape([-1,2])  #如果不知道具体的行或列可以使用-1代替 自动计算   如这里的 不知道多少行 每个行2个数据\n     #一定要整除 否则报错\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * ndarray.resize(new_shape) reshape并不会对原数组进行修改 而产生新数组 我们使用resize可以对原数组进行修改\n\n * ndarray.revel() 将数组变成一维 将全部元素存放为数组中\n\n * ndarray.t 将数组的行、列进行互换\n\n\n# 数组拼接\n\n * np.hstack((a,b)) 将两数组相连接\n * np.vstack((a,b)) 将两数组纵向拼接\n\n\n# 数组切分\n\n * np.hshplit(a,3) 将指定数组平均拆分成3个np数组 行的拆分 列数不变\n * np.hshplit(a,(3,4)) 如果传递元组则在指定索引下 拆分 在第3列拆分1个 在第4列拆分1个 其他拆分为1个\n * np.vshplit(a,3) 列拆分 行不变\n * np.hshplit(a,(3,4)) 在指定列数拆分\n\n\n# 类型修改\n\n * ndarray.astype(flaot) 转换元素类型\n   \n   * df[\'金牌数\'] = df[\'金牌数\'].fillna("0").astype(int) #先替换缺失值\n     \n     \n     1\n     \n\n * ndarray.tostring([order]) 转为字符串输出\n\n * ndarray.tobytes([order]) 转为字节数组\n\n\n# 复制\n\n * np.array(object, dtype) 将数组转为np数组 深拷贝\n * np.asarray(a, dtype) 浅拷贝\n * ndarray.copy() 浅拷贝\n * ndarray.view() 深拷贝\n\n\n# 数组的去重\n\n * np.unique(nparray) 去重\n\n\n# 数组运算\n\n * np数组支持两个np数组 之间运算 同时支持与常数运算\n   \n   * a = np.array([20,30,40,50])\n     b = np.array([1,2,3])\n     #数组在进行矢量化运算时，要求数组的形状是相等的。当形状不相等的数组执行算术运算的时候，就会出现广播机制，该机制会对数组进行扩展，使数组的shape属性值一样，这样，就可以进行矢量化运算了。\n     a + b # [21,32,43,50]\n     a - 1 # [19,29,39,49]\n     a * 2 # [40,60,80,100]\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * ndarray == 10 支持 > = <比较运算符 返回为同样维度的布尔数组\n   \n   * na = np.arryay([5,10,15,1])\n     t = na == 10\n     print (na[t])  #因为返回是布尔数组 所以可以根据索引获取具体值\n     print (na[na == 10])\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * 支持 逻辑运算\n   \n   * na = np.arryay([5,10,15,1])\n     t = (na >= 10) && (na <= 5)\n     \n     \n     1\n     2\n     \n\n * np.exp() 平方\n\n * np.sqrt() 开根号\n\n * np.tile(a,(2,2)) 将行数和列数扩展为指定倍数 传递数组和一个元组\n\n\n# 判断函数\n\n * np.all() 所有元素都符合条件则返回true np.all(arry >60 )\n * np.any() 有一个符合则返回true\n * np.where(temp > 60, 1, 0) 三元运算符 成立返回参数2 否则返回参数3\n * ndarray.argmax(axis=) 返回最大值的索引\n\n\n# 排序\n\n * ndarray.sort(axis=) 排序默认为升序\n * ndarray.argsort(axis=) 按元素排序 返回元素之前的索引\n\n\n# 统计运算\n\n * ndarray.min(axis) 获取最小值\n\n * ndarray.max(axis) 获取最大值\n\n * ndarray.sum(axis=1) 求和 axis=1则为行求和 axis=0为列切换\n\n * ndarray.median(axis) 返回中位值\n\n * ndarray.mean(axis,dtype) 返回平均值\n\n * ndarray.std(axis,dtype) 返回标准差\n\n * ndarray.var(axis,dtype) 返回方差\n\n\n# 正态分布\n\n正态分布是一种概率分布。正态分布是具有两个参数μ和σ的连续型随机变量的分布，第一参数μ是服从正态分布的随机变量的均值，第二个参 数σ是此随机变量的标准差，所以正态分布记作n(μ，σ )。\n\n\n# 矩阵\n\n矩阵必须是二维的 但array可以是多维的\n\n$$ a= \\left{ \\begin{matrix} a & b & c & d & e\\ f & g & h & i & j \\ k & l & m & n & o \\ p & q & r & s & t \\end{matrix} \\right} $$\n\n\n# 向量\n\n向量是一种特殊的矩阵 向量一般都是列向量\n\n如: (3x1)的矩阵\n\n向量是一维的\n\n\n# 加法和标量乘法\n\n * 加法:行列数相等时可以相加\n\n * 乘法: 标(常)量 乘以 矩阵 直接相乘 按原来位置\n\n * 矩阵向量乘法: (m行,n列) x (n行,l列) = (m行,l列) 并且 n列和n行相等\n\n\n\n * 矩阵乘法:\n\n即a的第一行 各个数 都乘以b的第一列各个数 相加 得到 c的第一行第一个数\n\n第一行 各个数 都乘以b的第二列各个数 相加 得到 c的第一行第二个数\n\n\n# 矩阵乘法的性质\n\n * 矩阵乘法不满足交换律 a*b != b*a\n * 矩阵的乘法满足结合律: a*(b*c)=(a*b)*c\n * 单位矩阵:在矩阵乘法中 有一种特殊矩阵 称为单位矩阵 它是个方针 一般用 i 或者 e表示 从左上角到右下角的对角线(称为主对角线) 上的元素均为为1 其他全为0 如果a*b=e 那么矩阵a和b互为逆矩阵\n\n\n\n\n# 逆、转置\n\n 1. 待定系数法\n\n给予一个2x2的矩阵 我们假设一个同样为2x2的矩阵 [a b] [b c]\n\na b矩阵相乘 转为单位矩阵\n\n\n\n\n\n求出 a b c d 的值 ,就可以得出 a的逆矩阵\n\n\n\n 2. 伴随矩阵求逆矩阵\n\n * 伴随矩阵是矩阵元素所对应的代数余子式，所构成的矩阵，转置后得到的新矩阵。\n\n\n\n原矩阵为 [1 2]\n\n[-1 -3]\n\na11 为 第一行和第一列的元素 都去掉剩下 -3 又因为a11为 1+1=2 为偶数是整数 1*-3=-3 a11=-3\n\na12 为 第一行和第二列的元素 都去掉剩下 -1 又因为a12为 1+2=3 为奇数 所以为负号 a12=-(-1)=1\n\na21 为 第二行和第一列的元素 都去掉剩下 2 2+1为奇数 a21=-(2)=-2\n\na22 为 第二行和第二列的元素 都去掉剩下 1 为偶数 a22=1\n\n得出[-3 1] 的矩阵 进行转置即行列变换第一行内容变成第一列内容 变成 [-3 -2]\n\n[-2 1] 第二行内容变成第二列内容 [1 1]\n\n接下来求出矩阵a的行列式\n\n * 行列式\n\n二阶行列式的计算方法是“对角线法则” 主对角线元素积与副对角线元素积的差\n\n\n\n二阶行列式并不适合三阶使用\n\n正对角线为正，反对角线为负。\n\n\n\nxsc + yt*a + ra* z - zsa - yr*c - tb* x\n\n由行列号性质得出\n\n|a|1*(-1)-(-1)*2=-1\n\na⁻¹=a*/|a| = a*/(-1)=-a* = [3 2]\n\n[-1 -1]\n\n 3. 初等变换求逆矩阵\n\n首先我们得出a的增广矩阵\n\n\n\n然后进行初等行变换。依次进行\n\n第1行加到第2行，得到\n\n第2行×2加到第1行，得到\n\n第2行×(-1)，得到\n\n\n\n因此逆矩阵a⁻¹=\n\n3 2\n\n-1 -1\n\n\n\n# 转置\n\n即将矩阵的行列互换\n\n\n\n\n# np方法\n\n * np.matmul()\n   \n   * a = np.array([[80, 86], [82, 80], [85, 78], [90, 90],\n                  [86, 82], [82, 90]])  # 是的多行2列的矩阵\n     b = np.array([[0.7], [0.3]])  # 两行一列的矩阵\n     np.matmul(a, b)  # 转为m行l列 即多行一列\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * np.dot()\n   \n   * #np.matmul(a, 10) #只支持矩阵相乘\n     np.dot(a, 10)  # matmul和dot功能一致 但dot支持点乘  标量运算 matmul不支持点乘\n     \n     \n     1\n     2\n     ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Pandas",frontmatter:{title:"Pandas",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/3f7274/",categories:["后端","机器学习"],tags:[null]},regularPath:"/%E5%90%8E%E7%AB%AF/08.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/04.Pandas.html",relativePath:"后端/08.机器学习/04.Pandas.md",key:"v-c631dd4e",path:"/pages/3f7274/",headers:[{level:2,title:"Series",slug:"series",normalizedTitle:"series",charIndex:60},{level:3,title:"创建",slug:"创建",normalizedTitle:"创建",charIndex:145},{level:3,title:"属性",slug:"属性",normalizedTitle:"属性",charIndex:560},{level:2,title:"DataFrame",slug:"dataframe",normalizedTitle:"dataframe",charIndex:698},{level:3,title:"创建",slug:"创建-2",normalizedTitle:"创建",charIndex:145},{level:3,title:"属性和查看",slug:"属性和查看",normalizedTitle:"属性和查看",charIndex:1847},{level:2,title:"列名修改",slug:"列名修改",normalizedTitle:"列名修改",charIndex:3607},{level:2,title:"单元格操作",slug:"单元格操作",normalizedTitle:"单元格操作",charIndex:3960},{level:3,title:"读取",slug:"读取",normalizedTitle:"读取",charIndex:3970},{level:3,title:"赋值",slug:"赋值",normalizedTitle:"赋值",charIndex:4455},{level:3,title:"替换值",slug:"替换值",normalizedTitle:"替换值",charIndex:4768},{level:2,title:"索引重置和设置",slug:"索引重置和设置",normalizedTitle:"索引重置和设置",charIndex:5068},{level:2,title:"增加列和行",slug:"增加列和行",normalizedTitle:"增加列和行",charIndex:6156},{level:2,title:"删除列或行",slug:"删除列或行",normalizedTitle:"删除列或行",charIndex:6704},{level:2,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:7096},{level:3,title:"DataFrame",slug:"dataframe-2",normalizedTitle:"dataframe",charIndex:698},{level:3,title:"Series",slug:"series-2",normalizedTitle:"series",charIndex:60},{level:2,title:"运算",slug:"运算",normalizedTitle:"运算",charIndex:7629},{level:2,title:"数据筛选",slug:"数据筛选",normalizedTitle:"数据筛选",charIndex:8725},{level:3,title:"筛选列",slug:"筛选列",normalizedTitle:"筛选列",charIndex:8734},{level:3,title:"筛选行",slug:"筛选行",normalizedTitle:"筛选行",charIndex:8974},{level:3,title:"组合",slug:"组合",normalizedTitle:"组合",charIndex:8932},{level:2,title:"自定义函数",slug:"自定义函数",normalizedTitle:"自定义函数",charIndex:9698},{level:2,title:"Pandas画图",slug:"pandas画图",normalizedTitle:"pandas画图",charIndex:10015},{level:2,title:"读取和保存",slug:"读取和保存",normalizedTitle:"读取和保存",charIndex:10151},{level:2,title:"set_option 选择设置",slug:"set-option-选择设置",normalizedTitle:"set_option 选择设置",charIndex:13366},{level:2,title:"reset_option",slug:"reset-option",normalizedTitle:"reset_option",charIndex:14596},{level:2,title:"style",slug:"style",normalizedTitle:"style",charIndex:14972},{level:2,title:"缺失值处理",slug:"缺失值处理",normalizedTitle:"缺失值处理",charIndex:18228},{level:3,title:"判断缺失值",slug:"判断缺失值",normalizedTitle:"判断缺失值",charIndex:18238},{level:3,title:"替换和替换缺失值或标记值",slug:"替换和替换缺失值或标记值",normalizedTitle:"替换和替换缺失值或标记值",charIndex:19094},{level:3,title:"缺失值转换",slug:"缺失值转换",normalizedTitle:"缺失值转换",charIndex:20228},{level:3,title:"缺失值标记",slug:"缺失值标记",normalizedTitle:"缺失值标记",charIndex:11334},{level:3,title:"缺失值忽略",slug:"缺失值忽略",normalizedTitle:"缺失值忽略",charIndex:20424},{level:2,title:"数据离散化",slug:"数据离散化",normalizedTitle:"数据离散化",charIndex:20511},{level:3,title:"one-hot编码",slug:"one-hot编码",normalizedTitle:"one-hot编码",charIndex:21389},{level:2,title:"数据合并",slug:"数据合并",normalizedTitle:"数据合并",charIndex:21525},{level:3,title:"concat",slug:"concat",normalizedTitle:"concat",charIndex:6081},{level:3,title:"merge",slug:"merge",normalizedTitle:"merge",charIndex:22630},{level:3,title:"join 组合",slug:"join-组合",normalizedTitle:"join 组合",charIndex:23841},{level:2,title:"交叉表与透视表",slug:"交叉表与透视表",normalizedTitle:"交叉表与透视表",charIndex:24272},{level:2,title:"分组与聚合",slug:"分组与聚合",normalizedTitle:"分组与聚合",charIndex:25898},{level:3,title:"分组可视化",slug:"分组可视化",normalizedTitle:"分组可视化",charIndex:29554},{level:2,title:"时间处理",slug:"时间处理",normalizedTitle:"时间处理",charIndex:30128},{level:2,title:"去重",slug:"去重",normalizedTitle:"去重",charIndex:3336},{level:2,title:"热力图",slug:"热力图",normalizedTitle:"热力图",charIndex:31242},{level:2,title:"热力地图",slug:"热力地图",normalizedTitle:"热力地图",charIndex:31656},{level:2,title:"直方图",slug:"直方图",normalizedTitle:"直方图",charIndex:10100}],headersStr:"Series 创建 属性 DataFrame 创建 属性和查看 列名修改 单元格操作 读取 赋值 替换值 索引重置和设置 增加列和行 删除列或行 排序 DataFrame Series 运算 数据筛选 筛选列 筛选行 组合 自定义函数 Pandas画图 读取和保存 set_option 选择设置 reset_option style 缺失值处理 判断缺失值 替换和替换缺失值或标记值 缺失值转换 缺失值标记 缺失值忽略 数据离散化 one-hot编码 数据合并 concat merge join 组合 交叉表与透视表 分组与聚合 分组可视化 时间处理 去重 热力图 热力地图 直方图",content:"# Pandas\n\nPandas 以Numpy为基础 和 matplotlib 开源的数据挖掘库 用于数据探索\n\n\n# Series\n\nSeries是一个类似于一维数组的数据结构，它能够保存任何类型的数据，比如整数、字符串、浮点数等，主要由一组数据和与之相关的索引两 部分构成\n\n\n\n\n# 创建\n\n * pd.Series(data=None, index=None, dtype=None)\n   \n   * data：传入的数据，可以是ndarray、list等\n   * index：索引，必须是唯一的，且与数据的长度相等。如果没有传入索引参数，则默认会自动创建一个从0-N的整数索引。\n   * dtype：数据的类型\n   \n   pd.Series(np.arange(10))\n   \n   \n   1\n   \n\n * 指定索引\n   \n   pd.Series([6.7,5.6,3,10,2], index=[1,2,3,4,5])\n   \n   \n   1\n   \n\n * 通过字典创建\n   \n   color_count = pd.Series({'red':100, 'blue':200, 'green': 500, 'yellow':1000})\n   \n   \n   1\n   \n\n\n# 属性\n\n * color_count.index 返回行索引\n * color_count.values 返回一个np数组 存放为列值\n * color_count.tolist() 返回一个np数组 存放为列值\n * color_count[n] 通过索引来获取值\n\n\n# DataFrame\n\nDataFrame是一个类似于二维数组或表格(如excel)的对象，既有行索引，又有列索引\n\n 1. 行索引，表明不同行，横向索引，叫index，0轴，axis=0\n 2. 列索引，表名不同列，纵向索引，叫columns，1轴，axis=1\n\n\n# 创建\n\n * 创建DataFrame 需要传递一个ndarray对象\n   \n   * stock_change = np.random.normal(0, 1, (10, 5))\n     #导入np数据\n     df = pd.DataFrame(stock_change)\n     #根据行数生成行索引\n     stock_code = [\"股票{}\".format(i+1) for i in range(stock_rise.shape[0])]\n     #修改行索引名称\n     pd.DataFrame(stock_change,index=stock_code)\n     #生成时间序列\n     #date_range(start=None,end=None,periods=None,freq=\"B\")\n     #start开始时间 end结束时间 periods时间天数 freq递进单位默认为一台   B为略过周末\n     date = pd.date_range(start=\"20211020\",periods=stock_rise.shape[1],freq=\"B\")\n     \n     #添加列索引名称\n     stock_c = pd.DataFrame(stock_change,index=stock_code,columns=date)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 增加行、列索引\n   \n   * # 构造行索引序列\n     subjects = [\"语文\", \"数学\", \"英语\", \"政治\", \"体育\"]\n     # 构造列索引序列\n     stu = ['同学' + str(i) for i in range(score_df.shape[0])]\n     # 添加行索引\n     data = pd.DataFrame(score, columns=subjects, index=stu)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# 属性和查看\n\n * df.shape 查看几行几列\n   \n   * #查看几行几列\n     stock_rise.shape\n     #查看行\n     stock_rise.shape[0]\n     #查看列\n     stock_rise.shape[1]\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * df.index 查询行索引列表\n\n * df.columns 查看当前df的列名列表\n\n * df.values 查询所有数据 返回为一个np数组\n\n * df.sample(n) 随机查看n条数据 默认为1条\n   \n   * df.sample(5)\n     \n     \n     1\n     \n\n * df.head() 查看前5行 可传递指定行数\n   \n   * df.head(12)\n     \n     \n     1\n     \n\n * df.tail() 后5行 可传递指定行数\n\n * df.info() 查看数据基本信息\n   \n   * df.info()\n     \n     <class 'pandas.core.frame.DataFrame'>\n     RangeIndex: 262 entries, 0 to 261\n     Data columns (total 11 columns):\n      #   Column  Non-Null Count  Dtype  \n     ---  ------  --------------  -----  \n      0   片名      262 non-null    object \n      1   上映年份    262 non-null    int64  \n      2   评分      257 non-null    float64\n      3   评价人数    259 non-null    float64\n      4   导演      262 non-null    object \n      5   编剧      262 non-null    object \n      6   主演      262 non-null    object \n      7   类型      262 non-null    object \n      8   国家/地区   256 non-null    object \n      9   语言      256 non-null    object \n      10  时长(分钟)  256 non-null    float64\n     dtypes: float64(3), int64(1), object(7)\n     memory usage: 22.6+ KB\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     \n\n * df.describe() 查看数据统计信息｜整体\n   \n   * df.describe(include='all')  #默认不包含 unique去重后数据量  top出现频率最高的  freq频率计数\n     df.describe().round(2).T #保留2为小数 并进行行列互换 方便查看\n     \n     \n     1\n     2\n     \n   \n   * \n\n * df.dtypes 查看各列数据类型\n\n * df.corr() 相关系数矩阵，也就是每两列之间的相关性系数\n\n * df支持切片\n   \n   * df[0:]\n     df[::2]\n     \n     \n     1\n     2\n     \n\n * df.T 行列互换\n\n\n# 列名修改\n\n * df.columns=[\"1\",\"2\"] 将全部列设置为指定列表\n * df.rename(columns={\"a\":\"A\",\"b\":\"B\"},inplace=True) 替换指定列\n   * columns:列名 如传递一个数组则需要与csv文件中列名数保持一致 如果传递为map则将指定列名替换为值 {\"positionId\":\"ID\",\"positionName\":\"岗位名称\",\"salary\":\"薪资\"}\n   * inplace: 是否替换原dataframe 默认为false\n\ndf.rename(columns={\"positionId\":\"ID\",\"positionName\":\"岗位名称\",\"salary\":\"薪资\"},inplace=True)\n\n\n1\n\n\n\n# 单元格操作\n\n\n# 读取\n\n * df[\"列名\"][\"行索引\"] df中默认是先列后行 不支持切片 如果单列则支持索引\n   \n   * data['open']['2018-02-27'] #先列后行  不支持切片\n     data['open'][1:2] #单列 支持切片\n     \n     \n     1\n     2\n     \n\n * df.loc[\"2018-02-27\":\"2018-02-23\",\"high\"] 通过loc 索引名称 实现先行后列的查询 支持切片\n\n * df.iloc[行下标,列下标] 通过索引下标值 支持切片\n   \n   * stock_data.iloc[3,5]\n     stock_data.iloc[:3,:5]\n     \n     \n     1\n     2\n     \n\n * df.at[\"行索引\",\"列名\"] at先行后列的查询\n\n * 多列名读取\n   \n   * df[[\"positionId\",\"companyId\",\"industryField\"]]\n     \n     \n     1\n     \n\n\n# 赋值\n\n * 指定列赋值\n   \n   * data['close'] = 1\n     data.close = 1\n     df[\"金牌大于30\"] = np.where(df[\"金牌数\"]>30,\"是\",\"否\") #支持比较\n     \n     \n     1\n     2\n     3\n     \n\n * 指定单元格赋值\n   \n   * stock_data[\"open\"][\"2018-02-27\"]=1\n     df.at[5,\"国家奥委会\"] = \"俄奥委会\"\n     df.iloc[4,0] = \"俄奥委会\"\n     \n     \n     1\n     2\n     3\n     \n\n\n# 替换值\n\n * df.replace(\"指定值\",\"替换值\",inplace=True)\n   \n   * df.replace(0,\"无\",inplace=True) #整个df替换\n     df[\"金牌数\"].replace(0,\"无\",inplace=True)\n     \n     #替换值（多值）\n     df.replace(\"无\",np.nan,inplace=True)\n     df.replace(0,\"None\",inplace=True)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# 索引重置和设置\n\n * 修改索引 传递一个相同个数的列表 可以获取下标获取值但不能单个修改\n   \n   * stu = [\"学生_\" + str(i) for i in range(score_df.shape[0])]\n     # 必须整体全部修改\n     data.index = stu\n     #索引可以通过下标获取 但无法直接进行修改\n     #stu.index[3] \n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * reset_index(drop=False) 重置索引 drop默认为Flase不删除原来索引\n   \n   * # 重置索引,drop=False  默认保留旧索引\n     data.reset_index()\n     # 重置索引,drop=True\n     data.reset_index(drop=True)\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * set_index(keys,drop = True)以某列值设置为新的索引 keys列名或列索引 drop默认为true 删除原来的索引列\n   \n   * df.set_index(\"year\") #以某列设置为新的索引\n     df.set_index(keys=[\"year\",\"month\"])  #设置多个索引\n     \n     \n     1\n     2\n     \n\n * df.rename_axis(keys,inplace=True) 更改当前索引列名\n   \n   * df.rename_axis(\"金牌排名\",inplace=True)\n     \n     \n     1\n     \n\n * df.set_index(\"列名\",inplace=True) 将指定列设定为索引\n   \n   * df.set_index(\"排名\",inplace=True)\n     df.set_index([\"排名\",\"总分\"],inplace=True) #支持多列行索引\n     \n     \n     1\n     2\n     \n\n * df.reindex([\"索引名1\",\"索引名2\",...]) 将索引重置为指定索引列表\n   \n   * pd.concat([df1, df4], axis=1).reindex(df1.index)\n     \n     \n     1\n     \n\n\n# 增加列和行\n\n * 增加列 直接df引用未定的列名 自动创建\n\ndf[\"比赛地点\"] = \"东京\"\n\n\n1\n\n\n * 在最后一行增加行 使用append\n   \n   * df1 = pd.DataFrame([[i for i in range(len(df.columns))]],columns=df.columns)\n     df_new = df.append(df1)\n     \n     \n     1\n     2\n     \n\n * 在指定行中 增加行 使用concat\n   \n   * df1 = df.iloc[:1, :]\n     df2 = df.iloc[1:, :]\n     df3 = pd.DataFrame([[i for i in range(len(df.columns))]], columns=df.columns)\n     df_new = pd.concat([df1, df3, df2], ignore_index=True)\n     \n     \n     \n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n\n# 删除列或行\n\n * df.drop(columns,axis=0) 删除列或行 传递指定行索引名或者列名\n   \n   * df.drop(1) #删除索引名为1的行\n     df.drop(df[df.金牌数<20].index) #条件删除\n     df.drop(['ma5', 'ma10', 'ma20', 'v_ma5', 'v_ma10', 'v_ma20'],axis=1 )  #删除多个列\n     df.drop(columns=[\"比赛地点\"],inplace=True) #删除指定列\n     df.drop(df.columns[[7,8,9,10]],axis=1,inplace=True) #按列号删除\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# 排序\n\n\n# DataFrame\n\n * df.sort_values(by=, ascending=) 按列的值排序 by: 列名 ascending: 默认为True升序 False降序\n   \n   * data.sort_values(by=\"open\", ascending=True)  #升序\n     data.sort_values(by=['open', 'high'], ascending=False) #多个列降序\n     \n     \n     1\n     2\n     \n\n * df.sort_index() 根据索引进行排序 默认升序\n\n\n# Series\n\n * series.sort_values(ascending=True) 对series中的值进行排序 默认升序\n   \n   * data['p_change'].sort_values(ascending=True)\n     \n     \n     1\n     \n\n * series.sort_index() 索引排序\n   \n   * data['p_change'].sort_index()\n     \n     \n     1\n     \n\n\n# 运算\n\n * data.add 相加 data['open'].add(1)\n\n * 逻辑运算\n   \n   * data[data[\"open\"] > 23]\n     data[(data[\"open\"] > 23) & (data[\"open\"] < 24)]  # 支持逻辑运算符\n     \n     \n     1\n     2\n     \n\n * data.query(expr) 逻辑运算函数\n   \n   * data.query(\"open<24 & open>23\")  # 只需要根据列名 + 条件就可以判断\n     \n     \n     1\n     \n\n * data.isin(values) 判断值是否在指定范围\n   \n   * data[data[\"open\"].isin([23.53, 23.85])]\n     \n     \n     1\n     \n\n * data.agg() 进行聚合操作 支持自定义min max mead sum等操作\n   \n   * df.agg({\n             \"总分\": [\"min\", \"max\", \"median\", \"mean\"],\n             \"高端人才得分\": [\"min\", \"max\", \"median\", \"mean\"],\n             \"办学层次得分\":[\"min\", \"max\", \"median\", \"mean\"]})\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * data.max(0) 最大值 默认为列计算 axis=0 传递1为行计算\n\n * data.min() 最小值\n\n * data.std() 标准差\n\n * data.var() 方差\n\n * data.median() 中位数\n\n * data.quantile() 分位数 默认返回50%的分位数 即中位数\n\n * data.idxmax(axis=0) 最大值的索引名\n\n * data.idxmin(axis=0) 最小值的索引名\n\n * data.cumsum() 连续求和 累加到最后一个单元格\n\n * data.cummax() 计算最大值\n\n * data.cummin() 计算最小值\n\n * data.cumprod() 累乘\n\n * data.mode() 众数\n\n * data.nsmallest(n) 返回当前列最小的n个数\n\n * data.nlargest(n) 返回当前列最大的n个数\n\n\n# 数据筛选\n\n\n# 筛选列\n\ndf.iloc[:,[0,1,2,3]]  #通过列号筛选\ndf[['金牌数','银牌数','铜牌数']] #通过列名\ndf.iloc[:,[i%2==0 for i in range(len(df.columns))]] #条件筛选\ndf.loc[:, df.columns.str.endswith('数')] #提取全部列名中包含 数 的列\ndf.iloc[9:20,-3:] #组合（行号+列名） 提取倒数后三列的10-20行\n\n\n1\n2\n3\n4\n5\n\n\n\n# 筛选行\n\ndf.loc[9:9] # 通过行号\ndf.iloc[9:,:] #切片\ndf.loc[:50:3] # 步进\ndf[df['金牌数'] > 30] # 判断\ndf[df[\"金牌数\"] == 10] \ndf[df[\"金牌数\"] != 10]\ndf[[i%2 != 0 for i in range(len(df.index))]] # 条件\ndf.loc[df['国家奥委会'].isin(['中国','美国','英国','日本','巴西'])] # 判断是否在列表中的值 \ndf.loc[((df['金牌数'] < 30) & df['国家奥委会'].isin(['中国', '美国', '英国', '日本', '巴西']))] #多条件  每个条件要用括号包起来\ndf[df.国家奥委会.str.contains('国')] # 提取 国家奥委会 列中，所有包含 国的行\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 组合\n\ndf.iloc[0:1,[1]] # 第 0 行第 2 列\ndf.iloc[0:2,0:2] #筛选多行多列\ndf.iloc[3,3] #行号+列号\ndf.loc[4,\"金牌数\"] # 行号+列名\ndf.loc[df['国家奥委会'] == '中国'].loc[1].at['金牌数'] # 条件\ndf.query(\"金牌数 + 银牌数 > 15\") # query 类sql筛选\n\n#query（引用变量）\nme = df[\"金牌数\"].mean()\ndf.query(f\"金牌数 > {me}\")  #需要要{}引用变量  类sql语句\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 自定义函数\n\n * df.apply(自定义函数名,axis=0) df会传递一个Series对象 通过自定义函数return一个结果 axis=0:默认是列，axis=1为行进行运算\n   \n   * def abc(ses):\n         c_null = pd.isnull(ses)\n         is_null = column[c_null]\n         return len(is_null)\n     \n     df.apply(abc)  #会返回每一列null的个数\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# Pandas画图\n\n * df.plot (kind='line') 默认为line折线图\n   * line 折线图\n   * bar 柱状图\n   * barh 横向直方图\n   * hist 直方图\n   * pie 饼图\n   * scatter 散点图\n\n\n# 读取和保存\n\n * pd.read_csv(filepath_or_buffer, sep =',', usecols=[1,3,5],skiprows=[1,3,5],index_col=[\"列名\"],names=[], keep_default_na=False,na_values=['[]'],na_filter= True,dtype={'positionId': str,'companyId':str})\n   \n   * filepath_or_buffer:文件路径\n     \n     * pd.read_csv(\"某招聘网站数据.csv\")\n       \n       \n       1\n       \n   \n   * sep :分隔符，默认用\",\"隔开\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",sep=\",\")\n       \n       \n       1\n       \n   \n   * usecols:指定读取的列名，列表形式 [1,3,5] 只读取2 4 6列 或指定列名 ['positionId','positionName','salary']\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",usecols = ['positionId','positionName','salary'])\n       pd.read_csv(\"某招聘网站数据.csv\",usecols = [1,3,5])\n       \n       \n       1\n       2\n       \n   \n   * skiprows: 跳过指定行数 [1,3,5] 跳过 2 4 6 行\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",skiprows = [i for i in range(1,21)])\n       \n       \n       1\n       \n   \n   * index_col: 将指定列设置为行索引\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",index_col=\"positionId\")\n       \n       \n       1\n       \n   \n   * names: 指定新的列名 ['ID','岗位名称','薪资'] 不支持map\n     \n     * pd.read_csv('某招聘网站数据.csv', usecols=[0,1,17],header = 0,names=['ID','岗位名称','薪资'])\n       \n       \n       1\n       \n   \n   * keep_default_na: 缺失值标记为NaN 默认为True\n   \n   * pd.read_csv('某招聘网站数据.csv', keep_default_na=False)\n     \n     \n     1\n     \n   \n   * na_values: 将缺失值标记为指定字符串 如['[]']\n     \n     * pd.read_csv('某招聘网站数据.csv',na_values=['[]'])\n       \n       \n       1\n       \n   \n   * na_filter: 缺失值是否处理 默认为True 处理\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",na_filter=False)\n       \n       \n       1\n       \n   \n   * dtype: 读取时指定列类型 传递map\n     \n     * pd.read_csv(\"某招聘网站数据.csv\", dtype={'positionId': str,'companyId':str}) \n       \n       \n       1\n       \n   \n   * parse_dates: 将指定列转为datatime类型\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",parse_dates=['createTime']) \n       \n       \n       1\n       \n   \n   * chunksize: 分块读取 返回一个可迭代对象，每次读取 n 行\n     \n     * data = pd.read_csv(\"某招聘网站数据.csv\", chunksize= 10)\n       for i in data:\n           print(i)\n       \n       \n       1\n       2\n       3\n       \n\n * df.to_csv(path_or_buf=None, sep=', ’, columns=None, header=True, index=True, mode='w', encoding=None)\n   \n   * path_or_buf :文件路径\n   \n   * sep :分隔符，默认用\",\"隔开\n   \n   * columns :选择需要的列索引\n     \n     * data.to_csv(\"234.csv\",columns=[\"positionName\",\"salary\"])\n       \n       \n       1\n       \n   \n   * header :boolean or list of string, default True,是否写进列索引值\n   \n   * index:是否写进行索引\n     \n     * data.to_csv(\"234.csv\",index=False) #不写入索引\n       \n       \n       1\n       \n   \n   * mode:'w'：重写, 'a' 追加\n   \n   * na_rep: 缺失值标记为指定字符\n     \n     * data.to_csv(\"234.csv\",index=False,na_rep=\"数据缺失\")\n       \n       \n       1\n       \n\n * pd.read_hdf(path_or_buf，key =None)\n   \n   * path_or_buffer:文件路径\n   * key:读取的键\n   * 需要先安装tables库 pip install tables\n\n * df.to_hdf(path_or_buf, key)\n\n * pd.read_json(\"./data//test.json\",orient=\"records\",lines=True) orient指定列 lines是否一行一个数据默认为false 一行一大串数据 推荐设置为true\n\n * df.to_json(\"./data/test.json\",orient=\"close\")\n\n * pd.read_clipboard() 从剪贴板读取数据\n\n * pd.read_sql(sql语句,conn连接对象)\n   \n   * from sqlite3 import connect\n     conn = connect(':memory:')\n     pd.read_sql(\"SELECT int_column, date_column FROM test_data\",conn)\n     \n     \n     1\n     2\n     3\n     \n\n\n# set_option 选择设置\n\noption 选择设置为全局配置\n\n * display.max_columns 显示全部列\n   \n   * pd.set_option('display.max_columns', None) #显示全部列\n     \n     \n     1\n     \n\n * display.max_columns 显示指定列\n   \n   * pd.set_option('display.max_columns', 10)  \n     \n     \n     1\n     \n\n * display.max_rows 显示指定行\n   \n   * pd.set_option('display.max_rows', 7)\n     \n     \n     1\n     \n\n * display.max_colwidth 每列最多显示10个字符，多余的会变成... 设置指定长度字符\n   \n   * pd.set_option ('display.max_colwidth',10)\n     \n     \n     1\n     \n\n * precision 修改默认显示精度为小数点后5位\n   \n   * pd.set_option('precision', 5)\n     \n     \n     1\n     \n\n * mode.chained_assignment 取消pandas相关warning提示\n   \n   * pd.set_option(\"mode.chained_assignment\", None) \n     # 全局取消warning\n     # import warnings\n     # warnings.filterwarnings('ignore')\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * chop_threshold 数值显示条件 如果数值小于 20 则显示为0\n   \n   * pd.set_option('chop_threshold', 20) \n     \n     \n     1\n     \n\n * display.html.use_mathjax 让dataframe中内容支持 Latex 显示（需要使用$$包住）\n   \n   * pd.set_option(\"display.html.use_mathjax\",True)\n     \n     \n     1\n     \n\n * plotting.backend 修改绘图引擎\n   \n   * #修改pandas默认绘图引擎为plotly（需要提前安装好plotly）\n     pd.set_option(\"plotting.backend\",\"plotly\")\n     \n     \n     1\n     2\n     \n\n\n# reset_option\n\n * 根据key还原指定选择设置 去除display.\n   \n   * pd.reset_option(\"max_rows\")\n     pd.reset_option(\"max_columns\")\n     \n     \n     1\n     2\n     \n\n * 还原全部显示设置 display\n   \n   * pd.reset_option(\"^display\")\n     \n     \n     1\n     \n\n * 还原全部 option 设置\n   \n   * pd.reset_option(\"all\") \n     \n     \n     \n     \n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# style\n\n基于 style 个性化设置同样不会修改数据，所有 data.style.xxxx 输出的数据均是一次性的（可以复用、导出），因此你应该在合适的时间选择使用该方法。\n\n * style.hide_index() 隐藏索引列\n   \n   * data.style.hide_index()\n     \n     \n     1\n     \n\n * style.set_precision(n) 调整小数的精度\n   \n   * # 将带有小数点的列精度调整为小数点后2位\n     data.style.set_precision(2)\n     \n     \n     \n     1\n     2\n     3\n     \n\n * style.set_na_rep(\"指定字符\") 标记缺失值为指定字符\n   \n   * (data.style.set_na_rep(\"数据缺失\"))\n     \n     \n     1\n     \n\n * style.highlight_null(null_color=\"red\") 高亮缺失值\n   \n   * null_color: 为高亮颜色 默认为红色\n   \n   * (data.style.set_na_rep(\"数据缺失\").highlight_null(null_color='skyblue'))\n     \n     \n     1\n     \n\n * style.highlight_max 高亮数值列最大值\n   \n   * (data.style.highlight_max)\n     \n     \n     1\n     \n\n * style.highlight_min 高亮数值列最小值\n   \n   * (data.style.highlight_min)\n     \n     \n     1\n     \n\n * 同时高亮最大最小值\n   \n   * (data\n     .style\n     .highlight_max(color='#F77802')\n     .highlight_min(color='#26BE49'))\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * style.highlight_between 设置指定列 指定范围内高亮\n   \n   * (data\n     .style\n     .highlight_between(left=3000, right=10000, subset=['salary']))\n     \n     \n     1\n     2\n     3\n     \n\n * style.background_gradient 渐变显示数值列\n   \n   * import seaborn as sns\n     \n     cm = sns.light_palette(\"green\", as_cmap=True)\n     \n     (data\n     .style\n     .background_gradient(cmap=cm))\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n * style.set_properties 修改指定properties\n   \n   * # 修改字体颜色\n     # 将 salary 列修改为红色字体\n     (data\n     .style\n     .set_properties(\n         subset=['salary'], **{'color': 'red'}))\n     \n     #修改背景颜色、对齐方式、字体大小\n     #1.居中 2.背景色修改为 #F8F8FF 3.字体:13px\n     (data\n     .style\n     .set_properties(**{'background-color': '#F8F8FF','text-align':'center', 'font-size': '13px'}))\n     \n     #链式多个设置进行结合\n     (data\n     .style\n     .set_properties(**{'background-color': '#F8F8FF','text-align':'center', 'font-size': '13px'})\n     .set_properties(\n         subset=['salary'], **{'color': 'red'}))\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     \n\n * 带样式导出\n   \n   * (data\n     .style\n     .set_properties(**{'background-color': '#F8F8FF','text-align':'center', 'font-size': '13px'})\n     .set_properties(\n         subset=['salary'], **{'color': 'red'})).to_excel('带有样式导出.xlsx')\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * style.bar 将指定列以横向条形图形式进行可视化\n   \n   * (data\n     .style\n     .bar(subset=['salary'],color='skyblue'))\n     \n     \n     1\n     2\n     3\n     \n\n * style.applymap 带条件样式 / 自定义样式\n   \n   * def my_style(val):\n     \n         color = 'red' if val > 30000 else 'black'\n         return 'color: %s' % color\n     \n     \n     data.style.applymap(my_style, subset=\"salary\")\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     \n\n * style.format 格式化值\n   \n   * #将 createTime 列格式化输出为 xx年xx月xx日\n     data.style.format({\"createTime\": lambda t: t.strftime(\"%Y年%m月%d日\")})\n     \n     #1.在 salary 列后增加\"元\"  2.对 matchScore 列保留两位小数并增加\"分\"\n     (data\n     .style\n     .format(\"{0:,.2f}分\", subset=\"matchScore\")\n     .format(\"{\"\"}元\", subset=\"salary\"))\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n * \n\n\n# 缺失值处理\n\n\n# 判断缺失值\n\n * type(np.NaN) 默认为float类型\n\n * pd.isnull(df) 判断df 是否有Nan 如有则返回True\n   \n   * \n   \n   * #查询每列Nan的数量\n     df.isnull().sum()\n     \n     片名        0\n     上映年份      0\n     评分        5\n     评价人数      3\n     导演        0\n     编剧        0\n     主演        0\n     类型        0\n     国家/地区     6\n     语言        6\n     时长(分钟)    6\n     dtype: int64\n         \n     #统计df中所有nan值\n     df.isna().sum().sum()\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     \n\n * pd.notnull(df) 判断df 是否有Nan 如有则返回False\n\n * np.all(pd.notnull(movie)) 里面如果有一个缺失值 则为false 则返回false\n\n * np.any(pd.isnull(movie)) 通过np.any 只有里面有一个为null就返回true\n\n * 高亮缺失值\n   \n   * (df[df.isnull().T.any() == True]\n     .style\n     .highlight_null(null_color='skyblue'))\n     \n     \n     1\n     2\n     3\n     \n\n\n# 替换和替换缺失值或标记值\n\n * df.dropna() 删除所有np.Nan 直接删除一行数据\n   \n   * df.dropna(axis=1) #按行删除\n     df.dropna(axis=0,subset=[\"列名1\",\"列名2\"]) #删除指定列中的缺失值\n     \n     \n     1\n     2\n     \n\n * df.fillna(value, inplace=False) 替换所有的缺失值为指定值\n   \n   * value:缺失值将要替换成的值\n   \n   * inplace:True:会修改原数据，默认为False:不替换修改原数据，生成新的对象\n   \n   * #可以替换单列\n     movie[\"Revenue (Millions)\"].fillna(value=movie[\"Revenue (Millions)\"].mean(),inplace=True)  \n     #更换指定列中的np.NaN为 Revenue (Millions)的中位值  inplace为是否替换原来数据 默认为false不替换原来数据\n     \n     #将评分列的缺失值，替换为上一个电影的评分\n     df['评分'] = df['评分'].fillna(axis=0,method='ffill')\n     \n     #上下均值填充\n     df['评价人数'] = df['评价人数'].fillna(df['评价人数'].interpolate())\n     \n     #匹配填充\n     df['语言']=df.groupby('国家/地区').语言.bfill() \n     #bfill:向前填充  ffill:向后填充  默认 axis=0(列方向)  vs  axis=1(行方向)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     \n\n * df.replace(to_replace=, value=)\n   \n   * #先把被标记的缺失值 也是? 替换成np.nan\n     wis = wis.replace(to_replace='?',value=np.nan)\n     #再进行缺失值处理\n     wis = wis.dropna()\n     wis.head(10)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# 缺失值转换\n\n不将缺失值标记为 NAN\n\ndata = pd.read_csv('某招聘网站数据.csv', keep_default_na=False)\ndata\n\n\n1\n2\n\n\nNaN会变成空字符\n\n\n# 缺失值标记\n\n将指定的字符标记为NaN缺失值\n\ndata = pd.read_csv('某招聘网站数据.csv',na_values=['[]'])\ndata\n\n\n1\n2\n\n\n\n# 缺失值忽略\n\n忽略缺失值 变为空字符串\n\ndata = pd.read_csv(\"某招聘网站数据.csv\",na_filter=False)\ndata\n\n\n1\n2\n\n\n\n# 数据离散化\n\n连续属性的离散化就是在连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数 值代表落在每个子区间中的属性值。\n\n\n\n * pd.qcut(data, q) 对数据进行分组将数据分组，一般会与value_counts搭配使用，统计每组的个数 即将数据划分为指定个数的区间\n   \n   * # 自行分组\n     qcut = pd.qcut(p_change, 10)  #将此数据集 离散化平均分10个组\n     # 计算分到每个组数据个数\n     qcut.value_counts()  #查看每个组共有多少个数据集\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * series.value_counts()：统计分组次数\n\n * pd.cut(data, bins) 自定义区间分组\n   \n   * bins = [-100, -7, -5, -3, 0, 3, 5, 7, 100]  # 通过自定义区间范围来分组\n     cut_r = pd.cut(data_p, bins=bins)\n     cut_r.value_counts()\n     \n     \n     (0, 3]        215\n     (-3, 0]       188\n     (3, 5]         57\n     (-5, -3]       51\n     (5, 7]         35\n     (7, 100]       35\n     (-100, -7]     34\n     (-7, -5]       28\n     Name: p_change, dtype: int64\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n\n# one-hot编码\n\n把每个类别生成一个布尔列，这些列中只有一列可以为这个样本取值为1.其又被称为独热编码。\n\n\n\n * pd.get_dummies(data,prefix=\"区间前缀测试\") 将据转为 one-hot编码 prefix为区间前缀方便阅读和理解\n\n\n# 数据合并\n\n\n# concat\n\n * pd.concat([data1, data2], axis=1) 将两个df进行合并\n   \n   * 按照行或列进行合并,axis=0为列索引，axis=1为行索引\n   \n   * #默认为行连接  通过指定axis=1为按列连接\n     pd.concat([data,data_dummies],axis=1) #横向拼接\n     pd.concat([df1,df2,df3]) #重置拼接\n     \n     \n     1\n     2\n     3\n     \n   \n   * ignore_index: 重置索引 默认为flase\n     \n     * pd.concat([df1,df4],ignore_index=True)\n       \n       \n       1\n       \n   \n   * join: 什么方式加入\n     \n     * inner:默认 内连接\n       \n       * pd.concat([df1,df4],axis=1,join='inner')\n         \n         \n         1\n         \n   \n   * keys: 每个表拆分 来区分不同的表数据来源\n     \n     * pd.concat([df1, df2, df3], keys=['x', 'y', 'z'])\n       \n       \n       A\tB\tC\tD\n       x\t0\tA0\tB0\tC0\tD0\n       1\tA1\tB1\tC1\tD1\n       2\tA2\tB2\tC2\tD2\n       3\tA3\tB3\tC3\tD3\n       y\t4\tA4\tB4\tC4\tD4\n       5\tA5\tB5\tC5\tD5\n       6\tA6\tB6\tC6\tD6\n       7\tA7\tB7\tC7\tD7\n       z\t8\tA8\tB8\tC8\tD8\n       9\tA9\tB9\tC9\tD9\n       10\tA10\tB10\tC10\tD10\n       11\tA11\tB11\tC11\tD11\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       15\n       16\n       \n\n\n# merge\n\n * pd.merge(left, right, how='inner', on=None)\n   \n   * 可以指定按照两组数据的共同键值对合并或者左右各自\n   \n   * left : DataFrame\n   \n   * right : 另一个DataFrame\n   \n   * on : 指定的共同键\n     \n     * pd.merge(left, right, on='key') #以名称为key的列为共同键连接\n       pd.merge(left, right, on=['key1', 'key2']) # 多键\n       \n       \n       1\n       2\n       \n   \n   * how:按照什么方式连接\n     \n     * left:左连接\n       \n       * pd.merge(left, right,how=\"left\")\n         \n         \n         1\n         \n     \n     * right:右连接\n     \n     * outer:全外连接\n     \n     * inner: 内连接\n   \n   * suffixes:将连接表除on键 以外的列 为前缀再区分数据\n     \n     * pd.merge(left, right, on='k', suffixes=['_l', '_r'])\n       \n       \n       k\tv_l\tv_r\n       0\tK0\t1\t4\n       1\tK0\t1\t5\n       \n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       \n   \n   * pd.merge(left,right,on=[\"key1\",\"key2\"])  #默认为内连接\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"inner\")   #通过how属性指定连接类型 默认inner为内连接\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"outer\")   #outer 满外连接 没有的值会以NaN存在 需要注意\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"left\") # 左连接\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"right\") # 右连接\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# join 组合\n\n * df1.join(right,how=\"outer\") 将两表进行组合 默认为内连接\n   \n   * left.join(right) \n     \n     \n     1\n     \n   \n   * how: 连接方式\n     \n     * left.join(right,how=\"outer\")\n       \n       \n       1\n       \n     \n     * outer\n     \n     * left\n     \n     * right\n     \n     * inner\n   \n   * on: 共同键 按列名\n     \n     * left.join(right, on='v1')\n       left.join(right, on=[\"key1\",\"key2\"]) #多个列\n       \n       \n       1\n       2\n       \n\n\n# 交叉表与透视表\n\n * pd.crosstab(value1, value2) pd.crosstab(value1, value2) 交叉表用于计算一列数据对于另外一列数据的分组个数(用于统计分组频率的特殊透视表)\n\n * df.pivot_table([], ,values = [],index=[],aggfunc=\"mean\") 透视表是将原有的DataFrame的列分别作为行索引和列索引，然后对指定的列应用聚集函数\n   \n   * data.pivot_table([\"p_n\"],index=\"week\")  \n     \n     \tp_n\n     week\t\n     0\t0.496000\n     1\t0.580153\n     2\t0.537879\n     3\t0.507812\n     4\t0.535433\n     \n     pd.pivot_table(df,values = ['销售额','利润','数量'],index = '类别',aggfunc = sum) #多列(指标)\n     pd.pivot_table(df,values = ['销售额'],index = ['省/自治区','类别'],aggfunc = sum) # 多索引\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     \n   \n   * values :指标\n     \n     * pd.pivot_table(df,values = ['销售额'],index = '省/自治区')\n       \n       \n       1\n       \n   \n   * index:索引名\n   \n   * columns: 列名\n     \n     * pd.pivot_table(df,values = ['销售额'],index = ['省/自治区'], columns='类别',aggfunc = sum) #多层\n       \n       \n       1\n       \n   \n   * aggfunc: 透视图使用的方法 默认为mean平均值\n     \n     * pd.pivot_table(df,values = ['销售额'],index = '省/自治区',aggfunc = sum)\n       pd.pivot_table(df,values = ['销售额'],index = '省/自治区',aggfunc = ['mean',sum]) #多个聚合方法\n       \n       \n       1\n       2\n       \n   \n   * margins: string，默认为‘ALL’，当参数margins为True时，ALL行和列的名字\n     \n     * pd.pivot_table(df,values = ['销售额','数量'],index = ['省/自治区','类别'],aggfunc = ['mean',sum],margins=True)\n       \n       \n       1\n       \n   \n   * df.melt() 逆透视\n     \n     * table = pd.pivot_table(df,values = ['销售额','利润','数量'],index = '类别',aggfunc = sum)\n       table.melt(id_vars=['数量'],var_name='分类',value_name='金额')\n       \n       \n       1\n       2\n       \n\n\n# 分组与聚合\n\n分组与聚合通常是分析数据的一种方式，通常与一些统计函数一起使用，查看数据的分组情况\n\n * df.groupby(key, as_index=False)\n   \n   * key:分组的列数据，可以多个\n   \n   * col.groupby([\"color\"])[\"price1\"].mean()  #按照列索引color分组  再根据分组后的price1列求中位值\n     col[\"price1\"].groupby(col[\"color\"]).mean()  #取出price1的值 再按color分组 再求中位值\n     col.groupby([\"color\"],as_index=False)[\"price1\"].mean()  #为分组聚合后的数据给行索引\n     df[['district','salary']].groupby(by='district').mean().sort_values('salary',ascending=False).head(1) #分组并排序\n     \n     pd.DataFrame(df.groupby(\"district\")['companySize'].value_counts()).rename_axis([\"行政区\", \"公司规模\"]) #分组后 修改索引名\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n   \n   * groupby().groups 以字典形式查看各分组内容\n     \n     * df.groupby([\"district\",\"salary\"]).groups\n       \n       {('上城区', 22500): [81], ('上城区', 30000): [97], ('下沙', 30000): [31], ('余杭区', 7500): [84], ('余杭区', 20000): [52, 103], ('余杭区', 22500): [23, 51], ('余杭区', 25000): [62], ('余杭区', 27500): [24, 49], ('余杭区', 29000): [93], ('余杭区', 30000): [4, 10, 13, 18, 59, 61, 65, 68, 74, 76, 86, 92, 94], ('余杭区', 35000): [101], ('余杭区', 37500): [0, 32, 38, 39, 41], ('余杭区', 40000): [60, 87], ('余杭区', 45000): [25], ('余杭区', 50000): [5, 64, 90], ('余杭区', 60000): [8, 82], ('拱墅区', 24000): [72], ('拱墅区', 30000): [54, 89, 96], ('江干区', 3500): [2], ('江干区', 22500): [45], ('江干区', 30000): [73], ('江干区', 45000): [3], ('滨江区', 7500): [83], ('滨江区', 15000): [1], ('滨江区', 20000): [21, 40], ('滨江区', 22500): [37], ('滨江区', 30000): [22, 53, 55, 58, 67, 80, 102], ('滨江区', 32500): [26], ('滨江区', 37500): [17, 28, 57, 77], ('滨江区', 42500): [91], ('滨江区', 45000): [43, 47], ('滨江区', 50000): [44], ('萧山区', 25000): [100], ('萧山区', 30000): [6], ('萧山区', 45000): [66, 69], ('西湖区', 6500): [71], ('西湖区', 20000): [12], ('西湖区', 21500): [104], ('西湖区', 22500): [48, 70], ('西湖区', 24000): [42], ('西湖区', 25000): [56], ('西湖区', 26500): [78], ('西湖区', 27000): [75], ('西湖区', 27500): [15, 20, 50, 63], ('西湖区', 30000): [11, 27, 33, 34, 85, 88, 98], ('西湖区', 35000): [7], ('西湖区', 36500): [99], ('西湖区', 37500): [14, 16, 19, 30, 36, 79, 95], ('西湖区', 40000): [9, 35], ('西湖区', 45000): [29, 46]}\n       \n       \n       1\n       2\n       3\n       \n   \n   * groupby().get_group 查看分组后指定值数据\n     \n     * df.groupby([\"district\", \"salary\"]).get_group((\"西湖区\", 30000))\n       \n       \n       1\n       \n   \n   * groupby().transform(\"聚合函数名\") 分组转换\n     \n     * df['该区平均工资'] = df[['district','salary']].groupby(by='district').transform('mean')\n       df\n       \n       \n       1\n       2\n       \n   \n   * groupby().filter() 分组过滤\n     \n     * df.groupby('district').filter(lambda x: x['salary'].mean() < 30000)\n       \n       \n       1\n       \n   \n   * groupby().agg([\"聚合函数名1\",\"聚合函数名2\"]) 将分组的数据 聚合统计\n     \n     * df.groupby('district')['salary'].agg([min, max, np.mean])\n       \n       df.groupby('positionName').agg({'salary': np.median, 'score': np.mean}) #指定以指定聚合函数来获取值\n       \n       df.groupby('district').agg(\n           {'salary': [np.mean, np.median, np.std], 'score': np.mean}) #多层聚合\n       \n       \n       #自定义函数聚合\n       def myfunc(x):\n       \n           return x.max()-x.mean()\n       \n       df.groupby('district').agg(最低工资=('salary', 'min'), 最高工资=(\n           'salary', 'max'), 平均工资=('salary', 'mean'), 最大值与均值差值=('salary', myfunc)).rename_axis([\"行政区\"])\n       \n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       15\n       16\n       \n     \n     * agg.rename_axis(\"列名\") 修改列名\n       \n       * df.groupby('district').agg(最低工资=('salary', 'min'), 最高工资=(\n             'salary', 'max'), 平均工资=('salary', 'mean')).rename_axis([\"行政区\"])\n         \n         \n         1\n         2\n         \n   \n   \n   # 分组可视化\n   \n   import matplotlib.pyplot as plt\n   %config InlineBackend.figure_format = 'retina'\n   plt.rcParams['font.sans-serif'] = ['Songti SC']\n   \n   \n   df.groupby(\"district\")['positionName'].count().plot(\n       kind='bar', figsize=(10, 6), color='#5172F0', fontsize=12)\n   \n   plt.rcParams['font.sans-serif'] = 'SimHei'\n   plt.rcParams['axes.unicode_minus'] = False\n   \n   plt.ylabel(\"公司数量\", fontsize=14)\n   plt.xlabel(\"杭州市各区\", fontsize=14)\n   \n   plt.show()\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n\n\n\n# 时间处理\n\n从文件中读取的时间默认我object类型 需要转为pd时间才能进行运算\n\n * pd.to_datetime(data) 将指定列转为时间序列 返回datatime对象\n * datetime.weekday 星期几\n * datetime.week 月中第几个星期\n * datetime.day 几号\n * datetime.year 年\n * datetime.total_seconds() 将时间转为秒\n\nt1 = datetime.datetime.strptime(\"2021-11-03 10:30:00\", \"%Y-%m-%d %H:%M:%S\")\nt2 = datetime.datetime.strptime(\"2021-11-03 12:30:00\", \"%Y-%m-%d %H:%M:%S\")\n\ntime = (t2-t1).total_seconds() #获取时间差\n\n\n1\n2\n3\n4\n\n\n\n# 去重\n\n * df.drop_duplicates(subset=['列名'], keep='first') 去重 只保留第一个\n   \n   * keep: first为保留重复值中第一个 last为保留最后一个\n     \n     * df.drop_duplicates(keep=\"last\")\n       \n       \n       1\n       \n   \n   * subset: 列名\n\n * df[\"列名\"].uniue()\n\n * 查找行是否存在重复值\n   \n   * df[df.duplicated()]\n     \n     2      True\n     3      True\n     4      True\n     5      True\n     6      True\n            ... \n     257    True\n     258    True\n     259    True\n     260    True\n     261    True\n     Length: 260, dtype: bool\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 查询指定列中的重复值\n   \n   * df[df.duplicated(['片名'])]\n     \n     \n     1\n     \n\n\n# 热力图\n\ndf.corr().style.background_gradient(cmap='coolwarm').set_precision(2)\n\n### 方法二 ###\n\n#借助 `matplotlib` 和 `seaborn` \n\n# 其中中文设置可以参考我的这篇文章 https://mp.weixin.qq.com/s/WKOGvQP-6QUAP00ZXjhweg\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (9,6),dpi=100)\nsns.set(font='Songti SC')\nsns.heatmap(df.corr().round(2),annot=True,cmap='RdBu')\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n\n\n# 热力地图\n\n结合 pyecharts 进行地图可视化\n\nfrom pyecharts import options as opts\nfrom pyecharts.charts import Map\nlist1 = list(pd.DataFrame(df.省市.value_counts()).index)\nlist2 = list(pd.DataFrame(df.省市.value_counts()).省市)\n\n\nc = (\n    Map()\n    .add('', [list(z) for z in zip(list1,list2)], \"china\",is_map_symbol_show=False)\n    .set_global_opts(\n        title_opts=opts.TitleOpts(title=\"排名前100高校各省市占比\"),\n        visualmap_opts=opts.VisualMapOpts(max_=20),\n       \n\n    )\n)\nc.render_notebook()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n\n# 直方图\n\nimport seaborn as sns\nsns.set(font='Songti SC')\nsns.distplot(df['总分'])\n\n\n1\n2\n3\n\n\n",normalizedContent:"# pandas\n\npandas 以numpy为基础 和 matplotlib 开源的数据挖掘库 用于数据探索\n\n\n# series\n\nseries是一个类似于一维数组的数据结构，它能够保存任何类型的数据，比如整数、字符串、浮点数等，主要由一组数据和与之相关的索引两 部分构成\n\n\n\n\n# 创建\n\n * pd.series(data=none, index=none, dtype=none)\n   \n   * data：传入的数据，可以是ndarray、list等\n   * index：索引，必须是唯一的，且与数据的长度相等。如果没有传入索引参数，则默认会自动创建一个从0-n的整数索引。\n   * dtype：数据的类型\n   \n   pd.series(np.arange(10))\n   \n   \n   1\n   \n\n * 指定索引\n   \n   pd.series([6.7,5.6,3,10,2], index=[1,2,3,4,5])\n   \n   \n   1\n   \n\n * 通过字典创建\n   \n   color_count = pd.series({'red':100, 'blue':200, 'green': 500, 'yellow':1000})\n   \n   \n   1\n   \n\n\n# 属性\n\n * color_count.index 返回行索引\n * color_count.values 返回一个np数组 存放为列值\n * color_count.tolist() 返回一个np数组 存放为列值\n * color_count[n] 通过索引来获取值\n\n\n# dataframe\n\ndataframe是一个类似于二维数组或表格(如excel)的对象，既有行索引，又有列索引\n\n 1. 行索引，表明不同行，横向索引，叫index，0轴，axis=0\n 2. 列索引，表名不同列，纵向索引，叫columns，1轴，axis=1\n\n\n# 创建\n\n * 创建dataframe 需要传递一个ndarray对象\n   \n   * stock_change = np.random.normal(0, 1, (10, 5))\n     #导入np数据\n     df = pd.dataframe(stock_change)\n     #根据行数生成行索引\n     stock_code = [\"股票{}\".format(i+1) for i in range(stock_rise.shape[0])]\n     #修改行索引名称\n     pd.dataframe(stock_change,index=stock_code)\n     #生成时间序列\n     #date_range(start=none,end=none,periods=none,freq=\"b\")\n     #start开始时间 end结束时间 periods时间天数 freq递进单位默认为一台   b为略过周末\n     date = pd.date_range(start=\"20211020\",periods=stock_rise.shape[1],freq=\"b\")\n     \n     #添加列索引名称\n     stock_c = pd.dataframe(stock_change,index=stock_code,columns=date)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 增加行、列索引\n   \n   * # 构造行索引序列\n     subjects = [\"语文\", \"数学\", \"英语\", \"政治\", \"体育\"]\n     # 构造列索引序列\n     stu = ['同学' + str(i) for i in range(score_df.shape[0])]\n     # 添加行索引\n     data = pd.dataframe(score, columns=subjects, index=stu)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# 属性和查看\n\n * df.shape 查看几行几列\n   \n   * #查看几行几列\n     stock_rise.shape\n     #查看行\n     stock_rise.shape[0]\n     #查看列\n     stock_rise.shape[1]\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * df.index 查询行索引列表\n\n * df.columns 查看当前df的列名列表\n\n * df.values 查询所有数据 返回为一个np数组\n\n * df.sample(n) 随机查看n条数据 默认为1条\n   \n   * df.sample(5)\n     \n     \n     1\n     \n\n * df.head() 查看前5行 可传递指定行数\n   \n   * df.head(12)\n     \n     \n     1\n     \n\n * df.tail() 后5行 可传递指定行数\n\n * df.info() 查看数据基本信息\n   \n   * df.info()\n     \n     <class 'pandas.core.frame.dataframe'>\n     rangeindex: 262 entries, 0 to 261\n     data columns (total 11 columns):\n      #   column  non-null count  dtype  \n     ---  ------  --------------  -----  \n      0   片名      262 non-null    object \n      1   上映年份    262 non-null    int64  \n      2   评分      257 non-null    float64\n      3   评价人数    259 non-null    float64\n      4   导演      262 non-null    object \n      5   编剧      262 non-null    object \n      6   主演      262 non-null    object \n      7   类型      262 non-null    object \n      8   国家/地区   256 non-null    object \n      9   语言      256 non-null    object \n      10  时长(分钟)  256 non-null    float64\n     dtypes: float64(3), int64(1), object(7)\n     memory usage: 22.6+ kb\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     \n\n * df.describe() 查看数据统计信息｜整体\n   \n   * df.describe(include='all')  #默认不包含 unique去重后数据量  top出现频率最高的  freq频率计数\n     df.describe().round(2).t #保留2为小数 并进行行列互换 方便查看\n     \n     \n     1\n     2\n     \n   \n   * \n\n * df.dtypes 查看各列数据类型\n\n * df.corr() 相关系数矩阵，也就是每两列之间的相关性系数\n\n * df支持切片\n   \n   * df[0:]\n     df[::2]\n     \n     \n     1\n     2\n     \n\n * df.t 行列互换\n\n\n# 列名修改\n\n * df.columns=[\"1\",\"2\"] 将全部列设置为指定列表\n * df.rename(columns={\"a\":\"a\",\"b\":\"b\"},inplace=true) 替换指定列\n   * columns:列名 如传递一个数组则需要与csv文件中列名数保持一致 如果传递为map则将指定列名替换为值 {\"positionid\":\"id\",\"positionname\":\"岗位名称\",\"salary\":\"薪资\"}\n   * inplace: 是否替换原dataframe 默认为false\n\ndf.rename(columns={\"positionid\":\"id\",\"positionname\":\"岗位名称\",\"salary\":\"薪资\"},inplace=true)\n\n\n1\n\n\n\n# 单元格操作\n\n\n# 读取\n\n * df[\"列名\"][\"行索引\"] df中默认是先列后行 不支持切片 如果单列则支持索引\n   \n   * data['open']['2018-02-27'] #先列后行  不支持切片\n     data['open'][1:2] #单列 支持切片\n     \n     \n     1\n     2\n     \n\n * df.loc[\"2018-02-27\":\"2018-02-23\",\"high\"] 通过loc 索引名称 实现先行后列的查询 支持切片\n\n * df.iloc[行下标,列下标] 通过索引下标值 支持切片\n   \n   * stock_data.iloc[3,5]\n     stock_data.iloc[:3,:5]\n     \n     \n     1\n     2\n     \n\n * df.at[\"行索引\",\"列名\"] at先行后列的查询\n\n * 多列名读取\n   \n   * df[[\"positionid\",\"companyid\",\"industryfield\"]]\n     \n     \n     1\n     \n\n\n# 赋值\n\n * 指定列赋值\n   \n   * data['close'] = 1\n     data.close = 1\n     df[\"金牌大于30\"] = np.where(df[\"金牌数\"]>30,\"是\",\"否\") #支持比较\n     \n     \n     1\n     2\n     3\n     \n\n * 指定单元格赋值\n   \n   * stock_data[\"open\"][\"2018-02-27\"]=1\n     df.at[5,\"国家奥委会\"] = \"俄奥委会\"\n     df.iloc[4,0] = \"俄奥委会\"\n     \n     \n     1\n     2\n     3\n     \n\n\n# 替换值\n\n * df.replace(\"指定值\",\"替换值\",inplace=true)\n   \n   * df.replace(0,\"无\",inplace=true) #整个df替换\n     df[\"金牌数\"].replace(0,\"无\",inplace=true)\n     \n     #替换值（多值）\n     df.replace(\"无\",np.nan,inplace=true)\n     df.replace(0,\"none\",inplace=true)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# 索引重置和设置\n\n * 修改索引 传递一个相同个数的列表 可以获取下标获取值但不能单个修改\n   \n   * stu = [\"学生_\" + str(i) for i in range(score_df.shape[0])]\n     # 必须整体全部修改\n     data.index = stu\n     #索引可以通过下标获取 但无法直接进行修改\n     #stu.index[3] \n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * reset_index(drop=false) 重置索引 drop默认为flase不删除原来索引\n   \n   * # 重置索引,drop=false  默认保留旧索引\n     data.reset_index()\n     # 重置索引,drop=true\n     data.reset_index(drop=true)\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * set_index(keys,drop = true)以某列值设置为新的索引 keys列名或列索引 drop默认为true 删除原来的索引列\n   \n   * df.set_index(\"year\") #以某列设置为新的索引\n     df.set_index(keys=[\"year\",\"month\"])  #设置多个索引\n     \n     \n     1\n     2\n     \n\n * df.rename_axis(keys,inplace=true) 更改当前索引列名\n   \n   * df.rename_axis(\"金牌排名\",inplace=true)\n     \n     \n     1\n     \n\n * df.set_index(\"列名\",inplace=true) 将指定列设定为索引\n   \n   * df.set_index(\"排名\",inplace=true)\n     df.set_index([\"排名\",\"总分\"],inplace=true) #支持多列行索引\n     \n     \n     1\n     2\n     \n\n * df.reindex([\"索引名1\",\"索引名2\",...]) 将索引重置为指定索引列表\n   \n   * pd.concat([df1, df4], axis=1).reindex(df1.index)\n     \n     \n     1\n     \n\n\n# 增加列和行\n\n * 增加列 直接df引用未定的列名 自动创建\n\ndf[\"比赛地点\"] = \"东京\"\n\n\n1\n\n\n * 在最后一行增加行 使用append\n   \n   * df1 = pd.dataframe([[i for i in range(len(df.columns))]],columns=df.columns)\n     df_new = df.append(df1)\n     \n     \n     1\n     2\n     \n\n * 在指定行中 增加行 使用concat\n   \n   * df1 = df.iloc[:1, :]\n     df2 = df.iloc[1:, :]\n     df3 = pd.dataframe([[i for i in range(len(df.columns))]], columns=df.columns)\n     df_new = pd.concat([df1, df3, df2], ignore_index=true)\n     \n     \n     \n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n\n# 删除列或行\n\n * df.drop(columns,axis=0) 删除列或行 传递指定行索引名或者列名\n   \n   * df.drop(1) #删除索引名为1的行\n     df.drop(df[df.金牌数<20].index) #条件删除\n     df.drop(['ma5', 'ma10', 'ma20', 'v_ma5', 'v_ma10', 'v_ma20'],axis=1 )  #删除多个列\n     df.drop(columns=[\"比赛地点\"],inplace=true) #删除指定列\n     df.drop(df.columns[[7,8,9,10]],axis=1,inplace=true) #按列号删除\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# 排序\n\n\n# dataframe\n\n * df.sort_values(by=, ascending=) 按列的值排序 by: 列名 ascending: 默认为true升序 false降序\n   \n   * data.sort_values(by=\"open\", ascending=true)  #升序\n     data.sort_values(by=['open', 'high'], ascending=false) #多个列降序\n     \n     \n     1\n     2\n     \n\n * df.sort_index() 根据索引进行排序 默认升序\n\n\n# series\n\n * series.sort_values(ascending=true) 对series中的值进行排序 默认升序\n   \n   * data['p_change'].sort_values(ascending=true)\n     \n     \n     1\n     \n\n * series.sort_index() 索引排序\n   \n   * data['p_change'].sort_index()\n     \n     \n     1\n     \n\n\n# 运算\n\n * data.add 相加 data['open'].add(1)\n\n * 逻辑运算\n   \n   * data[data[\"open\"] > 23]\n     data[(data[\"open\"] > 23) & (data[\"open\"] < 24)]  # 支持逻辑运算符\n     \n     \n     1\n     2\n     \n\n * data.query(expr) 逻辑运算函数\n   \n   * data.query(\"open<24 & open>23\")  # 只需要根据列名 + 条件就可以判断\n     \n     \n     1\n     \n\n * data.isin(values) 判断值是否在指定范围\n   \n   * data[data[\"open\"].isin([23.53, 23.85])]\n     \n     \n     1\n     \n\n * data.agg() 进行聚合操作 支持自定义min max mead sum等操作\n   \n   * df.agg({\n             \"总分\": [\"min\", \"max\", \"median\", \"mean\"],\n             \"高端人才得分\": [\"min\", \"max\", \"median\", \"mean\"],\n             \"办学层次得分\":[\"min\", \"max\", \"median\", \"mean\"]})\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * data.max(0) 最大值 默认为列计算 axis=0 传递1为行计算\n\n * data.min() 最小值\n\n * data.std() 标准差\n\n * data.var() 方差\n\n * data.median() 中位数\n\n * data.quantile() 分位数 默认返回50%的分位数 即中位数\n\n * data.idxmax(axis=0) 最大值的索引名\n\n * data.idxmin(axis=0) 最小值的索引名\n\n * data.cumsum() 连续求和 累加到最后一个单元格\n\n * data.cummax() 计算最大值\n\n * data.cummin() 计算最小值\n\n * data.cumprod() 累乘\n\n * data.mode() 众数\n\n * data.nsmallest(n) 返回当前列最小的n个数\n\n * data.nlargest(n) 返回当前列最大的n个数\n\n\n# 数据筛选\n\n\n# 筛选列\n\ndf.iloc[:,[0,1,2,3]]  #通过列号筛选\ndf[['金牌数','银牌数','铜牌数']] #通过列名\ndf.iloc[:,[i%2==0 for i in range(len(df.columns))]] #条件筛选\ndf.loc[:, df.columns.str.endswith('数')] #提取全部列名中包含 数 的列\ndf.iloc[9:20,-3:] #组合（行号+列名） 提取倒数后三列的10-20行\n\n\n1\n2\n3\n4\n5\n\n\n\n# 筛选行\n\ndf.loc[9:9] # 通过行号\ndf.iloc[9:,:] #切片\ndf.loc[:50:3] # 步进\ndf[df['金牌数'] > 30] # 判断\ndf[df[\"金牌数\"] == 10] \ndf[df[\"金牌数\"] != 10]\ndf[[i%2 != 0 for i in range(len(df.index))]] # 条件\ndf.loc[df['国家奥委会'].isin(['中国','美国','英国','日本','巴西'])] # 判断是否在列表中的值 \ndf.loc[((df['金牌数'] < 30) & df['国家奥委会'].isin(['中国', '美国', '英国', '日本', '巴西']))] #多条件  每个条件要用括号包起来\ndf[df.国家奥委会.str.contains('国')] # 提取 国家奥委会 列中，所有包含 国的行\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 组合\n\ndf.iloc[0:1,[1]] # 第 0 行第 2 列\ndf.iloc[0:2,0:2] #筛选多行多列\ndf.iloc[3,3] #行号+列号\ndf.loc[4,\"金牌数\"] # 行号+列名\ndf.loc[df['国家奥委会'] == '中国'].loc[1].at['金牌数'] # 条件\ndf.query(\"金牌数 + 银牌数 > 15\") # query 类sql筛选\n\n#query（引用变量）\nme = df[\"金牌数\"].mean()\ndf.query(f\"金牌数 > {me}\")  #需要要{}引用变量  类sql语句\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 自定义函数\n\n * df.apply(自定义函数名,axis=0) df会传递一个series对象 通过自定义函数return一个结果 axis=0:默认是列，axis=1为行进行运算\n   \n   * def abc(ses):\n         c_null = pd.isnull(ses)\n         is_null = column[c_null]\n         return len(is_null)\n     \n     df.apply(abc)  #会返回每一列null的个数\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# pandas画图\n\n * df.plot (kind='line') 默认为line折线图\n   * line 折线图\n   * bar 柱状图\n   * barh 横向直方图\n   * hist 直方图\n   * pie 饼图\n   * scatter 散点图\n\n\n# 读取和保存\n\n * pd.read_csv(filepath_or_buffer, sep =',', usecols=[1,3,5],skiprows=[1,3,5],index_col=[\"列名\"],names=[], keep_default_na=false,na_values=['[]'],na_filter= true,dtype={'positionid': str,'companyid':str})\n   \n   * filepath_or_buffer:文件路径\n     \n     * pd.read_csv(\"某招聘网站数据.csv\")\n       \n       \n       1\n       \n   \n   * sep :分隔符，默认用\",\"隔开\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",sep=\",\")\n       \n       \n       1\n       \n   \n   * usecols:指定读取的列名，列表形式 [1,3,5] 只读取2 4 6列 或指定列名 ['positionid','positionname','salary']\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",usecols = ['positionid','positionname','salary'])\n       pd.read_csv(\"某招聘网站数据.csv\",usecols = [1,3,5])\n       \n       \n       1\n       2\n       \n   \n   * skiprows: 跳过指定行数 [1,3,5] 跳过 2 4 6 行\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",skiprows = [i for i in range(1,21)])\n       \n       \n       1\n       \n   \n   * index_col: 将指定列设置为行索引\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",index_col=\"positionid\")\n       \n       \n       1\n       \n   \n   * names: 指定新的列名 ['id','岗位名称','薪资'] 不支持map\n     \n     * pd.read_csv('某招聘网站数据.csv', usecols=[0,1,17],header = 0,names=['id','岗位名称','薪资'])\n       \n       \n       1\n       \n   \n   * keep_default_na: 缺失值标记为nan 默认为true\n   \n   * pd.read_csv('某招聘网站数据.csv', keep_default_na=false)\n     \n     \n     1\n     \n   \n   * na_values: 将缺失值标记为指定字符串 如['[]']\n     \n     * pd.read_csv('某招聘网站数据.csv',na_values=['[]'])\n       \n       \n       1\n       \n   \n   * na_filter: 缺失值是否处理 默认为true 处理\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",na_filter=false)\n       \n       \n       1\n       \n   \n   * dtype: 读取时指定列类型 传递map\n     \n     * pd.read_csv(\"某招聘网站数据.csv\", dtype={'positionid': str,'companyid':str}) \n       \n       \n       1\n       \n   \n   * parse_dates: 将指定列转为datatime类型\n     \n     * pd.read_csv(\"某招聘网站数据.csv\",parse_dates=['createtime']) \n       \n       \n       1\n       \n   \n   * chunksize: 分块读取 返回一个可迭代对象，每次读取 n 行\n     \n     * data = pd.read_csv(\"某招聘网站数据.csv\", chunksize= 10)\n       for i in data:\n           print(i)\n       \n       \n       1\n       2\n       3\n       \n\n * df.to_csv(path_or_buf=none, sep=', ’, columns=none, header=true, index=true, mode='w', encoding=none)\n   \n   * path_or_buf :文件路径\n   \n   * sep :分隔符，默认用\",\"隔开\n   \n   * columns :选择需要的列索引\n     \n     * data.to_csv(\"234.csv\",columns=[\"positionname\",\"salary\"])\n       \n       \n       1\n       \n   \n   * header :boolean or list of string, default true,是否写进列索引值\n   \n   * index:是否写进行索引\n     \n     * data.to_csv(\"234.csv\",index=false) #不写入索引\n       \n       \n       1\n       \n   \n   * mode:'w'：重写, 'a' 追加\n   \n   * na_rep: 缺失值标记为指定字符\n     \n     * data.to_csv(\"234.csv\",index=false,na_rep=\"数据缺失\")\n       \n       \n       1\n       \n\n * pd.read_hdf(path_or_buf，key =none)\n   \n   * path_or_buffer:文件路径\n   * key:读取的键\n   * 需要先安装tables库 pip install tables\n\n * df.to_hdf(path_or_buf, key)\n\n * pd.read_json(\"./data//test.json\",orient=\"records\",lines=true) orient指定列 lines是否一行一个数据默认为false 一行一大串数据 推荐设置为true\n\n * df.to_json(\"./data/test.json\",orient=\"close\")\n\n * pd.read_clipboard() 从剪贴板读取数据\n\n * pd.read_sql(sql语句,conn连接对象)\n   \n   * from sqlite3 import connect\n     conn = connect(':memory:')\n     pd.read_sql(\"select int_column, date_column from test_data\",conn)\n     \n     \n     1\n     2\n     3\n     \n\n\n# set_option 选择设置\n\noption 选择设置为全局配置\n\n * display.max_columns 显示全部列\n   \n   * pd.set_option('display.max_columns', none) #显示全部列\n     \n     \n     1\n     \n\n * display.max_columns 显示指定列\n   \n   * pd.set_option('display.max_columns', 10)  \n     \n     \n     1\n     \n\n * display.max_rows 显示指定行\n   \n   * pd.set_option('display.max_rows', 7)\n     \n     \n     1\n     \n\n * display.max_colwidth 每列最多显示10个字符，多余的会变成... 设置指定长度字符\n   \n   * pd.set_option ('display.max_colwidth',10)\n     \n     \n     1\n     \n\n * precision 修改默认显示精度为小数点后5位\n   \n   * pd.set_option('precision', 5)\n     \n     \n     1\n     \n\n * mode.chained_assignment 取消pandas相关warning提示\n   \n   * pd.set_option(\"mode.chained_assignment\", none) \n     # 全局取消warning\n     # import warnings\n     # warnings.filterwarnings('ignore')\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * chop_threshold 数值显示条件 如果数值小于 20 则显示为0\n   \n   * pd.set_option('chop_threshold', 20) \n     \n     \n     1\n     \n\n * display.html.use_mathjax 让dataframe中内容支持 latex 显示（需要使用$$包住）\n   \n   * pd.set_option(\"display.html.use_mathjax\",true)\n     \n     \n     1\n     \n\n * plotting.backend 修改绘图引擎\n   \n   * #修改pandas默认绘图引擎为plotly（需要提前安装好plotly）\n     pd.set_option(\"plotting.backend\",\"plotly\")\n     \n     \n     1\n     2\n     \n\n\n# reset_option\n\n * 根据key还原指定选择设置 去除display.\n   \n   * pd.reset_option(\"max_rows\")\n     pd.reset_option(\"max_columns\")\n     \n     \n     1\n     2\n     \n\n * 还原全部显示设置 display\n   \n   * pd.reset_option(\"^display\")\n     \n     \n     1\n     \n\n * 还原全部 option 设置\n   \n   * pd.reset_option(\"all\") \n     \n     \n     \n     \n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n\n# style\n\n基于 style 个性化设置同样不会修改数据，所有 data.style.xxxx 输出的数据均是一次性的（可以复用、导出），因此你应该在合适的时间选择使用该方法。\n\n * style.hide_index() 隐藏索引列\n   \n   * data.style.hide_index()\n     \n     \n     1\n     \n\n * style.set_precision(n) 调整小数的精度\n   \n   * # 将带有小数点的列精度调整为小数点后2位\n     data.style.set_precision(2)\n     \n     \n     \n     1\n     2\n     3\n     \n\n * style.set_na_rep(\"指定字符\") 标记缺失值为指定字符\n   \n   * (data.style.set_na_rep(\"数据缺失\"))\n     \n     \n     1\n     \n\n * style.highlight_null(null_color=\"red\") 高亮缺失值\n   \n   * null_color: 为高亮颜色 默认为红色\n   \n   * (data.style.set_na_rep(\"数据缺失\").highlight_null(null_color='skyblue'))\n     \n     \n     1\n     \n\n * style.highlight_max 高亮数值列最大值\n   \n   * (data.style.highlight_max)\n     \n     \n     1\n     \n\n * style.highlight_min 高亮数值列最小值\n   \n   * (data.style.highlight_min)\n     \n     \n     1\n     \n\n * 同时高亮最大最小值\n   \n   * (data\n     .style\n     .highlight_max(color='#f77802')\n     .highlight_min(color='#26be49'))\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * style.highlight_between 设置指定列 指定范围内高亮\n   \n   * (data\n     .style\n     .highlight_between(left=3000, right=10000, subset=['salary']))\n     \n     \n     1\n     2\n     3\n     \n\n * style.background_gradient 渐变显示数值列\n   \n   * import seaborn as sns\n     \n     cm = sns.light_palette(\"green\", as_cmap=true)\n     \n     (data\n     .style\n     .background_gradient(cmap=cm))\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n\n * style.set_properties 修改指定properties\n   \n   * # 修改字体颜色\n     # 将 salary 列修改为红色字体\n     (data\n     .style\n     .set_properties(\n         subset=['salary'], **{'color': 'red'}))\n     \n     #修改背景颜色、对齐方式、字体大小\n     #1.居中 2.背景色修改为 #f8f8ff 3.字体:13px\n     (data\n     .style\n     .set_properties(**{'background-color': '#f8f8ff','text-align':'center', 'font-size': '13px'}))\n     \n     #链式多个设置进行结合\n     (data\n     .style\n     .set_properties(**{'background-color': '#f8f8ff','text-align':'center', 'font-size': '13px'})\n     .set_properties(\n         subset=['salary'], **{'color': 'red'}))\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     \n\n * 带样式导出\n   \n   * (data\n     .style\n     .set_properties(**{'background-color': '#f8f8ff','text-align':'center', 'font-size': '13px'})\n     .set_properties(\n         subset=['salary'], **{'color': 'red'})).to_excel('带有样式导出.xlsx')\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * style.bar 将指定列以横向条形图形式进行可视化\n   \n   * (data\n     .style\n     .bar(subset=['salary'],color='skyblue'))\n     \n     \n     1\n     2\n     3\n     \n\n * style.applymap 带条件样式 / 自定义样式\n   \n   * def my_style(val):\n     \n         color = 'red' if val > 30000 else 'black'\n         return 'color: %s' % color\n     \n     \n     data.style.applymap(my_style, subset=\"salary\")\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     \n\n * style.format 格式化值\n   \n   * #将 createtime 列格式化输出为 xx年xx月xx日\n     data.style.format({\"createtime\": lambda t: t.strftime(\"%y年%m月%d日\")})\n     \n     #1.在 salary 列后增加\"元\"  2.对 matchscore 列保留两位小数并增加\"分\"\n     (data\n     .style\n     .format(\"{0:,.2f}分\", subset=\"matchscore\")\n     .format(\"{\"\"}元\", subset=\"salary\"))\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     \n * \n\n\n# 缺失值处理\n\n\n# 判断缺失值\n\n * type(np.nan) 默认为float类型\n\n * pd.isnull(df) 判断df 是否有nan 如有则返回true\n   \n   * \n   \n   * #查询每列nan的数量\n     df.isnull().sum()\n     \n     片名        0\n     上映年份      0\n     评分        5\n     评价人数      3\n     导演        0\n     编剧        0\n     主演        0\n     类型        0\n     国家/地区     6\n     语言        6\n     时长(分钟)    6\n     dtype: int64\n         \n     #统计df中所有nan值\n     df.isna().sum().sum()\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     \n\n * pd.notnull(df) 判断df 是否有nan 如有则返回false\n\n * np.all(pd.notnull(movie)) 里面如果有一个缺失值 则为false 则返回false\n\n * np.any(pd.isnull(movie)) 通过np.any 只有里面有一个为null就返回true\n\n * 高亮缺失值\n   \n   * (df[df.isnull().t.any() == true]\n     .style\n     .highlight_null(null_color='skyblue'))\n     \n     \n     1\n     2\n     3\n     \n\n\n# 替换和替换缺失值或标记值\n\n * df.dropna() 删除所有np.nan 直接删除一行数据\n   \n   * df.dropna(axis=1) #按行删除\n     df.dropna(axis=0,subset=[\"列名1\",\"列名2\"]) #删除指定列中的缺失值\n     \n     \n     1\n     2\n     \n\n * df.fillna(value, inplace=false) 替换所有的缺失值为指定值\n   \n   * value:缺失值将要替换成的值\n   \n   * inplace:true:会修改原数据，默认为false:不替换修改原数据，生成新的对象\n   \n   * #可以替换单列\n     movie[\"revenue (millions)\"].fillna(value=movie[\"revenue (millions)\"].mean(),inplace=true)  \n     #更换指定列中的np.nan为 revenue (millions)的中位值  inplace为是否替换原来数据 默认为false不替换原来数据\n     \n     #将评分列的缺失值，替换为上一个电影的评分\n     df['评分'] = df['评分'].fillna(axis=0,method='ffill')\n     \n     #上下均值填充\n     df['评价人数'] = df['评价人数'].fillna(df['评价人数'].interpolate())\n     \n     #匹配填充\n     df['语言']=df.groupby('国家/地区').语言.bfill() \n     #bfill:向前填充  ffill:向后填充  默认 axis=0(列方向)  vs  axis=1(行方向)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     \n\n * df.replace(to_replace=, value=)\n   \n   * #先把被标记的缺失值 也是? 替换成np.nan\n     wis = wis.replace(to_replace='?',value=np.nan)\n     #再进行缺失值处理\n     wis = wis.dropna()\n     wis.head(10)\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# 缺失值转换\n\n不将缺失值标记为 nan\n\ndata = pd.read_csv('某招聘网站数据.csv', keep_default_na=false)\ndata\n\n\n1\n2\n\n\nnan会变成空字符\n\n\n# 缺失值标记\n\n将指定的字符标记为nan缺失值\n\ndata = pd.read_csv('某招聘网站数据.csv',na_values=['[]'])\ndata\n\n\n1\n2\n\n\n\n# 缺失值忽略\n\n忽略缺失值 变为空字符串\n\ndata = pd.read_csv(\"某招聘网站数据.csv\",na_filter=false)\ndata\n\n\n1\n2\n\n\n\n# 数据离散化\n\n连续属性的离散化就是在连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数 值代表落在每个子区间中的属性值。\n\n\n\n * pd.qcut(data, q) 对数据进行分组将数据分组，一般会与value_counts搭配使用，统计每组的个数 即将数据划分为指定个数的区间\n   \n   * # 自行分组\n     qcut = pd.qcut(p_change, 10)  #将此数据集 离散化平均分10个组\n     # 计算分到每个组数据个数\n     qcut.value_counts()  #查看每个组共有多少个数据集\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * series.value_counts()：统计分组次数\n\n * pd.cut(data, bins) 自定义区间分组\n   \n   * bins = [-100, -7, -5, -3, 0, 3, 5, 7, 100]  # 通过自定义区间范围来分组\n     cut_r = pd.cut(data_p, bins=bins)\n     cut_r.value_counts()\n     \n     \n     (0, 3]        215\n     (-3, 0]       188\n     (3, 5]         57\n     (-5, -3]       51\n     (5, 7]         35\n     (7, 100]       35\n     (-100, -7]     34\n     (-7, -5]       28\n     name: p_change, dtype: int64\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n\n# one-hot编码\n\n把每个类别生成一个布尔列，这些列中只有一列可以为这个样本取值为1.其又被称为独热编码。\n\n\n\n * pd.get_dummies(data,prefix=\"区间前缀测试\") 将据转为 one-hot编码 prefix为区间前缀方便阅读和理解\n\n\n# 数据合并\n\n\n# concat\n\n * pd.concat([data1, data2], axis=1) 将两个df进行合并\n   \n   * 按照行或列进行合并,axis=0为列索引，axis=1为行索引\n   \n   * #默认为行连接  通过指定axis=1为按列连接\n     pd.concat([data,data_dummies],axis=1) #横向拼接\n     pd.concat([df1,df2,df3]) #重置拼接\n     \n     \n     1\n     2\n     3\n     \n   \n   * ignore_index: 重置索引 默认为flase\n     \n     * pd.concat([df1,df4],ignore_index=true)\n       \n       \n       1\n       \n   \n   * join: 什么方式加入\n     \n     * inner:默认 内连接\n       \n       * pd.concat([df1,df4],axis=1,join='inner')\n         \n         \n         1\n         \n   \n   * keys: 每个表拆分 来区分不同的表数据来源\n     \n     * pd.concat([df1, df2, df3], keys=['x', 'y', 'z'])\n       \n       \n       a\tb\tc\td\n       x\t0\ta0\tb0\tc0\td0\n       1\ta1\tb1\tc1\td1\n       2\ta2\tb2\tc2\td2\n       3\ta3\tb3\tc3\td3\n       y\t4\ta4\tb4\tc4\td4\n       5\ta5\tb5\tc5\td5\n       6\ta6\tb6\tc6\td6\n       7\ta7\tb7\tc7\td7\n       z\t8\ta8\tb8\tc8\td8\n       9\ta9\tb9\tc9\td9\n       10\ta10\tb10\tc10\td10\n       11\ta11\tb11\tc11\td11\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       15\n       16\n       \n\n\n# merge\n\n * pd.merge(left, right, how='inner', on=none)\n   \n   * 可以指定按照两组数据的共同键值对合并或者左右各自\n   \n   * left : dataframe\n   \n   * right : 另一个dataframe\n   \n   * on : 指定的共同键\n     \n     * pd.merge(left, right, on='key') #以名称为key的列为共同键连接\n       pd.merge(left, right, on=['key1', 'key2']) # 多键\n       \n       \n       1\n       2\n       \n   \n   * how:按照什么方式连接\n     \n     * left:左连接\n       \n       * pd.merge(left, right,how=\"left\")\n         \n         \n         1\n         \n     \n     * right:右连接\n     \n     * outer:全外连接\n     \n     * inner: 内连接\n   \n   * suffixes:将连接表除on键 以外的列 为前缀再区分数据\n     \n     * pd.merge(left, right, on='k', suffixes=['_l', '_r'])\n       \n       \n       k\tv_l\tv_r\n       0\tk0\t1\t4\n       1\tk0\t1\t5\n       \n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       \n   \n   * pd.merge(left,right,on=[\"key1\",\"key2\"])  #默认为内连接\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"inner\")   #通过how属性指定连接类型 默认inner为内连接\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"outer\")   #outer 满外连接 没有的值会以nan存在 需要注意\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"left\") # 左连接\n     pd.merge(left,right,on=[\"key1\",\"key2\"],how=\"right\") # 右连接\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n\n# join 组合\n\n * df1.join(right,how=\"outer\") 将两表进行组合 默认为内连接\n   \n   * left.join(right) \n     \n     \n     1\n     \n   \n   * how: 连接方式\n     \n     * left.join(right,how=\"outer\")\n       \n       \n       1\n       \n     \n     * outer\n     \n     * left\n     \n     * right\n     \n     * inner\n   \n   * on: 共同键 按列名\n     \n     * left.join(right, on='v1')\n       left.join(right, on=[\"key1\",\"key2\"]) #多个列\n       \n       \n       1\n       2\n       \n\n\n# 交叉表与透视表\n\n * pd.crosstab(value1, value2) pd.crosstab(value1, value2) 交叉表用于计算一列数据对于另外一列数据的分组个数(用于统计分组频率的特殊透视表)\n\n * df.pivot_table([], ,values = [],index=[],aggfunc=\"mean\") 透视表是将原有的dataframe的列分别作为行索引和列索引，然后对指定的列应用聚集函数\n   \n   * data.pivot_table([\"p_n\"],index=\"week\")  \n     \n     \tp_n\n     week\t\n     0\t0.496000\n     1\t0.580153\n     2\t0.537879\n     3\t0.507812\n     4\t0.535433\n     \n     pd.pivot_table(df,values = ['销售额','利润','数量'],index = '类别',aggfunc = sum) #多列(指标)\n     pd.pivot_table(df,values = ['销售额'],index = ['省/自治区','类别'],aggfunc = sum) # 多索引\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     \n   \n   * values :指标\n     \n     * pd.pivot_table(df,values = ['销售额'],index = '省/自治区')\n       \n       \n       1\n       \n   \n   * index:索引名\n   \n   * columns: 列名\n     \n     * pd.pivot_table(df,values = ['销售额'],index = ['省/自治区'], columns='类别',aggfunc = sum) #多层\n       \n       \n       1\n       \n   \n   * aggfunc: 透视图使用的方法 默认为mean平均值\n     \n     * pd.pivot_table(df,values = ['销售额'],index = '省/自治区',aggfunc = sum)\n       pd.pivot_table(df,values = ['销售额'],index = '省/自治区',aggfunc = ['mean',sum]) #多个聚合方法\n       \n       \n       1\n       2\n       \n   \n   * margins: string，默认为‘all’，当参数margins为true时，all行和列的名字\n     \n     * pd.pivot_table(df,values = ['销售额','数量'],index = ['省/自治区','类别'],aggfunc = ['mean',sum],margins=true)\n       \n       \n       1\n       \n   \n   * df.melt() 逆透视\n     \n     * table = pd.pivot_table(df,values = ['销售额','利润','数量'],index = '类别',aggfunc = sum)\n       table.melt(id_vars=['数量'],var_name='分类',value_name='金额')\n       \n       \n       1\n       2\n       \n\n\n# 分组与聚合\n\n分组与聚合通常是分析数据的一种方式，通常与一些统计函数一起使用，查看数据的分组情况\n\n * df.groupby(key, as_index=false)\n   \n   * key:分组的列数据，可以多个\n   \n   * col.groupby([\"color\"])[\"price1\"].mean()  #按照列索引color分组  再根据分组后的price1列求中位值\n     col[\"price1\"].groupby(col[\"color\"]).mean()  #取出price1的值 再按color分组 再求中位值\n     col.groupby([\"color\"],as_index=false)[\"price1\"].mean()  #为分组聚合后的数据给行索引\n     df[['district','salary']].groupby(by='district').mean().sort_values('salary',ascending=false).head(1) #分组并排序\n     \n     pd.dataframe(df.groupby(\"district\")['companysize'].value_counts()).rename_axis([\"行政区\", \"公司规模\"]) #分组后 修改索引名\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n   \n   * groupby().groups 以字典形式查看各分组内容\n     \n     * df.groupby([\"district\",\"salary\"]).groups\n       \n       {('上城区', 22500): [81], ('上城区', 30000): [97], ('下沙', 30000): [31], ('余杭区', 7500): [84], ('余杭区', 20000): [52, 103], ('余杭区', 22500): [23, 51], ('余杭区', 25000): [62], ('余杭区', 27500): [24, 49], ('余杭区', 29000): [93], ('余杭区', 30000): [4, 10, 13, 18, 59, 61, 65, 68, 74, 76, 86, 92, 94], ('余杭区', 35000): [101], ('余杭区', 37500): [0, 32, 38, 39, 41], ('余杭区', 40000): [60, 87], ('余杭区', 45000): [25], ('余杭区', 50000): [5, 64, 90], ('余杭区', 60000): [8, 82], ('拱墅区', 24000): [72], ('拱墅区', 30000): [54, 89, 96], ('江干区', 3500): [2], ('江干区', 22500): [45], ('江干区', 30000): [73], ('江干区', 45000): [3], ('滨江区', 7500): [83], ('滨江区', 15000): [1], ('滨江区', 20000): [21, 40], ('滨江区', 22500): [37], ('滨江区', 30000): [22, 53, 55, 58, 67, 80, 102], ('滨江区', 32500): [26], ('滨江区', 37500): [17, 28, 57, 77], ('滨江区', 42500): [91], ('滨江区', 45000): [43, 47], ('滨江区', 50000): [44], ('萧山区', 25000): [100], ('萧山区', 30000): [6], ('萧山区', 45000): [66, 69], ('西湖区', 6500): [71], ('西湖区', 20000): [12], ('西湖区', 21500): [104], ('西湖区', 22500): [48, 70], ('西湖区', 24000): [42], ('西湖区', 25000): [56], ('西湖区', 26500): [78], ('西湖区', 27000): [75], ('西湖区', 27500): [15, 20, 50, 63], ('西湖区', 30000): [11, 27, 33, 34, 85, 88, 98], ('西湖区', 35000): [7], ('西湖区', 36500): [99], ('西湖区', 37500): [14, 16, 19, 30, 36, 79, 95], ('西湖区', 40000): [9, 35], ('西湖区', 45000): [29, 46]}\n       \n       \n       1\n       2\n       3\n       \n   \n   * groupby().get_group 查看分组后指定值数据\n     \n     * df.groupby([\"district\", \"salary\"]).get_group((\"西湖区\", 30000))\n       \n       \n       1\n       \n   \n   * groupby().transform(\"聚合函数名\") 分组转换\n     \n     * df['该区平均工资'] = df[['district','salary']].groupby(by='district').transform('mean')\n       df\n       \n       \n       1\n       2\n       \n   \n   * groupby().filter() 分组过滤\n     \n     * df.groupby('district').filter(lambda x: x['salary'].mean() < 30000)\n       \n       \n       1\n       \n   \n   * groupby().agg([\"聚合函数名1\",\"聚合函数名2\"]) 将分组的数据 聚合统计\n     \n     * df.groupby('district')['salary'].agg([min, max, np.mean])\n       \n       df.groupby('positionname').agg({'salary': np.median, 'score': np.mean}) #指定以指定聚合函数来获取值\n       \n       df.groupby('district').agg(\n           {'salary': [np.mean, np.median, np.std], 'score': np.mean}) #多层聚合\n       \n       \n       #自定义函数聚合\n       def myfunc(x):\n       \n           return x.max()-x.mean()\n       \n       df.groupby('district').agg(最低工资=('salary', 'min'), 最高工资=(\n           'salary', 'max'), 平均工资=('salary', 'mean'), 最大值与均值差值=('salary', myfunc)).rename_axis([\"行政区\"])\n       \n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       14\n       15\n       16\n       \n     \n     * agg.rename_axis(\"列名\") 修改列名\n       \n       * df.groupby('district').agg(最低工资=('salary', 'min'), 最高工资=(\n             'salary', 'max'), 平均工资=('salary', 'mean')).rename_axis([\"行政区\"])\n         \n         \n         1\n         2\n         \n   \n   \n   # 分组可视化\n   \n   import matplotlib.pyplot as plt\n   %config inlinebackend.figure_format = 'retina'\n   plt.rcparams['font.sans-serif'] = ['songti sc']\n   \n   \n   df.groupby(\"district\")['positionname'].count().plot(\n       kind='bar', figsize=(10, 6), color='#5172f0', fontsize=12)\n   \n   plt.rcparams['font.sans-serif'] = 'simhei'\n   plt.rcparams['axes.unicode_minus'] = false\n   \n   plt.ylabel(\"公司数量\", fontsize=14)\n   plt.xlabel(\"杭州市各区\", fontsize=14)\n   \n   plt.show()\n   \n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   \n\n\n\n\n# 时间处理\n\n从文件中读取的时间默认我object类型 需要转为pd时间才能进行运算\n\n * pd.to_datetime(data) 将指定列转为时间序列 返回datatime对象\n * datetime.weekday 星期几\n * datetime.week 月中第几个星期\n * datetime.day 几号\n * datetime.year 年\n * datetime.total_seconds() 将时间转为秒\n\nt1 = datetime.datetime.strptime(\"2021-11-03 10:30:00\", \"%y-%m-%d %h:%m:%s\")\nt2 = datetime.datetime.strptime(\"2021-11-03 12:30:00\", \"%y-%m-%d %h:%m:%s\")\n\ntime = (t2-t1).total_seconds() #获取时间差\n\n\n1\n2\n3\n4\n\n\n\n# 去重\n\n * df.drop_duplicates(subset=['列名'], keep='first') 去重 只保留第一个\n   \n   * keep: first为保留重复值中第一个 last为保留最后一个\n     \n     * df.drop_duplicates(keep=\"last\")\n       \n       \n       1\n       \n   \n   * subset: 列名\n\n * df[\"列名\"].uniue()\n\n * 查找行是否存在重复值\n   \n   * df[df.duplicated()]\n     \n     2      true\n     3      true\n     4      true\n     5      true\n     6      true\n            ... \n     257    true\n     258    true\n     259    true\n     260    true\n     261    true\n     length: 260, dtype: bool\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     \n\n * 查询指定列中的重复值\n   \n   * df[df.duplicated(['片名'])]\n     \n     \n     1\n     \n\n\n# 热力图\n\ndf.corr().style.background_gradient(cmap='coolwarm').set_precision(2)\n\n### 方法二 ###\n\n#借助 `matplotlib` 和 `seaborn` \n\n# 其中中文设置可以参考我的这篇文章 https://mp.weixin.qq.com/s/wkogvqp-6quap00zxjhweg\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (9,6),dpi=100)\nsns.set(font='songti sc')\nsns.heatmap(df.corr().round(2),annot=true,cmap='rdbu')\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n\n\n# 热力地图\n\n结合 pyecharts 进行地图可视化\n\nfrom pyecharts import options as opts\nfrom pyecharts.charts import map\nlist1 = list(pd.dataframe(df.省市.value_counts()).index)\nlist2 = list(pd.dataframe(df.省市.value_counts()).省市)\n\n\nc = (\n    map()\n    .add('', [list(z) for z in zip(list1,list2)], \"china\",is_map_symbol_show=false)\n    .set_global_opts(\n        title_opts=opts.titleopts(title=\"排名前100高校各省市占比\"),\n        visualmap_opts=opts.visualmapopts(max_=20),\n       \n\n    )\n)\nc.render_notebook()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n\n# 直方图\n\nimport seaborn as sns\nsns.set(font='songti sc')\nsns.distplot(df['总分'])\n\n\n1\n2\n3\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"winutils",frontmatter:{title:"winutils",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/981612/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/04.winutils.html",relativePath:"大数据/01.Hadoop/04.winutils.md",key:"v-9c664796",path:"/pages/981612/",headersStr:null,content:"# winutils\n\nhttps://github.com/steveloughran/winutils\n\n下载3.1.3版本 要与pom.xml api版本一致\n\n配置环境变量\n\n添加HADOOP_HOME = D:\\compile\\hadoop\n\n并且在path里面 添加 %HADOOP_HOME%\\bin\n\n#cmd中运行\nwinutils\n\n\n1\n2\n\n\n拷贝D:\\compile\\hadoop\\bin\\hadoop.dll文件 拷贝到 c:\\windows\\system32\n\n并重启电脑",normalizedContent:"# winutils\n\nhttps://github.com/steveloughran/winutils\n\n下载3.1.3版本 要与pom.xml api版本一致\n\n配置环境变量\n\n添加hadoop_home = d:\\compile\\hadoop\n\n并且在path里面 添加 %hadoop_home%\\bin\n\n#cmd中运行\nwinutils\n\n\n1\n2\n\n\n拷贝d:\\compile\\hadoop\\bin\\hadoop.dll文件 拷贝到 c:\\windows\\system32\n\n并重启电脑",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hadoop",frontmatter:{title:"Hadoop",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/8e0c98/",categories:["Hadoop"],tags:["Hadoop"]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/01.Hadoop.html",relativePath:"大数据/01.Hadoop/01.Hadoop.md",key:"v-3b8fa256",path:"/pages/8e0c98/",headers:[{level:2,title:"hadoop 三大发行版",slug:"hadoop-三大发行版",normalizedTitle:"hadoop 三大发行版",charIndex:105},{level:2,title:"Hadoop 的优势",slug:"hadoop-的优势",normalizedTitle:"hadoop 的优势",charIndex:288},{level:2,title:"Hadoop 组成",slug:"hadoop-组成",normalizedTitle:"hadoop 组成",charIndex:339},{level:3,title:"HDFS 架构",slug:"hdfs-架构",normalizedTitle:"hdfs 架构",charIndex:355},{level:3,title:"YARN 架构",slug:"yarn-架构",normalizedTitle:"yarn 架构",charIndex:540},{level:3,title:"MapReduce 框架",slug:"mapreduce-框架",normalizedTitle:"mapreduce 框架",charIndex:571},{level:3,title:"大数据技术生态体系",slug:"大数据技术生态体系",normalizedTitle:"大数据技术生态体系",charIndex:633}],headersStr:"hadoop 三大发行版 Hadoop 的优势 Hadoop 组成 HDFS 架构 YARN 架构 MapReduce 框架 大数据技术生态体系",content:"# Hadoop\n\nHadoop apache 基金会开发的分布式系统基础架构\n\n主要解决 海量数据的存储和海量数据的分析计算问题\n\n广义来说,hadoop通常是指一个更广泛的概念 hadoop生态圈\n\n\n# hadoop 三大发行版\n\napache 版本 最原始(基础)的版本 入门学习较好\n\nhttp://archive.apache.org/dist/hadoop/core/\n\ncloudera 内部集成了很多大数据框架 对应产品 CDH\n\nhortonworks 文档较好 对应产品 HDP\n\ncloudera 和 hortonworks 目前两家已经合并\n\n\n# Hadoop 的优势\n\n 1. 高可靠性\n 2. 高扩展性\n 3. 高效性\n 4. 高容错性\n\n\n# Hadoop 组成\n\n\n\n\n# HDFS 架构\n\n 1. NameNode(nn) 存储文件的元数据 如文件名,目录结构 文件失效 以及每个文件的块列表和块所在的DataNode\n 2. DataNode(dn) 在本地文件系统存储文件块数据 以及块数据的校验和\n 3. Secondary NameNode(2nn) 每隔一段时间对NameNode元数据备份 不是热备份 2nn是nn的辅助\n\n\n# YARN 架构\n\n负责资源调度 CPU和内存资源\n\n\n\n\n# MapReduce 框架\n\n * Map阶段并行处理输入数据 分发\n * Reduce阶段对Map结果进行汇总\n\n\n\n\n# 大数据技术生态体系\n\n",normalizedContent:"# hadoop\n\nhadoop apache 基金会开发的分布式系统基础架构\n\n主要解决 海量数据的存储和海量数据的分析计算问题\n\n广义来说,hadoop通常是指一个更广泛的概念 hadoop生态圈\n\n\n# hadoop 三大发行版\n\napache 版本 最原始(基础)的版本 入门学习较好\n\nhttp://archive.apache.org/dist/hadoop/core/\n\ncloudera 内部集成了很多大数据框架 对应产品 cdh\n\nhortonworks 文档较好 对应产品 hdp\n\ncloudera 和 hortonworks 目前两家已经合并\n\n\n# hadoop 的优势\n\n 1. 高可靠性\n 2. 高扩展性\n 3. 高效性\n 4. 高容错性\n\n\n# hadoop 组成\n\n\n\n\n# hdfs 架构\n\n 1. namenode(nn) 存储文件的元数据 如文件名,目录结构 文件失效 以及每个文件的块列表和块所在的datanode\n 2. datanode(dn) 在本地文件系统存储文件块数据 以及块数据的校验和\n 3. secondary namenode(2nn) 每隔一段时间对namenode元数据备份 不是热备份 2nn是nn的辅助\n\n\n# yarn 架构\n\n负责资源调度 cpu和内存资源\n\n\n\n\n# mapreduce 框架\n\n * map阶段并行处理输入数据 分发\n * reduce阶段对map结果进行汇总\n\n\n\n\n# 大数据技术生态体系\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"环境安装",frontmatter:{title:"环境安装",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/083fca/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/02.%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html",relativePath:"大数据/01.Hadoop/02.环境安装.md",key:"v-ef3e7878",path:"/pages/083fca/",headers:[{level:2,title:"虚拟机安装前置",slug:"虚拟机安装前置",normalizedTitle:"虚拟机安装前置",charIndex:56},{level:2,title:"修改静态地址",slug:"修改静态地址",normalizedTitle:"修改静态地址",charIndex:293},{level:2,title:"修改主机名",slug:"修改主机名",normalizedTitle:"修改主机名",charIndex:599},{level:2,title:"修改hosts",slug:"修改hosts",normalizedTitle:"修改hosts",charIndex:664},{level:2,title:"关闭防火墙",slug:"关闭防火墙",normalizedTitle:"关闭防火墙",charIndex:951},{level:2,title:"创建用户",slug:"创建用户",normalizedTitle:"创建用户",charIndex:1032},{level:2,title:"修改atguigu权限",slug:"修改atguigu权限",normalizedTitle:"修改atguigu权限",charIndex:1102},{level:2,title:"安装java",slug:"安装java",normalizedTitle:"安装java",charIndex:1370},{level:2,title:"安装hadoop",slug:"安装hadoop",normalizedTitle:"安装hadoop",charIndex:1752},{level:2,title:"SSH免密登陆",slug:"ssh免密登陆",normalizedTitle:"ssh免密登陆",charIndex:2057},{level:3,title:"xsync 脚本",slug:"xsync-脚本",normalizedTitle:"xsync 脚本",charIndex:2368},{level:3,title:"复制ssh免密key",slug:"复制ssh免密key",normalizedTitle:"复制ssh免密key",charIndex:3340},{level:3,title:"同步环境",slug:"同步环境",normalizedTitle:"同步环境",charIndex:3391},{level:2,title:"hadoop配置文件",slug:"hadoop配置文件",normalizedTitle:"hadoop配置文件",charIndex:3480},{level:3,title:"配置core-site.xml",slug:"配置core-site-xml",normalizedTitle:"配置core-site.xml",charIndex:3540},{level:3,title:"配置hdfs-site.xml",slug:"配置hdfs-site-xml",normalizedTitle:"配置hdfs-site.xml",charIndex:4309},{level:3,title:"配置yarn-site.xml",slug:"配置yarn-site-xml",normalizedTitle:"配置yarn-site.xml",charIndex:4534},{level:3,title:"配置mapred-site.xml",slug:"配置mapred-site-xml",normalizedTitle:"配置mapred-site.xml",charIndex:5878},{level:3,title:"配置workers",slug:"配置workers",normalizedTitle:"配置workers",charIndex:6182},{level:3,title:"同步",slug:"同步",normalizedTitle:"同步",charIndex:251},{level:2,title:"启动集群",slug:"启动集群",normalizedTitle:"启动集群",charIndex:6365},{level:3,title:"dfs启动",slug:"dfs启动",normalizedTitle:"dfs启动",charIndex:6606},{level:3,title:"JPS",slug:"jps",normalizedTitle:"jps",charIndex:6645},{level:2,title:"配置历史服务器",slug:"配置历史服务器",normalizedTitle:"配置历史服务器",charIndex:6663},{level:3,title:"配置mapred-site.xml",slug:"配置mapred-site-xml-2",normalizedTitle:"配置mapred-site.xml",charIndex:5878},{level:2,title:"配置日志聚集",slug:"配置日志聚集",normalizedTitle:"配置日志聚集",charIndex:7087},{level:3,title:"配置 yarn-site.xml",slug:"配置-yarn-site-xml",normalizedTitle:"配置 yarn-site.xml",charIndex:7098},{level:2,title:"配置完成重新同步 并启动",slug:"配置完成重新同步-并启动",normalizedTitle:"配置完成重新同步 并启动",charIndex:8107},{level:2,title:"集群时间同步",slug:"集群时间同步",normalizedTitle:"集群时间同步",charIndex:8441},{level:3,title:"修改ntp配置文件",slug:"修改ntp配置文件",normalizedTitle:"修改ntp配置文件",charIndex:8551},{level:3,title:"修改ntpd文件",slug:"修改ntpd文件",normalizedTitle:"修改ntpd文件",charIndex:8894},{level:3,title:"其他机器配置",slug:"其他机器配置",normalizedTitle:"其他机器配置",charIndex:9050},{level:2,title:"常用端口",slug:"常用端口",normalizedTitle:"常用端口",charIndex:9174},{level:3,title:"hadopp2.x和3.x端口变化",slug:"hadopp2-x和3-x端口变化",normalizedTitle:"hadopp2.x和3.x端口变化",charIndex:9183},{level:3,title:"HDFS",slug:"hdfs",normalizedTitle:"hdfs",charIndex:5022},{level:3,title:"Yarn",slug:"yarn",normalizedTitle:"yarn",charIndex:10081},{level:3,title:"Hbase",slug:"hbase",normalizedTitle:"hbase",charIndex:11135},{level:3,title:"Hive",slug:"hive",normalizedTitle:"hive",charIndex:11873},{level:3,title:"Zookeeper",slug:"zookeeper",normalizedTitle:"zookeeper",charIndex:12209}],headersStr:"虚拟机安装前置 修改静态地址 修改主机名 修改hosts 关闭防火墙 创建用户 修改atguigu权限 安装java 安装hadoop SSH免密登陆 xsync 脚本 复制ssh免密key 同步环境 hadoop配置文件 配置core-site.xml 配置hdfs-site.xml 配置yarn-site.xml 配置mapred-site.xml 配置workers 同步 启动集群 dfs启动 JPS 配置历史服务器 配置mapred-site.xml 配置日志聚集 配置 yarn-site.xml 配置完成重新同步 并启动 集群时间同步 修改ntp配置文件 修改ntpd文件 其他机器配置 常用端口 hadopp2.x和3.x端口变化 HDFS Yarn Hbase Hive Zookeeper",content:'# 环境安装\n\nhttps://hadoop.apache.org/docs/stable/ 官方文档\n\n\n# 虚拟机安装前置\n\nsudo yum install -y epel-release  #额外软件源\nsudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git wget #安装 psmisc工具包  nc工具包 net-tools工具包 rsync 远程同步  vim编辑器 lrzsz上传下载  ntp时间同步 \n\n\n1\n2\n\n\n\n# 修改静态地址\n\nsudo vim /etc/sysconfig/network-scripts/ifcfg-ens33\n#如手写可以 ctrl+alt+tab 补全  \n\n\n1\n2\n\n\n配置为符合的 记得改UUID和Mac地址\n\nDEVICE=ens33\nTYPE=Ethernet\nONBOOT=yes\nBOOTPROTO=static\nNAME="ens33"\nIPADDR=192.168.1.101\nPREFIX=24\nGATEWAY=192.168.1.2\nDNS1=192.168.1.2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nservice network restart\n\n\n1\n\n\n\n# 修改主机名\n\nsudo hostnamectl --static set-hostname hadoop102\n\n\n1\n\n\n\n# 修改hosts\n\nsudo vim /etc/hosts\n\n\n1\n\n\n192.168.1.100 hadoop100\n192.168.1.101 hadoop101\n192.168.1.102 hadoop102\n192.168.1.103 hadoop103\n192.168.1.104 hadoop104\n192.168.1.105 hadoop105\n192.168.1.106 hadoop106\n192.168.1.107 hadoop107\n192.168.1.108 hadoop108\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n物理机也改host\n\n\n# 关闭防火墙\n\nsudo systemctl stop firewalld\nsudo systemctl disable firewalld\n\n\n1\n2\n\n\n\n# 创建用户\n\nsudo useradd atguigu\nsudo passwd atguigu\n\nreboot\n\n\n1\n2\n3\n4\n\n\n\n# 修改atguigu权限\n\nvisudo\n\n\n1\n\n\n#第91行\n## Allow root to run any commands anywhere\nroot    ALL=(ALL)     ALL\natguigu   ALL=(ALL)     ALL\n\n\n1\n2\n3\n4\n\n\n创建opt下的存放目录\n\ncd /opt\nsudo mkdir module\nsudo mkdir software\nsudo chown atguigu:atguigu /opt/module /opt/software\n\n\n1\n2\n3\n4\n\n\n\n# 安装java\n\n先卸载\n\nrpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps\n\n\n1\n\n\n解压\n\ncd /opt/software/\ntar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/\n\n\n1\n2\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n#JAVA_HOME\n#yum 为/usr/lib/jvm/java\nexport JAVA_HOME=/opt/module/jdk1.8.0_212 \nexport PATH=$PATH:$JAVA_HOME/bin\n\n\n1\n2\n3\n4\n\n\n更新环境变量\n\nsource /etc/profile\njava -version\n\n\n1\n2\n\n\n\n# 安装hadoop\n\n解压\n\ntar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/\n\n\n1\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n##HADOOP_HOME\nexport HADOOP_HOME=/opt/module/hadoop-3.1.3\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\n\n\n1\n2\n3\n4\n\n\n更新\n\nsource /etc/profile\nhadoop version\n\n\n1\n2\n\n\n\n# SSH免密登陆\n\n生成密钥 全部回车 为空即可\n\nssh-keygen -t rsa\n\n\n1\n\n\n发送密钥 给另外一台主机\n\nssh-copy-id hadoop102\n\nssh-copy-id hadoop103\n\nssh-copy-id hadoop104\n\n\n1\n2\n3\n4\n5\n\n\n默认存储在 /home/用户/.ssh文件下\n\n注意：\n\n还需要在hadoop102上采用root账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；\n\n还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。\n\n\n# xsync 脚本\n\n安装\n\nyum install -y rsync\n\n\n1\n\n\n创建xsync脚本\n\ncd /home/atguigu\nvim xsync\n\n\n1\n2\n\n\n#!/bin/bash\n#1. 判断参数个数\nif [ $# -lt 1 ]\nthen\n  echo Not Enough Arguement!\n  exit;\nfi\n#2. 遍历集群所有机器\nfor host in hadoop102 hadoop103 hadoop104\ndo\n  echo ====================  $host  ====================\n  #3. 遍历所有目录，挨个发送\n  for file in $@\n  do\n    #4 判断文件是否存在\n    if [ -e $file ]\n    then\n      #5. 获取父目录\n      pdir=$(cd -P $(dirname $file); pwd)\n      #6. 获取当前文件的名称\n      fname=$(basename $file)\n      ssh $host "mkdir -p $pdir"\n      rsync -av $pdir/$fname $host:$pdir\n    else\n      echo $file does not exists!\n    fi\n  done\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nchmod +x xsync\nsudo mv xsync /bin/   #将脚本移动到/bin中，以便全局调用\n\n\n1\n2\n\n\nhttps://rsync.samba.org/\n\nhttps://vault.centos.org/6.5/os/x86_64/\n\n安装包安装\n\nwget https://vault.centos.org/6.5/os/x86_64/Packages/rsync-3.0.6-9.el6_4.1.x86_64.rpm\nrpm -ivh rsync-3.0.6-9.el6_4.1.x86_64.rpm\n\n\n1\n2\n\n\n\n# 复制ssh免密key\n\ncd /home/atguigu\nxsync .ssh\n\n\n1\n2\n\n\n\n# 同步环境\n\n同步环境变量\n\nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n\n\n同步软件\n\ncd /\nsxsync opt\n\n\n1\n2\n\n\n\n# hadoop配置文件\n\ncd /opt/module/hadoop-3.1.3/etc/hadoop\n\n\n1\n\n\n\n# 配置core-site.xml\n\nvim core-site.xml \n\n\n1\n\n\n在<configuration>标签中追加\n\n<property>\n        <name>fs.defaultFS</name>\n    \x3c!-- 配置hdfs默认的地址 --\x3e\n        <value>hdfs://hadoop102:8020</value>\n    </property>\n    <property>\n        \x3c!-- 配置hadoop临时存放路径--\x3e\n        <name>hadoop.tmp.dir</name>\n        <value>/opt/module/hadoop-3.1.3/data</value>\n    </property>\n    <property>\n        \x3c!-- 兼容性配置hive --\x3e\n        <name>hadoop.proxyuser.atguigu.hosts</name>\n        <value>*</value>\n    </property>\n    <property>\n        <name>hadoop.proxyuser.atguigu.groups</name>\n        <value>*</value>\n    </property>\n    <property>\n        <name>hadoop.http.staticuser.user</name>\n        <value>atguigu</value>\n    </property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 配置hdfs-site.xml\n\nvim hdfs-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n<property>\n    \x3c!-- 2nn的地址 --\x3e\n        <name>dfs.namenode.secondary.http-address</name>\n        <value>hadoop104:9868</value>\n    </property>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 配置yarn-site.xml\n\nvim yarn-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n<property>\n    \x3c!--设置NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序--\x3e\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <value>hadoop103</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.env-whitelist</name>\n        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n    </property>\n\t\x3c!--设定单个容器可以申领到的最小内存资源--\x3e\n    <property>\n        <name>yarn.scheduler.minimum-allocation-mb</name>\n        <value>2048</value>\n    </property>\n    <property>\n        <name>yarn.scheduler.maximum-allocation-mb</name>\n        <value>4096</value>\n    </property>\n\x3c!--设定物理节点有4G内存加入资源池--\x3e\n    <property>\n        <name>yarn.nodemanager.resource.memory-mb</name>\n        <value>4096</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.pmem-check-enabled</name>\n        <value>false</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.vmem-check-enabled</name>\n        <value>false</value>\n    </property>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# 配置mapred-site.xml\n\nvim mapred-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n\x3c!--Hadoop对MapReduce运行框架一共提供了3种实现，在mapred-site.xml中通过"mapreduce.framework.name"这个属性来设置为"classic"、"yarn"或者"local"--\x3e\n<property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 配置workers\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/workers\n\n\n1\n\n\n删除localhost 文件中添加的内容结尾不允许有空格，文件中不允许有空行。\n\nhadoop102\nhadoop103\nhadoop104\n\n\n1\n2\n3\n\n\n\n# 同步\n\ncd ..\nxsync hadoop\n\n\n1\n2\n\n\n\n# 启动集群\n\n格式化hdfs\n\nhdfs namenode -format  #在节点机子上\n\n\n1\n\n\n启动集群\n\nstart-dfs.sh\n\n\n1\n\n\n如果报java未找到 修改hadoop.env.sh文件\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/hadoop.env.sh\n\n\n1\n\n\n修改JAVA_HOME为 并同步\n\nexport JAVA_HOME=/usr/lib/jvm/java\n\n\n1\n\n\n\n\n显示这样启动成功\n\n\n# dfs启动\n\n在103中启动\n\nstart-yarn.sh\n\n\n1\n\n\n\n# JPS\n\nJPS\n\n\n1\n\n\n\n# 配置历史服务器\n\n关闭dfs和yarn\n\nstop-yarn.sh  #103\nstop-dfs.sh  #102\n\n\n1\n2\n\n\n\n# 配置mapred-site.xml\n\nvi mapred-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n\x3c!-- 历史服务器端地址 --\x3e\n<property>\n    <name>mapreduce.jobhistory.address</name>\n    <value>hadoop102:10020</value>\n</property>\n\n\x3c!-- 历史服务器web端地址 --\x3e\n<property>\n    <name>mapreduce.jobhistory.webapp.address</name>\n    <value>hadoop102:19888</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 配置日志聚集\n\n\n# 配置 yarn-site.xml\n\nvim yarn-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n<property>\n    \x3c!-- 开启日志聚集--\x3e\n    <name>yarn.log-aggregation-enable</name>\n    <value>true</value>\n</property>\n<property>  \n    \x3c!-- 日志服务器--\x3e\n    <name>yarn.log.server.url</name>  \n    <value>http://${yarn.timeline-service.webapp.address}/applicationhistory/logs</value>\n</property>\n<property>\n    \x3c!-- 日志保存时间--\x3e\n    <name>yarn.log-aggregation.retain-seconds</name>\n    <value>604800</value>\n</property>\n<property>\n    <name>yarn.timeline-service.enabled</name>\n    <value>true</value>\n</property>\n<property>\n    <name>yarn.timeline-service.hostname</name>\n    <value>${yarn.resourcemanager.hostname}</value>\n</property>\n<property>\n    <name>yarn.timeline-service.http-cross-origin.enabled</name>\n    <value>true</value>\n</property>\n<property>\n    <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>\n    <value>true</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# 配置完成重新同步 并启动\n\ncd ..\nxsync hadoop/\n\n\n1\n2\n\n\n重新启动\n\nstart-dfs.sh  #102\n\nstart-yarn.sh  #103\n\nmapred --daemon start historyserver  #102 启动历史服务器\n\n\n1\n2\n3\n4\n5\n\n\n配置完成后不要同步整个hadoop文件夹 只需要etc配置文件夹 千万不要同步data文件夹\n\n如想修改data路径\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml  #hadoop.data.dir\n\n\n1\n\n\n如果想重新按配置开 删除掉logs文件夹里的东西 重新格式化HDFS 再启动\n\n\n# 集群时间同步\n\n在所有节点关闭ntp服务和自启动\n\n建议root用户下操作\n\nsu -\nsudo systemctl stop ntpd\nsudo systemctl disable ntpd\n\n\n1\n2\n3\n\n\n\n# 修改ntp配置文件\n\nvim /etc/ntp.conf\n\n\n1\n\n\nrestrict 192.168.130.2 mask 255.255.255.0 nomodify notrap  #去掉#号修改即可\n\n#加上井号\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\n\n#添加\nserver 127.127.1.0\nfudge 127.127.1.0 stratum 10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 修改ntpd文件\n\nvim /etc/sysconfig/ntpd\n\n\n1\n\n\n#追加\nSYNC_HWCLOCK=yes\n\n\n1\n2\n\n\n#重启服务\nsystemctl start ntpd\n\n#开机启动\nsystemctl enable ntpd\n\n\n1\n2\n3\n4\n5\n\n\n以上修改文件在102进行\n\n\n# 其他机器配置\n\n切换root用户 进行\n\nsu -\ncrontab -e\n\n\n1\n2\n\n\n#追加\n*/1 * * * * /usr/sbin/ntpdate hadoop102\n\n\n1\n2\n\n\ndate  #查看当前时间是否同步\n\n\n1\n\n\n\n# 常用端口\n\n\n# hadopp2.x和3.x端口变化\n\nHADOOP2.X    HADOOP3.X   \n访问HDFS端口     50070       9870\n访问MR执行情况端口   8088        8088\n历史服务器        19888       19888\n客户端访问集群端口    9000        8020\n\n\n\n\n# HDFS\n\n组件     节点            端口     配置                             用途说明\nHDFS   Namenode      9870   dfs.namenode.http-address      http服务的端口\nHDFS   NameNode      9871   dfs.namenode.https-address     https服务的端口\nHDFS   NameNode      9820   fs.defaultFS                   接收Client连接的RPC端口，用于获取文件系统metadata信息。\nHDFS   DataNode      9866   dfs.datanode.address           datanode服务端口，用于数据传输\nHDFS   DataNode      9864   dfs.datanode.http.address      http服务的端口\nHDFS   DataNode      9865   dfs.datanode.https.address     https服务的端口\nHDFS   DataNode      9867   dfs.datanode.ipc.address       ipc服务的端口\nHDFS   journalnode   8485   dfs.journalnode.rpc-address    RPC服务\nHDFS   journalnode   8480   dfs.journalnode.http-address   HTTP服务\n\n\n# Yarn\n\n组件     节点                  端口      配置                                              用途说明\nYARN   ResourceManager     8032    yarn.resourcemanager.address                    RM的applications manager(ASM)端口\nYARN   ResourceManager     8030    yarn.resourcemanager.scheduler.address          scheduler组件的IPC端口\nYARN   ResourceManager     8031    yarn.resourcemanager.resource-tracker.address   IPC\nYARN   ResourceManager     8033    yarn.resourcemanager.admin.address              IPC\nYARN   ResourceManager     8088    yarn.resourcemanager.webapp.address             http服务端口\nYARN   NodeManager         8040    yarn.nodemanager.localizer.address              localizer IPC\nYARN   NodeManager         8042    yarn.nodemanager.webapp.address                 http服务端口\nYARN   NodeManager         8041    yarn.nodemanager.address                        NM中container manager的端口\nYARN   JobHistory Server   10020   mapreduce.jobhistory.address                    IPC\nYARN   JobHistory Server   19888   mapreduce.jobhistory.webapp.address             http服务端口\n\n\n# Hbase\n\n组件      节点             端口      配置                                    用途说明\nHBase   Master         60000   hbase.master.port                     IPC\nHBase   Master         60010   hbase.master.info.port                http服务端口\nHBase   RegionServer   60020   hbase.regionserver.port               IPC\nHBase   RegionServer   60030   hbase.regionserver.info.port          http服务端口\nHBase   HQuorumPeer    2181    hbase.zookeeper.property.clientPort   HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。\nHBase   HQuorumPeer    2888    hbase.zookeeper.peerport              HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。\nHBase   HQuorumPeer    3888    hbase.zookeeper.leaderport            HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。\n\n\n# Hive\n\n组件     节点           端口      配置                                                      用途说明\nHive   Metastore    9083    /etc/default/hive-metastore中export PORT=<port>来更新默认端口   \nHive   HiveServer   10000   /etc/hive/conf/hive-env.sh中export                       \n                            HIVE_SERVER2_THRIFT_PORT=<port>来更新默认端口\n\n\n# Zookeeper\n\n组件          节点       端口     配置                                                                   用途说明\nZooKeeper   Server   2181   /etc/zookeeper/conf/zoo.cfg中clientPort=<port>                        对客户端提供服务的端口\nZooKeeper   Server   2888   /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分   follower用来连接到leader，只在leader上监听该端口。\nZooKeeper   Server   3888   /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分   用于leader选举的。只在electionAlg是1,2或3(默认)时需要。',normalizedContent:'# 环境安装\n\nhttps://hadoop.apache.org/docs/stable/ 官方文档\n\n\n# 虚拟机安装前置\n\nsudo yum install -y epel-release  #额外软件源\nsudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git wget #安装 psmisc工具包  nc工具包 net-tools工具包 rsync 远程同步  vim编辑器 lrzsz上传下载  ntp时间同步 \n\n\n1\n2\n\n\n\n# 修改静态地址\n\nsudo vim /etc/sysconfig/network-scripts/ifcfg-ens33\n#如手写可以 ctrl+alt+tab 补全  \n\n\n1\n2\n\n\n配置为符合的 记得改uuid和mac地址\n\ndevice=ens33\ntype=ethernet\nonboot=yes\nbootproto=static\nname="ens33"\nipaddr=192.168.1.101\nprefix=24\ngateway=192.168.1.2\ndns1=192.168.1.2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nservice network restart\n\n\n1\n\n\n\n# 修改主机名\n\nsudo hostnamectl --static set-hostname hadoop102\n\n\n1\n\n\n\n# 修改hosts\n\nsudo vim /etc/hosts\n\n\n1\n\n\n192.168.1.100 hadoop100\n192.168.1.101 hadoop101\n192.168.1.102 hadoop102\n192.168.1.103 hadoop103\n192.168.1.104 hadoop104\n192.168.1.105 hadoop105\n192.168.1.106 hadoop106\n192.168.1.107 hadoop107\n192.168.1.108 hadoop108\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n物理机也改host\n\n\n# 关闭防火墙\n\nsudo systemctl stop firewalld\nsudo systemctl disable firewalld\n\n\n1\n2\n\n\n\n# 创建用户\n\nsudo useradd atguigu\nsudo passwd atguigu\n\nreboot\n\n\n1\n2\n3\n4\n\n\n\n# 修改atguigu权限\n\nvisudo\n\n\n1\n\n\n#第91行\n## allow root to run any commands anywhere\nroot    all=(all)     all\natguigu   all=(all)     all\n\n\n1\n2\n3\n4\n\n\n创建opt下的存放目录\n\ncd /opt\nsudo mkdir module\nsudo mkdir software\nsudo chown atguigu:atguigu /opt/module /opt/software\n\n\n1\n2\n3\n4\n\n\n\n# 安装java\n\n先卸载\n\nrpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps\n\n\n1\n\n\n解压\n\ncd /opt/software/\ntar -zxvf jdk-8u212-linux-x64.tar.gz -c /opt/module/\n\n\n1\n2\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n#java_home\n#yum 为/usr/lib/jvm/java\nexport java_home=/opt/module/jdk1.8.0_212 \nexport path=$path:$java_home/bin\n\n\n1\n2\n3\n4\n\n\n更新环境变量\n\nsource /etc/profile\njava -version\n\n\n1\n2\n\n\n\n# 安装hadoop\n\n解压\n\ntar -zxvf hadoop-3.1.3.tar.gz -c /opt/module/\n\n\n1\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n##hadoop_home\nexport hadoop_home=/opt/module/hadoop-3.1.3\nexport path=$path:$hadoop_home/bin\nexport path=$path:$hadoop_home/sbin\n\n\n1\n2\n3\n4\n\n\n更新\n\nsource /etc/profile\nhadoop version\n\n\n1\n2\n\n\n\n# ssh免密登陆\n\n生成密钥 全部回车 为空即可\n\nssh-keygen -t rsa\n\n\n1\n\n\n发送密钥 给另外一台主机\n\nssh-copy-id hadoop102\n\nssh-copy-id hadoop103\n\nssh-copy-id hadoop104\n\n\n1\n2\n3\n4\n5\n\n\n默认存储在 /home/用户/.ssh文件下\n\n注意：\n\n还需要在hadoop102上采用root账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；\n\n还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。\n\n\n# xsync 脚本\n\n安装\n\nyum install -y rsync\n\n\n1\n\n\n创建xsync脚本\n\ncd /home/atguigu\nvim xsync\n\n\n1\n2\n\n\n#!/bin/bash\n#1. 判断参数个数\nif [ $# -lt 1 ]\nthen\n  echo not enough arguement!\n  exit;\nfi\n#2. 遍历集群所有机器\nfor host in hadoop102 hadoop103 hadoop104\ndo\n  echo ====================  $host  ====================\n  #3. 遍历所有目录，挨个发送\n  for file in $@\n  do\n    #4 判断文件是否存在\n    if [ -e $file ]\n    then\n      #5. 获取父目录\n      pdir=$(cd -p $(dirname $file); pwd)\n      #6. 获取当前文件的名称\n      fname=$(basename $file)\n      ssh $host "mkdir -p $pdir"\n      rsync -av $pdir/$fname $host:$pdir\n    else\n      echo $file does not exists!\n    fi\n  done\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nchmod +x xsync\nsudo mv xsync /bin/   #将脚本移动到/bin中，以便全局调用\n\n\n1\n2\n\n\nhttps://rsync.samba.org/\n\nhttps://vault.centos.org/6.5/os/x86_64/\n\n安装包安装\n\nwget https://vault.centos.org/6.5/os/x86_64/packages/rsync-3.0.6-9.el6_4.1.x86_64.rpm\nrpm -ivh rsync-3.0.6-9.el6_4.1.x86_64.rpm\n\n\n1\n2\n\n\n\n# 复制ssh免密key\n\ncd /home/atguigu\nxsync .ssh\n\n\n1\n2\n\n\n\n# 同步环境\n\n同步环境变量\n\nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n\n\n同步软件\n\ncd /\nsxsync opt\n\n\n1\n2\n\n\n\n# hadoop配置文件\n\ncd /opt/module/hadoop-3.1.3/etc/hadoop\n\n\n1\n\n\n\n# 配置core-site.xml\n\nvim core-site.xml \n\n\n1\n\n\n在<configuration>标签中追加\n\n<property>\n        <name>fs.defaultfs</name>\n    \x3c!-- 配置hdfs默认的地址 --\x3e\n        <value>hdfs://hadoop102:8020</value>\n    </property>\n    <property>\n        \x3c!-- 配置hadoop临时存放路径--\x3e\n        <name>hadoop.tmp.dir</name>\n        <value>/opt/module/hadoop-3.1.3/data</value>\n    </property>\n    <property>\n        \x3c!-- 兼容性配置hive --\x3e\n        <name>hadoop.proxyuser.atguigu.hosts</name>\n        <value>*</value>\n    </property>\n    <property>\n        <name>hadoop.proxyuser.atguigu.groups</name>\n        <value>*</value>\n    </property>\n    <property>\n        <name>hadoop.http.staticuser.user</name>\n        <value>atguigu</value>\n    </property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 配置hdfs-site.xml\n\nvim hdfs-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n<property>\n    \x3c!-- 2nn的地址 --\x3e\n        <name>dfs.namenode.secondary.http-address</name>\n        <value>hadoop104:9868</value>\n    </property>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 配置yarn-site.xml\n\nvim yarn-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n<property>\n    \x3c!--设置nodemanager上运行的附属服务，需配置成mapreduce_shuffle才可运行mapreduce程序--\x3e\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <value>hadoop103</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.env-whitelist</name>\n        <value>java_home,hadoop_common_home,hadoop_hdfs_home,hadoop_conf_dir,classpath_prepend_distcache,hadoop_yarn_home,hadoop_mapred_home</value>\n    </property>\n\t\x3c!--设定单个容器可以申领到的最小内存资源--\x3e\n    <property>\n        <name>yarn.scheduler.minimum-allocation-mb</name>\n        <value>2048</value>\n    </property>\n    <property>\n        <name>yarn.scheduler.maximum-allocation-mb</name>\n        <value>4096</value>\n    </property>\n\x3c!--设定物理节点有4g内存加入资源池--\x3e\n    <property>\n        <name>yarn.nodemanager.resource.memory-mb</name>\n        <value>4096</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.pmem-check-enabled</name>\n        <value>false</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.vmem-check-enabled</name>\n        <value>false</value>\n    </property>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# 配置mapred-site.xml\n\nvim mapred-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n\x3c!--hadoop对mapreduce运行框架一共提供了3种实现，在mapred-site.xml中通过"mapreduce.framework.name"这个属性来设置为"classic"、"yarn"或者"local"--\x3e\n<property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n\n\n1\n2\n3\n4\n5\n\n\n\n# 配置workers\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/workers\n\n\n1\n\n\n删除localhost 文件中添加的内容结尾不允许有空格，文件中不允许有空行。\n\nhadoop102\nhadoop103\nhadoop104\n\n\n1\n2\n3\n\n\n\n# 同步\n\ncd ..\nxsync hadoop\n\n\n1\n2\n\n\n\n# 启动集群\n\n格式化hdfs\n\nhdfs namenode -format  #在节点机子上\n\n\n1\n\n\n启动集群\n\nstart-dfs.sh\n\n\n1\n\n\n如果报java未找到 修改hadoop.env.sh文件\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/hadoop.env.sh\n\n\n1\n\n\n修改java_home为 并同步\n\nexport java_home=/usr/lib/jvm/java\n\n\n1\n\n\n\n\n显示这样启动成功\n\n\n# dfs启动\n\n在103中启动\n\nstart-yarn.sh\n\n\n1\n\n\n\n# jps\n\njps\n\n\n1\n\n\n\n# 配置历史服务器\n\n关闭dfs和yarn\n\nstop-yarn.sh  #103\nstop-dfs.sh  #102\n\n\n1\n2\n\n\n\n# 配置mapred-site.xml\n\nvi mapred-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n\x3c!-- 历史服务器端地址 --\x3e\n<property>\n    <name>mapreduce.jobhistory.address</name>\n    <value>hadoop102:10020</value>\n</property>\n\n\x3c!-- 历史服务器web端地址 --\x3e\n<property>\n    <name>mapreduce.jobhistory.webapp.address</name>\n    <value>hadoop102:19888</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 配置日志聚集\n\n\n# 配置 yarn-site.xml\n\nvim yarn-site.xml\n\n\n1\n\n\n同样在<configuration>标签中追加\n\n<property>\n    \x3c!-- 开启日志聚集--\x3e\n    <name>yarn.log-aggregation-enable</name>\n    <value>true</value>\n</property>\n<property>  \n    \x3c!-- 日志服务器--\x3e\n    <name>yarn.log.server.url</name>  \n    <value>http://${yarn.timeline-service.webapp.address}/applicationhistory/logs</value>\n</property>\n<property>\n    \x3c!-- 日志保存时间--\x3e\n    <name>yarn.log-aggregation.retain-seconds</name>\n    <value>604800</value>\n</property>\n<property>\n    <name>yarn.timeline-service.enabled</name>\n    <value>true</value>\n</property>\n<property>\n    <name>yarn.timeline-service.hostname</name>\n    <value>${yarn.resourcemanager.hostname}</value>\n</property>\n<property>\n    <name>yarn.timeline-service.http-cross-origin.enabled</name>\n    <value>true</value>\n</property>\n<property>\n    <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>\n    <value>true</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# 配置完成重新同步 并启动\n\ncd ..\nxsync hadoop/\n\n\n1\n2\n\n\n重新启动\n\nstart-dfs.sh  #102\n\nstart-yarn.sh  #103\n\nmapred --daemon start historyserver  #102 启动历史服务器\n\n\n1\n2\n3\n4\n5\n\n\n配置完成后不要同步整个hadoop文件夹 只需要etc配置文件夹 千万不要同步data文件夹\n\n如想修改data路径\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml  #hadoop.data.dir\n\n\n1\n\n\n如果想重新按配置开 删除掉logs文件夹里的东西 重新格式化hdfs 再启动\n\n\n# 集群时间同步\n\n在所有节点关闭ntp服务和自启动\n\n建议root用户下操作\n\nsu -\nsudo systemctl stop ntpd\nsudo systemctl disable ntpd\n\n\n1\n2\n3\n\n\n\n# 修改ntp配置文件\n\nvim /etc/ntp.conf\n\n\n1\n\n\nrestrict 192.168.130.2 mask 255.255.255.0 nomodify notrap  #去掉#号修改即可\n\n#加上井号\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\n\n#添加\nserver 127.127.1.0\nfudge 127.127.1.0 stratum 10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 修改ntpd文件\n\nvim /etc/sysconfig/ntpd\n\n\n1\n\n\n#追加\nsync_hwclock=yes\n\n\n1\n2\n\n\n#重启服务\nsystemctl start ntpd\n\n#开机启动\nsystemctl enable ntpd\n\n\n1\n2\n3\n4\n5\n\n\n以上修改文件在102进行\n\n\n# 其他机器配置\n\n切换root用户 进行\n\nsu -\ncrontab -e\n\n\n1\n2\n\n\n#追加\n*/1 * * * * /usr/sbin/ntpdate hadoop102\n\n\n1\n2\n\n\ndate  #查看当前时间是否同步\n\n\n1\n\n\n\n# 常用端口\n\n\n# hadopp2.x和3.x端口变化\n\nhadoop2.x    hadoop3.x   \n访问hdfs端口     50070       9870\n访问mr执行情况端口   8088        8088\n历史服务器        19888       19888\n客户端访问集群端口    9000        8020\n\n\n\n\n# hdfs\n\n组件     节点            端口     配置                             用途说明\nhdfs   namenode      9870   dfs.namenode.http-address      http服务的端口\nhdfs   namenode      9871   dfs.namenode.https-address     https服务的端口\nhdfs   namenode      9820   fs.defaultfs                   接收client连接的rpc端口，用于获取文件系统metadata信息。\nhdfs   datanode      9866   dfs.datanode.address           datanode服务端口，用于数据传输\nhdfs   datanode      9864   dfs.datanode.http.address      http服务的端口\nhdfs   datanode      9865   dfs.datanode.https.address     https服务的端口\nhdfs   datanode      9867   dfs.datanode.ipc.address       ipc服务的端口\nhdfs   journalnode   8485   dfs.journalnode.rpc-address    rpc服务\nhdfs   journalnode   8480   dfs.journalnode.http-address   http服务\n\n\n# yarn\n\n组件     节点                  端口      配置                                              用途说明\nyarn   resourcemanager     8032    yarn.resourcemanager.address                    rm的applications manager(asm)端口\nyarn   resourcemanager     8030    yarn.resourcemanager.scheduler.address          scheduler组件的ipc端口\nyarn   resourcemanager     8031    yarn.resourcemanager.resource-tracker.address   ipc\nyarn   resourcemanager     8033    yarn.resourcemanager.admin.address              ipc\nyarn   resourcemanager     8088    yarn.resourcemanager.webapp.address             http服务端口\nyarn   nodemanager         8040    yarn.nodemanager.localizer.address              localizer ipc\nyarn   nodemanager         8042    yarn.nodemanager.webapp.address                 http服务端口\nyarn   nodemanager         8041    yarn.nodemanager.address                        nm中container manager的端口\nyarn   jobhistory server   10020   mapreduce.jobhistory.address                    ipc\nyarn   jobhistory server   19888   mapreduce.jobhistory.webapp.address             http服务端口\n\n\n# hbase\n\n组件      节点             端口      配置                                    用途说明\nhbase   master         60000   hbase.master.port                     ipc\nhbase   master         60010   hbase.master.info.port                http服务端口\nhbase   regionserver   60020   hbase.regionserver.port               ipc\nhbase   regionserver   60030   hbase.regionserver.info.port          http服务端口\nhbase   hquorumpeer    2181    hbase.zookeeper.property.clientport   hbase-managed zk mode，使用独立的zookeeper集群则不会启用该端口。\nhbase   hquorumpeer    2888    hbase.zookeeper.peerport              hbase-managed zk mode，使用独立的zookeeper集群则不会启用该端口。\nhbase   hquorumpeer    3888    hbase.zookeeper.leaderport            hbase-managed zk mode，使用独立的zookeeper集群则不会启用该端口。\n\n\n# hive\n\n组件     节点           端口      配置                                                      用途说明\nhive   metastore    9083    /etc/default/hive-metastore中export port=<port>来更新默认端口   \nhive   hiveserver   10000   /etc/hive/conf/hive-env.sh中export                       \n                            hive_server2_thrift_port=<port>来更新默认端口\n\n\n# zookeeper\n\n组件          节点       端口     配置                                                                   用途说明\nzookeeper   server   2181   /etc/zookeeper/conf/zoo.cfg中clientport=<port>                        对客户端提供服务的端口\nzookeeper   server   2888   /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分   follower用来连接到leader，只在leader上监听该端口。\nzookeeper   server   3888   /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分   用于leader选举的。只在electionalg是1,2或3(默认)时需要。',charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:39:06",lastUpdatedTimestamp:1647567546e3},{title:"java操作",frontmatter:{title:"java操作",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/9ce9a0/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/06.java%E6%93%8D%E4%BD%9C.html",relativePath:"大数据/01.Hadoop/06.java操作.md",key:"v-91bab1e0",path:"/pages/9ce9a0/",headers:[{level:2,title:"HDFS文件上传",slug:"hdfs文件上传",normalizedTitle:"hdfs文件上传",charIndex:13},{level:2,title:"HDFS下载到本地",slug:"hdfs下载到本地",normalizedTitle:"hdfs下载到本地",charIndex:556},{level:2,title:"HDFS追加",slug:"hdfs追加",normalizedTitle:"hdfs追加",charIndex:688},{level:2,title:"ls",slug:"ls",normalizedTitle:"ls",charIndex:849},{level:2,title:"listFiles",slug:"listfiles",normalizedTitle:"listfiles",charIndex:1171},{level:2,title:"移动或重命名",slug:"移动或重命名",normalizedTitle:"移动或重命名",charIndex:1989}],headersStr:"HDFS文件上传 HDFS下载到本地 HDFS追加 ls listFiles 移动或重命名",content:'# java操作\n\n\n# HDFS文件上传\n\n//获取配置对象\nConfiguration configuration = new Configuration();\n//设置配置  将默认分块设置为2\nconfiguration.set("dfs.replication","2");\n//新建HDFS对象\nFileSystem fileSystem = FileSystem.get(URI.create("hdfs://hadoop102:8020"), configuration, "atguigu");\n\n//操作集群\nfileSystem.copyFromLocalFile(new Path("D:\\\\图\\\\QQ图片20190517141148.jpg"),new Path("/"));\n\n//关闭资源\nfileSystem.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n * get(URI.crate("hdfs://hadoop地址"),new Configuation(), 集群用户名); 获取HDFS操作对象\n   * copyFromLocalFile(new Path("本地路径"),new Path(集群路径)); 上传文件到HDFS\n\n\n# HDFS下载到本地\n\nfileSystem.copyToLocalFile(\n        new Path("/QQ图片20190517141148.jpg"),\n        new Path("d:/images")\n);\n\n\n1\n2\n3\n4\n\n\n\n# HDFS追加\n\nFSDataOutputStream append = fileSystem.append(\n        new Path("1.txt")\n);\n\n//写入流\nappend.write("testApi".getBytes(StandardCharsets.UTF_8));\n\n//关闭流\nIOUtils.closeStream(append);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# ls\n\n//ls操作 返回数组\nFileStatus[] fileStatuses = fileSystem.listStatus(new Path("/"));\n\nfor (FileStatus fileStatus : fileStatuses) {\n    //获取文件对象路径\n    System.out.println(fileStatus.getPath());\n    //获取文件所属者\n    System.out.println(fileStatus.getOwner());\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# listFiles\n\n//获取当前所有文件的迭代器 无论是文件夹还是文件  true返回一个迭代器 \nRemoteIterator<LocatedFileStatus> locatedFileStatusRemoteIterator = fileSystem.listFiles(\n        new Path("/"),\n        true);\nwhile (locatedFileStatusRemoteIterator.hasNext()) {\n    LocatedFileStatus fileStatus = locatedFileStatusRemoteIterator.next();\n\n    System.out.println(fileStatus.getPath());\n    //获取当前文件的块 返回数组\n    BlockLocation[] blockLocations = fileStatus.getBlockLocations();\n    for (int i = 0; i < blockLocations.length; i++) {\n        System.out.println("第" + i + "块");\n        //获取当前文件 每个块存储的集群情况\n        String[] hosts = blockLocations[i].getHosts();\n        for (String host : hosts) {\n            System.out.print(host + " ");\n\n        }\n        System.out.println();\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 移动或重命名\n\n//重命名或者移动文件\nfileSystem.rename(new Path("/QQ图片20190517141148.jpg"),new Path("/test/233.jpg"));\n\n\n1\n2\n',normalizedContent:'# java操作\n\n\n# hdfs文件上传\n\n//获取配置对象\nconfiguration configuration = new configuration();\n//设置配置  将默认分块设置为2\nconfiguration.set("dfs.replication","2");\n//新建hdfs对象\nfilesystem filesystem = filesystem.get(uri.create("hdfs://hadoop102:8020"), configuration, "atguigu");\n\n//操作集群\nfilesystem.copyfromlocalfile(new path("d:\\\\图\\\\qq图片20190517141148.jpg"),new path("/"));\n\n//关闭资源\nfilesystem.close();\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n * get(uri.crate("hdfs://hadoop地址"),new configuation(), 集群用户名); 获取hdfs操作对象\n   * copyfromlocalfile(new path("本地路径"),new path(集群路径)); 上传文件到hdfs\n\n\n# hdfs下载到本地\n\nfilesystem.copytolocalfile(\n        new path("/qq图片20190517141148.jpg"),\n        new path("d:/images")\n);\n\n\n1\n2\n3\n4\n\n\n\n# hdfs追加\n\nfsdataoutputstream append = filesystem.append(\n        new path("1.txt")\n);\n\n//写入流\nappend.write("testapi".getbytes(standardcharsets.utf_8));\n\n//关闭流\nioutils.closestream(append);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# ls\n\n//ls操作 返回数组\nfilestatus[] filestatuses = filesystem.liststatus(new path("/"));\n\nfor (filestatus filestatus : filestatuses) {\n    //获取文件对象路径\n    system.out.println(filestatus.getpath());\n    //获取文件所属者\n    system.out.println(filestatus.getowner());\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# listfiles\n\n//获取当前所有文件的迭代器 无论是文件夹还是文件  true返回一个迭代器 \nremoteiterator<locatedfilestatus> locatedfilestatusremoteiterator = filesystem.listfiles(\n        new path("/"),\n        true);\nwhile (locatedfilestatusremoteiterator.hasnext()) {\n    locatedfilestatus filestatus = locatedfilestatusremoteiterator.next();\n\n    system.out.println(filestatus.getpath());\n    //获取当前文件的块 返回数组\n    blocklocation[] blocklocations = filestatus.getblocklocations();\n    for (int i = 0; i < blocklocations.length; i++) {\n        system.out.println("第" + i + "块");\n        //获取当前文件 每个块存储的集群情况\n        string[] hosts = blocklocations[i].gethosts();\n        for (string host : hosts) {\n            system.out.print(host + " ");\n\n        }\n        system.out.println();\n    }\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 移动或重命名\n\n//重命名或者移动文件\nfilesystem.rename(new path("/qq图片20190517141148.jpg"),new path("/test/233.jpg"));\n\n\n1\n2\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"IDEA中创建hadoop项目",frontmatter:{title:"IDEA中创建hadoop项目",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/7e6b01/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/05.IDEA%E4%B8%AD%E5%88%9B%E5%BB%BAhadoop%E9%A1%B9%E7%9B%AE.html",relativePath:"大数据/01.Hadoop/05.IDEA中创建hadoop项目.md",key:"v-9c36cd5e",path:"/pages/7e6b01/",headersStr:null,content:'# IDEA中创建hadoop项目\n\n要配置了winutils\n\n 1. 创建maven项目\n\n 2. 添加依赖\n    \n    <dependencies>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>log4j-slf4j-impl</artifactId>\n            <version>2.12.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>3.1.3</version>\n        </dependency>\n    </dependencies>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    \n\n 3. 在main的resources 新建 log4j2.xml\n    \n    <?xml version="1.0" encoding="UTF-8"?>\n    <Configuration status="error" strict="true" name="XMLConfig">\n        <Appenders>\n            \x3c!-- 类型名为Console，名称为必须属性 --\x3e\n            <Appender type="Console" name="STDOUT">\n                \x3c!-- 布局为PatternLayout的方式，\n                输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I\'m here --\x3e\n                <Layout type="PatternLayout"\n                        pattern="[%p] [%d{yyyy-MM-dd HH:mm:ss}][%c{10}]%m%n"/>\n            </Appender>\n        </Appenders>\n    \n        <Loggers>\n            \x3c!-- 可加性为false --\x3e\n            <Logger name="test" level="info" additivity="false">\n                <AppenderRef ref="STDOUT"/>\n            </Logger>\n    \n            \x3c!-- root loggerConfig设置 --\x3e\n            <Root level="info">\n                <AppenderRef ref="STDOUT"/>\n            </Root>\n        </Loggers>\n    </Configuration>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    ',normalizedContent:'# idea中创建hadoop项目\n\n要配置了winutils\n\n 1. 创建maven项目\n\n 2. 添加依赖\n    \n    <dependencies>\n        <dependency>\n            <groupid>junit</groupid>\n            <artifactid>junit</artifactid>\n            <version>4.12</version>\n        </dependency>\n        <dependency>\n            <groupid>org.apache.logging.log4j</groupid>\n            <artifactid>log4j-slf4j-impl</artifactid>\n            <version>2.12.0</version>\n        </dependency>\n        <dependency>\n            <groupid>org.apache.hadoop</groupid>\n            <artifactid>hadoop-client</artifactid>\n            <version>3.1.3</version>\n        </dependency>\n    </dependencies>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    \n\n 3. 在main的resources 新建 log4j2.xml\n    \n    <?xml version="1.0" encoding="utf-8"?>\n    <configuration status="error" strict="true" name="xmlconfig">\n        <appenders>\n            \x3c!-- 类型名为console，名称为必须属性 --\x3e\n            <appender type="console" name="stdout">\n                \x3c!-- 布局为patternlayout的方式，\n                输出样式为[info] [2018-01-22 17:34:01][org.test.console]i\'m here --\x3e\n                <layout type="patternlayout"\n                        pattern="[%p] [%d{yyyy-mm-dd hh:mm:ss}][%c{10}]%m%n"/>\n            </appender>\n        </appenders>\n    \n        <loggers>\n            \x3c!-- 可加性为false --\x3e\n            <logger name="test" level="info" additivity="false">\n                <appenderref ref="stdout"/>\n            </logger>\n    \n            \x3c!-- root loggerconfig设置 --\x3e\n            <root level="info">\n                <appenderref ref="stdout"/>\n            </root>\n        </loggers>\n    </configuration>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"HDFS的数据流",frontmatter:{title:"HDFS的数据流",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/bbfc59/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/07.HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81.html",relativePath:"大数据/01.Hadoop/07.HDFS的数据流.md",key:"v-0cd4d5ee",path:"/pages/bbfc59/",headers:[{level:2,title:"网络拓扑",slug:"网络拓扑",normalizedTitle:"网络拓扑",charIndex:464}],headersStr:"网络拓扑",content:"# HDFS的数据流\n\n\n\n（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。\n\n（2）NameNode返回是否可以上传。\n\n（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。\n\n（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。\n\n（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。\n\n（6）dn1、dn2、dn3逐级应答客户端。\n\n（7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。\n\n（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。\n\n\n# 网络拓扑\n\n\n\n",normalizedContent:"# hdfs的数据流\n\n\n\n（1）客户端通过distributed filesystem模块向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。\n\n（2）namenode返回是否可以上传。\n\n（3）客户端请求第一个 block上传到哪几个datanode服务器上。\n\n（4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。\n\n（5）客户端通过fsdataoutputstream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。\n\n（6）dn1、dn2、dn3逐级应答客户端。\n\n（7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。\n\n（8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。\n\n\n# 网络拓扑\n\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"HDFS",frontmatter:{title:"HDFS",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/7afcbc/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/03.HDFS.html",relativePath:"大数据/01.Hadoop/03.HDFS.md",key:"v-4672ef15",path:"/pages/7afcbc/",headers:[{level:2,title:"命令行操作",slug:"命令行操作",normalizedTitle:"命令行操作",charIndex:293},{level:3,title:"HDFS 到 HDFS",slug:"hdfs-到-hdfs",normalizedTitle:"hdfs 到 hdfs",charIndex:333},{level:3,title:"本地 到 HDFS",slug:"本地-到-hdfs",normalizedTitle:"本地 到 hdfs",charIndex:610},{level:3,title:"HDFS 到 本地",slug:"hdfs-到-本地",normalizedTitle:"hdfs 到 本地",charIndex:1224}],headersStr:"命令行操作 HDFS 到 HDFS 本地 到 HDFS HDFS 到 本地",content:"# HDFS\n\nHDFS(Hadoop Distributed File System) 它是一个文件系统 用于存储文件 通过目录树来定位文件 其次它是分布式的\n\nHDFS的使用场景: 适合一次写入 多次读出的场景 且不支持文件的修改\n\n优点:\n\n 1. 高容错性 自动保存多个副本 某一个副本丢失 可以自动恢复\n 2. 适合处理大数据\n 3. 可构建在廉价机器上 通过多副本机制 提高可靠性\n\n缺点:\n\n 1. 不适合低延时时数据访问 比如毫秒级的数据\n 2. 无法高效的对大量小文件进行存储\n 3. 不支持并发写入 文件随机修改 仅支持数据的append 追加\n\n\n\n\n\n\n\n\n# 命令行操作\n\n以 hadoop fs 或者 hdfs dfs 为关键字\n\n\n# HDFS 到 HDFS\n\n大部分linux命令都支持 需要在hadoop fs -拼接命令\n\n * cp 如:hadoop fs -cp 1.txt 2.txt\n\n * mv\n\n * chown\n\n * chmod\n\n * mkdir\n\n * du 统计文件夹的大小信息\n\n * df\n\n * cat\n\n * rm\n\n * setrep 设置HDFS中文件的副本数据 默认为节点(集群)数 并且不可以超节点数最大为节点数\n   \n   * hadoop fs -setrep 10 /1.txt\n     \n     \n     1\n     \n\n\n# 本地 到 HDFS\n\n * put 上传\n   \n   * #hadoop fs -put 文件 HDFS内文件路径\n     hadoop fs -put 1.txt /\n     \n     \n     1\n     2\n     \n\n * copyFromLocal 从本地复制到HDFS 支持多线程\n   \n   * #hadoop fs -copyFromLocal 文件 HDFS内文件路径\n     hadoop fs -copyFromLocal  1.txt /\n     \n     \n     1\n     2\n     \n\n * moveFromLocal 从本地复制到HDFS\n   \n   * #hadoop fs -moveFromLocal  文件 HDFS内文件路径\n     hadoop fs -moveFromLocal   1.txt /\n     \n     \n     1\n     2\n     \n\n * appendToFile 将指定文件内容或者指定内容追加到HDFS文件末尾\n   \n   * #hadoop fs -appendToFile  文件 HDFS内文件路径  如要指定内容则文件名忽略填写 - 既可\n     hadoop fs -appendToFile   2.txt /1.txt\n     \n     \n     1\n     2\n     \n\n\n# HDFS 到 本地\n\n * get 从HDFS下载到本地\n   \n   * #hadoop fs -get  HDFS内文件  本地文件路径\n     hadoop fs -get   /2.txt /\n     \n     \n     1\n     2\n     \n\n * copyToLocal 与get一致\n\n * getmerge 合并下载\n   \n   * #hadoop fs -get  HDFS内多个文件  本地文件\n     hadoop fs -get   /*.txt /1.txt\n     \n     \n     1\n     2\n     ",normalizedContent:"# hdfs\n\nhdfs(hadoop distributed file system) 它是一个文件系统 用于存储文件 通过目录树来定位文件 其次它是分布式的\n\nhdfs的使用场景: 适合一次写入 多次读出的场景 且不支持文件的修改\n\n优点:\n\n 1. 高容错性 自动保存多个副本 某一个副本丢失 可以自动恢复\n 2. 适合处理大数据\n 3. 可构建在廉价机器上 通过多副本机制 提高可靠性\n\n缺点:\n\n 1. 不适合低延时时数据访问 比如毫秒级的数据\n 2. 无法高效的对大量小文件进行存储\n 3. 不支持并发写入 文件随机修改 仅支持数据的append 追加\n\n\n\n\n\n\n\n\n# 命令行操作\n\n以 hadoop fs 或者 hdfs dfs 为关键字\n\n\n# hdfs 到 hdfs\n\n大部分linux命令都支持 需要在hadoop fs -拼接命令\n\n * cp 如:hadoop fs -cp 1.txt 2.txt\n\n * mv\n\n * chown\n\n * chmod\n\n * mkdir\n\n * du 统计文件夹的大小信息\n\n * df\n\n * cat\n\n * rm\n\n * setrep 设置hdfs中文件的副本数据 默认为节点(集群)数 并且不可以超节点数最大为节点数\n   \n   * hadoop fs -setrep 10 /1.txt\n     \n     \n     1\n     \n\n\n# 本地 到 hdfs\n\n * put 上传\n   \n   * #hadoop fs -put 文件 hdfs内文件路径\n     hadoop fs -put 1.txt /\n     \n     \n     1\n     2\n     \n\n * copyfromlocal 从本地复制到hdfs 支持多线程\n   \n   * #hadoop fs -copyfromlocal 文件 hdfs内文件路径\n     hadoop fs -copyfromlocal  1.txt /\n     \n     \n     1\n     2\n     \n\n * movefromlocal 从本地复制到hdfs\n   \n   * #hadoop fs -movefromlocal  文件 hdfs内文件路径\n     hadoop fs -movefromlocal   1.txt /\n     \n     \n     1\n     2\n     \n\n * appendtofile 将指定文件内容或者指定内容追加到hdfs文件末尾\n   \n   * #hadoop fs -appendtofile  文件 hdfs内文件路径  如要指定内容则文件名忽略填写 - 既可\n     hadoop fs -appendtofile   2.txt /1.txt\n     \n     \n     1\n     2\n     \n\n\n# hdfs 到 本地\n\n * get 从hdfs下载到本地\n   \n   * #hadoop fs -get  hdfs内文件  本地文件路径\n     hadoop fs -get   /2.txt /\n     \n     \n     1\n     2\n     \n\n * copytolocal 与get一致\n\n * getmerge 合并下载\n   \n   * #hadoop fs -get  hdfs内多个文件  本地文件\n     hadoop fs -get   /*.txt /1.txt\n     \n     \n     1\n     2\n     ",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"NameNode 工作机制",frontmatter:{title:"NameNode 工作机制",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/785074/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/08.NameNode%20%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.html",relativePath:"大数据/01.Hadoop/08.NameNode 工作机制.md",key:"v-00dc2e8f",path:"/pages/785074/",headers:[{level:2,title:"Fsimage和Edits解析",slug:"fsimage和edits解析",normalizedTitle:"fsimage和edits解析",charIndex:22},{level:2,title:"CheckPoint时间设置",slug:"checkpoint时间设置",normalizedTitle:"checkpoint时间设置",charIndex:452},{level:2,title:"集群安全模式",slug:"集群安全模式",normalizedTitle:"集群安全模式",charIndex:969}],headersStr:"Fsimage和Edits解析 CheckPoint时间设置 集群安全模式",content:"# NameNode 工作机制\n\n\n\n\n# Fsimage和Edits解析\n\nFsimage和Edits 文件存放在data中 /opt/module/hadoop-3.1.3/data/dfs/name/current\n\nhdfs内置命令可以查看\n\n * hdfs oiv -p XML -i /opt/module/hadoop-3.1.3/data/dfs/name/current/fsimage_00000000000000000000xxxxxx -o /opt/module/hadoop-3.1.3/fsimage.xml\n   * hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径\n * hdfs oev -p XML edists_0000000000000000000000xxxxx -o /opt/module/hadoop-3.1.3/edits.xml\n   * hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径\n\n\n# CheckPoint时间设置\n\n通常情况下，SecondaryNameNode每隔一小时执行一次\n\nhdfs-default.xml\n\n<property>\n  <name>dfs.namenode.checkpoint.period</name>\n  <value>3600</value>\n</property>\n\n\n1\n2\n3\n4\n\n\n一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次\n\n<property>\n  <name>dfs.namenode.checkpoint.txns</name>\n  <value>1000000</value>\n<description>操作动作次数</description>\n</property>\n\n<property>\n  <name>dfs.namenode.checkpoint.check.period</name>\n  <value>60</value>\n<description> 1分钟检查一次操作次数</description>\n</property >\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 集群安全模式\n\n集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。",normalizedContent:"# namenode 工作机制\n\n\n\n\n# fsimage和edits解析\n\nfsimage和edits 文件存放在data中 /opt/module/hadoop-3.1.3/data/dfs/name/current\n\nhdfs内置命令可以查看\n\n * hdfs oiv -p xml -i /opt/module/hadoop-3.1.3/data/dfs/name/current/fsimage_00000000000000000000xxxxxx -o /opt/module/hadoop-3.1.3/fsimage.xml\n   * hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径\n * hdfs oev -p xml edists_0000000000000000000000xxxxx -o /opt/module/hadoop-3.1.3/edits.xml\n   * hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径\n\n\n# checkpoint时间设置\n\n通常情况下，secondarynamenode每隔一小时执行一次\n\nhdfs-default.xml\n\n<property>\n  <name>dfs.namenode.checkpoint.period</name>\n  <value>3600</value>\n</property>\n\n\n1\n2\n3\n4\n\n\n一分钟检查一次操作次数，3当操作次数达到1百万时，secondarynamenode执行一次\n\n<property>\n  <name>dfs.namenode.checkpoint.txns</name>\n  <value>1000000</value>\n<description>操作动作次数</description>\n</property>\n\n<property>\n  <name>dfs.namenode.checkpoint.check.period</name>\n  <value>60</value>\n<description> 1分钟检查一次操作次数</description>\n</property >\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 集群安全模式\n\n集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"DataNode",frontmatter:{title:"DataNode",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/49d0d4/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/09.DataNode.html",relativePath:"大数据/01.Hadoop/09.DataNode.md",key:"v-724628f5",path:"/pages/49d0d4/",headers:[{level:2,title:"数据完整性",slug:"数据完整性",normalizedTitle:"数据完整性",charIndex:17},{level:2,title:"扩展集群",slug:"扩展集群",normalizedTitle:"扩展集群",charIndex:135},{level:3,title:"添加黑/白名单",slug:"添加黑-白名单",normalizedTitle:"添加黑/白名单",charIndex:796},{level:3,title:"黑名单退役",slug:"黑名单退役",normalizedTitle:"黑名单退役",charIndex:1391},{level:2,title:"DateNode 多目录配置",slug:"datenode-多目录配置",normalizedTitle:"datenode 多目录配置",charIndex:1544}],headersStr:"数据完整性 扩展集群 添加黑/白名单 黑名单退役 DateNode 多目录配置",content:"# DataNode\n\n\n\n\n# 数据完整性\n\n奇偶校验位 如果传输数据1为偶数个数则为0 如果为奇数个1则为1\n\n如果原始数据与接收数据发生改变又恰好奇偶性一致 则这个现象我们称为校验碰撞\n\ncrc校验位 32位\n\nmd5 128位\n\nsha1 160位\n\n\n\n\n# 扩展集群\n\n服役新数据节点\n\n 1. 克隆 修改主机名 ip\n\n 2. #复制module文件夹 在其他集群中复制  和环境变量\n    sudo rsync -av /opt/module hadoop105:/opt\n    sudo rsync -av /etc/profile.d hadoop105:/etc\n    \n    \n    1\n    2\n    3\n    \n\n 3. #在扩展机中删除logs data 文件夹\n    cd /opt/module/hadoop-3.1.3/\n    rm -rf data logs\n    source /etc/profile\n    hadoop version\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 4. hdfs --daemon start datanode \n    yarn --daemon start nodemanager  \n    jps\n    \n    \n    1\n    2\n    3\n    \n\n此方式是手动启动 如果想要群启动则需要配置免密登陆\n\n#在102中配置\nvim /opt/module/hadoop-3.1.3/etc/hadoop/workers \n\n\n1\n2\n\n\n追加上地址\n\nhadoop105\n\n\n1\n\n\n同步所有集群\n\nxsync /opt/module/hadoop-3.1.3/etc/hadoop/workers \n\n\n1\n\n\n\n# 添加黑/白名单\n\n在hadoop文件夹下 新建balcklist和whitelist\n\n#主机上写 102\ncd /opt/module/hadoop-3.1.3/etc/hadoop\ntouch balcklist\ntouch whitelist\n\n\n1\n2\n3\n4\n\n\n添加白名单 一般whitelist内容和workers相同\n\nvim whitelist\n\n\n1\n\n\nhadoop102\nhadoop103\nhadoop104\nhadoop105\n\n\n1\n2\n3\n4\n\n\n再编辑hdfs-size.xml\n\nvim hdfs-site.xml \n\n\n1\n\n\n添加以下内容\n\n<property>\n<name>dfs.hosts.exclude</name>\n      <value>/opt/module/hadoop-3.1.3/etc/hadoop/balcklist</value>\n</property>\n<property>\n<name>dfs.hosts</name>\n      <value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n重启集群\n\nstop-dfs.sh\n\n\n1\n\n\n开启\n\nstart-dfs.sh\n\n\n1\n\n\n\n# 黑名单退役\n\n编辑 balcklist 添加黑名单地址\n\nvim balcklist\nhdfs dfsadmin -refreshNodes  #刷新节点\n\n\n1\n2\n\n\n退役的机器 自动上传文件到服役中的其他主机中\n\n关闭节点\n\nhdfs --daemon stop datanode\n\n\n1\n\n\n\n# DateNode 多目录配置\n\nDataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本\n\nvim hdfs-site.xml\n\n\n1\n\n\n多个目录之间逗号隔开\n\n<property>\n        <name>dfs.datanode.data.dir</name>\n<value>file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2</value>\n</property>\n\n\n1\n2\n3\n4\n\n\n配置完成后重启\n\nhdfs --daemon stop datanode\nhdfs --daemon start datanode\n\n\n1\n2\n",normalizedContent:"# datanode\n\n\n\n\n# 数据完整性\n\n奇偶校验位 如果传输数据1为偶数个数则为0 如果为奇数个1则为1\n\n如果原始数据与接收数据发生改变又恰好奇偶性一致 则这个现象我们称为校验碰撞\n\ncrc校验位 32位\n\nmd5 128位\n\nsha1 160位\n\n\n\n\n# 扩展集群\n\n服役新数据节点\n\n 1. 克隆 修改主机名 ip\n\n 2. #复制module文件夹 在其他集群中复制  和环境变量\n    sudo rsync -av /opt/module hadoop105:/opt\n    sudo rsync -av /etc/profile.d hadoop105:/etc\n    \n    \n    1\n    2\n    3\n    \n\n 3. #在扩展机中删除logs data 文件夹\n    cd /opt/module/hadoop-3.1.3/\n    rm -rf data logs\n    source /etc/profile\n    hadoop version\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 4. hdfs --daemon start datanode \n    yarn --daemon start nodemanager  \n    jps\n    \n    \n    1\n    2\n    3\n    \n\n此方式是手动启动 如果想要群启动则需要配置免密登陆\n\n#在102中配置\nvim /opt/module/hadoop-3.1.3/etc/hadoop/workers \n\n\n1\n2\n\n\n追加上地址\n\nhadoop105\n\n\n1\n\n\n同步所有集群\n\nxsync /opt/module/hadoop-3.1.3/etc/hadoop/workers \n\n\n1\n\n\n\n# 添加黑/白名单\n\n在hadoop文件夹下 新建balcklist和whitelist\n\n#主机上写 102\ncd /opt/module/hadoop-3.1.3/etc/hadoop\ntouch balcklist\ntouch whitelist\n\n\n1\n2\n3\n4\n\n\n添加白名单 一般whitelist内容和workers相同\n\nvim whitelist\n\n\n1\n\n\nhadoop102\nhadoop103\nhadoop104\nhadoop105\n\n\n1\n2\n3\n4\n\n\n再编辑hdfs-size.xml\n\nvim hdfs-site.xml \n\n\n1\n\n\n添加以下内容\n\n<property>\n<name>dfs.hosts.exclude</name>\n      <value>/opt/module/hadoop-3.1.3/etc/hadoop/balcklist</value>\n</property>\n<property>\n<name>dfs.hosts</name>\n      <value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n重启集群\n\nstop-dfs.sh\n\n\n1\n\n\n开启\n\nstart-dfs.sh\n\n\n1\n\n\n\n# 黑名单退役\n\n编辑 balcklist 添加黑名单地址\n\nvim balcklist\nhdfs dfsadmin -refreshnodes  #刷新节点\n\n\n1\n2\n\n\n退役的机器 自动上传文件到服役中的其他主机中\n\n关闭节点\n\nhdfs --daemon stop datanode\n\n\n1\n\n\n\n# datenode 多目录配置\n\ndatanode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本\n\nvim hdfs-site.xml\n\n\n1\n\n\n多个目录之间逗号隔开\n\n<property>\n        <name>dfs.datanode.data.dir</name>\n<value>file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2</value>\n</property>\n\n\n1\n2\n3\n4\n\n\n配置完成后重启\n\nhdfs --daemon stop datanode\nhdfs --daemon start datanode\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"MapReduce",frontmatter:{title:"MapReduce",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/070d96/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/10.MapReduce.html",relativePath:"大数据/01.Hadoop/10.MapReduce.md",key:"v-1c17faff",path:"/pages/070d96/",headers:[{level:2,title:"Hadoop原生MapReduce",slug:"hadoop原生mapreduce",normalizedTitle:"hadoop原生mapreduce",charIndex:174},{level:2,title:"常用数据序列化类型",slug:"常用数据序列化类型",normalizedTitle:"常用数据序列化类型",charIndex:509},{level:2,title:"WordCount编写",slug:"wordcount编写",normalizedTitle:"wordcount编写",charIndex:525},{level:2,title:"HaDoop序列化",slug:"hadoop序列化",normalizedTitle:"hadoop序列化",charIndex:4381},{level:3,title:"统计流量案例",slug:"统计流量案例",normalizedTitle:"统计流量案例",charIndex:4520}],headersStr:"Hadoop原生MapReduce 常用数据序列化类型 WordCount编写 HaDoop序列化 统计流量案例",content:'# MapReduce\n\nMapReduce是一个分布式运算程序的编程框架 将用户编写的业务逻辑代码和自带默认组件整合一贯完整的分布式运算程序 并发运行在一个Haoop集群上\n\n优点:它简单的实现一些接口，就可以完成一个分布式程序\n\n缺点:每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。\n\n\n\n\n# Hadoop原生MapReduce\n\n存储在/opt/module/hadoop-3.1.3/share/hadoop/mapreduce\n\n * yarn jar MapReduce路径 wordcount 输入hdfs文件路径 输出hdfs文件保存路径还是在hdfs上(必须是不存在的文件夹否则报错)\n\ncd /opt/module/hadoop-3.1.3\nyarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /fiddler.md /output\n\n\n1\n2\n\n\n打开hdfs中output中的part-r-xxxx 里面会统计出每个字词出现的次数\n\n\n# 常用数据序列化类型\n\n\n\n\n# WordCount编写\n\n使用IDEA中创建hadoop项目 创建maven项目\n\nWcDriver类\n\npackage com.mywordcount;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\n\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class WcDriver {\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n        //1.获取Job实例\n        Job job = Job.getInstance(new Configuration());\n        //2.设置jar包\n        job.setJarByClass(WcDriver.class);\n\n        //设置Mapper和Reducer\n        job.setMapOutputKeyClass(WcDriver.class);\n        job.setReducerClass(WcReducer.class);\n\n        //设置Map和Reduce的输出类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        //设置输入输出文件\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //提交job\n        boolean result = job.waitForCompletion(true);\n\n        System.exit(result ? 0 : 1);\n\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\nWcMapper\n\npackage com.mywordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class WcMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n    private IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    /**\n     * 框架将数据拆成一行一行输入进来 把数据变成(单词,1)的形式\n     *\n     * @param key     行号\n     * @param value   行内容\n     * @param context 任务本身\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {\n        //行数据\n        String line = value.toString();\n\n        //拆分成若干个单词\n        String[] words = line.split(" ");\n\n        //将(单词,1)写回框架\n        for (String word : words) {\n            this.word.set(word);\n            context.write(this.word, this.one);\n        }\n\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nWcReducer\n\npackage com.mywordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class WcReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n    private IntWritable result = new IntWritable();\n\n    /**\n     * 框架把单词分好组给我们, 我们将同一个单词的次数进行增加\n     *\n     * @param key     单词\n     * @param values  此时为1 数量\n     * @param context 任务本身\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {\n        //累加\n        int sum = 0;\n        for (IntWritable value : values) {\n            sum += value.get();\n        }\n        result.set(sum);\n        context.write(key, result);\n\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n\n打包项目 把maprduce1-1.0上传到集群中 打包前注意java版本 请用1.8打包\n\nhttps://zhuanlan.zhihu.com/p/348660719 还有pom.xml版本要设置\n\n\n\n * yarn jar mapreduce1-1.0-SNAPSHOT.jar 全类名引用路径 /fiddler.md /output2\n\nyarn jar mapreduce1-1.0-SNAPSHOT.jar com.mywordcount.WcDriver /fiddler.md /output2\n\n\n1\n\n\n\n# HaDoop序列化\n\n序列号就是把内存中的对象,转换为二进制序列 以便持久化\n\nJAVA序列化是一个重量级序列化框架 会附带额外的信息(校验信息 header 继承体系等)\n\n但Hadoop不需要这么多信息,所以Hadoop拥有自己的一套序列化体系(Writable)\n\n\n# 统计流量案例\n\n 1. FlowBean类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.io.Writable;\n    \n    import java.io.DataInput;\n    import java.io.DataOutput;\n    import java.io.IOException;\n    \n    public class FlowBean implements Writable {\n        private long upFlow;\n        private long downFlow;\n        private long sumFlow;\n    \n        @Override\n        public String toString() {\n            return "FlowBean{" +\n                    "upFlow=" + upFlow +\n                    ", downFlow=" + downFlow +\n                    ", sumFlow=" + sumFlow +\n                    \'}\';\n        }\n    \n        public void set(long upFlow, long downFlow) {\n            this.downFlow = downFlow;\n            this.upFlow = upFlow;\n            this.sumFlow = upFlow + downFlow;\n        }\n    \n    \n        public long getUpFlow() {\n            return upFlow;\n        }\n    \n        public void setUpFlow(long upFlow) {\n            this.upFlow = upFlow;\n        }\n    \n        public long getDownFlow() {\n            return downFlow;\n        }\n    \n        public void setDownFlow(long downFlow) {\n            this.downFlow = downFlow;\n        }\n    \n        public long getSumFlow() {\n            return sumFlow;\n        }\n    \n        public void setSumFlow(long sumFlow) {\n            this.sumFlow = sumFlow;\n        }\n    \n        /**\n         * 将对象数据写出到框架指定地方  序列化\n         *\n         * @param dataOutput 数据的容器\n         * @throws IOException\n         */\n        @Override\n        public void write(DataOutput dataOutput) throws IOException {\n            dataOutput.writeLong(upFlow);\n            dataOutput.writeLong(downFlow);\n            dataOutput.writeLong(sumFlow);\n        }\n    \n    \n        /**\n         * 从框架指定地方读取数据填充对象  反序列化\n         *\n         * @param dataInput\n         * @throws IOException\n         */\n        @Override\n        public void readFields(DataInput dataInput) throws IOException {\n            //读写顺序要一致\n            this.upFlow = dataInput.readLong();\n            this.downFlow = dataInput.readLong();\n            this.sumFlow = dataInput.readLong();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    68\n    69\n    70\n    71\n    72\n    73\n    74\n    75\n    76\n    77\n    78\n    79\n    80\n    81\n    \n\n 2. FlowMapper类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Mapper;\n    \n    import java.io.IOException;\n    \n    public class FlowMapper extends Mapper<LongWritable, Text, Text, FlowBean> {\n        private Text phone = new Text();\n        private FlowBean flow = new FlowBean();\n    \n    \n        @Override\n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, FlowBean>.Context context) throws IOException, InterruptedException {\n            //拿到一行数据\n            String line = value.toString();\n            //切分\n            String[] split = line.split("\\t");\n            //封装\n            phone.set(split[1]);\n            flow.set(\n                    Long.parseLong(split[split.length - 3]),//upFlow\n                    Long.parseLong(split[split.length - 2]) //downFlow\n            );\n            context.write(phone, flow);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    \n\n 3. FlowReducer类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Reducer;\n    \n    import java.io.IOException;\n    \n    public class FlowReducer extends Reducer<Text, FlowBean, Text, FlowBean> {\n    \n        private FlowBean flow = new FlowBean();\n    \n        @Override\n        protected void reduce(Text key, Iterable<FlowBean> values, Reducer<Text, FlowBean, Text, FlowBean>.Context context) throws IOException, InterruptedException {\n            //累加流量\n            long sumUpFlow = 0;\n            long sumDownFlow = 0;\n    \n            for (FlowBean value : values) {\n                sumUpFlow += value.getUpFlow();\n                sumDownFlow += value.getDownFlow();\n            }\n    \n            //封装为flow对象\n            flow.set(sumUpFlow, sumDownFlow);\n    \n            context.write(key, flow);\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    \n\n 4. FlowDriver类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    \n    public class FlowDriver {\n    \n        public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n            Job job = Job.getInstance(new Configuration());\n    \n            job.setJarByClass(FlowDriver.class);\n    \n            job.setMapperClass(FlowMapper.class);\n            job.setReducerClass(FlowReducer.class);\n    \n            job.setMapOutputKeyClass(Text.class);\n            job.setMapOutputValueClass(FlowBean.class);\n    \n            job.setOutputKeyClass(Text.class);\n            job.setOutputValueClass(FlowBean.class);\n    \n            FileInputFormat.setInputPaths(job, new Path(args[0]));\n            FileOutputFormat.setOutputPath(job, new Path(args[1]));\n    \n            boolean completion = job.waitForCompletion(true);\n            System.exit(completion ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    \n\n 5. 打包成jar文件上传到hadoop中\n    \n    #创建目录\n    hadoop fs -mkdir /input\n    hadoop fs -put /home/atguigu/phone_data.txt /input\n    yarn jar mapreduce1-1.0-SNAPSHOT.jar com.flow.FlowDriver /input /output3\n    \n    \n    1\n    2\n    3\n    4\n    ',normalizedContent:'# mapreduce\n\nmapreduce是一个分布式运算程序的编程框架 将用户编写的业务逻辑代码和自带默认组件整合一贯完整的分布式运算程序 并发运行在一个haoop集群上\n\n优点:它简单的实现一些接口，就可以完成一个分布式程序\n\n缺点:每个mapreduce作业的输出结果都会写入到磁盘，会造成大量的磁盘io，导致性能非常的低下。\n\n\n\n\n# hadoop原生mapreduce\n\n存储在/opt/module/hadoop-3.1.3/share/hadoop/mapreduce\n\n * yarn jar mapreduce路径 wordcount 输入hdfs文件路径 输出hdfs文件保存路径还是在hdfs上(必须是不存在的文件夹否则报错)\n\ncd /opt/module/hadoop-3.1.3\nyarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /fiddler.md /output\n\n\n1\n2\n\n\n打开hdfs中output中的part-r-xxxx 里面会统计出每个字词出现的次数\n\n\n# 常用数据序列化类型\n\n\n\n\n# wordcount编写\n\n使用idea中创建hadoop项目 创建maven项目\n\nwcdriver类\n\npackage com.mywordcount;\n\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.intwritable;\nimport org.apache.hadoop.io.text;\n\nimport org.apache.hadoop.mapreduce.job;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\nimport org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n\nimport java.io.ioexception;\n\npublic class wcdriver {\n    public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n        //1.获取job实例\n        job job = job.getinstance(new configuration());\n        //2.设置jar包\n        job.setjarbyclass(wcdriver.class);\n\n        //设置mapper和reducer\n        job.setmapoutputkeyclass(wcdriver.class);\n        job.setreducerclass(wcreducer.class);\n\n        //设置map和reduce的输出类型\n        job.setmapoutputkeyclass(text.class);\n        job.setmapoutputvalueclass(intwritable.class);\n\n        job.setoutputkeyclass(text.class);\n        job.setoutputvalueclass(intwritable.class);\n\n        //设置输入输出文件\n        fileinputformat.setinputpaths(job, new path(args[0]));\n        fileoutputformat.setoutputpath(job, new path(args[1]));\n\n        //提交job\n        boolean result = job.waitforcompletion(true);\n\n        system.exit(result ? 0 : 1);\n\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\nwcmapper\n\npackage com.mywordcount;\n\nimport org.apache.hadoop.io.intwritable;\nimport org.apache.hadoop.io.longwritable;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.mapper;\n\nimport java.io.ioexception;\n\npublic class wcmapper extends mapper<longwritable, text, text, intwritable> {\n    private intwritable one = new intwritable(1);\n    private text word = new text();\n\n    /**\n     * 框架将数据拆成一行一行输入进来 把数据变成(单词,1)的形式\n     *\n     * @param key     行号\n     * @param value   行内容\n     * @param context 任务本身\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    protected void map(longwritable key, text value, mapper<longwritable, text, text, intwritable>.context context) throws ioexception, interruptedexception {\n        //行数据\n        string line = value.tostring();\n\n        //拆分成若干个单词\n        string[] words = line.split(" ");\n\n        //将(单词,1)写回框架\n        for (string word : words) {\n            this.word.set(word);\n            context.write(this.word, this.one);\n        }\n\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nwcreducer\n\npackage com.mywordcount;\n\nimport org.apache.hadoop.io.intwritable;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.reducer;\n\nimport java.io.ioexception;\n\npublic class wcreducer extends reducer<text, intwritable, text, intwritable> {\n    private intwritable result = new intwritable();\n\n    /**\n     * 框架把单词分好组给我们, 我们将同一个单词的次数进行增加\n     *\n     * @param key     单词\n     * @param values  此时为1 数量\n     * @param context 任务本身\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    protected void reduce(text key, iterable<intwritable> values, reducer<text, intwritable, text, intwritable>.context context) throws ioexception, interruptedexception {\n        //累加\n        int sum = 0;\n        for (intwritable value : values) {\n            sum += value.get();\n        }\n        result.set(sum);\n        context.write(key, result);\n\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n\n打包项目 把maprduce1-1.0上传到集群中 打包前注意java版本 请用1.8打包\n\nhttps://zhuanlan.zhihu.com/p/348660719 还有pom.xml版本要设置\n\n\n\n * yarn jar mapreduce1-1.0-snapshot.jar 全类名引用路径 /fiddler.md /output2\n\nyarn jar mapreduce1-1.0-snapshot.jar com.mywordcount.wcdriver /fiddler.md /output2\n\n\n1\n\n\n\n# hadoop序列化\n\n序列号就是把内存中的对象,转换为二进制序列 以便持久化\n\njava序列化是一个重量级序列化框架 会附带额外的信息(校验信息 header 继承体系等)\n\n但hadoop不需要这么多信息,所以hadoop拥有自己的一套序列化体系(writable)\n\n\n# 统计流量案例\n\n 1. flowbean类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.io.writable;\n    \n    import java.io.datainput;\n    import java.io.dataoutput;\n    import java.io.ioexception;\n    \n    public class flowbean implements writable {\n        private long upflow;\n        private long downflow;\n        private long sumflow;\n    \n        @override\n        public string tostring() {\n            return "flowbean{" +\n                    "upflow=" + upflow +\n                    ", downflow=" + downflow +\n                    ", sumflow=" + sumflow +\n                    \'}\';\n        }\n    \n        public void set(long upflow, long downflow) {\n            this.downflow = downflow;\n            this.upflow = upflow;\n            this.sumflow = upflow + downflow;\n        }\n    \n    \n        public long getupflow() {\n            return upflow;\n        }\n    \n        public void setupflow(long upflow) {\n            this.upflow = upflow;\n        }\n    \n        public long getdownflow() {\n            return downflow;\n        }\n    \n        public void setdownflow(long downflow) {\n            this.downflow = downflow;\n        }\n    \n        public long getsumflow() {\n            return sumflow;\n        }\n    \n        public void setsumflow(long sumflow) {\n            this.sumflow = sumflow;\n        }\n    \n        /**\n         * 将对象数据写出到框架指定地方  序列化\n         *\n         * @param dataoutput 数据的容器\n         * @throws ioexception\n         */\n        @override\n        public void write(dataoutput dataoutput) throws ioexception {\n            dataoutput.writelong(upflow);\n            dataoutput.writelong(downflow);\n            dataoutput.writelong(sumflow);\n        }\n    \n    \n        /**\n         * 从框架指定地方读取数据填充对象  反序列化\n         *\n         * @param datainput\n         * @throws ioexception\n         */\n        @override\n        public void readfields(datainput datainput) throws ioexception {\n            //读写顺序要一致\n            this.upflow = datainput.readlong();\n            this.downflow = datainput.readlong();\n            this.sumflow = datainput.readlong();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    68\n    69\n    70\n    71\n    72\n    73\n    74\n    75\n    76\n    77\n    78\n    79\n    80\n    81\n    \n\n 2. flowmapper类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.mapper;\n    \n    import java.io.ioexception;\n    \n    public class flowmapper extends mapper<longwritable, text, text, flowbean> {\n        private text phone = new text();\n        private flowbean flow = new flowbean();\n    \n    \n        @override\n        protected void map(longwritable key, text value, mapper<longwritable, text, text, flowbean>.context context) throws ioexception, interruptedexception {\n            //拿到一行数据\n            string line = value.tostring();\n            //切分\n            string[] split = line.split("\\t");\n            //封装\n            phone.set(split[1]);\n            flow.set(\n                    long.parselong(split[split.length - 3]),//upflow\n                    long.parselong(split[split.length - 2]) //downflow\n            );\n            context.write(phone, flow);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    \n\n 3. flowreducer类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.reducer;\n    \n    import java.io.ioexception;\n    \n    public class flowreducer extends reducer<text, flowbean, text, flowbean> {\n    \n        private flowbean flow = new flowbean();\n    \n        @override\n        protected void reduce(text key, iterable<flowbean> values, reducer<text, flowbean, text, flowbean>.context context) throws ioexception, interruptedexception {\n            //累加流量\n            long sumupflow = 0;\n            long sumdownflow = 0;\n    \n            for (flowbean value : values) {\n                sumupflow += value.getupflow();\n                sumdownflow += value.getdownflow();\n            }\n    \n            //封装为flow对象\n            flow.set(sumupflow, sumdownflow);\n    \n            context.write(key, flow);\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    \n\n 4. flowdriver类\n    \n    package com.flow;\n    \n    import org.apache.hadoop.conf.configuration;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.job;\n    import org.apache.hadoop.mapreduce.lib.input.fileinputformat;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    \n    public class flowdriver {\n    \n        public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n            job job = job.getinstance(new configuration());\n    \n            job.setjarbyclass(flowdriver.class);\n    \n            job.setmapperclass(flowmapper.class);\n            job.setreducerclass(flowreducer.class);\n    \n            job.setmapoutputkeyclass(text.class);\n            job.setmapoutputvalueclass(flowbean.class);\n    \n            job.setoutputkeyclass(text.class);\n            job.setoutputvalueclass(flowbean.class);\n    \n            fileinputformat.setinputpaths(job, new path(args[0]));\n            fileoutputformat.setoutputpath(job, new path(args[1]));\n    \n            boolean completion = job.waitforcompletion(true);\n            system.exit(completion ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    \n\n 5. 打包成jar文件上传到hadoop中\n    \n    #创建目录\n    hadoop fs -mkdir /input\n    hadoop fs -put /home/atguigu/phone_data.txt /input\n    yarn jar mapreduce1-1.0-snapshot.jar com.flow.flowdriver /input /output3\n    \n    \n    1\n    2\n    3\n    4\n    ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Yarn",frontmatter:{title:"Yarn",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/89b57e/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/12.Yarn.html",relativePath:"大数据/01.Hadoop/12.Yarn.md",key:"v-7f22fd75",path:"/pages/89b57e/",headers:[{level:2,title:"windows向yarn提交源码任务",slug:"windows向yarn提交源码任务",normalizedTitle:"windows向yarn提交源码任务",charIndex:11},{level:2,title:"数据压缩",slug:"数据压缩",normalizedTitle:"数据压缩",charIndex:1984},{level:2,title:"Yarn架构",slug:"yarn架构",normalizedTitle:"yarn架构",charIndex:5652},{level:2,title:"资源调度器",slug:"资源调度器",normalizedTitle:"资源调度器",charIndex:6324},{level:2,title:"容器调度器多队列配置",slug:"容器调度器多队列配置",normalizedTitle:"容器调度器多队列配置",charIndex:6829},{level:3,title:"多队列提交任务",slug:"多队列提交任务",normalizedTitle:"多队列提交任务",charIndex:1e4},{level:2,title:"任务的推测执行",slug:"任务的推测执行",normalizedTitle:"任务的推测执行",charIndex:10116}],headersStr:"windows向yarn提交源码任务 数据压缩 Yarn架构 资源调度器 容器调度器多队列配置 多队列提交任务 任务的推测执行",content:'# Yarn\n\n\n# windows向yarn提交源码任务\n\n 1. 在Configuration配置文件添加yarn的配置属性\n 2. 用Maven 构建jar\n 3. 修改job加载驱动类为 打包后的jar包\n 4. \n\n驱动类编码\n\npackage com.mywordcount;\n\nimport java.io.IOException;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WcDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取配置信息以及封装任务\n        Configuration configuration = new Configuration();\n\n        configuration.set("fs.defaultFS", "hdfs://hadoop102:8020");\n        configuration.set("mapreduce.framework.name", "yarn");\n        configuration.set("mapreduce.app-submission.cross-platform", "true");\n        configuration.set("yarn.resourcemanager.hostname", "hadoop103");\n\n        Job job = Job.getInstance(configuration);\n\n        // 2 设置jar加载路径\n//        job.setJarByClass(WcDriver.class);\n        job.setJar("D:\\\\code\\\\mapreduce1\\\\target\\\\mapreduce1-1.0-SNAPSHOT.jar");\n        // 3 设置map和reduce类\n        job.setMapperClass(WcMapper.class);\n        job.setReducerClass(WcReducer.class);\n\n        // 4 设置map输出\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        // 5 设置最终输出kv类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 6 设置输入和输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        // 7 提交\n        boolean result = job.waitForCompletion(true);\n\n        System.exit(result ? 0 : 1);\n    }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 数据压缩\n\n采用压缩技术减少了磁盘IO 但同时增加了CPU运算负担 所以压缩特性运用得当能提高性能 但运用不当也可能降低性能\n\n压缩格式      HADOOP自带？   算法        文件扩展名      是否可切分   换成压缩格式后，原来的程序是否需要修改\nDEFLATE   是，直接使用      DEFLATE   .deflate   否       和文本处理一样，不需要修改\nGzip      是，直接使用      DEFLATE   .gz        否       和文本处理一样，不需要修改\nbzip2     是，直接使用      bzip2     .bz2       是       和文本处理一样，不需要修改\nLZO       否，需要安装      LZO       .lzo       是       需要建索引，还需要指定输入格式\nSnappy    否，需要安装      Snappy    .snappy    否       和文本处理一样，不需要修改\n\n常用Snappy压缩 因为较高 其次是LZO\n\n不同阶段开启压缩:\n\n 1. 如果输入阶段时为压缩包 则直接传递即可无需更改 Hadoop自动解压缩并处理\n\n 2. shuffle阶段 在驱动类设置开启压缩 并指定压缩格式\n    \n    //开启压缩模式\n    configuration.setBoolean("mapreduce.map.output.compress", true);\n    //压缩格式为\n    configuration.setClass("mapreduce.map.output.compress.codec", BZip2Codec.class,\n            CompressionCodec.class);\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 3. reduce阶段 输出压缩\n    \n    //reduce阶段压缩\n    configuration.setBoolean("mapreduce.output.fileoutputformat.compress", true);\n    //指定压缩格式\n    configuration.setClass("mapreduce.output.fileoutputformat.compress.codec", SnappyCodec.class,\n            CompressionCodec.class);\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n# Hadoop压缩和解压\n\npackage com.compression;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.compress.*;\nimport org.apache.hadoop.util.ReflectionUtils;\n\nimport java.io.IOException;\n\npublic class TestCompression {\n    public static void main(String[] args) throws IOException {\n        compress("d:/phone_data.txt", BZip2Codec.class);\n        decompress("d:/phone_data.txt.bz2");\n    }\n\n    //解压\n    private static void decompress(String file) throws IOException {\n        Configuration configuration = new Configuration();\n        //生成压缩格式工厂对象\n        CompressionCodecFactory codecFactory = new CompressionCodecFactory(configuration);\n\n        //根据压缩格式工厂获取压缩对象\n        CompressionCodec codec = codecFactory.getCodec(new Path(file));\n\n        //输入流\n        FileSystem fileSystem = FileSystem.get(configuration);\n\n        FSDataInputStream fsDataInputStream = fileSystem.open(new Path(file));\n        CompressionInputStream cis = codec.createInputStream(fsDataInputStream);\n\n        //输出流\n        String outputFile = file.substring(0, file.length() - codec.getDefaultExtension().length()); //获取文件名\n        FSDataOutputStream fos = fileSystem.create(new Path(outputFile));\n        IOUtils.copyBytes(cis, fos, 1024);//复制流 缓存为1024字节\n\n        //关闭流\n        IOUtils.closeStream(cis);\n        IOUtils.closeStream(fos);\n\n\n    }\n\n    //压缩\n    private static void compress(String file, Class<? extends CompressionCodec> codecClass) throws IOException {\n        Configuration configuration = new Configuration();\n        FileSystem fileSystem = FileSystem.get(configuration);\n\n        //生成压缩格式对象\n        CompressionCodec codec = ReflectionUtils.newInstance(codecClass, configuration);\n\n\n        //开输入流\n        FSDataInputStream fis = fileSystem.open(new Path(file));\n        //输出流\n        FSDataOutputStream fos = fileSystem.create(new Path(file + codec.getDefaultExtension()));\n\n        //用压缩格式包装输出流\n        CompressionOutputStream cos = codec.createOutputStream(fos);\n        IOUtils.copyBytes(fis, cos, 1024);\n        IOUtils.closeStream(fis);\n        IOUtils.closeStream(cos);\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n\n# Yarn架构\n\nYARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。\n\n\n\n\n\n 1.  MR程序提交到客户端所在的节点。\n 2.  YarnRunner向ResourceManager申请一个Application。\n 3.  RM将该应用程序的资源路径返回给YarnRunner。\n 4.  该程序将运行所需资源提交到HDFS上。\n 5.  程序资源提交完毕后，申请运行mrAppMaster。\n 6.  RM将用户的请求初始化成一个Task。\n 7.  其中一个NodeManager领取到Task任务。\n 8.  该NodeManager创建容器Container，并产生MRAppmaster。\n 9.  Container从HDFS上拷贝资源到本地。\n 10. MRAppmaster向RM 申请运行MapTask资源。\n 11. RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。\n 12. MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。\n 13. MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。\n 14. ReduceTask向MapTask获取相应分区的数据。\n 15. 程序运行完毕后，MR会向RM申请注销自己。\n\n\n# 资源调度器\n\n目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。\n\n通过yarn-default.xml配置\n\n<property>\n    <description>The class to use as the resource scheduler.</description>\n    <name>yarn.resourcemanager.scheduler.class</name>\n<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\n</property>\n\n\n1\n2\n3\n4\n5\n\n\n 1. 先进先出调度器（FIFO）\n    \n    \n\n 2. 容量调度器（Capacity Scheduler)\n    \n    \n\n 3. 公平调度器（Fair Scheduler）\n    \n    \n\n\n\n\n# 容器调度器多队列配置\n\n容量调度器默认为1个队列 default 通过修改capacity-scheduler.xml文件来配置多队列\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml  #建议用图像界面\n\n\n1\n\n\n 1. 修改yarn.scheduler.capacity.root.queues的value 添加新的队列\n    \n    \x3c!-- 默认为default队列 可以设置多条队列--\x3e\n      <property>\n        <name>yarn.scheduler.capacity.root.queues</name>\n        <value>default,hive</value>\n        <description>\n          The queues at the this level (root is the root queue).\n        </description>\n      </property>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n 2. 修改default队列占比为40\n    \n    \x3c!-- default队列默认占比为100  改为百分之40  剩下交给hive --\x3e\n      <property>\n        <name>yarn.scheduler.capacity.root.default.capacity</name>\n        <value>40</value>\n        <description>Default queue target capacity.</description>\n      </property>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 修改default队列允许的最大占比为60\n    \n    \x3c!--default队列最大占比默认为100  改为60 --\x3e\n      <property>\n        <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\n        <value>60</value>\n        <description>\n          The maximum capacity of the default queue. \n        </description>\n      </property>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n 4. 把default队列的配置属性复制一份 修改为新增队列名hive 并删除其中的description标签\n    \n    \x3c!--hive队列设置--\x3e\n     <property>\n        <name>yarn.scheduler.capacity.root.hive.capacity</name>\n        <value>60</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>\n        <value>1</value>\n      </property>\n      <property>\n        <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>\n        <value>80</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.state</name>\n        <value>RUNNING</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>\n        <value>*</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>\n        <value>*</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>\n        <value>*</value>\n      </property>\n    \n       <property>\n         <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime\n         </name>\n         <value>-1</value>\n       </property>\n    \n       <property>\n         <name>yarn.scheduler.capacity.root.hive.default-application-lifetime\n         </name>\n         <value>-1</value>\n       </property>\n     \x3c!--hive队列设置结束--\x3e\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    \n\n 5. 同步到其他集群中\n    \n    xsync /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml\n    \n    \n    1\n    \n\n 6. 重启hadoop yarn\n    \n    stop-yarn.sh  #103中\n    start-yarn.sh\n    \n    \n    1\n    2\n    \n\n\n\n\n# 多队列提交任务\n\n通过configuration设置 mapred.job.queue.name为指定队列名\n\nconfiguration.set("mapred.job.queue.name","hive");\n\n\n1\n\n\n\n# 任务的推测执行\n\n 1. 推测执行机制\n    \n    APPmstr 会监控任务的运行速度如果某个任务运行速度远慢于平均任务 则为拖后腿的任务启动一个备份任务同时运行 谁先运行完 则采取谁的结果\n\n 2. 执行推测任务的前提\n    \n    1. 每个task只能有一个备份任务\n    \n    2. 当前job已经完成的task必须不小于 5%\n    \n    3. 开启了推测执行设置 默认为打开的 在mapred-site.xml设置\n       \n       <property>\n               <name>mapreduce.map.speculative</name>\n               <value>true</value>\n       </property>\n       <property>\n               <name>mapreduce.reduce.speculative</name>\n               <value>true</value>\n       </property>\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       \n\n 3. 不能启用推测执行机制情况\n    \n    1. 任务间存在严重的负载倾斜\n    2. 特殊任务 如任务向数据库中写数据\n\n',normalizedContent:'# yarn\n\n\n# windows向yarn提交源码任务\n\n 1. 在configuration配置文件添加yarn的配置属性\n 2. 用maven 构建jar\n 3. 修改job加载驱动类为 打包后的jar包\n 4. \n\n驱动类编码\n\npackage com.mywordcount;\n\nimport java.io.ioexception;\n\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.intwritable;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.job;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\nimport org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n\npublic class wcdriver {\n\n    public static void main(string[] args) throws ioexception, classnotfoundexception, interruptedexception {\n\n        // 1 获取配置信息以及封装任务\n        configuration configuration = new configuration();\n\n        configuration.set("fs.defaultfs", "hdfs://hadoop102:8020");\n        configuration.set("mapreduce.framework.name", "yarn");\n        configuration.set("mapreduce.app-submission.cross-platform", "true");\n        configuration.set("yarn.resourcemanager.hostname", "hadoop103");\n\n        job job = job.getinstance(configuration);\n\n        // 2 设置jar加载路径\n//        job.setjarbyclass(wcdriver.class);\n        job.setjar("d:\\\\code\\\\mapreduce1\\\\target\\\\mapreduce1-1.0-snapshot.jar");\n        // 3 设置map和reduce类\n        job.setmapperclass(wcmapper.class);\n        job.setreducerclass(wcreducer.class);\n\n        // 4 设置map输出\n        job.setmapoutputkeyclass(text.class);\n        job.setmapoutputvalueclass(intwritable.class);\n\n        // 5 设置最终输出kv类型\n        job.setoutputkeyclass(text.class);\n        job.setoutputvalueclass(intwritable.class);\n\n        // 6 设置输入和输出路径\n        fileinputformat.setinputpaths(job, new path(args[0]));\n        fileoutputformat.setoutputpath(job, new path(args[1]));\n        // 7 提交\n        boolean result = job.waitforcompletion(true);\n\n        system.exit(result ? 0 : 1);\n    }\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n\n# 数据压缩\n\n采用压缩技术减少了磁盘io 但同时增加了cpu运算负担 所以压缩特性运用得当能提高性能 但运用不当也可能降低性能\n\n压缩格式      hadoop自带？   算法        文件扩展名      是否可切分   换成压缩格式后，原来的程序是否需要修改\ndeflate   是，直接使用      deflate   .deflate   否       和文本处理一样，不需要修改\ngzip      是，直接使用      deflate   .gz        否       和文本处理一样，不需要修改\nbzip2     是，直接使用      bzip2     .bz2       是       和文本处理一样，不需要修改\nlzo       否，需要安装      lzo       .lzo       是       需要建索引，还需要指定输入格式\nsnappy    否，需要安装      snappy    .snappy    否       和文本处理一样，不需要修改\n\n常用snappy压缩 因为较高 其次是lzo\n\n不同阶段开启压缩:\n\n 1. 如果输入阶段时为压缩包 则直接传递即可无需更改 hadoop自动解压缩并处理\n\n 2. shuffle阶段 在驱动类设置开启压缩 并指定压缩格式\n    \n    //开启压缩模式\n    configuration.setboolean("mapreduce.map.output.compress", true);\n    //压缩格式为\n    configuration.setclass("mapreduce.map.output.compress.codec", bzip2codec.class,\n            compressioncodec.class);\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 3. reduce阶段 输出压缩\n    \n    //reduce阶段压缩\n    configuration.setboolean("mapreduce.output.fileoutputformat.compress", true);\n    //指定压缩格式\n    configuration.setclass("mapreduce.output.fileoutputformat.compress.codec", snappycodec.class,\n            compressioncodec.class);\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n# hadoop压缩和解压\n\npackage com.compression;\n\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.fsdatainputstream;\nimport org.apache.hadoop.fs.fsdataoutputstream;\nimport org.apache.hadoop.fs.filesystem;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.ioutils;\nimport org.apache.hadoop.io.compress.*;\nimport org.apache.hadoop.util.reflectionutils;\n\nimport java.io.ioexception;\n\npublic class testcompression {\n    public static void main(string[] args) throws ioexception {\n        compress("d:/phone_data.txt", bzip2codec.class);\n        decompress("d:/phone_data.txt.bz2");\n    }\n\n    //解压\n    private static void decompress(string file) throws ioexception {\n        configuration configuration = new configuration();\n        //生成压缩格式工厂对象\n        compressioncodecfactory codecfactory = new compressioncodecfactory(configuration);\n\n        //根据压缩格式工厂获取压缩对象\n        compressioncodec codec = codecfactory.getcodec(new path(file));\n\n        //输入流\n        filesystem filesystem = filesystem.get(configuration);\n\n        fsdatainputstream fsdatainputstream = filesystem.open(new path(file));\n        compressioninputstream cis = codec.createinputstream(fsdatainputstream);\n\n        //输出流\n        string outputfile = file.substring(0, file.length() - codec.getdefaultextension().length()); //获取文件名\n        fsdataoutputstream fos = filesystem.create(new path(outputfile));\n        ioutils.copybytes(cis, fos, 1024);//复制流 缓存为1024字节\n\n        //关闭流\n        ioutils.closestream(cis);\n        ioutils.closestream(fos);\n\n\n    }\n\n    //压缩\n    private static void compress(string file, class<? extends compressioncodec> codecclass) throws ioexception {\n        configuration configuration = new configuration();\n        filesystem filesystem = filesystem.get(configuration);\n\n        //生成压缩格式对象\n        compressioncodec codec = reflectionutils.newinstance(codecclass, configuration);\n\n\n        //开输入流\n        fsdatainputstream fis = filesystem.open(new path(file));\n        //输出流\n        fsdataoutputstream fos = filesystem.create(new path(file + codec.getdefaultextension()));\n\n        //用压缩格式包装输出流\n        compressionoutputstream cos = codec.createoutputstream(fos);\n        ioutils.copybytes(fis, cos, 1024);\n        ioutils.closestream(fis);\n        ioutils.closestream(cos);\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n\n# yarn架构\n\nyarn主要由resourcemanager、nodemanager、applicationmaster和container等组件构成。\n\n\n\n\n\n 1.  mr程序提交到客户端所在的节点。\n 2.  yarnrunner向resourcemanager申请一个application。\n 3.  rm将该应用程序的资源路径返回给yarnrunner。\n 4.  该程序将运行所需资源提交到hdfs上。\n 5.  程序资源提交完毕后，申请运行mrappmaster。\n 6.  rm将用户的请求初始化成一个task。\n 7.  其中一个nodemanager领取到task任务。\n 8.  该nodemanager创建容器container，并产生mrappmaster。\n 9.  container从hdfs上拷贝资源到本地。\n 10. mrappmaster向rm 申请运行maptask资源。\n 11. rm将运行maptask任务分配给另外两个nodemanager，另两个nodemanager分别领取任务并创建容器。\n 12. mr向两个接收到任务的nodemanager发送程序启动脚本，这两个nodemanager分别启动maptask，maptask对数据分区排序。\n 13. mrappmaster等待所有maptask运行完毕后，向rm申请容器，运行reducetask。\n 14. reducetask向maptask获取相应分区的数据。\n 15. 程序运行完毕后，mr会向rm申请注销自己。\n\n\n# 资源调度器\n\n目前，hadoop作业调度器主要有三种：fifo、capacity scheduler和fair scheduler。hadoop3.1.3默认的资源调度器是capacity scheduler。\n\n通过yarn-default.xml配置\n\n<property>\n    <description>the class to use as the resource scheduler.</description>\n    <name>yarn.resourcemanager.scheduler.class</name>\n<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.capacityscheduler</value>\n</property>\n\n\n1\n2\n3\n4\n5\n\n\n 1. 先进先出调度器（fifo）\n    \n    \n\n 2. 容量调度器（capacity scheduler)\n    \n    \n\n 3. 公平调度器（fair scheduler）\n    \n    \n\n\n\n\n# 容器调度器多队列配置\n\n容量调度器默认为1个队列 default 通过修改capacity-scheduler.xml文件来配置多队列\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml  #建议用图像界面\n\n\n1\n\n\n 1. 修改yarn.scheduler.capacity.root.queues的value 添加新的队列\n    \n    \x3c!-- 默认为default队列 可以设置多条队列--\x3e\n      <property>\n        <name>yarn.scheduler.capacity.root.queues</name>\n        <value>default,hive</value>\n        <description>\n          the queues at the this level (root is the root queue).\n        </description>\n      </property>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n 2. 修改default队列占比为40\n    \n    \x3c!-- default队列默认占比为100  改为百分之40  剩下交给hive --\x3e\n      <property>\n        <name>yarn.scheduler.capacity.root.default.capacity</name>\n        <value>40</value>\n        <description>default queue target capacity.</description>\n      </property>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 修改default队列允许的最大占比为60\n    \n    \x3c!--default队列最大占比默认为100  改为60 --\x3e\n      <property>\n        <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\n        <value>60</value>\n        <description>\n          the maximum capacity of the default queue. \n        </description>\n      </property>\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n\n 4. 把default队列的配置属性复制一份 修改为新增队列名hive 并删除其中的description标签\n    \n    \x3c!--hive队列设置--\x3e\n     <property>\n        <name>yarn.scheduler.capacity.root.hive.capacity</name>\n        <value>60</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>\n        <value>1</value>\n      </property>\n      <property>\n        <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>\n        <value>80</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.state</name>\n        <value>running</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>\n        <value>*</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>\n        <value>*</value>\n      </property>\n    \n      <property>\n        <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>\n        <value>*</value>\n      </property>\n    \n       <property>\n         <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime\n         </name>\n         <value>-1</value>\n       </property>\n    \n       <property>\n         <name>yarn.scheduler.capacity.root.hive.default-application-lifetime\n         </name>\n         <value>-1</value>\n       </property>\n     \x3c!--hive队列设置结束--\x3e\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    \n\n 5. 同步到其他集群中\n    \n    xsync /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml\n    \n    \n    1\n    \n\n 6. 重启hadoop yarn\n    \n    stop-yarn.sh  #103中\n    start-yarn.sh\n    \n    \n    1\n    2\n    \n\n\n\n\n# 多队列提交任务\n\n通过configuration设置 mapred.job.queue.name为指定队列名\n\nconfiguration.set("mapred.job.queue.name","hive");\n\n\n1\n\n\n\n# 任务的推测执行\n\n 1. 推测执行机制\n    \n    appmstr 会监控任务的运行速度如果某个任务运行速度远慢于平均任务 则为拖后腿的任务启动一个备份任务同时运行 谁先运行完 则采取谁的结果\n\n 2. 执行推测任务的前提\n    \n    1. 每个task只能有一个备份任务\n    \n    2. 当前job已经完成的task必须不小于 5%\n    \n    3. 开启了推测执行设置 默认为打开的 在mapred-site.xml设置\n       \n       <property>\n               <name>mapreduce.map.speculative</name>\n               <value>true</value>\n       </property>\n       <property>\n               <name>mapreduce.reduce.speculative</name>\n               <value>true</value>\n       </property>\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       \n\n 3. 不能启用推测执行机制情况\n    \n    1. 任务间存在严重的负载倾斜\n    2. 特殊任务 如任务向数据库中写数据\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hadoop企业优化",frontmatter:{title:"Hadoop企业优化",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/ce529c/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/13.Hadoop%E4%BC%81%E4%B8%9A%E4%BC%98%E5%8C%96.html",relativePath:"大数据/01.Hadoop/13.Hadoop企业优化.md",key:"v-2e69f406",path:"/pages/ce529c/",headers:[{level:2,title:"MapReduce优化方法",slug:"mapreduce优化方法",normalizedTitle:"mapreduce优化方法",charIndex:19},{level:2,title:"常用的调优参数",slug:"常用的调优参数",normalizedTitle:"常用的调优参数",charIndex:89},{level:2,title:"HDFS 小文件优化方法",slug:"hdfs-小文件优化方法",normalizedTitle:"hdfs 小文件优化方法",charIndex:2283}],headersStr:"MapReduce优化方法 常用的调优参数 HDFS 小文件优化方法",content:"# Hadoop企业优化\n\n\n\n\n# MapReduce优化方法\n\nMapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 常用的调优参数\n\n1）资源相关参数\n\n（1）以下参数是在用户自己的MR应用程序中配置就可以生效（mapred-default.xml）\n\n配置参数                                            参数说明\nmapreduce.map.memory.mb                         一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。\nmapreduce.reduce.memory.mb                      一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。\nmapreduce.map.cpu.vcores                        每个MapTask可使用的最多cpu core数目，默认值: 1\nmapreduce.reduce.cpu.vcores                     每个ReduceTask可使用的最多cpu core数目，默认值: 1\nmapreduce.reduce.shuffle.parallelcopies         每个Reduce去Map中取数据的并行数。默认值是5\nmapreduce.reduce.shuffle.merge.percent          Buffer中的数据达到多少比例开始写入磁盘。默认值0.66\nmapreduce.reduce.shuffle.input.buffer.percent   Buffer大小占Reduce可用内存的比例。默认值0.7\nmapreduce.reduce.input.buffer.percent           指定多少比例的内存用来存放Buffer中的数据，默认值是0.0\n\n（2）应该在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）\n\n配置参数                                       参数说明\nyarn.scheduler.minimum-allocation-mb       给应用程序Container分配的最小内存，默认值：1024\nyarn.scheduler.maximum-allocation-mb       给应用程序Container分配的最大内存，默认值：8192\nyarn.scheduler.minimum-allocation-vcores   每个Container申请的最小CPU核数，默认值：1\nyarn.scheduler.maximum-allocation-vcores   每个Container申请的最大CPU核数，默认值：32\nyarn.nodemanager.resource.memory-mb        给Containers分配的最大物理内存，默认值：8192\n\n（3）Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml）\n\n配置参数                               参数说明\nmapreduce.task.io.sort.mb          Shuffle的环形缓冲区大小，默认100m\nmapreduce.map.sort.spill.percent   环形缓冲区溢出的阈值，默认80%\n\n2）容错相关参数（MapReduce 性能优化）\n\n配置参数                           参数说明\nmapreduce.map.maxattempts      每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。\nmapreduce.reduce.maxattempts   每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。\nmapreduce.task.timeout         Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0\n                               Timed out after 300 secsContainer killed by the\n                               ApplicationMaster.”。\n\n\n# HDFS 小文件优化方法\n\n小文件的优化无非以下几种方式：\n\n * 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。\n * 在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。\n * 在MapReduce处理时，可采用CombineTextInputFormat提高效率。\n\n\n\n",normalizedContent:"# hadoop企业优化\n\n\n\n\n# mapreduce优化方法\n\nmapreduce优化方法主要从六个方面考虑：数据输入、map阶段、reduce阶段、io传输、数据倾斜问题和常用的调优参数。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 常用的调优参数\n\n1）资源相关参数\n\n（1）以下参数是在用户自己的mr应用程序中配置就可以生效（mapred-default.xml）\n\n配置参数                                            参数说明\nmapreduce.map.memory.mb                         一个maptask可使用的资源上限（单位:mb），默认为1024。如果maptask实际使用的资源量超过该值，则会被强制杀死。\nmapreduce.reduce.memory.mb                      一个reducetask可使用的资源上限（单位:mb），默认为1024。如果reducetask实际使用的资源量超过该值，则会被强制杀死。\nmapreduce.map.cpu.vcores                        每个maptask可使用的最多cpu core数目，默认值: 1\nmapreduce.reduce.cpu.vcores                     每个reducetask可使用的最多cpu core数目，默认值: 1\nmapreduce.reduce.shuffle.parallelcopies         每个reduce去map中取数据的并行数。默认值是5\nmapreduce.reduce.shuffle.merge.percent          buffer中的数据达到多少比例开始写入磁盘。默认值0.66\nmapreduce.reduce.shuffle.input.buffer.percent   buffer大小占reduce可用内存的比例。默认值0.7\nmapreduce.reduce.input.buffer.percent           指定多少比例的内存用来存放buffer中的数据，默认值是0.0\n\n（2）应该在yarn启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）\n\n配置参数                                       参数说明\nyarn.scheduler.minimum-allocation-mb       给应用程序container分配的最小内存，默认值：1024\nyarn.scheduler.maximum-allocation-mb       给应用程序container分配的最大内存，默认值：8192\nyarn.scheduler.minimum-allocation-vcores   每个container申请的最小cpu核数，默认值：1\nyarn.scheduler.maximum-allocation-vcores   每个container申请的最大cpu核数，默认值：32\nyarn.nodemanager.resource.memory-mb        给containers分配的最大物理内存，默认值：8192\n\n（3）shuffle性能优化的关键参数，应在yarn启动之前就配置好（mapred-default.xml）\n\n配置参数                               参数说明\nmapreduce.task.io.sort.mb          shuffle的环形缓冲区大小，默认100m\nmapreduce.map.sort.spill.percent   环形缓冲区溢出的阈值，默认80%\n\n2）容错相关参数（mapreduce 性能优化）\n\n配置参数                           参数说明\nmapreduce.map.maxattempts      每个map task最大重试次数，一旦重试参数超过该值，则认为map task运行失败，默认值：4。\nmapreduce.reduce.maxattempts   每个reduce task最大重试次数，一旦重试参数超过该值，则认为map task运行失败，默认值：4。\nmapreduce.task.timeout         task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“attemptid:attempt_14267829456721_123456_m_000224_0\n                               timed out after 300 secscontainer killed by the\n                               applicationmaster.”。\n\n\n# hdfs 小文件优化方法\n\n小文件的优化无非以下几种方式：\n\n * 在数据采集的时候，就将小文件或小批数据合成大文件再上传hdfs。\n * 在业务处理之前，在hdfs上使用mapreduce程序对小文件进行合并。\n * 在mapreduce处理时，可采用combinetextinputformat提高效率。\n\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"日志",frontmatter:{title:"日志",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/1cc354/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/15.%E6%97%A5%E5%BF%97.html",relativePath:"大数据/01.Hadoop/15.日志.md",key:"v-5c5d9838",path:"/pages/1cc354/",headersStr:null,content:"# 日志\n\n 1. 找到日志文件\n    1. 查看 框架目录 etc/hadoop 下的log4j文件 查询日志文件存放配置\n    2. 框架解压目录 或者 /tmp\n    3. 框架是rpm包安装 去/var/log目录下找\n 2. 看日志\n    1. 不要用vim看日用 用tail命令 tail -n 500 文件名\n    2. 下载到windows中看\n 3. 看什么\n    1. 一般日志一行内容 由 发生时间 日志级别 日志输出主体(谁) 做什么操作\n    2. 如果找错误应该 找日志级别为 ERROR 或 FATAL的行 或者在上下文附近 找到问题原因",normalizedContent:"# 日志\n\n 1. 找到日志文件\n    1. 查看 框架目录 etc/hadoop 下的log4j文件 查询日志文件存放配置\n    2. 框架解压目录 或者 /tmp\n    3. 框架是rpm包安装 去/var/log目录下找\n 2. 看日志\n    1. 不要用vim看日用 用tail命令 tail -n 500 文件名\n    2. 下载到windows中看\n 3. 看什么\n    1. 一般日志一行内容 由 发生时间 日志级别 日志输出主体(谁) 做什么操作\n    2. 如果找错误应该 找日志级别为 error 或 fatal的行 或者在上下文附近 找到问题原因",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hadoop HA高可用",frontmatter:{title:"Hadoop HA高可用",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/0dc6f9/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/16.Hadoop%20HA%E9%AB%98%E5%8F%AF%E7%94%A8.html",relativePath:"大数据/01.Hadoop/16.Hadoop HA高可用.md",key:"v-f87607aa",path:"/pages/0dc6f9/",headers:[{level:2,title:"配置HDFS-HA集群",slug:"配置hdfs-ha集群",normalizedTitle:"配置hdfs-ha集群",charIndex:401},{level:2,title:"HA的选举机制",slug:"ha的选举机制",normalizedTitle:"ha的选举机制",charIndex:5260},{level:2,title:"YARN-HA",slug:"yarn-ha",normalizedTitle:"yarn-ha",charIndex:5870}],headersStr:"配置HDFS-HA集群 HA的选举机制 YARN-HA",content:"# Hadoop HA高可用\n\n（1）所谓HA（High Availablity），即高可用（7*24小时不中断服务）。\n\n（2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。\n\n（3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。\n\n（4）NameNode主要在以下两个方面影响HDFS集群\n\n * NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启\n\n * NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用\n\nHDFS HA功能通过配置Active/Standby两个NameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。\n\n\n\n\n# 配置HDFS-HA集群\n\nHADOOP102     HADOOP103         HADOOP104\nNameNode      NameNode          NameNode\nZKFC          ZKFC              ZKFC\nJournalNode   JournalNode       JournalNode\nDataNode      DataNode          DataNode\nZK            ZK                ZK\n              ResourceManager   \nNodeManager   NodeManager       NodeManager\n\n要先配置好zookeeper 并启动\n\nsudo mkdir /opt/ha\nsudo chown atguigu:atguigu /opt/ha\n#将/opt/module/下的 hadoop-3.1.3拷贝到/opt/ha目录下\ncp -r /opt/module/hadoop-3.1.3 /opt/ha/\n\n#删除复制后hadoop文件夹中的 data文件夹和logs文件夹\ncd /opt/ha/hadoop-3.1.3/\nrm -rf data/ logs mapreduce1-1.0-SNAPSHOT.jar\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n配置core-site\n\nvim etc/hadoop/core-site.xml \n#修改以下两个key的值  追加一个zookeeper集群地址\n\n\n1\n2\n\n\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://mycluster</value>\n  </property>\n   \x3c!-- hadoop数据存放路径 注意为data.dir 而不tmp.dir --\x3e \n  <property>\n    <name>hadoop.data.dir</name>\n    <value>/opt/ha/hadoop-3.1.3/data</value>\n  </property>\n    \n    \x3c!--追加 zookeeper  quorum 地址  配置ZooKeeper服务集群，用于活跃NameNode节点的选举--\x3e\n    <property>\n\t<name>ha.zookeeper.quorum</name>\n\t<value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>\n</property>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n配置hdfs-site.xml\n\nvim etc/hadoop/core-site.xml \n#覆盖原有配置\n\n\n1\n2\n\n\n<configuration>\n    \x3c!--配置NameNode的数据存储目录，需要与上文创建的目录相对应--\x3e\n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>file://${hadoop.data.dir}/name</value>\n  </property>\n   \n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>file://${hadoop.data.dir}/data</value>\n  </property>\n\x3c!-- nameservices名称 自定义的HDFS服务名，在高可用集群中，无法配置单一HDFS服务器入口，所以需要指定一个逻辑上的服务名，当访问服务名时，会自动选择NameNode节点进行访问 --\x3e\n  <property>\n    <name>dfs.nameservices</name>\n    <value>mycluster</value>\n  </property>\n\x3c!-- 三个namenode 定义HDFS服务名所指向的NameNode主机名称 --\x3e\n  <property>\n    <name>dfs.ha.namenodes.mycluster</name>\n    <value>nn1,nn2, nn3</value>\n  </property>\n\x3c!-- namenode地址  设置NameNode的完整监听地址 --\x3e\n  <property>\n    <name>dfs.namenode.rpc-address.mycluster.nn1</name>\n    <value>hadoop102:8020</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.mycluster.nn2</name>\n    <value>hadoop103:8020</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.mycluster.nn3</name>\n    <value>hadoop104:8020</value>\n  </property>\n        \x3c!--设置NameNode的HTTP访问地址--\x3e\n  <property>\n    <name>dfs.namenode.http-address.mycluster.nn1</name>\n    <value>hadoop102:9870</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.mycluster.nn2</name>\n    <value>hadoop103:9870</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.mycluster.nn3</name>\n    <value>hadoop104:9870</value>\n  </property>\n\x3c!-- 配置qjn集群  设置主从NameNode元数据同步地址，官方推荐将nameservice作为最后的journal ID --\x3e\n  <property>\n    <name>dfs.namenode.shared.edits.dir</name>\n    <value>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster</value>\n  </property>\n\x3c!-- qjn数据存放目录 配置JournalNode的数据存储目录，需要与上文创建的目录相对应 --\x3e\n    <property>\n    <name>dfs.journalnode.edits.dir</name>\n    <value>${hadoop.data.dir}/jn</value>\n  </property>\n\x3c!-- 代理类  设置HDFS客户端用来连接集群中活动状态NameNode节点的Java类 --\x3e\n  <property>\n    <name>dfs.client.failover.proxy.provider.mycluster</name>\n    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n  </property>\n\x3c!-- 隔离方式 为sshfence ssh远程代理  启动fence过程，确保集群高可用性 --\x3e\n  <property>\n    <name>dfs.ha.fencing.methods</name>\n    <value>sshfence</value>\n  </property>\n\x3c!-- ssh私有路径 设置SSH登录的私钥文件地址  --\x3e\n  <property>\n    <name>dfs.ha.fencing.ssh.private-key-files</name>\n    <value>/home/atguigu/.ssh/id_rsa</value>\n  </property>\n    \x3c!-- 开启自动切换 设置自动切换活跃节点，保证集群高可用性 --\x3e\n<property>\n\t<name>dfs.ha.automatic-failover.enabled</name>\n\t<value>true</value>\n</property>\n\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n\n\n同步\n\nsudo xsync /opt/ha/\n\n\n1\n\n\n临时修改环境变量 因为hdfs.sh 默认根据环境变量来启动的 无法正常格式化ha中的namenode\n\nsudo vim /etc/profile.d/my_env.sh\n#修改hadoop路径 临时为ha下的hadoop\nexport HADOOP_HOME=/opt/ha/hadoop-3.1.3\n\nsource /etc/profile.d/my_env.sh\nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n2\n3\n4\n5\n6\n\n\n启动JQM集群\n\n#三台都会启动 jps检查一下三台有没有journalnode\n/opt/ha/hadoop-3.1.3/bin/hdfs --workers --daemon start journalnode\n#如果不行删除data 和logs文件夹 重新启动  再不行看日志 JQM的数据存储路径必须为绝对路径 \n\n\n1\n2\n3\n\n\n格式化nn\n\n#在102节点中格式化nn\nhdfs namenode -format\nhdfs --daemon start namenode #启动nn\n\n#其他节点通过同步nn1的元数据信息 \n#切记要在atguigu下 不要root用户下  如果无法启动nn删除data 和logs文件夹 同步\n#在103和104中使用\nhdfs namenode -bootstrapStandby #同步\nhdfs --daemon start namenode  #启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n启动zk并初始化HA在Zookeeper中状态\n\n#先启动三台的zookeeper\nzkServer.sh start\n\n#格式一台即可以 其他集群会同步\nhdfs zkfc -formatZK\n\n\n1\n2\n3\n4\n5\n\n\n启动dfs\n\n#102\nstart-dfs.sh\n\n\n1\n2\n\n\n\n# HA的选举机制\n\n通过查看3台集群的web页面 发现只有一台活动(active状态) 其他都是standby状态, 如果我把active的nn杀掉,那么ha会自动故障转移把某台standby状态的变为active状态\n\n 1. 通过zookeeper选取的\n 2. 当我们启动nn时 hadoop会自动向zookeeper上的hadoop-ha/mycluster/ActiveStandbyElectorLock 注册一个临时节点 此节点的值就是active节点名称 先在此临时节点注册的集群则成为active\n 3. 如果active宕机 则节点会远程访问active 8020端口 并测试杀掉服务 无响应/杀掉 在zookeeper上的临时节点上注册 (此过程所有节点都会执行 谁先在临时节点注册上就是谁做active)\n 4. hadoop-ha/mycluster/ActiveBreadCrumb 此zookeeper节点记录上一个active 名称 如果重新上线则为standby状态\n 5. 如果active断网 则会尝试不断的隔离active节点 但无法连接到active 并且在隔离期间standby都无法转换为active 因为我们配置的隔离方法 是sshfence(ssh连接并尝试杀死进程) 我们可以通过逗号配置多value值\n    1. shell(脚本文件路径) 自定义脚本方法\n\n\n# YARN-HA\n\nHADOOP102         HADOOP103         HADOOP104\nNameNode          NameNode          \nJournalNode       JournalNode       JournalNode\nDataNode          DataNode          DataNode\nZK                ZK                ZK\nResourceManager   ResourceManager   \nNodeManager       NodeManager       NodeManager\n\n在配置完HDFS-HA的基础上 修改配置\n\nvim /opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n#追加\n\n\n1\n2\n\n\n\x3c!--启用resourcemanager ha 开启resourcemanager 的高可用性功能--\x3e\n    <property>\n        <name> yarn.resourcemanager.ha.enabled</name>\n        <value>true</value>\n    </property>\n \n    \x3c!--声明两台resourcemanager的地址 标识集群中的resourcemanager，如果设置该选项，需要确保所有的resourcemanager节点在配置中都有自己的逻辑id--\x3e\n    <property>\n        <name>yarn.resourcemanager.cluster-id</name>\n        <value>cluster-yarn1</value>\n    </property>\n\t\x3c!--设置resourcemanager节点的逻辑id--\x3e\n    <property>\n        <name>yarn.resourcemanager.ha.rm-ids</name>\n        <value>rm1,rm2</value>\n    </property>\n\t\x3c!--为每个逻辑id绑定实际的主机名称--\x3e\n    <property>\n        <name>yarn.resourcemanager.hostname.rm1</name>\n        <value>hadoop102</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.hostname.rm2</name>\n        <value>hadoop103</value>\n    </property>\n \n    \x3c!--指定zookeeper集群的地址--\x3e \n    <property>\n        <name>yarn.resourcemanager.zk-address</name>\n        <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>\n    </property>\n\n    \x3c!--启用自动恢复--\x3e \n    <property>\n        <name>yarn.resourcemanager.recovery.enabled</name>\n        <value>true</value>\n    </property>\n \n    \x3c!--指定resourcemanager的状态信息存储在zookeeper集群--\x3e \n    <property>\n        <name>yarn.resourcemanager.store.class</name>     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\nxsync /opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n启动yarn\n\n#103中启动\nstart-yarn.sh\n\n\n1\n2\n\n\n查询当前yarn是standby状态 还是active\n\nyarn rmadmin -getServiceState rm1\n\n\n1\n\n\nyarn在zookeeper下的节点名为yarn-leader-election 两个节点互相独立",normalizedContent:"# hadoop ha高可用\n\n（1）所谓ha（high availablity），即高可用（7*24小时不中断服务）。\n\n（2）实现高可用最关键的策略是消除单点故障。ha严格来说应该分成各个组件的ha机制：hdfs的ha和yarn的ha。\n\n（3）hadoop2.0之前，在hdfs集群中namenode存在单点故障（spof）。\n\n（4）namenode主要在以下两个方面影响hdfs集群\n\n * namenode机器发生意外，如宕机，集群将无法使用，直到管理员重启\n\n * namenode机器需要升级，包括软件、硬件升级，此时集群也将无法使用\n\nhdfs ha功能通过配置active/standby两个namenodes实现在集群中对namenode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将namenode很快的切换到另外一台机器。\n\n\n\n\n# 配置hdfs-ha集群\n\nhadoop102     hadoop103         hadoop104\nnamenode      namenode          namenode\nzkfc          zkfc              zkfc\njournalnode   journalnode       journalnode\ndatanode      datanode          datanode\nzk            zk                zk\n              resourcemanager   \nnodemanager   nodemanager       nodemanager\n\n要先配置好zookeeper 并启动\n\nsudo mkdir /opt/ha\nsudo chown atguigu:atguigu /opt/ha\n#将/opt/module/下的 hadoop-3.1.3拷贝到/opt/ha目录下\ncp -r /opt/module/hadoop-3.1.3 /opt/ha/\n\n#删除复制后hadoop文件夹中的 data文件夹和logs文件夹\ncd /opt/ha/hadoop-3.1.3/\nrm -rf data/ logs mapreduce1-1.0-snapshot.jar\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n配置core-site\n\nvim etc/hadoop/core-site.xml \n#修改以下两个key的值  追加一个zookeeper集群地址\n\n\n1\n2\n\n\n<configuration>\n  <property>\n    <name>fs.defaultfs</name>\n    <value>hdfs://mycluster</value>\n  </property>\n   \x3c!-- hadoop数据存放路径 注意为data.dir 而不tmp.dir --\x3e \n  <property>\n    <name>hadoop.data.dir</name>\n    <value>/opt/ha/hadoop-3.1.3/data</value>\n  </property>\n    \n    \x3c!--追加 zookeeper  quorum 地址  配置zookeeper服务集群，用于活跃namenode节点的选举--\x3e\n    <property>\n\t<name>ha.zookeeper.quorum</name>\n\t<value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>\n</property>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n配置hdfs-site.xml\n\nvim etc/hadoop/core-site.xml \n#覆盖原有配置\n\n\n1\n2\n\n\n<configuration>\n    \x3c!--配置namenode的数据存储目录，需要与上文创建的目录相对应--\x3e\n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>file://${hadoop.data.dir}/name</value>\n  </property>\n   \n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>file://${hadoop.data.dir}/data</value>\n  </property>\n\x3c!-- nameservices名称 自定义的hdfs服务名，在高可用集群中，无法配置单一hdfs服务器入口，所以需要指定一个逻辑上的服务名，当访问服务名时，会自动选择namenode节点进行访问 --\x3e\n  <property>\n    <name>dfs.nameservices</name>\n    <value>mycluster</value>\n  </property>\n\x3c!-- 三个namenode 定义hdfs服务名所指向的namenode主机名称 --\x3e\n  <property>\n    <name>dfs.ha.namenodes.mycluster</name>\n    <value>nn1,nn2, nn3</value>\n  </property>\n\x3c!-- namenode地址  设置namenode的完整监听地址 --\x3e\n  <property>\n    <name>dfs.namenode.rpc-address.mycluster.nn1</name>\n    <value>hadoop102:8020</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.mycluster.nn2</name>\n    <value>hadoop103:8020</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.mycluster.nn3</name>\n    <value>hadoop104:8020</value>\n  </property>\n        \x3c!--设置namenode的http访问地址--\x3e\n  <property>\n    <name>dfs.namenode.http-address.mycluster.nn1</name>\n    <value>hadoop102:9870</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.mycluster.nn2</name>\n    <value>hadoop103:9870</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.mycluster.nn3</name>\n    <value>hadoop104:9870</value>\n  </property>\n\x3c!-- 配置qjn集群  设置主从namenode元数据同步地址，官方推荐将nameservice作为最后的journal id --\x3e\n  <property>\n    <name>dfs.namenode.shared.edits.dir</name>\n    <value>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster</value>\n  </property>\n\x3c!-- qjn数据存放目录 配置journalnode的数据存储目录，需要与上文创建的目录相对应 --\x3e\n    <property>\n    <name>dfs.journalnode.edits.dir</name>\n    <value>${hadoop.data.dir}/jn</value>\n  </property>\n\x3c!-- 代理类  设置hdfs客户端用来连接集群中活动状态namenode节点的java类 --\x3e\n  <property>\n    <name>dfs.client.failover.proxy.provider.mycluster</name>\n    <value>org.apache.hadoop.hdfs.server.namenode.ha.configuredfailoverproxyprovider</value>\n  </property>\n\x3c!-- 隔离方式 为sshfence ssh远程代理  启动fence过程，确保集群高可用性 --\x3e\n  <property>\n    <name>dfs.ha.fencing.methods</name>\n    <value>sshfence</value>\n  </property>\n\x3c!-- ssh私有路径 设置ssh登录的私钥文件地址  --\x3e\n  <property>\n    <name>dfs.ha.fencing.ssh.private-key-files</name>\n    <value>/home/atguigu/.ssh/id_rsa</value>\n  </property>\n    \x3c!-- 开启自动切换 设置自动切换活跃节点，保证集群高可用性 --\x3e\n<property>\n\t<name>dfs.ha.automatic-failover.enabled</name>\n\t<value>true</value>\n</property>\n\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n\n\n同步\n\nsudo xsync /opt/ha/\n\n\n1\n\n\n临时修改环境变量 因为hdfs.sh 默认根据环境变量来启动的 无法正常格式化ha中的namenode\n\nsudo vim /etc/profile.d/my_env.sh\n#修改hadoop路径 临时为ha下的hadoop\nexport hadoop_home=/opt/ha/hadoop-3.1.3\n\nsource /etc/profile.d/my_env.sh\nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n2\n3\n4\n5\n6\n\n\n启动jqm集群\n\n#三台都会启动 jps检查一下三台有没有journalnode\n/opt/ha/hadoop-3.1.3/bin/hdfs --workers --daemon start journalnode\n#如果不行删除data 和logs文件夹 重新启动  再不行看日志 jqm的数据存储路径必须为绝对路径 \n\n\n1\n2\n3\n\n\n格式化nn\n\n#在102节点中格式化nn\nhdfs namenode -format\nhdfs --daemon start namenode #启动nn\n\n#其他节点通过同步nn1的元数据信息 \n#切记要在atguigu下 不要root用户下  如果无法启动nn删除data 和logs文件夹 同步\n#在103和104中使用\nhdfs namenode -bootstrapstandby #同步\nhdfs --daemon start namenode  #启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n启动zk并初始化ha在zookeeper中状态\n\n#先启动三台的zookeeper\nzkserver.sh start\n\n#格式一台即可以 其他集群会同步\nhdfs zkfc -formatzk\n\n\n1\n2\n3\n4\n5\n\n\n启动dfs\n\n#102\nstart-dfs.sh\n\n\n1\n2\n\n\n\n# ha的选举机制\n\n通过查看3台集群的web页面 发现只有一台活动(active状态) 其他都是standby状态, 如果我把active的nn杀掉,那么ha会自动故障转移把某台standby状态的变为active状态\n\n 1. 通过zookeeper选取的\n 2. 当我们启动nn时 hadoop会自动向zookeeper上的hadoop-ha/mycluster/activestandbyelectorlock 注册一个临时节点 此节点的值就是active节点名称 先在此临时节点注册的集群则成为active\n 3. 如果active宕机 则节点会远程访问active 8020端口 并测试杀掉服务 无响应/杀掉 在zookeeper上的临时节点上注册 (此过程所有节点都会执行 谁先在临时节点注册上就是谁做active)\n 4. hadoop-ha/mycluster/activebreadcrumb 此zookeeper节点记录上一个active 名称 如果重新上线则为standby状态\n 5. 如果active断网 则会尝试不断的隔离active节点 但无法连接到active 并且在隔离期间standby都无法转换为active 因为我们配置的隔离方法 是sshfence(ssh连接并尝试杀死进程) 我们可以通过逗号配置多value值\n    1. shell(脚本文件路径) 自定义脚本方法\n\n\n# yarn-ha\n\nhadoop102         hadoop103         hadoop104\nnamenode          namenode          \njournalnode       journalnode       journalnode\ndatanode          datanode          datanode\nzk                zk                zk\nresourcemanager   resourcemanager   \nnodemanager       nodemanager       nodemanager\n\n在配置完hdfs-ha的基础上 修改配置\n\nvim /opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n#追加\n\n\n1\n2\n\n\n\x3c!--启用resourcemanager ha 开启resourcemanager 的高可用性功能--\x3e\n    <property>\n        <name> yarn.resourcemanager.ha.enabled</name>\n        <value>true</value>\n    </property>\n \n    \x3c!--声明两台resourcemanager的地址 标识集群中的resourcemanager，如果设置该选项，需要确保所有的resourcemanager节点在配置中都有自己的逻辑id--\x3e\n    <property>\n        <name>yarn.resourcemanager.cluster-id</name>\n        <value>cluster-yarn1</value>\n    </property>\n\t\x3c!--设置resourcemanager节点的逻辑id--\x3e\n    <property>\n        <name>yarn.resourcemanager.ha.rm-ids</name>\n        <value>rm1,rm2</value>\n    </property>\n\t\x3c!--为每个逻辑id绑定实际的主机名称--\x3e\n    <property>\n        <name>yarn.resourcemanager.hostname.rm1</name>\n        <value>hadoop102</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.hostname.rm2</name>\n        <value>hadoop103</value>\n    </property>\n \n    \x3c!--指定zookeeper集群的地址--\x3e \n    <property>\n        <name>yarn.resourcemanager.zk-address</name>\n        <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>\n    </property>\n\n    \x3c!--启用自动恢复--\x3e \n    <property>\n        <name>yarn.resourcemanager.recovery.enabled</name>\n        <value>true</value>\n    </property>\n \n    \x3c!--指定resourcemanager的状态信息存储在zookeeper集群--\x3e \n    <property>\n        <name>yarn.resourcemanager.store.class</name>     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.zkrmstatestore</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\nxsync /opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n启动yarn\n\n#103中启动\nstart-yarn.sh\n\n\n1\n2\n\n\n查询当前yarn是standby状态 还是active\n\nyarn rmadmin -getservicestate rm1\n\n\n1\n\n\nyarn在zookeeper下的节点名为yarn-leader-election 两个节点互相独立",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hadoop 新特性",frontmatter:{title:"Hadoop 新特性",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/8dd6c9/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/14.Hadoop%20%E6%96%B0%E7%89%B9%E6%80%A7.html",relativePath:"大数据/01.Hadoop/14.Hadoop 新特性.md",key:"v-6594ad68",path:"/pages/8dd6c9/",headers:[{level:2,title:"集群间数据拷贝",slug:"集群间数据拷贝",normalizedTitle:"集群间数据拷贝",charIndex:17},{level:2,title:"小文件存档",slug:"小文件存档",normalizedTitle:"小文件存档",charIndex:479},{level:2,title:"回收站",slug:"回收站",normalizedTitle:"回收站",charIndex:847}],headersStr:"集群间数据拷贝 小文件存档 回收站",content:"# Hadoop 新特性\n\n\n# 集群间数据拷贝\n\n 1. 使用scp实现两个远程主机之间的文件复制\n\nscp -r hello.txt root@hadoop103:/user/atguigu/hello.txt\t\t# 推 push\nscp -r root@hadoop103:/user/atguigu/hello.txt  hello.txt\t\t# 拉 pull\nscp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu   #是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。\n\n\n1\n2\n3\n\n 2. 采用distcp命令实现两个Hadoop集群之间的递归数据复制\n\nbin/hadoop distcp hdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt\n\n\n1\n\n\n\n# 小文件存档\n\n\n\nstart-yarn.sh #需要启动yarn服务\nbin/hadoop archive -archiveName input.har –p /user/atguigu/input /user/atguigu/output #将/user/atguigu/input目录里的所有文件都归档为 input.har\n\nhadoop fs -lsr /user/atguigu/output/input.har\nhadoop fs -lsr har:///user/atguigu/output/input.har  #查看文档\n\nhadoop fs -cp har:///user/atguigu/output/input.har/* /user/atguigu  #解归档文件\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 回收站\n\n开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用\n\n\n\n 1. 启用回收站修改core-site.xml，配置垃圾回收时间为1分钟。\n\n<property>\n   <name>fs.trash.interval</name>\n<value>1</value>\n</property>\n\n\n1\n2\n3\n4\n\n 2. 回收站默认路径在集群 /user/atguigu/.Trash/…. 路径下\n 3. 修改访问垃圾回收站用户名称 进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户 修改core-site.xml文件\n\n<property>\n  <name>hadoop.http.staticuser.user</name>\n  <value>atguigu</value>\n</property>\n\n\n1\n2\n3\n4\n\n 4. 通过java程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站\n\nTrash trash = New Trash(conf);\ntrash.moveToTrash(path);\n\n\n1\n2\n\n 5. 恢复回收站数据\n\nhadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input /user/atguigu/input\n\n\n1\n\n 6. 清空回收站\n\nhadoop fs -expunge\n\n\n1\n",normalizedContent:"# hadoop 新特性\n\n\n# 集群间数据拷贝\n\n 1. 使用scp实现两个远程主机之间的文件复制\n\nscp -r hello.txt root@hadoop103:/user/atguigu/hello.txt\t\t# 推 push\nscp -r root@hadoop103:/user/atguigu/hello.txt  hello.txt\t\t# 拉 pull\nscp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu   #是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。\n\n\n1\n2\n3\n\n 2. 采用distcp命令实现两个hadoop集群之间的递归数据复制\n\nbin/hadoop distcp hdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt\n\n\n1\n\n\n\n# 小文件存档\n\n\n\nstart-yarn.sh #需要启动yarn服务\nbin/hadoop archive -archivename input.har –p /user/atguigu/input /user/atguigu/output #将/user/atguigu/input目录里的所有文件都归档为 input.har\n\nhadoop fs -lsr /user/atguigu/output/input.har\nhadoop fs -lsr har:///user/atguigu/output/input.har  #查看文档\n\nhadoop fs -cp har:///user/atguigu/output/input.har/* /user/atguigu  #解归档文件\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 回收站\n\n开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用\n\n\n\n 1. 启用回收站修改core-site.xml，配置垃圾回收时间为1分钟。\n\n<property>\n   <name>fs.trash.interval</name>\n<value>1</value>\n</property>\n\n\n1\n2\n3\n4\n\n 2. 回收站默认路径在集群 /user/atguigu/.trash/…. 路径下\n 3. 修改访问垃圾回收站用户名称 进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户 修改core-site.xml文件\n\n<property>\n  <name>hadoop.http.staticuser.user</name>\n  <value>atguigu</value>\n</property>\n\n\n1\n2\n3\n4\n\n 4. 通过java程序删除的文件不会经过回收站，需要调用movetotrash()才进入回收站\n\ntrash trash = new trash(conf);\ntrash.movetotrash(path);\n\n\n1\n2\n\n 5. 恢复回收站数据\n\nhadoop fs -mv /user/atguigu/.trash/current/user/atguigu/input /user/atguigu/input\n\n\n1\n\n 6. 清空回收站\n\nhadoop fs -expunge\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"MapReduce原理",frontmatter:{title:"MapReduce原理",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/65db7d/",categories:["Hadoop"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/01.Hadoop/11.MapReduce%E5%8E%9F%E7%90%86.html",relativePath:"大数据/01.Hadoop/11.MapReduce原理.md",key:"v-4b94238d",path:"/pages/65db7d/",headers:[{level:2,title:"InputFormat 数据输入",slug:"inputformat-数据输入",normalizedTitle:"inputformat 数据输入",charIndex:47},{level:3,title:"FileInputFormat切片源码",slug:"fileinputformat切片源码",normalizedTitle:"fileinputformat切片源码",charIndex:70},{level:3,title:"CombineTextInputFormat切片机制",slug:"combinetextinputformat切片机制",normalizedTitle:"combinetextinputformat切片机制",charIndex:100},{level:3,title:"自定义InputFormat",slug:"自定义inputformat",normalizedTitle:"自定义inputformat",charIndex:238},{level:2,title:"Shuffle(混洗)  整理数据",slug:"shuffle-混洗-整理数据",normalizedTitle:"shuffle(混洗)  整理数据",charIndex:null},{level:3,title:"Partition分区",slug:"partition分区",normalizedTitle:"partition分区",charIndex:5841},{level:3,title:"WritableComparable 排序",slug:"writablecomparable-排序",normalizedTitle:"writablecomparable 排序",charIndex:10013},{level:3,title:"RawComparator 排序",slug:"rawcomparator-排序",normalizedTitle:"rawcomparator 排序",charIndex:15822},{level:3,title:"Combiner 合并",slug:"combiner-合并",normalizedTitle:"combiner 合并",charIndex:18136},{level:3,title:"GroupingComparator分组",slug:"groupingcomparator分组",normalizedTitle:"groupingcomparator分组",charIndex:18638},{level:2,title:"OutputFormat 数据输出",slug:"outputformat-数据输出",normalizedTitle:"outputformat 数据输出",charIndex:26135},{level:2,title:"Reduce Join",slug:"reduce-join",normalizedTitle:"reduce join",charIndex:31073},{level:2,title:"MapJoin",slug:"mapjoin",normalizedTitle:"mapjoin",charIndex:40062},{level:2,title:"数据清洗(ETL)和计数器",slug:"数据清洗-etl-和计数器",normalizedTitle:"数据清洗(etl)和计数器",charIndex:44451},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:18441}],headersStr:"InputFormat 数据输入 FileInputFormat切片源码 CombineTextInputFormat切片机制 自定义InputFormat Shuffle(混洗)  整理数据 Partition分区 WritableComparable 排序 RawComparator 排序 Combiner 合并 GroupingComparator分组 OutputFormat 数据输出 Reduce Join MapJoin 数据清洗(ETL)和计数器 总结",content:'# MapReduce原理\n\n通过前面的Hadoop序列化 此处难以理解\n\n\n\n\n\n\n\n\n# InputFormat 数据输入\n\n\n\n\n# FileInputFormat切片源码\n\n\n\n\n\n\n\n\n# CombineTextInputFormat切片机制\n\n框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。\n\n\n\n\n# 自定义InputFormat\n\nDriver\n\npackage com.inputformat;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.BytesWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n\nimport java.io.IOException;\n\npublic class MyInputDriver {\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n        Job job = Job.getInstance(new Configuration());\n\n        job.setJarByClass(MyInputDriver.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(BytesWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(BytesWritable.class);\n\n        job.setInputFormatClass(MyInputFormat.class);\n        job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n        FileInputFormat.setInputPaths(job, new Path("D:/input"));\n        FileOutputFormat.setOutputPath(job, new Path("D:/output"));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\nInputFormat\n\npackage com.inputformat;\n\nimport org.apache.hadoop.io.BytesWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.InputSplit;\nimport org.apache.hadoop.mapreduce.RecordReader;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n\nimport java.io.IOException;\n\npublic class MyInputFormat extends FileInputFormat<Text, BytesWritable> {\n    /**\n     * 返回一个自定义RecordReader\n     * @param inputSplit\n     * @param taskAttemptContext\n     * @return\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    public RecordReader<Text, BytesWritable> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {\n        return new MyRecordReader();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\nRecordReader\n\npackage com.inputformat;\n\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.io.BytesWritable;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.InputSplit;\nimport org.apache.hadoop.mapreduce.RecordReader;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\n\nimport java.io.IOException;\n\n//负责将整个文件转化成一组Key Value对\npublic class MyRecordReader extends RecordReader<Text, BytesWritable> {\n\n    //文件是否读完 默认为false\n    private boolean isRead;\n\n    //键值对\n    private Text key=new Text();\n    private BytesWritable value= new BytesWritable();\n\n    FSDataInputStream inputStream;\n    FileSplit fs;\n\n\n    /**\n     * 初始化方法 一般执行一些初始化操作\n     *\n     * @param inputSplit\n     * @param taskAttemptContext\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {\n        //开流\n        fs = (FileSplit) inputSplit;//强转为实现子类\n        FileSystem fileSystem = FileSystem.get(taskAttemptContext.getConfiguration()); //获取config对象\n        inputStream = fileSystem.open(fs.getPath());//获取路径\n\n    }\n\n    /**\n     * 读取下一个键值对 是否存在\n     *\n     * @return\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    public boolean nextKeyValue() throws IOException, InterruptedException {\n        if (!isRead){\n            //读取这个文件\n\n            //填充key\n            key.set(fs.getPath().toString()); //key路径\n            //value\n            byte[] buffer = new byte[(int) fs.getLength()];\n            value.set(buffer,0,buffer.length);\n            //标记文件读完\n            isRead = true;\n            return true;\n        }\n        return false;\n    }\n\n    /*\n    获取当前key\n     */\n    @Override\n    public Text getCurrentKey() throws IOException, InterruptedException {\n        return key;\n    }\n\n    //获取当前value\n    @Override\n    public BytesWritable getCurrentValue() throws IOException, InterruptedException {\n        return value;\n    }\n\n    /**\n     * 显示进度\n     *\n     * @return\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    public float getProgress() throws IOException, InterruptedException {\n        return isRead ? 0 : 1;\n    }\n\n    /*\n    关闭方法\n     */\n    @Override\n    public void close() throws IOException {\n        IOUtils.closeStream(inputStream);  //关流\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# Shuffle(混洗) 整理数据\n\nMapReduce框架会确保每一个Reducer的输入都是按Key进行排序的。一般，将排序以及Map的输出传输到Reduce的过程称为混洗（shuffle)。每一个Map都包含一个环形的缓存，默认100M，Map首先将输出写到缓存当中。当缓存的内容达到“阈值”时（阈值默认的大小是缓存的80%），一个后台线程负责将结果写到硬盘，这个过程称为“spill”。Spill过程中，Map仍可以向缓存写入结果，如果缓存已经写满，那么Map进行等待。\n\nMap方法之后，Reduce方法之前的数据处理过程称之为Shuffle。\n\n\n\nShuffle将map中无序的键值对,分区 排序 归并后输出给Reduce\n\nShuffle阶段数据是存放在内存(栈)中,如果数据写满了缓冲区,则会进行分区 并排序 然后进行归并排序 并且写入磁盘的操作,以释放缓冲区 让新数据进入缓冲区\n\n一次排序比多次排序效率要高 因为归并次数越多效率下降 但如果是数据集庞大 我们只有牺牲时间来换取空间\n\n\n# Partition分区\n\n实体类\n\npackage com.flow;\n\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class FlowBean implements Writable {\n    private long upFlow;\n    private long downFlow;\n    private long sumFlow;\n\n    @Override\n    public String toString() {\n        return upFlow + "\\t" + downFlow + "\\t" + sumFlow;\n    }\n\n    public void set(long upFlow, long downFlow) {\n        this.downFlow = downFlow;\n        this.upFlow = upFlow;\n        this.sumFlow = upFlow + downFlow;\n    }\n\n\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n    /**\n     * 将对象数据写出到框架指定地方  序列化\n     *\n     * @param dataOutput 数据的容器\n     * @throws IOException\n     */\n    @Override\n    public void write(DataOutput dataOutput) throws IOException {\n        dataOutput.writeLong(upFlow);\n        dataOutput.writeLong(downFlow);\n        dataOutput.writeLong(sumFlow);\n    }\n\n\n    /**\n     * 从框架指定地方读取数据填充对象  反序列化\n     *\n     * @param dataInput\n     * @throws IOException\n     */\n    @Override\n    public void readFields(DataInput dataInput) throws IOException {\n        //读写顺序要一致\n        this.upFlow = dataInput.readLong();\n        this.downFlow = dataInput.readLong();\n        this.sumFlow = dataInput.readLong();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\n分区类\n\npackage com.partitioner;\n\nimport com.flow.FlowBean;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\npublic class MyPartitioner extends Partitioner<Text, FlowBean> {\n    /**\n     * 对每一个键值对 返回对应的分区号\n     *\n     * @param text 手机号\n     * @param flowBean 流量\n     * @param numPartitions\n     * @return\n     */\n    @Override\n    public int getPartition(Text text, FlowBean flowBean, int numPartitions) {\n        switch (text.toString().substring(0, 3)) {  //根据手机号前3位\n            case "136":\n                return 0;\n            case "137":\n                return 1;\n            case "138":\n                return 2;\n            case "139":\n                return 3;\n            default:\n                return 4;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n驱动类\n\npackage com.partitioner;\n\nimport com.flow.FlowBean;\nimport com.flow.FlowMapper;\nimport com.flow.FlowReducer;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class NewFlowDriver {\n\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n        Job job = Job.getInstance(new Configuration());\n\n        job.setJarByClass(NewFlowDriver.class);\n\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n\n        job.setNumReduceTasks(5); //设置分区数/并行数\n        job.setPartitionerClass(MyPartitioner.class);  //设置分区类\n\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        FileInputFormat.setInputPaths(job, new Path("file:///d:/input"));\n        FileOutputFormat.setOutputPath(job, new Path("file:///d:/output"));\n\n        boolean completion = job.waitForCompletion(true);\n        System.exit(completion ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n# WritableComparable 排序\n\nWritableComparable 是MapReduce中默认的排序接口 实现类为WritableComparator\n\nMapTask和ReduceTask均会对数据按照key进行排序 hadoop的默认行为 默认排序为字典顺序排序 底层为快速排序\n\n如果要重写排序方法 则让实体类继承WritableComparable接口 并实现compareTo方法\n\n实现类\n\npackage com.compare;\n\nimport org.apache.hadoop.io.Writable;\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//实现WritableComparable接口\npublic class FlowBean implements WritableComparable<FlowBean> {\n    private long upFlow;\n    private long downFlow;\n    private long sumFlow;\n\n    @Override\n    public String toString() {\n        return upFlow + "\\t" + downFlow + "\\t" + sumFlow;\n    }\n\n    public void set(long upFlow, long downFlow) {\n        this.downFlow = downFlow;\n        this.upFlow = upFlow;\n        this.sumFlow = upFlow + downFlow;\n    }\n\n\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n    /**\n     * 将对象数据写出到框架指定地方  序列化\n     *\n     * @param dataOutput 数据的容器\n     * @throws IOException\n     */\n    @Override\n    public void write(DataOutput dataOutput) throws IOException {\n        dataOutput.writeLong(upFlow);\n        dataOutput.writeLong(downFlow);\n        dataOutput.writeLong(sumFlow);\n    }\n\n\n    /**\n     * 从框架指定地方读取数据填充对象  反序列化\n     *\n     * @param dataInput\n     * @throws IOException\n     */\n    @Override\n    public void readFields(DataInput dataInput) throws IOException {\n        //读写顺序要一致\n        this.upFlow = dataInput.readLong();\n        this.downFlow = dataInput.readLong();\n        this.sumFlow = dataInput.readLong();\n    }\n\n    //比较器\n    @Override\n    public int compareTo(FlowBean o) {\n//        if (this.sumFlow < o.sumFlow) {\n//            return 1;  //降序\n//        } else if (\n//                this.sumFlow == o.sumFlow\n//        ) {\n//            return 0;\n//        }\n//        return -1;  //升序\n        return Long.compare(o.sumFlow,this.sumFlow);\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n\n\nmapper类\n\npackage com.compare;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class CompareMapper extends Mapper<LongWritable, Text,FlowBean,Text> {\n    private Text phone =new Text();\n    private FlowBean flow = new FlowBean();\n\n\n    @Override\n    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, FlowBean, Text>.Context context) throws IOException, InterruptedException {\n        //一行数据\n        String line = value.toString();\n\n\n        //切分\n        String[] fields = line.split("\\t");\n\n        //封装\n        phone.set(fields[0]);\n        flow.setUpFlow(Long.parseLong(fields[1]));\n        flow.setDownFlow(Long.parseLong(fields[2]));\n        flow.setSumFlow(Long.parseLong(fields[3]));\n\n        //写到上下文\n        context.write(flow,phone);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nReducer类\n\npackage com.compare;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n//收的数据为 流量key  手机号value  输出为 手机key 流量value\npublic class CompareReducer extends Reducer<FlowBean, Text,Text,FlowBean> {\n    /**\n     * Reduce收到的数据已经排完序了 我们只需要将键和值 反着输出到文件中就可以\n     * @param key\n     * @param values\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void reduce(FlowBean key, Iterable<Text> values, Reducer<FlowBean, Text, Text, FlowBean>.Context context) throws IOException, InterruptedException {\n        for (Text value : values) {\n            context.write(value,key);\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n驱动类\n\npackage com.compare;\n\nimport com.partitioner.MyPartitioner;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.WritableComparator;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class CompareDriver {\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n        Job job = Job.getInstance(new Configuration());\n\n        job.setJarByClass(CompareDriver.class);\n\n        job.setMapperClass(CompareMapper.class);\n        job.setReducerClass(CompareReducer.class);\n\n        job.setMapOutputKeyClass(FlowBean.class);\n        job.setMapOutputValueClass(Text.class);\n\n//        job.setSortComparatorClass(WritableComparator.class);  //默认排序\n//        job.setGroupingComparatorClass(WritableComparator.class);  //分区排序也是使用这个Comparato类\n\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        FileInputFormat.setInputPaths(job, new Path("file:///d:/output"));\n        FileOutputFormat.setOutputPath(job, new Path("file:///d:/output2"));\n\n        boolean completion = job.waitForCompletion(true);\n        System.exit(completion ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# RawComparator 排序\n\nWritableComparable 类已经帮我实现好了RawComparator 排序中方法 所有我们可以直接继承WritableComparable 而不是实现RawComparator 接口\n\npackage com.compare;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\npublic class FlowComparator extends WritableComparator {\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n        FlowBean fa = (FlowBean) a;\n        FlowBean fb = (FlowBean) b;\n        return Long.compare(fb.getSumFlow(), fa.getSumFlow());\n    }\n\n    protected FlowComparator() {\n        super(FlowBean.class, true);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n驱动类要set为自定义后的排序类\n\npackage com.compare;\n\nimport com.partitioner.MyPartitioner;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.WritableComparator;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class CompareDriver {\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n        Job job = Job.getInstance(new Configuration());\n\n        job.setJarByClass(CompareDriver.class);\n\n        job.setMapperClass(CompareMapper.class);\n        job.setReducerClass(CompareReducer.class);\n\n        job.setMapOutputKeyClass(FlowBean.class);\n        job.setMapOutputValueClass(Text.class);\n\n//        job.setSortComparatorClass(WritableComparator.class);  //默认排序\n//        job.setGroupingComparatorClass(WritableComparator.class);  //分区排序也是使用这个Comparator类\n        job.setSortComparatorClass(FlowComparator.class);  //设置为重写的Comparator类\n\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        FileInputFormat.setInputPaths(job, new Path("file:///d:/output"));\n        FileOutputFormat.setOutputPath(job, new Path("file:///d:/output2"));\n\n        boolean completion = job.waitForCompletion(true);\n        System.exit(completion ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nMappring类和实体类一致 但实体类中的compareTo 因为job已经设置了自定义的排序类 所有不会执行实体类中的compareTo方法\n\n\n# Combiner 合并\n\n 1. Combiner是MR程序中Mapper和Reducer之外的一种组件\n\n 2. Combiner组件的父类就是Reducer\n\n 3. Combiner和Reducer的区别在于运行的位置\n    \n    Combiner是在每一个MapTask所在堆叠节点运行\n    \n    Reducer是接受全局所有Mapper的输出结果\n\n 4. Combiner的意义就是对每一个MapTask的输出进行局部汇总,以减少网络传输量\n\n 5. Combiner能够应用的前提是不能影响最终的业务逻辑,而且Combiner的输出kv应用更Reducer的输入kv类型对应起来\n\n总结:Combiner就是在MapTask时 提前将数据分组归并 减少相同数据的分区 排序 再归并,但前提条件是合并后的数据不影响产生的结果 否则空间换取时间的做法不可取\n\n使用 在driver中传入Reducer类启用 不影响Reducer的使用\n\njob.setCombinerClass(CompareReducer.class); //提前归并分组 减少数据处理时间\n\n\n1\n\n\n\n# GroupingComparator分组\n\nGroupingComparator是在reduce阶段分组来使用的，由于reduce阶段，如果key相同的一组，只取第一个key作为key，迭代所有的values。 如果reduce的key是自定义的bean，我们只需要bean里面的某个属性相同就认为这样的key是相同的，这是我们就需要之定义GroupCoparator来“欺骗”reduce了。\n\n 1. 实体类继承WritableComparable接口 实现compareTo方法\n    \n    package com.grouping;\n    \n    \n    import org.apache.hadoop.io.WritableComparable;\n    \n    import java.io.DataInput;\n    import java.io.DataOutput;\n    import java.io.IOException;\n    \n    public class OrderBean implements WritableComparable<OrderBean> {\n    \n        private String orderId;\n        private String productId;\n        private double price;\n    \n        @Override\n        public String toString() {\n            return orderId + "\\t" + productId + "\\t" + price;\n        }\n    \n        public String getOrderId() {\n            return orderId;\n        }\n    \n        public String getProductId() {\n            return productId;\n        }\n    \n        public void setProductId(String productId) {\n            this.productId = productId;\n        }\n    \n        public double getPrice() {\n            return price;\n        }\n    \n        public void setPrice(double price) {\n            this.price = price;\n        }\n    \n        public void setOrderId(String orderId) {\n            this.orderId = orderId;\n        }\n    \n        //先按订单排序再根据订单相同价格降序\n        @Override\n        public int compareTo(OrderBean o) {\n            int compare = this.orderId.compareTo(o.orderId);  //比较订单号是否相同\n            if (compare != 0) {\n                return compare;  //不相同则返回差值\n            } else {\n                return Double.compare(o.price, this.price);  //相同按价格升序\n            }\n        }\n    \n        @Override\n        public void write(DataOutput dataOutput) throws IOException {\n            dataOutput.writeUTF(orderId);\n            dataOutput.writeUTF(productId);\n            dataOutput.writeDouble(price);\n        }\n    \n        @Override\n        public void readFields(DataInput dataInput) throws IOException {\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    \n\n 2. mapper封装数据到实体类中\n    \n    package com.grouping;\n    \n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Mapper;\n    \n    import java.io.IOException;\n    \n    //封装OrderBean\n    public class OrderMapper extends Mapper<LongWritable, Text,OrderBean, NullWritable> {\n    \n        private  OrderBean order =new OrderBean();\n    \n        //mapper封装方法\n        @Override\n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, OrderBean, NullWritable>.Context context) throws IOException, InterruptedException {\n            String[] split = value.toString().split("\\t");\n    \n            order.setOrderId(split[0]);\n            order.setProductId(split[1]);\n            order.setPrice(Double.parseDouble(split[2]));\n    \n            //key为一个OrderBean\n            context.write(order,NullWritable.get());\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    \n\n 3. 比较器 继承WritableComparator实现类 重写compare和无参构造方法\n    \n    package com.grouping;\n    \n    import org.apache.hadoop.io.WritableComparable;\n    import org.apache.hadoop.io.WritableComparator;\n    \n    //按照订单编号对数据进行分组\n    public class OrderComparator extends WritableComparator {\n        //按照相同订单进入一组进行比较\n        @Override\n        public int compare(WritableComparable a, WritableComparable b) {\n            OrderBean oa = (OrderBean) a;\n            OrderBean ob = (OrderBean) b;\n    \n            return oa.getOrderId().compareTo(ob.getOrderId());\n        }\n    \n        protected OrderComparator() {\n            super(OrderBean.class,true);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 4. Reducer类 此时key为实体类 value为null\n    \n    package com.grouping;\n    \n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.mapreduce.Reducer;\n    \n    import java.io.IOException;\n    import java.util.Iterator;\n    \n    //取每个订单的最高价格\n    public class OrderReducer extends Reducer<OrderBean, NullWritable,OrderBean,NullWritable> {\n    \n    \n        @Override\n        protected void reduce(OrderBean key, Iterable<NullWritable> values, Reducer<OrderBean, NullWritable, OrderBean, NullWritable>.Context context) throws IOException, InterruptedException {\n            Iterator<NullWritable> iterator = values.iterator();\n            for (int i = 0; i < 2; i++) { //输出当前订单组中前两个最高价格\n                if (iterator.hasNext()){\n                    iterator.next();\n                    context.write(key,NullWritable.get());\n                }\n            }\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n\n 5. 驱动类 setGroupingComparatorClass开启分组\n    \n    package com.grouping;\n    \n    import com.flow.FlowBean;\n    import com.flow.FlowDriver;\n    import com.flow.FlowMapper;\n    import com.flow.FlowReducer;\n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    \n    public class OrderDriver {\n        public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n            Job job = Job.getInstance(new Configuration());\n    \n            job.setJarByClass(OrderDriver.class);\n    \n            job.setMapperClass(OrderMapper.class);\n            job.setReducerClass(OrderReducer.class);\n    \n            job.setMapOutputKeyClass(OrderBean.class);\n            job.setMapOutputValueClass(NullWritable.class);\n    \n            job.setGroupingComparatorClass(OrderComparator.class); //分组比较器\n    \n            job.setOutputKeyClass(OrderBean.class);\n            job.setOutputValueClass(NullWritable.class);\n    \n            FileInputFormat.setInputPaths(job, new Path("file:///d:/input"));\n            FileOutputFormat.setOutputPath(job, new Path("file:///d:/output"));\n    \n            boolean completion = job.waitForCompletion(true);\n            System.exit(completion ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    \n\n上面Reducer中获取当前订单组中前两个最高价格 利用了shuffle中数据序列化的特性 如果在写入到磁盘中每次输出一个值创建一个映射实体类那么效率太低下 进入Mapper后数据就默认内部序列化了 写入到磁盘时只需创建一次映射实体类通过序列化迭代下一个键值对改变实体类的值 这样无需多次创建实体类浪费资源\n\n\n# OutputFormat 数据输出\n\n\n\n 1. Record类\n    \n    package com.outputformat;\n    \n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.FSDataOutputStream;\n    import org.apache.hadoop.fs.FileSystem;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.IOUtils;\n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.RecordWriter;\n    import org.apache.hadoop.mapreduce.TaskAttemptContext;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    import java.nio.charset.StandardCharsets;\n    \n    //将数据按照不包含 atguigu 的数据 分别输出到两个文件中\n    public class MyRecordWriter extends RecordWriter<LongWritable, Text> {\n        FSDataOutputStream atguigu = null;\n        FSDataOutputStream other = null;\n    \n        public MyRecordWriter(TaskAttemptContext job) throws IOException {\n            Configuration configuration = job.getConfiguration();//通过job获取配置文件\n            FileSystem fileSystem = FileSystem.get(configuration);  //通过配置文件获取文件对象\n            String outdir = configuration.get(FileOutputFormat.OUTDIR);  //获取配置文件中的输出路径地址\n            atguigu = fileSystem.create(new Path(outdir + "/atguigu.log"));//拼接\n            other = fileSystem.create(new Path(outdir + "/other.log"));\n        }\n    \n        /**\n         * 接受键值对 并按照值的不同输出到不同文件中\n         *\n         * @param key   读取的一行的偏移量\n         * @param value 这一行的内容\n         * @throws IOException\n         * @throws InterruptedException\n         */\n        @Override\n        public void write(LongWritable key, Text value) throws IOException, InterruptedException {\n            String line = value.toString() + "\\n";\n            if (line.contains("atguigu")) {//判断此行是否包含atguigu\n                //往atguigu文件写出数据\n                atguigu.write(line.getBytes(StandardCharsets.UTF_8));\n    \n            } else {\n                //往other文件写出数据\n                other.write(line.getBytes(StandardCharsets.UTF_8));\n            }\n        }\n    \n        //关闭资源\n        @Override\n        public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n            IOUtils.closeStream(atguigu);\n            IOUtils.closeStream(other);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    \n\n 2. OutputFormat类\n    \n    package com.outputformat;\n    \n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.RecordWriter;\n    import org.apache.hadoop.mapreduce.TaskAttemptContext;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    //mapping默认输出为LongWritable, Text\n    public class MyOutputFormat extends FileOutputFormat<LongWritable, Text> {\n        //返回一个处理数据的Record Writer\n        @Override\n        public RecordWriter<LongWritable, Text> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {\n            return new MyRecordWriter(job);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    \n\n 3. driver类\n    \n    package com.outputformat;\n    \n    \n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.IntWritable;\n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    \n    public class OutputDrive {\n        public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n            Configuration configuration = new Configuration();\n            Job job = Job.getInstance(configuration);\n    \n            job.setJarByClass(OutputDrive.class);\n    \n            job.setOutputFormatClass(MyOutputFormat.class);\n    \n    \n            FileInputFormat.setInputPaths(job, new Path("d:/input"));\n            //必须保证配置文件配置正常才能正常运行\n            FileOutputFormat.setOutputPath(job, new Path("d:/output"));\n    \n            boolean result = job.waitForCompletion(true);\n    \n            System.exit(result ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    \n\n\n# Reduce Join\n\nreduce side join是一种最简单的join方式，其主要思想如下： 在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签> （tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。> 在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作.\n\n * Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。\n * Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志）分开，最后进行合并就ok了。\n * 该方法的缺点：这种方式的缺点很明显就是会造成Map和Reduce端也就是shuffle阶段出现大量的数据传输，效率很低。\n\n\n\n 1. 创建实体类 并排序\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.io.WritableComparable;\n    \n    import java.io.DataInput;\n    import java.io.DataOutput;\n    import java.io.IOException;\n    \n    public class OrderBean implements WritableComparable<OrderBean> {\n    \n        private String id;\n        private String pid;\n        private int amount;\n        private String pname;\n    \n        @Override\n        public String toString() {\n            return id + "\\t" + pname + "\\t" + amount;\n        }\n    \n        public String getId() {\n            return id;\n        }\n    \n        public void setId(String id) {\n            this.id = id;\n        }\n    \n        public String getPid() {\n            return pid;\n        }\n    \n        public void setPid(String pid) {\n            this.pid = pid;\n        }\n    \n        public int getAmount() {\n            return amount;\n        }\n    \n        public void setAmount(int amount) {\n            this.amount = amount;\n        }\n    \n        public String getPname() {\n            return pname;\n        }\n    \n        public void setPname(String pname) {\n            this.pname = pname;\n        }\n    \n    \n        @Override\n        public int compareTo(OrderBean o) {\n            //按pid分组 组内按照pname降序排序\n            int i = this.pid.compareTo(o.pid);\n            if (i !=0){\n                return i;\n            }else {\n                return o.pname.compareTo(this.pname);\n            }\n        }\n    \n        @Override\n        public void write(DataOutput dataOutput) throws IOException {\n            dataOutput.writeUTF(id);\n            dataOutput.writeUTF(pid);\n            dataOutput.writeInt(amount);\n            dataOutput.writeUTF(pname);\n        }\n    \n        @Override\n        public void readFields(DataInput dataInput) throws IOException {\n            this.id = dataInput.readUTF();\n            this.pid = dataInput.readUTF();\n            this.amount = dataInput.readInt();\n            this.pname = dataInput.readUTF();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    68\n    69\n    70\n    71\n    72\n    73\n    74\n    75\n    76\n    77\n    78\n    79\n    80\n    \n\n 2. Mapper类 根据文件名的不同来封装实体类不同的实现\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Mapper;\n    import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n    \n    import java.io.IOException;\n    \n    public class OrderMapper extends Mapper<LongWritable, Text, OrderBean, NullWritable> {\n    \n        private OrderBean order = new OrderBean();\n        private String filename;  //获取当前文件名\n    \n        @Override\n        protected void setup(Mapper<LongWritable, Text, OrderBean, NullWritable>.Context context) throws IOException, InterruptedException {\n            //获取数据文件名\n            FileSplit fs = (FileSplit) context.getInputSplit();\n            filename = fs.getPath().getName();\n        }\n    \n        @Override\n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, OrderBean, NullWritable>.Context context) throws IOException, InterruptedException {\n            String[] split = value.toString().split("\\t");\n    \n            //封装,按数据来源不同分别封装\n            if ("order.txt".equals(filename)){\n                //封装order\n                order.setId(split[0]);\n                order.setPid(split[1]);\n                order.setAmount(Integer.parseInt(split[2]));\n                order.setPname(""); //不能为null\n            }else{\n                //封装pd\n                order.setPid(split[0]);\n                order.setPname(split[1]);\n                order.setAmount(0);  //不能为null\n                order.setId("");\n            }\n    \n            context.write(order,NullWritable.get());\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    \n\n 3. comparator 根据pid进行分组\n    \n    package com.reducejoin;\n    \n    \n    import org.apache.hadoop.io.WritableComparable;\n    import org.apache.hadoop.io.WritableComparator;\n    \n    //分组比较器 按照order对象的pid分组\n    public class OrderComparator extends WritableComparator {\n    \n        protected OrderComparator() {\n            super(OrderBean.class,true);\n        }\n    \n        //按照pid比较a和b\n        @Override\n        public int compare(WritableComparable a, WritableComparable b) {\n            OrderBean oa= (OrderBean) a;\n            OrderBean ob= (OrderBean) b;\n            return oa.getPid().compareTo(ob.getPid());\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n\n 4. Reducer 进行替换合并处理好/标志好数据\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.mapreduce.Reducer;\n    \n    import java.io.IOException;\n    import java.util.Iterator;\n    \n    //数据替换工作 将pid换成对应的pname\n    public class OrderReducer extends Reducer<OrderBean, NullWritable, OrderBean, NullWritable> {\n    \n    \n        @Override\n        protected void reduce(OrderBean key, Iterable<NullWritable> values, Reducer<OrderBean, NullWritable, OrderBean, NullWritable>.Context context) throws IOException, InterruptedException {\n    \n            /*for (NullWritable value : values) {\n            if (!"".equals(key.getPname())){\n                pName=key.getPname();  //遍历panme查找当前分组中有值的pname即品牌\n                break;  //但迭代器无法进行第二次迭代遍历\n            }*/\n    \n            //已经根据pname再次排序 并进行分组 第一个为需要的品牌名pname\n            Iterator<NullWritable> iterator = values.iterator();\n            iterator.next();\n            String pName = key.getPname(); //获取品牌名\n            while (iterator.hasNext()) {\n                iterator.next();\n                key.setPname(pName); //替换为对应的品牌名\n                context.write(key, NullWritable.get());  //写出\n            }\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    \n\n 5. 驱动类\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    \n    public class OrderDriver {\n        public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n            Job job = Job.getInstance(new Configuration());\n    \n            job.setJarByClass(OrderDriver.class);\n    \n            job.setMapperClass(OrderMapper.class);\n            job.setReducerClass(OrderReducer.class);\n    \n            job.setMapOutputKeyClass(OrderBean.class);\n            job.setMapOutputValueClass(NullWritable.class);\n    \n            job.setOutputKeyClass(OrderBean.class);\n            job.setOutputValueClass(NullWritable.class);\n    \n            job.setGroupingComparatorClass(OrderComparator.class);  //分组比较器\n    \n            FileInputFormat.setInputPaths(job, new Path("d:/input"));\n            FileOutputFormat.setOutputPath(job, new Path("d:/output"));\n    \n            boolean b = job.waitForCompletion(true);\n            System.exit(b ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    \n\n\n# MapJoin\n\nMap Join适用于一张表十分小、一张表很大的场景。在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。\n\n而使用MapJoin只需编写 driver和map类 无需编写reduce类 因为不涉及到reduce阶段 我们在map阶段就处理完成\n\n 1. driver 开启分布式缓存并传递小文件路径\n    \n    package com.mapjoin;\n    \n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    import java.net.URI;\n    \n    public class MJDriver {\n        public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n            Job job = Job.getInstance(new Configuration());\n    \n            job.setJarByClass(MJDriver.class);\n    \n            job.setMapperClass(MJMapper.class);\n            job.setNumReduceTasks(0); //Map端的join不需要Reduce阶段 所以设置ReduceTask数0\n    \n            //添加分布式缓存可以添加多值 传递为数组\n            job.addCacheFile(URI.create("file:///d:/input/pd.txt"));  //设置加载缓存数据\n    \n            job.setMapOutputKeyClass(Text.class);\n            job.setMapOutputValueClass(NullWritable.class);\n    \n            FileInputFormat.setInputPaths(job, new Path("D:/input/order.txt"));\n            FileOutputFormat.setOutputPath(job, new Path("d:/output"));\n    \n            boolean b = job.waitForCompletion(true);\n            System.exit(b ? 0 : 1);\n    \n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    \n\n 2. mapper setup加载分布式缓存字节流put到map集合当中 在map中替换和处理要处理的数据\n    \n    package com.mapjoin;\n    \n    import org.apache.commons.lang.StringUtils;\n    import org.apache.hadoop.fs.FSDataInputStream;\n    import org.apache.hadoop.fs.FileSystem;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.IOUtils;\n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Mapper;\n    \n    import java.io.BufferedReader;\n    import java.io.IOException;\n    import java.io.InputStreamReader;\n    import java.net.URI;\n    import java.util.HashMap;\n    import java.util.Map;\n    \n    public class MJMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n    \n        private Map<String, String> pMap = new HashMap<>();\n    \n        private Text k = new Text();\n    \n        @Override\n        protected void setup(Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {\n            //读取pd.txt到pMap\n            //开流\n            URI[] cacheFiles = context.getCacheFiles();  //读取分布式缓存文件路径数组\n            FileSystem fileSystem = FileSystem.get(context.getConfiguration());\n            FSDataInputStream pd = fileSystem.open(new Path(cacheFiles[0])); //pd文件\n    \n            //将文件按行处理 读取到pMap中\n            BufferedReader br = new BufferedReader(new InputStreamReader(pd)); //将字节流转为字符流\n            String line;\n            while (StringUtils.isNotEmpty(line = br.readLine())) {\n                String[] split = line.split("\\t");\n                pMap.put(split[0], split[1]); //转为map集合\n            }\n            IOUtils.closeStream(br);\n        }\n    \n        //处理order.txt的数据\n        @Override\n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {\n            String[] split = value.toString().split("\\t");\n    \n            k.set(split[0] + "\\t" + pMap.get(split[1]) + "\\t" + split[2]); //从map中根据pid获取value替换\n            context.write(k,NullWritable.get());\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    \n\n\n\n 1. mapJoin效率比ReduceJoin高\n 2. mapJoin因为是提前缓存数据到内存中 如果数据量庞大那么则无法使用\n\n\n# 数据清洗(ETL)和计数器\n\n在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序\n\n 1. 创建枚举类 方便构造计数器\n    \n    package com.etl;\n    \n    public enum ETL {\n        PASS,FAIL\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 2. mapper类 在setup方法中构造Counter 计数器 在map中通过数据清洗 计算出符合条件的条数\n    \n    package com.etl;\n    \n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Counter;\n    import org.apache.hadoop.mapreduce.Mapper;\n    \n    import java.io.IOException;\n    \n    public class ETLMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n    \n        private Counter pass;\n        private Counter fail;\n    \n    \n        @Override\n        protected void setup(Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {\n    //        pass = context.getCounter("ETL", "PASS"); //通过上下文构造一个计数器对象\n    //        fail = context.getCounter("ETL", "Fail");  //通过key value赋值\n            pass = context.getCounter(ETL.PASS); //通过上下文构造一个计数器对象\n            fail = context.getCounter(ETL.FAIL);  //通过枚举类来构造\n    \n        }\n    \n        //判断日志是否需要清洗\n        @Override\n        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {\n    \n            String[] splits = value.toString().split(" ");\n            if (splits.length > 11) {\n                context.write(value, NullWritable.get());\n                pass.increment(1); //计数器+1\n            } else {\n                fail.increment(1); //不符合条件的计数器+1\n            }\n            //此处没有作上下文写入 默认为不改变传递给reduce\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    \n\n 3. 驱动类\n    \n    package com.etl;\n    \n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    \n    import java.io.IOException;\n    \n    public class ETLDriver {\n        public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n            Job job = Job.getInstance(new Configuration());\n    \n            job.setJarByClass(ETLDriver.class);\n    \n            job.setMapperClass(ETLMapper.class);\n            job.setNumReduceTasks(0);\n    \n            job.setMapOutputKeyClass(Text.class);\n            job.setMapOutputValueClass(NullWritable.class);\n    \n            FileInputFormat.setInputPaths(job, new Path("d:/input"));\n            FileOutputFormat.setOutputPath(job, new Path("d:/output"));\n    \n            boolean b = job.waitForCompletion(true);\n            System.exit(b ? 0 : 1);\n    \n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    \n\n\n\nFail为符合条件的内容条数\n\nPASS为不符合条件的内容条数\n\n\n# 总结\n\n 1. 输入数据接口: InputFormat\n    \n    * 默认使用的实现是: TextInputFormat\n    * TextInputFormat的功能逻辑是: 一次读一行文本 然后将该行的起始偏移量作为key 行内容作为valuie返回\n    * KeyVlaueTextInputFormat每一行均为一条记录 被分隔符分割为key value 默认的分隔符为 \\t\n    * NlineInputFormat 按照指定的行数N来划分切片\n    * CombineTextInputFormat可以把多个小文件合并成一个切片处理 提高处理效率\n    * 用户还可以自定义InputFormat\n\n 2. 逻辑处理接口: Mapper\n    \n    * 根据业务需求实现 map() setup cleanup() 这三个方法\n\n 3. Partitioner分区\n    \n    * 默认实现类 HashPartitioner 逻辑是根据key的哈希值 和 numReduces来返回一个分区号\n      \n      (key.hashCode() & Integer.MAXVALUE) % numReduces\n    \n    * 可以自定义分区\n\n 4. Comparable 排序\n    \n    * 当我们用自定义的对象作为key来输出时 必须要实现 WritableComparable接口 重写其中的compareTo()方法\n    * 部分排序: 对最终输出的每个文件进行内部排序\n    * 全排序:对所有数据进行排序 通常只有一个Reduse\n    * 二次排序: 排序的条件有两个\n\n 5. Combiner 合并\n    \n    * Combiner合并可以提高程序的效率,减少IO传输.但是使用时必须不能影响原有的业务处理结果\n\n 6. Reduce端分组\n    \n    * GroupingComparator 在Reduce端对key进行分组 应用于:在接收的key为bean对象时,想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时,可以采用分组排序\n\n 7. 逻辑处理接口 Reducer\n    \n    * 根据业务需求实现 reduce() setup cleanup() 这三个方法\n\n 8. 输出数据接口 OutputFormat\n    \n    * 默认实现类是TextOutputFormat 功能逻辑是 将每一个键值对 想目标文本文件输出一行\n    * 将SequenceFileOutputFormat输出作为后续MapReduce任务的输入,这个是一种比较好的输出格式,y我它格式紧凑 容易被压缩\n    * 可以自定义OutputFormat',normalizedContent:'# mapreduce原理\n\n通过前面的hadoop序列化 此处难以理解\n\n\n\n\n\n\n\n\n# inputformat 数据输入\n\n\n\n\n# fileinputformat切片源码\n\n\n\n\n\n\n\n\n# combinetextinputformat切片机制\n\n框架默认的textinputformat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。\n\n\n\n\n# 自定义inputformat\n\ndriver\n\npackage com.inputformat;\n\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.byteswritable;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.job;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\nimport org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\nimport org.apache.hadoop.mapreduce.lib.output.sequencefileoutputformat;\n\nimport java.io.ioexception;\n\npublic class myinputdriver {\n    public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n        job job = job.getinstance(new configuration());\n\n        job.setjarbyclass(myinputdriver.class);\n\n        job.setmapoutputkeyclass(text.class);\n        job.setmapoutputvalueclass(byteswritable.class);\n\n        job.setoutputkeyclass(text.class);\n        job.setoutputvalueclass(byteswritable.class);\n\n        job.setinputformatclass(myinputformat.class);\n        job.setoutputformatclass(sequencefileoutputformat.class);\n\n        fileinputformat.setinputpaths(job, new path("d:/input"));\n        fileoutputformat.setoutputpath(job, new path("d:/output"));\n\n        boolean b = job.waitforcompletion(true);\n        system.exit(b ? 0 : 1);\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\ninputformat\n\npackage com.inputformat;\n\nimport org.apache.hadoop.io.byteswritable;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.inputsplit;\nimport org.apache.hadoop.mapreduce.recordreader;\nimport org.apache.hadoop.mapreduce.taskattemptcontext;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\n\nimport java.io.ioexception;\n\npublic class myinputformat extends fileinputformat<text, byteswritable> {\n    /**\n     * 返回一个自定义recordreader\n     * @param inputsplit\n     * @param taskattemptcontext\n     * @return\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    public recordreader<text, byteswritable> createrecordreader(inputsplit inputsplit, taskattemptcontext taskattemptcontext) throws ioexception, interruptedexception {\n        return new myrecordreader();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\nrecordreader\n\npackage com.inputformat;\n\nimport org.apache.hadoop.fs.fsdatainputstream;\nimport org.apache.hadoop.fs.filesystem;\nimport org.apache.hadoop.io.byteswritable;\nimport org.apache.hadoop.io.ioutils;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.inputsplit;\nimport org.apache.hadoop.mapreduce.recordreader;\nimport org.apache.hadoop.mapreduce.taskattemptcontext;\nimport org.apache.hadoop.mapreduce.lib.input.filesplit;\n\nimport java.io.ioexception;\n\n//负责将整个文件转化成一组key value对\npublic class myrecordreader extends recordreader<text, byteswritable> {\n\n    //文件是否读完 默认为false\n    private boolean isread;\n\n    //键值对\n    private text key=new text();\n    private byteswritable value= new byteswritable();\n\n    fsdatainputstream inputstream;\n    filesplit fs;\n\n\n    /**\n     * 初始化方法 一般执行一些初始化操作\n     *\n     * @param inputsplit\n     * @param taskattemptcontext\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    public void initialize(inputsplit inputsplit, taskattemptcontext taskattemptcontext) throws ioexception, interruptedexception {\n        //开流\n        fs = (filesplit) inputsplit;//强转为实现子类\n        filesystem filesystem = filesystem.get(taskattemptcontext.getconfiguration()); //获取config对象\n        inputstream = filesystem.open(fs.getpath());//获取路径\n\n    }\n\n    /**\n     * 读取下一个键值对 是否存在\n     *\n     * @return\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    public boolean nextkeyvalue() throws ioexception, interruptedexception {\n        if (!isread){\n            //读取这个文件\n\n            //填充key\n            key.set(fs.getpath().tostring()); //key路径\n            //value\n            byte[] buffer = new byte[(int) fs.getlength()];\n            value.set(buffer,0,buffer.length);\n            //标记文件读完\n            isread = true;\n            return true;\n        }\n        return false;\n    }\n\n    /*\n    获取当前key\n     */\n    @override\n    public text getcurrentkey() throws ioexception, interruptedexception {\n        return key;\n    }\n\n    //获取当前value\n    @override\n    public byteswritable getcurrentvalue() throws ioexception, interruptedexception {\n        return value;\n    }\n\n    /**\n     * 显示进度\n     *\n     * @return\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    public float getprogress() throws ioexception, interruptedexception {\n        return isread ? 0 : 1;\n    }\n\n    /*\n    关闭方法\n     */\n    @override\n    public void close() throws ioexception {\n        ioutils.closestream(inputstream);  //关流\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n\n\n\n# shuffle(混洗) 整理数据\n\nmapreduce框架会确保每一个reducer的输入都是按key进行排序的。一般，将排序以及map的输出传输到reduce的过程称为混洗（shuffle)。每一个map都包含一个环形的缓存，默认100m，map首先将输出写到缓存当中。当缓存的内容达到“阈值”时（阈值默认的大小是缓存的80%），一个后台线程负责将结果写到硬盘，这个过程称为“spill”。spill过程中，map仍可以向缓存写入结果，如果缓存已经写满，那么map进行等待。\n\nmap方法之后，reduce方法之前的数据处理过程称之为shuffle。\n\n\n\nshuffle将map中无序的键值对,分区 排序 归并后输出给reduce\n\nshuffle阶段数据是存放在内存(栈)中,如果数据写满了缓冲区,则会进行分区 并排序 然后进行归并排序 并且写入磁盘的操作,以释放缓冲区 让新数据进入缓冲区\n\n一次排序比多次排序效率要高 因为归并次数越多效率下降 但如果是数据集庞大 我们只有牺牲时间来换取空间\n\n\n# partition分区\n\n实体类\n\npackage com.flow;\n\nimport org.apache.hadoop.io.writable;\n\nimport java.io.datainput;\nimport java.io.dataoutput;\nimport java.io.ioexception;\n\npublic class flowbean implements writable {\n    private long upflow;\n    private long downflow;\n    private long sumflow;\n\n    @override\n    public string tostring() {\n        return upflow + "\\t" + downflow + "\\t" + sumflow;\n    }\n\n    public void set(long upflow, long downflow) {\n        this.downflow = downflow;\n        this.upflow = upflow;\n        this.sumflow = upflow + downflow;\n    }\n\n\n    public long getupflow() {\n        return upflow;\n    }\n\n    public void setupflow(long upflow) {\n        this.upflow = upflow;\n    }\n\n    public long getdownflow() {\n        return downflow;\n    }\n\n    public void setdownflow(long downflow) {\n        this.downflow = downflow;\n    }\n\n    public long getsumflow() {\n        return sumflow;\n    }\n\n    public void setsumflow(long sumflow) {\n        this.sumflow = sumflow;\n    }\n\n    /**\n     * 将对象数据写出到框架指定地方  序列化\n     *\n     * @param dataoutput 数据的容器\n     * @throws ioexception\n     */\n    @override\n    public void write(dataoutput dataoutput) throws ioexception {\n        dataoutput.writelong(upflow);\n        dataoutput.writelong(downflow);\n        dataoutput.writelong(sumflow);\n    }\n\n\n    /**\n     * 从框架指定地方读取数据填充对象  反序列化\n     *\n     * @param datainput\n     * @throws ioexception\n     */\n    @override\n    public void readfields(datainput datainput) throws ioexception {\n        //读写顺序要一致\n        this.upflow = datainput.readlong();\n        this.downflow = datainput.readlong();\n        this.sumflow = datainput.readlong();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\n\n分区类\n\npackage com.partitioner;\n\nimport com.flow.flowbean;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.partitioner;\n\npublic class mypartitioner extends partitioner<text, flowbean> {\n    /**\n     * 对每一个键值对 返回对应的分区号\n     *\n     * @param text 手机号\n     * @param flowbean 流量\n     * @param numpartitions\n     * @return\n     */\n    @override\n    public int getpartition(text text, flowbean flowbean, int numpartitions) {\n        switch (text.tostring().substring(0, 3)) {  //根据手机号前3位\n            case "136":\n                return 0;\n            case "137":\n                return 1;\n            case "138":\n                return 2;\n            case "139":\n                return 3;\n            default:\n                return 4;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n驱动类\n\npackage com.partitioner;\n\nimport com.flow.flowbean;\nimport com.flow.flowmapper;\nimport com.flow.flowreducer;\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.job;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\nimport org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n\nimport java.io.ioexception;\n\npublic class newflowdriver {\n\n    public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n        job job = job.getinstance(new configuration());\n\n        job.setjarbyclass(newflowdriver.class);\n\n        job.setmapperclass(flowmapper.class);\n        job.setreducerclass(flowreducer.class);\n\n        job.setmapoutputkeyclass(text.class);\n        job.setmapoutputvalueclass(flowbean.class);\n\n        job.setnumreducetasks(5); //设置分区数/并行数\n        job.setpartitionerclass(mypartitioner.class);  //设置分区类\n\n\n        job.setoutputkeyclass(text.class);\n        job.setoutputvalueclass(flowbean.class);\n\n        fileinputformat.setinputpaths(job, new path("file:///d:/input"));\n        fileoutputformat.setoutputpath(job, new path("file:///d:/output"));\n\n        boolean completion = job.waitforcompletion(true);\n        system.exit(completion ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n# writablecomparable 排序\n\nwritablecomparable 是mapreduce中默认的排序接口 实现类为writablecomparator\n\nmaptask和reducetask均会对数据按照key进行排序 hadoop的默认行为 默认排序为字典顺序排序 底层为快速排序\n\n如果要重写排序方法 则让实体类继承writablecomparable接口 并实现compareto方法\n\n实现类\n\npackage com.compare;\n\nimport org.apache.hadoop.io.writable;\nimport org.apache.hadoop.io.writablecomparable;\n\nimport java.io.datainput;\nimport java.io.dataoutput;\nimport java.io.ioexception;\n\n//实现writablecomparable接口\npublic class flowbean implements writablecomparable<flowbean> {\n    private long upflow;\n    private long downflow;\n    private long sumflow;\n\n    @override\n    public string tostring() {\n        return upflow + "\\t" + downflow + "\\t" + sumflow;\n    }\n\n    public void set(long upflow, long downflow) {\n        this.downflow = downflow;\n        this.upflow = upflow;\n        this.sumflow = upflow + downflow;\n    }\n\n\n    public long getupflow() {\n        return upflow;\n    }\n\n    public void setupflow(long upflow) {\n        this.upflow = upflow;\n    }\n\n    public long getdownflow() {\n        return downflow;\n    }\n\n    public void setdownflow(long downflow) {\n        this.downflow = downflow;\n    }\n\n    public long getsumflow() {\n        return sumflow;\n    }\n\n    public void setsumflow(long sumflow) {\n        this.sumflow = sumflow;\n    }\n\n    /**\n     * 将对象数据写出到框架指定地方  序列化\n     *\n     * @param dataoutput 数据的容器\n     * @throws ioexception\n     */\n    @override\n    public void write(dataoutput dataoutput) throws ioexception {\n        dataoutput.writelong(upflow);\n        dataoutput.writelong(downflow);\n        dataoutput.writelong(sumflow);\n    }\n\n\n    /**\n     * 从框架指定地方读取数据填充对象  反序列化\n     *\n     * @param datainput\n     * @throws ioexception\n     */\n    @override\n    public void readfields(datainput datainput) throws ioexception {\n        //读写顺序要一致\n        this.upflow = datainput.readlong();\n        this.downflow = datainput.readlong();\n        this.sumflow = datainput.readlong();\n    }\n\n    //比较器\n    @override\n    public int compareto(flowbean o) {\n//        if (this.sumflow < o.sumflow) {\n//            return 1;  //降序\n//        } else if (\n//                this.sumflow == o.sumflow\n//        ) {\n//            return 0;\n//        }\n//        return -1;  //升序\n        return long.compare(o.sumflow,this.sumflow);\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n\n\nmapper类\n\npackage com.compare;\n\nimport org.apache.hadoop.io.longwritable;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.mapper;\n\nimport java.io.ioexception;\n\npublic class comparemapper extends mapper<longwritable, text,flowbean,text> {\n    private text phone =new text();\n    private flowbean flow = new flowbean();\n\n\n    @override\n    protected void map(longwritable key, text value, mapper<longwritable, text, flowbean, text>.context context) throws ioexception, interruptedexception {\n        //一行数据\n        string line = value.tostring();\n\n\n        //切分\n        string[] fields = line.split("\\t");\n\n        //封装\n        phone.set(fields[0]);\n        flow.setupflow(long.parselong(fields[1]));\n        flow.setdownflow(long.parselong(fields[2]));\n        flow.setsumflow(long.parselong(fields[3]));\n\n        //写到上下文\n        context.write(flow,phone);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nreducer类\n\npackage com.compare;\n\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.reducer;\n\nimport java.io.ioexception;\n\n//收的数据为 流量key  手机号value  输出为 手机key 流量value\npublic class comparereducer extends reducer<flowbean, text,text,flowbean> {\n    /**\n     * reduce收到的数据已经排完序了 我们只需要将键和值 反着输出到文件中就可以\n     * @param key\n     * @param values\n     * @param context\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    protected void reduce(flowbean key, iterable<text> values, reducer<flowbean, text, text, flowbean>.context context) throws ioexception, interruptedexception {\n        for (text value : values) {\n            context.write(value,key);\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n驱动类\n\npackage com.compare;\n\nimport com.partitioner.mypartitioner;\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.io.writablecomparator;\nimport org.apache.hadoop.mapreduce.job;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\nimport org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n\nimport java.io.ioexception;\n\npublic class comparedriver {\n    public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n        job job = job.getinstance(new configuration());\n\n        job.setjarbyclass(comparedriver.class);\n\n        job.setmapperclass(comparemapper.class);\n        job.setreducerclass(comparereducer.class);\n\n        job.setmapoutputkeyclass(flowbean.class);\n        job.setmapoutputvalueclass(text.class);\n\n//        job.setsortcomparatorclass(writablecomparator.class);  //默认排序\n//        job.setgroupingcomparatorclass(writablecomparator.class);  //分区排序也是使用这个comparato类\n\n\n        job.setoutputkeyclass(text.class);\n        job.setoutputvalueclass(flowbean.class);\n\n        fileinputformat.setinputpaths(job, new path("file:///d:/output"));\n        fileoutputformat.setoutputpath(job, new path("file:///d:/output2"));\n\n        boolean completion = job.waitforcompletion(true);\n        system.exit(completion ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# rawcomparator 排序\n\nwritablecomparable 类已经帮我实现好了rawcomparator 排序中方法 所有我们可以直接继承writablecomparable 而不是实现rawcomparator 接口\n\npackage com.compare;\n\nimport org.apache.hadoop.io.writablecomparable;\nimport org.apache.hadoop.io.writablecomparator;\n\npublic class flowcomparator extends writablecomparator {\n    @override\n    public int compare(writablecomparable a, writablecomparable b) {\n        flowbean fa = (flowbean) a;\n        flowbean fb = (flowbean) b;\n        return long.compare(fb.getsumflow(), fa.getsumflow());\n    }\n\n    protected flowcomparator() {\n        super(flowbean.class, true);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n驱动类要set为自定义后的排序类\n\npackage com.compare;\n\nimport com.partitioner.mypartitioner;\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.io.writablecomparator;\nimport org.apache.hadoop.mapreduce.job;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\nimport org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n\nimport java.io.ioexception;\n\npublic class comparedriver {\n    public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n        job job = job.getinstance(new configuration());\n\n        job.setjarbyclass(comparedriver.class);\n\n        job.setmapperclass(comparemapper.class);\n        job.setreducerclass(comparereducer.class);\n\n        job.setmapoutputkeyclass(flowbean.class);\n        job.setmapoutputvalueclass(text.class);\n\n//        job.setsortcomparatorclass(writablecomparator.class);  //默认排序\n//        job.setgroupingcomparatorclass(writablecomparator.class);  //分区排序也是使用这个comparator类\n        job.setsortcomparatorclass(flowcomparator.class);  //设置为重写的comparator类\n\n\n        job.setoutputkeyclass(text.class);\n        job.setoutputvalueclass(flowbean.class);\n\n        fileinputformat.setinputpaths(job, new path("file:///d:/output"));\n        fileoutputformat.setoutputpath(job, new path("file:///d:/output2"));\n\n        boolean completion = job.waitforcompletion(true);\n        system.exit(completion ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nmappring类和实体类一致 但实体类中的compareto 因为job已经设置了自定义的排序类 所有不会执行实体类中的compareto方法\n\n\n# combiner 合并\n\n 1. combiner是mr程序中mapper和reducer之外的一种组件\n\n 2. combiner组件的父类就是reducer\n\n 3. combiner和reducer的区别在于运行的位置\n    \n    combiner是在每一个maptask所在堆叠节点运行\n    \n    reducer是接受全局所有mapper的输出结果\n\n 4. combiner的意义就是对每一个maptask的输出进行局部汇总,以减少网络传输量\n\n 5. combiner能够应用的前提是不能影响最终的业务逻辑,而且combiner的输出kv应用更reducer的输入kv类型对应起来\n\n总结:combiner就是在maptask时 提前将数据分组归并 减少相同数据的分区 排序 再归并,但前提条件是合并后的数据不影响产生的结果 否则空间换取时间的做法不可取\n\n使用 在driver中传入reducer类启用 不影响reducer的使用\n\njob.setcombinerclass(comparereducer.class); //提前归并分组 减少数据处理时间\n\n\n1\n\n\n\n# groupingcomparator分组\n\ngroupingcomparator是在reduce阶段分组来使用的，由于reduce阶段，如果key相同的一组，只取第一个key作为key，迭代所有的values。 如果reduce的key是自定义的bean，我们只需要bean里面的某个属性相同就认为这样的key是相同的，这是我们就需要之定义groupcoparator来“欺骗”reduce了。\n\n 1. 实体类继承writablecomparable接口 实现compareto方法\n    \n    package com.grouping;\n    \n    \n    import org.apache.hadoop.io.writablecomparable;\n    \n    import java.io.datainput;\n    import java.io.dataoutput;\n    import java.io.ioexception;\n    \n    public class orderbean implements writablecomparable<orderbean> {\n    \n        private string orderid;\n        private string productid;\n        private double price;\n    \n        @override\n        public string tostring() {\n            return orderid + "\\t" + productid + "\\t" + price;\n        }\n    \n        public string getorderid() {\n            return orderid;\n        }\n    \n        public string getproductid() {\n            return productid;\n        }\n    \n        public void setproductid(string productid) {\n            this.productid = productid;\n        }\n    \n        public double getprice() {\n            return price;\n        }\n    \n        public void setprice(double price) {\n            this.price = price;\n        }\n    \n        public void setorderid(string orderid) {\n            this.orderid = orderid;\n        }\n    \n        //先按订单排序再根据订单相同价格降序\n        @override\n        public int compareto(orderbean o) {\n            int compare = this.orderid.compareto(o.orderid);  //比较订单号是否相同\n            if (compare != 0) {\n                return compare;  //不相同则返回差值\n            } else {\n                return double.compare(o.price, this.price);  //相同按价格升序\n            }\n        }\n    \n        @override\n        public void write(dataoutput dataoutput) throws ioexception {\n            dataoutput.writeutf(orderid);\n            dataoutput.writeutf(productid);\n            dataoutput.writedouble(price);\n        }\n    \n        @override\n        public void readfields(datainput datainput) throws ioexception {\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    \n\n 2. mapper封装数据到实体类中\n    \n    package com.grouping;\n    \n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.mapper;\n    \n    import java.io.ioexception;\n    \n    //封装orderbean\n    public class ordermapper extends mapper<longwritable, text,orderbean, nullwritable> {\n    \n        private  orderbean order =new orderbean();\n    \n        //mapper封装方法\n        @override\n        protected void map(longwritable key, text value, mapper<longwritable, text, orderbean, nullwritable>.context context) throws ioexception, interruptedexception {\n            string[] split = value.tostring().split("\\t");\n    \n            order.setorderid(split[0]);\n            order.setproductid(split[1]);\n            order.setprice(double.parsedouble(split[2]));\n    \n            //key为一个orderbean\n            context.write(order,nullwritable.get());\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    \n\n 3. 比较器 继承writablecomparator实现类 重写compare和无参构造方法\n    \n    package com.grouping;\n    \n    import org.apache.hadoop.io.writablecomparable;\n    import org.apache.hadoop.io.writablecomparator;\n    \n    //按照订单编号对数据进行分组\n    public class ordercomparator extends writablecomparator {\n        //按照相同订单进入一组进行比较\n        @override\n        public int compare(writablecomparable a, writablecomparable b) {\n            orderbean oa = (orderbean) a;\n            orderbean ob = (orderbean) b;\n    \n            return oa.getorderid().compareto(ob.getorderid());\n        }\n    \n        protected ordercomparator() {\n            super(orderbean.class,true);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n\n 4. reducer类 此时key为实体类 value为null\n    \n    package com.grouping;\n    \n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.mapreduce.reducer;\n    \n    import java.io.ioexception;\n    import java.util.iterator;\n    \n    //取每个订单的最高价格\n    public class orderreducer extends reducer<orderbean, nullwritable,orderbean,nullwritable> {\n    \n    \n        @override\n        protected void reduce(orderbean key, iterable<nullwritable> values, reducer<orderbean, nullwritable, orderbean, nullwritable>.context context) throws ioexception, interruptedexception {\n            iterator<nullwritable> iterator = values.iterator();\n            for (int i = 0; i < 2; i++) { //输出当前订单组中前两个最高价格\n                if (iterator.hasnext()){\n                    iterator.next();\n                    context.write(key,nullwritable.get());\n                }\n            }\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    \n\n 5. 驱动类 setgroupingcomparatorclass开启分组\n    \n    package com.grouping;\n    \n    import com.flow.flowbean;\n    import com.flow.flowdriver;\n    import com.flow.flowmapper;\n    import com.flow.flowreducer;\n    import org.apache.hadoop.conf.configuration;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.job;\n    import org.apache.hadoop.mapreduce.lib.input.fileinputformat;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    \n    public class orderdriver {\n        public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n            job job = job.getinstance(new configuration());\n    \n            job.setjarbyclass(orderdriver.class);\n    \n            job.setmapperclass(ordermapper.class);\n            job.setreducerclass(orderreducer.class);\n    \n            job.setmapoutputkeyclass(orderbean.class);\n            job.setmapoutputvalueclass(nullwritable.class);\n    \n            job.setgroupingcomparatorclass(ordercomparator.class); //分组比较器\n    \n            job.setoutputkeyclass(orderbean.class);\n            job.setoutputvalueclass(nullwritable.class);\n    \n            fileinputformat.setinputpaths(job, new path("file:///d:/input"));\n            fileoutputformat.setoutputpath(job, new path("file:///d:/output"));\n    \n            boolean completion = job.waitforcompletion(true);\n            system.exit(completion ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    \n\n上面reducer中获取当前订单组中前两个最高价格 利用了shuffle中数据序列化的特性 如果在写入到磁盘中每次输出一个值创建一个映射实体类那么效率太低下 进入mapper后数据就默认内部序列化了 写入到磁盘时只需创建一次映射实体类通过序列化迭代下一个键值对改变实体类的值 这样无需多次创建实体类浪费资源\n\n\n# outputformat 数据输出\n\n\n\n 1. record类\n    \n    package com.outputformat;\n    \n    import org.apache.hadoop.conf.configuration;\n    import org.apache.hadoop.fs.fsdataoutputstream;\n    import org.apache.hadoop.fs.filesystem;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.ioutils;\n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.recordwriter;\n    import org.apache.hadoop.mapreduce.taskattemptcontext;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    import java.nio.charset.standardcharsets;\n    \n    //将数据按照不包含 atguigu 的数据 分别输出到两个文件中\n    public class myrecordwriter extends recordwriter<longwritable, text> {\n        fsdataoutputstream atguigu = null;\n        fsdataoutputstream other = null;\n    \n        public myrecordwriter(taskattemptcontext job) throws ioexception {\n            configuration configuration = job.getconfiguration();//通过job获取配置文件\n            filesystem filesystem = filesystem.get(configuration);  //通过配置文件获取文件对象\n            string outdir = configuration.get(fileoutputformat.outdir);  //获取配置文件中的输出路径地址\n            atguigu = filesystem.create(new path(outdir + "/atguigu.log"));//拼接\n            other = filesystem.create(new path(outdir + "/other.log"));\n        }\n    \n        /**\n         * 接受键值对 并按照值的不同输出到不同文件中\n         *\n         * @param key   读取的一行的偏移量\n         * @param value 这一行的内容\n         * @throws ioexception\n         * @throws interruptedexception\n         */\n        @override\n        public void write(longwritable key, text value) throws ioexception, interruptedexception {\n            string line = value.tostring() + "\\n";\n            if (line.contains("atguigu")) {//判断此行是否包含atguigu\n                //往atguigu文件写出数据\n                atguigu.write(line.getbytes(standardcharsets.utf_8));\n    \n            } else {\n                //往other文件写出数据\n                other.write(line.getbytes(standardcharsets.utf_8));\n            }\n        }\n    \n        //关闭资源\n        @override\n        public void close(taskattemptcontext context) throws ioexception, interruptedexception {\n            ioutils.closestream(atguigu);\n            ioutils.closestream(other);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    \n\n 2. outputformat类\n    \n    package com.outputformat;\n    \n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.recordwriter;\n    import org.apache.hadoop.mapreduce.taskattemptcontext;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    //mapping默认输出为longwritable, text\n    public class myoutputformat extends fileoutputformat<longwritable, text> {\n        //返回一个处理数据的record writer\n        @override\n        public recordwriter<longwritable, text> getrecordwriter(taskattemptcontext job) throws ioexception, interruptedexception {\n            return new myrecordwriter(job);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    \n\n 3. driver类\n    \n    package com.outputformat;\n    \n    \n    import org.apache.hadoop.conf.configuration;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.intwritable;\n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.job;\n    import org.apache.hadoop.mapreduce.lib.input.fileinputformat;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    \n    public class outputdrive {\n        public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n            configuration configuration = new configuration();\n            job job = job.getinstance(configuration);\n    \n            job.setjarbyclass(outputdrive.class);\n    \n            job.setoutputformatclass(myoutputformat.class);\n    \n    \n            fileinputformat.setinputpaths(job, new path("d:/input"));\n            //必须保证配置文件配置正常才能正常运行\n            fileoutputformat.setoutputpath(job, new path("d:/output"));\n    \n            boolean result = job.waitforcompletion(true);\n    \n            system.exit(result ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    \n\n\n# reduce join\n\nreduce side join是一种最简单的join方式，其主要思想如下： 在map阶段，map函数同时读取两个文件file1和file2，为了区分两种来源的key/value数据对，对每条数据打一个标签> （tag）,比如：tag=0表示来自文件file1，tag=2表示来自文件file2。即：map阶段的主要任务是对不同文件中的数据打标签。> 在reduce阶段，reduce函数获取key相同的来自file1和file2文件的value list， 然后对于同一个key，对file1和file2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作.\n\n * map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。\n * reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在map阶段已经打标志）分开，最后进行合并就ok了。\n * 该方法的缺点：这种方式的缺点很明显就是会造成map和reduce端也就是shuffle阶段出现大量的数据传输，效率很低。\n\n\n\n 1. 创建实体类 并排序\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.io.writablecomparable;\n    \n    import java.io.datainput;\n    import java.io.dataoutput;\n    import java.io.ioexception;\n    \n    public class orderbean implements writablecomparable<orderbean> {\n    \n        private string id;\n        private string pid;\n        private int amount;\n        private string pname;\n    \n        @override\n        public string tostring() {\n            return id + "\\t" + pname + "\\t" + amount;\n        }\n    \n        public string getid() {\n            return id;\n        }\n    \n        public void setid(string id) {\n            this.id = id;\n        }\n    \n        public string getpid() {\n            return pid;\n        }\n    \n        public void setpid(string pid) {\n            this.pid = pid;\n        }\n    \n        public int getamount() {\n            return amount;\n        }\n    \n        public void setamount(int amount) {\n            this.amount = amount;\n        }\n    \n        public string getpname() {\n            return pname;\n        }\n    \n        public void setpname(string pname) {\n            this.pname = pname;\n        }\n    \n    \n        @override\n        public int compareto(orderbean o) {\n            //按pid分组 组内按照pname降序排序\n            int i = this.pid.compareto(o.pid);\n            if (i !=0){\n                return i;\n            }else {\n                return o.pname.compareto(this.pname);\n            }\n        }\n    \n        @override\n        public void write(dataoutput dataoutput) throws ioexception {\n            dataoutput.writeutf(id);\n            dataoutput.writeutf(pid);\n            dataoutput.writeint(amount);\n            dataoutput.writeutf(pname);\n        }\n    \n        @override\n        public void readfields(datainput datainput) throws ioexception {\n            this.id = datainput.readutf();\n            this.pid = datainput.readutf();\n            this.amount = datainput.readint();\n            this.pname = datainput.readutf();\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    53\n    54\n    55\n    56\n    57\n    58\n    59\n    60\n    61\n    62\n    63\n    64\n    65\n    66\n    67\n    68\n    69\n    70\n    71\n    72\n    73\n    74\n    75\n    76\n    77\n    78\n    79\n    80\n    \n\n 2. mapper类 根据文件名的不同来封装实体类不同的实现\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.mapper;\n    import org.apache.hadoop.mapreduce.lib.input.filesplit;\n    \n    import java.io.ioexception;\n    \n    public class ordermapper extends mapper<longwritable, text, orderbean, nullwritable> {\n    \n        private orderbean order = new orderbean();\n        private string filename;  //获取当前文件名\n    \n        @override\n        protected void setup(mapper<longwritable, text, orderbean, nullwritable>.context context) throws ioexception, interruptedexception {\n            //获取数据文件名\n            filesplit fs = (filesplit) context.getinputsplit();\n            filename = fs.getpath().getname();\n        }\n    \n        @override\n        protected void map(longwritable key, text value, mapper<longwritable, text, orderbean, nullwritable>.context context) throws ioexception, interruptedexception {\n            string[] split = value.tostring().split("\\t");\n    \n            //封装,按数据来源不同分别封装\n            if ("order.txt".equals(filename)){\n                //封装order\n                order.setid(split[0]);\n                order.setpid(split[1]);\n                order.setamount(integer.parseint(split[2]));\n                order.setpname(""); //不能为null\n            }else{\n                //封装pd\n                order.setpid(split[0]);\n                order.setpname(split[1]);\n                order.setamount(0);  //不能为null\n                order.setid("");\n            }\n    \n            context.write(order,nullwritable.get());\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    \n\n 3. comparator 根据pid进行分组\n    \n    package com.reducejoin;\n    \n    \n    import org.apache.hadoop.io.writablecomparable;\n    import org.apache.hadoop.io.writablecomparator;\n    \n    //分组比较器 按照order对象的pid分组\n    public class ordercomparator extends writablecomparator {\n    \n        protected ordercomparator() {\n            super(orderbean.class,true);\n        }\n    \n        //按照pid比较a和b\n        @override\n        public int compare(writablecomparable a, writablecomparable b) {\n            orderbean oa= (orderbean) a;\n            orderbean ob= (orderbean) b;\n            return oa.getpid().compareto(ob.getpid());\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n\n 4. reducer 进行替换合并处理好/标志好数据\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.mapreduce.reducer;\n    \n    import java.io.ioexception;\n    import java.util.iterator;\n    \n    //数据替换工作 将pid换成对应的pname\n    public class orderreducer extends reducer<orderbean, nullwritable, orderbean, nullwritable> {\n    \n    \n        @override\n        protected void reduce(orderbean key, iterable<nullwritable> values, reducer<orderbean, nullwritable, orderbean, nullwritable>.context context) throws ioexception, interruptedexception {\n    \n            /*for (nullwritable value : values) {\n            if (!"".equals(key.getpname())){\n                pname=key.getpname();  //遍历panme查找当前分组中有值的pname即品牌\n                break;  //但迭代器无法进行第二次迭代遍历\n            }*/\n    \n            //已经根据pname再次排序 并进行分组 第一个为需要的品牌名pname\n            iterator<nullwritable> iterator = values.iterator();\n            iterator.next();\n            string pname = key.getpname(); //获取品牌名\n            while (iterator.hasnext()) {\n                iterator.next();\n                key.setpname(pname); //替换为对应的品牌名\n                context.write(key, nullwritable.get());  //写出\n            }\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    \n\n 5. 驱动类\n    \n    package com.reducejoin;\n    \n    import org.apache.hadoop.conf.configuration;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.mapreduce.job;\n    import org.apache.hadoop.mapreduce.lib.input.fileinputformat;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    \n    public class orderdriver {\n        public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n            job job = job.getinstance(new configuration());\n    \n            job.setjarbyclass(orderdriver.class);\n    \n            job.setmapperclass(ordermapper.class);\n            job.setreducerclass(orderreducer.class);\n    \n            job.setmapoutputkeyclass(orderbean.class);\n            job.setmapoutputvalueclass(nullwritable.class);\n    \n            job.setoutputkeyclass(orderbean.class);\n            job.setoutputvalueclass(nullwritable.class);\n    \n            job.setgroupingcomparatorclass(ordercomparator.class);  //分组比较器\n    \n            fileinputformat.setinputpaths(job, new path("d:/input"));\n            fileoutputformat.setoutputpath(job, new path("d:/output"));\n    \n            boolean b = job.waitforcompletion(true);\n            system.exit(b ? 0 : 1);\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    \n\n\n# mapjoin\n\nmap join适用于一张表十分小、一张表很大的场景。在map端缓存多张表，提前处理业务逻辑，这样增加map端业务，减少reduce端数据的压力，尽可能的减少数据倾斜。\n\n而使用mapjoin只需编写 driver和map类 无需编写reduce类 因为不涉及到reduce阶段 我们在map阶段就处理完成\n\n 1. driver 开启分布式缓存并传递小文件路径\n    \n    package com.mapjoin;\n    \n    import org.apache.hadoop.conf.configuration;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.job;\n    import org.apache.hadoop.mapreduce.lib.input.fileinputformat;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    import java.net.uri;\n    \n    public class mjdriver {\n        public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n            job job = job.getinstance(new configuration());\n    \n            job.setjarbyclass(mjdriver.class);\n    \n            job.setmapperclass(mjmapper.class);\n            job.setnumreducetasks(0); //map端的join不需要reduce阶段 所以设置reducetask数0\n    \n            //添加分布式缓存可以添加多值 传递为数组\n            job.addcachefile(uri.create("file:///d:/input/pd.txt"));  //设置加载缓存数据\n    \n            job.setmapoutputkeyclass(text.class);\n            job.setmapoutputvalueclass(nullwritable.class);\n    \n            fileinputformat.setinputpaths(job, new path("d:/input/order.txt"));\n            fileoutputformat.setoutputpath(job, new path("d:/output"));\n    \n            boolean b = job.waitforcompletion(true);\n            system.exit(b ? 0 : 1);\n    \n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    \n\n 2. mapper setup加载分布式缓存字节流put到map集合当中 在map中替换和处理要处理的数据\n    \n    package com.mapjoin;\n    \n    import org.apache.commons.lang.stringutils;\n    import org.apache.hadoop.fs.fsdatainputstream;\n    import org.apache.hadoop.fs.filesystem;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.ioutils;\n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.mapper;\n    \n    import java.io.bufferedreader;\n    import java.io.ioexception;\n    import java.io.inputstreamreader;\n    import java.net.uri;\n    import java.util.hashmap;\n    import java.util.map;\n    \n    public class mjmapper extends mapper<longwritable, text, text, nullwritable> {\n    \n        private map<string, string> pmap = new hashmap<>();\n    \n        private text k = new text();\n    \n        @override\n        protected void setup(mapper<longwritable, text, text, nullwritable>.context context) throws ioexception, interruptedexception {\n            //读取pd.txt到pmap\n            //开流\n            uri[] cachefiles = context.getcachefiles();  //读取分布式缓存文件路径数组\n            filesystem filesystem = filesystem.get(context.getconfiguration());\n            fsdatainputstream pd = filesystem.open(new path(cachefiles[0])); //pd文件\n    \n            //将文件按行处理 读取到pmap中\n            bufferedreader br = new bufferedreader(new inputstreamreader(pd)); //将字节流转为字符流\n            string line;\n            while (stringutils.isnotempty(line = br.readline())) {\n                string[] split = line.split("\\t");\n                pmap.put(split[0], split[1]); //转为map集合\n            }\n            ioutils.closestream(br);\n        }\n    \n        //处理order.txt的数据\n        @override\n        protected void map(longwritable key, text value, mapper<longwritable, text, text, nullwritable>.context context) throws ioexception, interruptedexception {\n            string[] split = value.tostring().split("\\t");\n    \n            k.set(split[0] + "\\t" + pmap.get(split[1]) + "\\t" + split[2]); //从map中根据pid获取value替换\n            context.write(k,nullwritable.get());\n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    41\n    42\n    43\n    44\n    45\n    46\n    47\n    48\n    49\n    50\n    51\n    52\n    \n\n\n\n 1. mapjoin效率比reducejoin高\n 2. mapjoin因为是提前缓存数据到内存中 如果数据量庞大那么则无法使用\n\n\n# 数据清洗(etl)和计数器\n\n在运行核心业务mapreduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行mapper程序，不需要运行reduce程序\n\n 1. 创建枚举类 方便构造计数器\n    \n    package com.etl;\n    \n    public enum etl {\n        pass,fail\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n\n 2. mapper类 在setup方法中构造counter 计数器 在map中通过数据清洗 计算出符合条件的条数\n    \n    package com.etl;\n    \n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.counter;\n    import org.apache.hadoop.mapreduce.mapper;\n    \n    import java.io.ioexception;\n    \n    public class etlmapper extends mapper<longwritable, text, text, nullwritable> {\n    \n        private counter pass;\n        private counter fail;\n    \n    \n        @override\n        protected void setup(mapper<longwritable, text, text, nullwritable>.context context) throws ioexception, interruptedexception {\n    //        pass = context.getcounter("etl", "pass"); //通过上下文构造一个计数器对象\n    //        fail = context.getcounter("etl", "fail");  //通过key value赋值\n            pass = context.getcounter(etl.pass); //通过上下文构造一个计数器对象\n            fail = context.getcounter(etl.fail);  //通过枚举类来构造\n    \n        }\n    \n        //判断日志是否需要清洗\n        @override\n        protected void map(longwritable key, text value, mapper<longwritable, text, text, nullwritable>.context context) throws ioexception, interruptedexception {\n    \n            string[] splits = value.tostring().split(" ");\n            if (splits.length > 11) {\n                context.write(value, nullwritable.get());\n                pass.increment(1); //计数器+1\n            } else {\n                fail.increment(1); //不符合条件的计数器+1\n            }\n            //此处没有作上下文写入 默认为不改变传递给reduce\n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    36\n    37\n    38\n    39\n    40\n    \n\n 3. 驱动类\n    \n    package com.etl;\n    \n    import org.apache.hadoop.conf.configuration;\n    import org.apache.hadoop.fs.path;\n    import org.apache.hadoop.io.longwritable;\n    import org.apache.hadoop.io.nullwritable;\n    import org.apache.hadoop.io.text;\n    import org.apache.hadoop.mapreduce.job;\n    import org.apache.hadoop.mapreduce.lib.input.fileinputformat;\n    import org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n    \n    import java.io.ioexception;\n    \n    public class etldriver {\n        public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n            job job = job.getinstance(new configuration());\n    \n            job.setjarbyclass(etldriver.class);\n    \n            job.setmapperclass(etlmapper.class);\n            job.setnumreducetasks(0);\n    \n            job.setmapoutputkeyclass(text.class);\n            job.setmapoutputvalueclass(nullwritable.class);\n    \n            fileinputformat.setinputpaths(job, new path("d:/input"));\n            fileoutputformat.setoutputpath(job, new path("d:/output"));\n    \n            boolean b = job.waitforcompletion(true);\n            system.exit(b ? 0 : 1);\n    \n    \n        }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    \n\n\n\nfail为符合条件的内容条数\n\npass为不符合条件的内容条数\n\n\n# 总结\n\n 1. 输入数据接口: inputformat\n    \n    * 默认使用的实现是: textinputformat\n    * textinputformat的功能逻辑是: 一次读一行文本 然后将该行的起始偏移量作为key 行内容作为valuie返回\n    * keyvlauetextinputformat每一行均为一条记录 被分隔符分割为key value 默认的分隔符为 \\t\n    * nlineinputformat 按照指定的行数n来划分切片\n    * combinetextinputformat可以把多个小文件合并成一个切片处理 提高处理效率\n    * 用户还可以自定义inputformat\n\n 2. 逻辑处理接口: mapper\n    \n    * 根据业务需求实现 map() setup cleanup() 这三个方法\n\n 3. partitioner分区\n    \n    * 默认实现类 hashpartitioner 逻辑是根据key的哈希值 和 numreduces来返回一个分区号\n      \n      (key.hashcode() & integer.maxvalue) % numreduces\n    \n    * 可以自定义分区\n\n 4. comparable 排序\n    \n    * 当我们用自定义的对象作为key来输出时 必须要实现 writablecomparable接口 重写其中的compareto()方法\n    * 部分排序: 对最终输出的每个文件进行内部排序\n    * 全排序:对所有数据进行排序 通常只有一个reduse\n    * 二次排序: 排序的条件有两个\n\n 5. combiner 合并\n    \n    * combiner合并可以提高程序的效率,减少io传输.但是使用时必须不能影响原有的业务处理结果\n\n 6. reduce端分组\n    \n    * groupingcomparator 在reduce端对key进行分组 应用于:在接收的key为bean对象时,想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时,可以采用分组排序\n\n 7. 逻辑处理接口 reducer\n    \n    * 根据业务需求实现 reduce() setup cleanup() 这三个方法\n\n 8. 输出数据接口 outputformat\n    \n    * 默认实现类是textoutputformat 功能逻辑是 将每一个键值对 想目标文本文件输出一行\n    * 将sequencefileoutputformat输出作为后续mapreduce任务的输入,这个是一种比较好的输出格式,y我它格式紧凑 容易被压缩\n    * 可以自定义outputformat',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"概述",frontmatter:{title:"概述",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/f38fc8/",categories:["大数据","Zookeeper"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/02.Zookeeper/01.%E6%A6%82%E8%BF%B0.html",relativePath:"大数据/02.Zookeeper/01.概述.md",key:"v-369ccb08",path:"/pages/f38fc8/",headersStr:null,content:"# 概述\n\nZookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目\n\n\n\n\n\n",normalizedContent:"# 概述\n\nzookeeper是一个开源的分布式的，为分布式应用提供协调服务的apache项目\n\n\n\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Api",frontmatter:{title:"Api",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/4a9c91/",categories:["大数据","Zookeeper"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/02.Zookeeper/05.Api.html",relativePath:"大数据/02.Zookeeper/05.Api.md",key:"v-72b26eaf",path:"/pages/4a9c91/",headersStr:null,content:'# Api\n\n坐标\n\n<dependencies>\n\t\t<dependency>\n\t\t\t<groupId>junit</groupId>\n\t\t\t<artifactId>junit</artifactId>\n\t\t\t<version>RELEASE</version>\n\t\t</dependency>\n\t\t<dependency>\n\t\t\t<groupId>org.apache.logging.log4j</groupId>\n\t\t\t<artifactId>log4j-core</artifactId>\n\t\t\t<version>2.8.2</version>\n\t\t</dependency>\n\t\t\x3c!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --\x3e\n\t\t<dependency>\n\t\t\t<groupId>org.apache.zookeeper</groupId>\n\t\t\t<artifactId>zookeeper</artifactId>\n\t\t\t<version>3.5.7</version>\n\t\t</dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nlog4j.properties\n\nlog4j.rootLogger=INFO, stdout  \nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender  \nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  \nlog4j.appender.logfile=org.apache.log4j.FileAppender  \nlog4j.appender.logfile.File=target/spring.log  \nlog4j.appender.logfile.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\ntest\n\npackage com.atguigu.zkclient;\n\nimport org.apache.zookeeper.*;\nimport org.apache.zookeeper.data.Stat;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.util.List;\n\npublic class ZkClient {\n    private ZooKeeper zooKeeper = null;\n\n    @Before\n    public void before() throws IOException {\n        String connectString = "hadoop102:2181,hadoop103:2181,hadoop104:2181";\n        int sessionTimeout = 2000;\n        //创建zk对象\n        zooKeeper = new ZooKeeper(connectString,  //连接地址\n                sessionTimeout,  //超时时间\n                new Watcher() {\n                    //Zookeeper监听的回调函数\n                    @Override\n                    public void process(WatchedEvent watchedEvent) {\n                        System.out.println("回调函数");\n                    }\n                }\n\n\n        );\n    }\n\n    @After\n    public void after() throws InterruptedException {\n        zooKeeper.close();\n    }\n\n    //创建新节点\n    @Test\n    public void create() throws IOException, InterruptedException, KeeperException {\n\n        zooKeeper.create(\n                "/testApi", //节点名\n                "haha".getBytes(), //值\n                ZooDefs.Ids.OPEN_ACL_UNSAFE,  //访问控制列表 相对应权限\n                CreateMode.PERSISTENT  //永久节点\n        );\n    }\n\n    //查询子节点\n    @Test\n    public void ls() throws InterruptedException, KeeperException {\n        List<String> children = zooKeeper.getChildren(\n                "/",\n                new Watcher() {\n                    @Override\n                    public void process(WatchedEvent watchedEvent) {\n                        System.out.println("自定义回调函数");\n                    }\n                }\n        );\n        for (String child : children) {\n            System.out.println(child);\n        }\n\n        Thread.sleep(Long.MAX_VALUE);\n    }\n\n    //查询节点的值\n    @Test\n    public void get() throws InterruptedException, KeeperException, IOException {\n        Stat stat = new Stat();  //节点状态(Stat结构体)\n        byte[] data = zooKeeper.getData("/testApi", false, stat);\n        System.out.write(data);\n        System.out.println();\n        System.out.println(stat.getMzxid());\n    }\n\n    //查询一个节点的状态\n    @Test\n    public void stat() throws InterruptedException, KeeperException {\n        Stat stat = zooKeeper.exists("/testApi", false);\n        if (stat == null) {\n            System.out.println("节点不存在");\n        } else {\n            System.out.println(stat);\n        }\n    }\n\n    //修改节点内容\n    @Test\n    public void set() throws InterruptedException, KeeperException {\n        String node = "/testApi";\n        int version = 0;\n        Stat stat = zooKeeper.exists(node, false);\n\n        if (stat == null) {\n            System.out.println("节点不存在");\n        } else {\n            version = stat.getVersion();\n        }\n        //乐观锁 修改前先比较版本号 不一致则报错\n        zooKeeper.setData(node,\n                "123".getBytes(StandardCharsets.UTF_8),\n                version\n        );\n    }\n\n    //删除节点\n    @Test\n    public void delete() throws InterruptedException, KeeperException {\n        zooKeeper.delete("/testApi",1);\n    }\n\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n',normalizedContent:'# api\n\n坐标\n\n<dependencies>\n\t\t<dependency>\n\t\t\t<groupid>junit</groupid>\n\t\t\t<artifactid>junit</artifactid>\n\t\t\t<version>release</version>\n\t\t</dependency>\n\t\t<dependency>\n\t\t\t<groupid>org.apache.logging.log4j</groupid>\n\t\t\t<artifactid>log4j-core</artifactid>\n\t\t\t<version>2.8.2</version>\n\t\t</dependency>\n\t\t\x3c!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --\x3e\n\t\t<dependency>\n\t\t\t<groupid>org.apache.zookeeper</groupid>\n\t\t\t<artifactid>zookeeper</artifactid>\n\t\t\t<version>3.5.7</version>\n\t\t</dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nlog4j.properties\n\nlog4j.rootlogger=info, stdout  \nlog4j.appender.stdout=org.apache.log4j.consoleappender  \nlog4j.appender.stdout.layout=org.apache.log4j.patternlayout  \nlog4j.appender.stdout.layout.conversionpattern=%d %p [%c] - %m%n  \nlog4j.appender.logfile=org.apache.log4j.fileappender  \nlog4j.appender.logfile.file=target/spring.log  \nlog4j.appender.logfile.layout=org.apache.log4j.patternlayout  \nlog4j.appender.logfile.layout.conversionpattern=%d %p [%c] - %m%n  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\ntest\n\npackage com.atguigu.zkclient;\n\nimport org.apache.zookeeper.*;\nimport org.apache.zookeeper.data.stat;\nimport org.junit.after;\nimport org.junit.before;\nimport org.junit.test;\n\nimport java.io.ioexception;\nimport java.nio.charset.standardcharsets;\nimport java.util.list;\n\npublic class zkclient {\n    private zookeeper zookeeper = null;\n\n    @before\n    public void before() throws ioexception {\n        string connectstring = "hadoop102:2181,hadoop103:2181,hadoop104:2181";\n        int sessiontimeout = 2000;\n        //创建zk对象\n        zookeeper = new zookeeper(connectstring,  //连接地址\n                sessiontimeout,  //超时时间\n                new watcher() {\n                    //zookeeper监听的回调函数\n                    @override\n                    public void process(watchedevent watchedevent) {\n                        system.out.println("回调函数");\n                    }\n                }\n\n\n        );\n    }\n\n    @after\n    public void after() throws interruptedexception {\n        zookeeper.close();\n    }\n\n    //创建新节点\n    @test\n    public void create() throws ioexception, interruptedexception, keeperexception {\n\n        zookeeper.create(\n                "/testapi", //节点名\n                "haha".getbytes(), //值\n                zoodefs.ids.open_acl_unsafe,  //访问控制列表 相对应权限\n                createmode.persistent  //永久节点\n        );\n    }\n\n    //查询子节点\n    @test\n    public void ls() throws interruptedexception, keeperexception {\n        list<string> children = zookeeper.getchildren(\n                "/",\n                new watcher() {\n                    @override\n                    public void process(watchedevent watchedevent) {\n                        system.out.println("自定义回调函数");\n                    }\n                }\n        );\n        for (string child : children) {\n            system.out.println(child);\n        }\n\n        thread.sleep(long.max_value);\n    }\n\n    //查询节点的值\n    @test\n    public void get() throws interruptedexception, keeperexception, ioexception {\n        stat stat = new stat();  //节点状态(stat结构体)\n        byte[] data = zookeeper.getdata("/testapi", false, stat);\n        system.out.write(data);\n        system.out.println();\n        system.out.println(stat.getmzxid());\n    }\n\n    //查询一个节点的状态\n    @test\n    public void stat() throws interruptedexception, keeperexception {\n        stat stat = zookeeper.exists("/testapi", false);\n        if (stat == null) {\n            system.out.println("节点不存在");\n        } else {\n            system.out.println(stat);\n        }\n    }\n\n    //修改节点内容\n    @test\n    public void set() throws interruptedexception, keeperexception {\n        string node = "/testapi";\n        int version = 0;\n        stat stat = zookeeper.exists(node, false);\n\n        if (stat == null) {\n            system.out.println("节点不存在");\n        } else {\n            version = stat.getversion();\n        }\n        //乐观锁 修改前先比较版本号 不一致则报错\n        zookeeper.setdata(node,\n                "123".getbytes(standardcharsets.utf_8),\n                version\n        );\n    }\n\n    //删除节点\n    @test\n    public void delete() throws interruptedexception, keeperexception {\n        zookeeper.delete("/testapi",1);\n    }\n\n\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"集群搭建",frontmatter:{title:"集群搭建",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/1597a2/",categories:["大数据","Zookeeper"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/02.Zookeeper/02.%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html",relativePath:"大数据/02.Zookeeper/02.集群搭建.md",key:"v-98591bee",path:"/pages/1597a2/",headersStr:null,content:"# 集群搭建\n\ntar -zxvf /opt/software/apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/\nmv /opt/module/apache-zookeeper-3.5.7-bin/ /opt/module/zookeeper\n\n#环境变量\nsudo vim /etc/profile.d/my_env.sh \n#追加\n#ZOOKEEPER_HOME\nexport ZOOKEEPER_HOME=/opt/module/zookeeper\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin\n\n\n#同步环境变量\nsource /etc/profile.d/my_env.sh \nsudo xsync /etc/profile.d/my_env.sh \n\n#将配置文件改名称为zoo.cfg\ncd /opt/module/zookeeper/conf/\ncp zoo_sample.cfg zoo.cfg\n\n#配置zookeeper文件\nvim zoo.cfg \n#追加下内容\nserver.2=hadoop102:2888:3888\nserver.3=hadoop103:2888:3888\nserver.4=hadoop104:2888:3888\n#修改数据存储位置\ndataDir=/opt/module/zookeeper/zkData\n\ncd /opt/module/zookeeper\nmkdir zkData\n\n#创建myid用于zookeeper标记机器\n#编辑为102的2用于唯一标识 103为3 104为4\necho 2 > /opt/module/zookeeper/zkData/myid\n\n#同步\nxsync /opt/module/zookeeper/\n\n#103\necho 3 > /opt/module/zookeeper/zkData/myid\n#104\necho 4 > /opt/module/zookeeper/zkData/myid\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n启动\n\n#在三台机器上分别启动\nzkServer.sh start\n\n\n1\n2\n\n\nzookeeper默认web端口为8080 和tomcat冲突 可以在配置文件同 admin.serverPort=8081 进行配置\n\nvim /opt/module/zookeeper/conf/zoo.cfg \n\n#web端口\nadmin.serverPort=8081\n\n\n1\n2\n3\n4\n",normalizedContent:"# 集群搭建\n\ntar -zxvf /opt/software/apache-zookeeper-3.5.7-bin.tar.gz -c /opt/module/\nmv /opt/module/apache-zookeeper-3.5.7-bin/ /opt/module/zookeeper\n\n#环境变量\nsudo vim /etc/profile.d/my_env.sh \n#追加\n#zookeeper_home\nexport zookeeper_home=/opt/module/zookeeper\nexport path=$path:$zookeeper_home/bin\n\n\n#同步环境变量\nsource /etc/profile.d/my_env.sh \nsudo xsync /etc/profile.d/my_env.sh \n\n#将配置文件改名称为zoo.cfg\ncd /opt/module/zookeeper/conf/\ncp zoo_sample.cfg zoo.cfg\n\n#配置zookeeper文件\nvim zoo.cfg \n#追加下内容\nserver.2=hadoop102:2888:3888\nserver.3=hadoop103:2888:3888\nserver.4=hadoop104:2888:3888\n#修改数据存储位置\ndatadir=/opt/module/zookeeper/zkdata\n\ncd /opt/module/zookeeper\nmkdir zkdata\n\n#创建myid用于zookeeper标记机器\n#编辑为102的2用于唯一标识 103为3 104为4\necho 2 > /opt/module/zookeeper/zkdata/myid\n\n#同步\nxsync /opt/module/zookeeper/\n\n#103\necho 3 > /opt/module/zookeeper/zkdata/myid\n#104\necho 4 > /opt/module/zookeeper/zkdata/myid\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n启动\n\n#在三台机器上分别启动\nzkserver.sh start\n\n\n1\n2\n\n\nzookeeper默认web端口为8080 和tomcat冲突 可以在配置文件同 admin.serverport=8081 进行配置\n\nvim /opt/module/zookeeper/conf/zoo.cfg \n\n#web端口\nadmin.serverport=8081\n\n\n1\n2\n3\n4\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"内部原理",frontmatter:{title:"内部原理",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/75493d/",categories:["大数据","Zookeeper"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/02.Zookeeper/04.%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86.html",relativePath:"大数据/02.Zookeeper/04.内部原理.md",key:"v-c1183f22",path:"/pages/75493d/",headers:[{level:2,title:"节点类型",slug:"节点类型",normalizedTitle:"节点类型",charIndex:11},{level:2,title:"Stat结构体",slug:"stat结构体",normalizedTitle:"stat结构体",charIndex:22},{level:2,title:"监听器原理",slug:"监听器原理",normalizedTitle:"监听器原理",charIndex:532},{level:2,title:"选举机制",slug:"选举机制",normalizedTitle:"选举机制",charIndex:544},{level:2,title:"写数据流程",slug:"写数据流程",normalizedTitle:"写数据流程",charIndex:1206}],headersStr:"节点类型 Stat结构体 监听器原理 选举机制 写数据流程",content:"# 内部原理\n\n\n# 节点类型\n\n\n\n\n# Stat结构体\n\n（1）czxid-创建节点的事务zxid\n\n每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。\n\n事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。\n\n（2）ctime - znode被创建的毫秒数(从1970年开始)\n\n（3）mzxid - znode最后更新的事务zxid\n\n（4）mtime - znode最后修改的毫秒数(从1970年开始)\n\n（5）pZxid-znode最后更新的子节点zxid\n\n（6）cversion - znode子节点变化号，znode子节点修改次数\n\n（7）dataversion - znode数据变化号\n\n（8）aclVersion - znode访问控制列表的变化号\n\n（9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。\n\n（10）dataLength- znode的数据长度\n\n（11）numChildren - znode子节点数量\n\n\n# 监听器原理\n\n\n\n\n# 选举机制\n\n（1）半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。\n\n（2）Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。\n\n（3）选举流程\n\n\n\n( 1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING；\n\n（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING\n\n（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；\n\n（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；\n\n（5）服务器5启动，同4一样当小弟。\n\n\n# 写数据流程\n\n\n\n 1. 客户向服务端发起请求\n 2. 请求转发给Leader\n 3. Leader将写请求广播给各个server(即Follower) 进行投票\n 4. 投票超半数 则进行Leader会向各个server发送提交信息 各个server收到写请求放入队列中 写入数据\n    1. 如果某个server投不同意 而超过半数投同意票 则此server会进行重启 并向leader重新同步数据\n    2. 如果server 的 czxid 比 leader当前写数据的czxid大 则此server会投不同意票 反之投同意票\n 5. server会进一步通知客户端 写数据成功",normalizedContent:"# 内部原理\n\n\n# 节点类型\n\n\n\n\n# stat结构体\n\n（1）czxid-创建节点的事务zxid\n\n每次修改zookeeper状态都会收到一个zxid形式的时间戳，也就是zookeeper事务id。\n\n事务id是zookeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。\n\n（2）ctime - znode被创建的毫秒数(从1970年开始)\n\n（3）mzxid - znode最后更新的事务zxid\n\n（4）mtime - znode最后修改的毫秒数(从1970年开始)\n\n（5）pzxid-znode最后更新的子节点zxid\n\n（6）cversion - znode子节点变化号，znode子节点修改次数\n\n（7）dataversion - znode数据变化号\n\n（8）aclversion - znode访问控制列表的变化号\n\n（9）ephemeralowner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。\n\n（10）datalength- znode的数据长度\n\n（11）numchildren - znode子节点数量\n\n\n# 监听器原理\n\n\n\n\n# 选举机制\n\n（1）半数机制：集群中半数以上机器存活，集群可用。所以zookeeper适合安装奇数台服务器。\n\n（2）zookeeper虽然在配置文件中并没有指定master和slave。但是，zookeeper工作时，是有一个节点为leader，其他则为follower，leader是通过内部的选举机制临时产生的。\n\n（3）选举流程\n\n\n\n( 1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为looking；\n\n（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的id比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持looking\n\n（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选leader。服务器1，2更改状态为following，服务器3更改状态为leading；\n\n（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是looking状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为following；\n\n（5）服务器5启动，同4一样当小弟。\n\n\n# 写数据流程\n\n\n\n 1. 客户向服务端发起请求\n 2. 请求转发给leader\n 3. leader将写请求广播给各个server(即follower) 进行投票\n 4. 投票超半数 则进行leader会向各个server发送提交信息 各个server收到写请求放入队列中 写入数据\n    1. 如果某个server投不同意 而超过半数投同意票 则此server会进行重启 并向leader重新同步数据\n    2. 如果server 的 czxid 比 leader当前写数据的czxid大 则此server会投不同意票 反之投同意票\n 5. server会进一步通知客户端 写数据成功",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"环境",frontmatter:{title:"环境",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/66f25c/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/02.%E7%8E%AF%E5%A2%83.html",relativePath:"大数据/03.Hive/02.环境.md",key:"v-b7e4068e",path:"/pages/66f25c/",headers:[{level:2,title:"查询mysql版本",slug:"查询mysql版本",normalizedTitle:"查询mysql版本",charIndex:9},{level:2,title:"安装hive",slug:"安装hive",normalizedTitle:"安装hive",charIndex:1357},{level:2,title:"Hive的元数据配置到mysql",slug:"hive的元数据配置到mysql",normalizedTitle:"hive的元数据配置到mysql",charIndex:1749},{level:2,title:"Tez引擎",slug:"tez引擎",normalizedTitle:"tez引擎",charIndex:3537},{level:2,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:884},{level:2,title:"更换Tez引擎后，执行任务卡住",slug:"更换tez引擎后-执行任务卡住",normalizedTitle:"更换tez引擎后，执行任务卡住",charIndex:7847},{level:2,title:"遍历删除yarn中所有app",slug:"遍历删除yarn中所有app",normalizedTitle:"遍历删除yarn中所有app",charIndex:8375},{level:2,title:"Tez UI",slug:"tez-ui",normalizedTitle:"tez ui",charIndex:8706}],headersStr:"查询mysql版本 安装hive Hive的元数据配置到mysql Tez引擎 启动 更换Tez引擎后，执行任务卡住 遍历删除yarn中所有app Tez UI",content:'# 环境\n\n\n# 查询mysql版本\n\nrpm -qa | grep -i -E mysql\\|mariadb \n\n\n1\n\n\n卸载\n\nrpm -qa | grep -i -E mysql\\|mariadb | xargs -n1 sudo rpm -e --nodeps\n\n\n1\n\n\n安装\n\nsudo rpm -ivh 01_mysql-community-common-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 02_mysql-community-libs-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 03_mysql-community-libs-compat-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 04_mysql-community-client-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 05_mysql-community-server-5.7.29-1.el7.x86_64.rpm\nsudo systemctl start mysqld\n\n\n1\n2\n3\n4\n5\n6\n\n\n6.5安装\n\nwget https://cdn.mysql.com//archives/mysql-5.7/mysql-5.7.14-1.el6.x86_64.rpm-bundle.tar\nrpm -ivh mysql-community-common-5.7.14-1.el6.x86_64.rpm\nrpm -ivh mysql-community-libs-5.7.14-1.el6.x86_64.rpm\nrpm -ivh mysql-community-client-5.7.14-1.el6.x86_64.rpm\nrpm -ivh mysql-community-server-5.7.14-1.el6.x86_64.rpm\n\nservice mysqld start\nchkconfig mysqld on       #设置开机自启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果不行则 在后面加上 --nodeps --force 安装时不再分析包之间的依赖关系而直接安装\n\n查询mysql密码\n\nsudo cat /var/log/mysqld.log | grep password\n\n\n1\n\n\n修改mysql密码\n\n mysql -uroot -p’password’\n\n\n1\n\n\n设置复杂密码(由于mysql密码策略，此密码必须足够复杂)\n\nset global validate_password_length=4;\nset global validate_password_policy=0;\n\n\n1\n2\n\n\n设置密码\n\nset password=password("A373213257s");\n\n\n1\n\n\nuse mysql\nselect user, host from user;\nupdate user set host="%" where user="root";  \nflush privileges;\nquit;\n\n\n1\n2\n3\n4\n5\n\n\n\n# 安装hive\n\ntar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/\nmv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive\n#环境变量\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n2\n3\n4\n\n\n#HIVE_HOME\nexport HIVE_HOME=/opt/module/hive\nexport PATH=$PATH:$HIVE_HOME/bin\n\n\n1\n2\n3\n\n\nsource /etc/profile\nmv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.bak\n\n\n1\n2\n\n\n\n# Hive的元数据配置到mysql\n\ncp /opt/software/mysql-connector-java-5.1.48.jar $HIVE_HOME/lib\nvim $HIVE_HOME/conf/hive-site.xml\n\n\n1\n2\n\n\n配置hive-site.xml\n\n<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        \x3c!-- mysql 连接地址--\x3e\n        <name>javax.jdo.option.ConnectionURL</name>\n        <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>\n    </property>\n\n    <property>\n         \x3c!--驱动 --\x3e\n        <name>javax.jdo.option.ConnectionDriverName</name>\n        <value>com.mysql.jdbc.Driver</value>\n    </property>\n\n    <property>\n         \x3c!--用户 --\x3e\n        <name>javax.jdo.option.ConnectionUserName</name>\n        <value>root</value>\n    </property>\n\n    <property>\n         \x3c!--密码 --\x3e\n        <name>javax.jdo.option.ConnectionPassword</name>\n        <value>A373213257s</value>\n    </property>\n\n    <property>\n         \x3c!--hive元数据仓库位置 --\x3e\n        <name>hive.metastore.warehouse.dir</name>\n        <value>/user/hive/warehouse</value>\n    </property>\n\n    <property>\n        <name>hive.metastore.schema.verification</name>\n        <value>false</value>\n    </property>\n\n    <property>\n        <name>hive.metastore.uris</name>\n        <value>thrift://hadoop102:9083</value>\n    </property>\n\n    <property>\n    <name>hive.server2.thrift.port</name>\n    <value>10000</value>\n    </property>\n\n    <property>\n        <name>hive.server2.thrift.bind.host</name>\n        <value>hadoop102</value>\n    </property>\n\n    <property>\n        <name>hive.metastore.event.db.notification.api.auth</name>\n        <value>false</value>\n    </property>\n\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n\n\n\n# Tez引擎\n\nTez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能\n\n#解压\nmkdir /opt/module/tez\ntar -zxvf /opt/software/tez-0.10.1-SNAPSHOT.tar.gz -C /opt/module/tez\n\n#上传Tez到hdfs\nhadoop fs -mkdir /tez\nhadoop fs -put /opt/software/tez-0.10.1-SNAPSHOT.tar.gz /tez\n\n#新建tez-size.xml\nvim $HADOOP_HOME/etc/hadoop/tez-site.xml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n配置tez-size.xml\n\n<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n<property>\n\t<name>tez.lib.uris</name>\n    <value>${fs.defaultFS}/tez/tez-0.10.1-SNAPSHOT.tar.gz</value>\n</property>\n<property>\n     <name>tez.use.cluster.hadoop-libs</name>\n     <value>true</value>\n</property>\n<property>\n     <name>tez.am.resource.memory.mb</name>\n     <value>1024</value>\n</property>\n<property>\n     <name>tez.am.resource.cpu.vcores</name>\n     <value>1</value>\n</property>\n    \x3c!--task\\AM占用JVM Xmx的比例，该参数建议调整，需根据具体业务情况修改 默认0.8 --\x3e\n<property>\n     <name>tez.container.max.java.heap.fraction</name>\n     <value>0.8</value>\n</property>\n<property>\n     <name>tez.task.resource.memory.mb</name>\n     <value>1024</value>\n</property>\n<property>\n     <name>tez.task.resource.cpu.vcores</name>\n     <value>1</value>\n</property>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n修改Hadoop环境变量\n\nvim $HADOOP_HOME/etc/hadoop/shellprofile.d/tez.sh\n\n\n1\n\n\nhadoop_add_profile tez\nfunction _tez_hadoop_classpath\n{\n    hadoop_add_classpath "$HADOOP_HOME/etc/hadoop" after\n    hadoop_add_classpath "/opt/module/tez/*" after\n    hadoop_add_classpath "/opt/module/tez/lib/*" after\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n修改Hive的计算模型\n\nvim $HIVE_HOME/conf/hive-site.xml\n\n\n1\n\n\n追加\n\n<property>\n    <name>hive.execution.engine</name>\n    <value>tez</value>\n</property>\n<property>\n    <name>hive.tez.container.size</name>\n    <value>1024</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n解决日志jar包冲突\n\nrm /opt/module/tez/lib/slf4j-log4j12-1.7.10.jar\n\n\n1\n\n\n\n# 启动\n\nmysql -uroot -pA373213257s\n\n#建库\ncreate database metastore;\nquit;\n\n#初始化hive\nschematool -initSchema -dbType mysql -verbose\n\n#编写启动脚本\nvim $HIVE_HOME/bin/hiveservices.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n#!/bin/bash\nHIVE_LOG_DIR=$HIVE_HOME/logs\n\nmkdir -p $HIVE_LOG_DIR\n\n#检查进程是否运行正常，参数1为进程名，参数2为进程端口\nfunction check_process()\n{\n    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk \'{print $2}\')\n    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk \'{print $7}\' | cut -d \'/\' -f 1)\n    echo $pid\n    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1\n}\n\nfunction hive_start()\n{\n    metapid=$(check_process HiveMetastore 9083)\n    cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"\n    cmd=$cmd" sleep4; hdfs dfsadmin -safemode wait >/dev/null 2>&1"\n    [ -z "$metapid" ] && eval $cmd || echo "Metastroe服务已启动"\n    server2pid=$(check_process HiveServer2 10000)\n    cmd="nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"\n    [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2服务已启动"\n}\n\nfunction hive_stop()\n{\n    metapid=$(check_process HiveMetastore 9083)\n    [ "$metapid" ] && kill $metapid || echo "Metastore服务未启动"\n    server2pid=$(check_process HiveServer2 10000)\n    [ "$server2pid" ] && kill $server2pid || echo "HiveServer2服务未启动"\n}\n\ncase $1 in\n"start")\n    hive_start\n    ;;\n"stop")\n    hive_stop\n    ;;\n"restart")\n    hive_stop\n    sleep 2\n    hive_start\n    ;;\n"status")\n    check_process HiveMetastore 9083 >/dev/null && echo "Metastore服务运行正常" || echo "Metastore服务运行异常"\n    check_process HiveServer2 10000 >/dev/null && echo "HiveServer2服务运行正常" || echo "HiveServer2服务运行异常"\n    ;;\n*)\n    echo Invalid Args!\n    echo \'Usage: \'$(basename $0)\' start|stop|restart|status\'\n    ;;\nesac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n修改日志配置\n\nvim /opt/module/hive/conf/hive-log4j2.properties.template\n#修改以下属性的值\nproperty.hive.log.dir = /opt/module/hive/logs\n\n\n1\n2\n3\n\n\n启动脚本\n\nchmod +x $HIVE_HOME/bin/hiveservices.sh\nhiveservices.sh start\n\njps\n#如果看到2个RunJar则成功\n\n\n1\n2\n3\n4\n5\n\n\n以官方命令行客户端 beeline 启动HiveJDBC\n\n在哪台机器启动服务就以哪台服务器ip为准\n\nbeeline -u jdbc:hive2://hadoop102:10000 -n atguigu\n\n\n1\n\n\nHive 2.x以上版本，要先启动这两个服务，否则会报错\n\n如果不以脚本方式启动必须先启动 hive服务端元数据服务 否则无法打开hive客户端\n\nhive --service metastore &\nhive\n\n\n1\n2\n\n\n\n# 更换Tez引擎后，执行任务卡住\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml\n\n\n1\n\n\n<property>\n    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\n    \x3c!-- 更改为1  --\x3e\n    <value>1</value>\n    <description>\n      Maximum percent of resources in the cluster which can be used to run \n      application masters i.e. controls number of concurrent running\n      applications.\n    </description>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nsudo xsync /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml #同步\n\n\n1\n\n\n\n# 遍历删除yarn中所有app\n\nfor i in  `yarn application  -list | grep -w  ACCEPTED | awk \'{print $1}\' | grep application_`; do yarn  application -kill $i; done\nhadoop job -list #任务列表\nyarn application -list # yarn的app\nyarn node -list all # 查看所有节点\nhdfs dfsadmin -safemode get # 查看是否是安全模式\nhadoop dfsadmin -report  #查看每个节点的具体\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n# Tez UI\n\n以下操作均在103节点中操作\n\ntez-site.xml\n\n追加\n\n<property>\n  <description>Enable Tez to use the Timeline Server for History Logging</description>\n  <name>tez.history.logging.service.class</name>\n  <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>\n</property>\n\n<property>\n  <description>URL for where the Tez UI is hosted</description>\n  <name>tez.tez-ui.history-url.base</name>\n  <value>http://hadoop103/tez-ui/</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nyarn-site.xml\n\n追加\n\n<property>\n  <description>Indicate to clients whether Timeline service is enabled or not.\n  If enabled, the TimelineClient library used by end-users will post entities\n  and events to the Timeline server.</description>\n  <name>yarn.timeline-service.enabled</name>\n  <value>true</value>\n</property>\n\n<property>\n  <description>The hostname of the Timeline service web application.</description>\n  <name>yarn.timeline-service.hostname</name>\n  <value>hadoop103</value>\n</property>\n\n<property>\n  <description>Enables cross-origin support (CORS) for web services where\n  cross-origin web response headers are needed. For example, javascript making\n  a web services request to the timeline server.</description>\n  <name>yarn.timeline-service.http-cross-origin.enabled</name>\n  <value>true</value>\n</property>\n\n<property>\n  <description>Publish YARN information to Timeline Server</description>\n  <name> yarn.resourcemanager.system-metrics-publisher.enabled</name>\n  <value>true</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n启用tomcat服务器\n\nsudo yum install -y tomcat\nsudo groupmems -g tomcat -a atguigu # 将atguigu添加到,重新连接shell\n#rpm -qa | grep java- | xargs -n sudo rpm -e --nodeps # 删除tomcat自带的java\n\n\n\n1\n2\n3\n4\n\n\n下载tez版本对应的war包并解压\n\nhttps://repository.apache.org/content/repositories/releases/org/apache/tez/tez-ui/\n\nsudo yum -y install unzip\ncd /var/lib/tomcat/wabapps #上传到此目录\n\nmkdir tez-ui\nuzip tez-ui-0.10.1.war -d tez-ui #解压 到tocat中\nrm tez-ui-0.10.1.war\n\ncd tez-ui/config\nvim configs.env\n\n#将里面的web地址 全部改成hadoop103\nsudo systemctl start tomcat #启动服务\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n重启yarn,并启动timeline server\n\nstop-yarn.sh\nstart-yarn.sh\nyarn --daemon start timelineserver #启动timeline server服务\n\n\n1\n2\n3\n',normalizedContent:'# 环境\n\n\n# 查询mysql版本\n\nrpm -qa | grep -i -e mysql\\|mariadb \n\n\n1\n\n\n卸载\n\nrpm -qa | grep -i -e mysql\\|mariadb | xargs -n1 sudo rpm -e --nodeps\n\n\n1\n\n\n安装\n\nsudo rpm -ivh 01_mysql-community-common-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 02_mysql-community-libs-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 03_mysql-community-libs-compat-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 04_mysql-community-client-5.7.29-1.el7.x86_64.rpm\nsudo rpm -ivh 05_mysql-community-server-5.7.29-1.el7.x86_64.rpm\nsudo systemctl start mysqld\n\n\n1\n2\n3\n4\n5\n6\n\n\n6.5安装\n\nwget https://cdn.mysql.com//archives/mysql-5.7/mysql-5.7.14-1.el6.x86_64.rpm-bundle.tar\nrpm -ivh mysql-community-common-5.7.14-1.el6.x86_64.rpm\nrpm -ivh mysql-community-libs-5.7.14-1.el6.x86_64.rpm\nrpm -ivh mysql-community-client-5.7.14-1.el6.x86_64.rpm\nrpm -ivh mysql-community-server-5.7.14-1.el6.x86_64.rpm\n\nservice mysqld start\nchkconfig mysqld on       #设置开机自启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n如果不行则 在后面加上 --nodeps --force 安装时不再分析包之间的依赖关系而直接安装\n\n查询mysql密码\n\nsudo cat /var/log/mysqld.log | grep password\n\n\n1\n\n\n修改mysql密码\n\n mysql -uroot -p’password’\n\n\n1\n\n\n设置复杂密码(由于mysql密码策略，此密码必须足够复杂)\n\nset global validate_password_length=4;\nset global validate_password_policy=0;\n\n\n1\n2\n\n\n设置密码\n\nset password=password("a373213257s");\n\n\n1\n\n\nuse mysql\nselect user, host from user;\nupdate user set host="%" where user="root";  \nflush privileges;\nquit;\n\n\n1\n2\n3\n4\n5\n\n\n\n# 安装hive\n\ntar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -c /opt/module/\nmv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive\n#环境变量\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n2\n3\n4\n\n\n#hive_home\nexport hive_home=/opt/module/hive\nexport path=$path:$hive_home/bin\n\n\n1\n2\n3\n\n\nsource /etc/profile\nmv $hive_home/lib/log4j-slf4j-impl-2.10.0.jar $hive_home/lib/log4j-slf4j-impl-2.10.0.bak\n\n\n1\n2\n\n\n\n# hive的元数据配置到mysql\n\ncp /opt/software/mysql-connector-java-5.1.48.jar $hive_home/lib\nvim $hive_home/conf/hive-site.xml\n\n\n1\n2\n\n\n配置hive-site.xml\n\n<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        \x3c!-- mysql 连接地址--\x3e\n        <name>javax.jdo.option.connectionurl</name>\n        <value>jdbc:mysql://hadoop102:3306/metastore?usessl=false</value>\n    </property>\n\n    <property>\n         \x3c!--驱动 --\x3e\n        <name>javax.jdo.option.connectiondrivername</name>\n        <value>com.mysql.jdbc.driver</value>\n    </property>\n\n    <property>\n         \x3c!--用户 --\x3e\n        <name>javax.jdo.option.connectionusername</name>\n        <value>root</value>\n    </property>\n\n    <property>\n         \x3c!--密码 --\x3e\n        <name>javax.jdo.option.connectionpassword</name>\n        <value>a373213257s</value>\n    </property>\n\n    <property>\n         \x3c!--hive元数据仓库位置 --\x3e\n        <name>hive.metastore.warehouse.dir</name>\n        <value>/user/hive/warehouse</value>\n    </property>\n\n    <property>\n        <name>hive.metastore.schema.verification</name>\n        <value>false</value>\n    </property>\n\n    <property>\n        <name>hive.metastore.uris</name>\n        <value>thrift://hadoop102:9083</value>\n    </property>\n\n    <property>\n    <name>hive.server2.thrift.port</name>\n    <value>10000</value>\n    </property>\n\n    <property>\n        <name>hive.server2.thrift.bind.host</name>\n        <value>hadoop102</value>\n    </property>\n\n    <property>\n        <name>hive.metastore.event.db.notification.api.auth</name>\n        <value>false</value>\n    </property>\n\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n\n\n\n# tez引擎\n\ntez可以将多个有依赖的作业转换为一个作业，这样只需写一次hdfs，且中间节点较少，从而大大提升作业的计算性能\n\n#解压\nmkdir /opt/module/tez\ntar -zxvf /opt/software/tez-0.10.1-snapshot.tar.gz -c /opt/module/tez\n\n#上传tez到hdfs\nhadoop fs -mkdir /tez\nhadoop fs -put /opt/software/tez-0.10.1-snapshot.tar.gz /tez\n\n#新建tez-size.xml\nvim $hadoop_home/etc/hadoop/tez-site.xml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n配置tez-size.xml\n\n<?xml version="1.0" encoding="utf-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n<property>\n\t<name>tez.lib.uris</name>\n    <value>${fs.defaultfs}/tez/tez-0.10.1-snapshot.tar.gz</value>\n</property>\n<property>\n     <name>tez.use.cluster.hadoop-libs</name>\n     <value>true</value>\n</property>\n<property>\n     <name>tez.am.resource.memory.mb</name>\n     <value>1024</value>\n</property>\n<property>\n     <name>tez.am.resource.cpu.vcores</name>\n     <value>1</value>\n</property>\n    \x3c!--task\\am占用jvm xmx的比例，该参数建议调整，需根据具体业务情况修改 默认0.8 --\x3e\n<property>\n     <name>tez.container.max.java.heap.fraction</name>\n     <value>0.8</value>\n</property>\n<property>\n     <name>tez.task.resource.memory.mb</name>\n     <value>1024</value>\n</property>\n<property>\n     <name>tez.task.resource.cpu.vcores</name>\n     <value>1</value>\n</property>\n</configuration>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n修改hadoop环境变量\n\nvim $hadoop_home/etc/hadoop/shellprofile.d/tez.sh\n\n\n1\n\n\nhadoop_add_profile tez\nfunction _tez_hadoop_classpath\n{\n    hadoop_add_classpath "$hadoop_home/etc/hadoop" after\n    hadoop_add_classpath "/opt/module/tez/*" after\n    hadoop_add_classpath "/opt/module/tez/lib/*" after\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n修改hive的计算模型\n\nvim $hive_home/conf/hive-site.xml\n\n\n1\n\n\n追加\n\n<property>\n    <name>hive.execution.engine</name>\n    <value>tez</value>\n</property>\n<property>\n    <name>hive.tez.container.size</name>\n    <value>1024</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n解决日志jar包冲突\n\nrm /opt/module/tez/lib/slf4j-log4j12-1.7.10.jar\n\n\n1\n\n\n\n# 启动\n\nmysql -uroot -pa373213257s\n\n#建库\ncreate database metastore;\nquit;\n\n#初始化hive\nschematool -initschema -dbtype mysql -verbose\n\n#编写启动脚本\nvim $hive_home/bin/hiveservices.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n#!/bin/bash\nhive_log_dir=$hive_home/logs\n\nmkdir -p $hive_log_dir\n\n#检查进程是否运行正常，参数1为进程名，参数2为进程端口\nfunction check_process()\n{\n    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk \'{print $2}\')\n    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk \'{print $7}\' | cut -d \'/\' -f 1)\n    echo $pid\n    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1\n}\n\nfunction hive_start()\n{\n    metapid=$(check_process hivemetastore 9083)\n    cmd="nohup hive --service metastore >$hive_log_dir/metastore.log 2>&1 &"\n    cmd=$cmd" sleep4; hdfs dfsadmin -safemode wait >/dev/null 2>&1"\n    [ -z "$metapid" ] && eval $cmd || echo "metastroe服务已启动"\n    server2pid=$(check_process hiveserver2 10000)\n    cmd="nohup hive --service hiveserver2 >$hive_log_dir/hiveserver2.log 2>&1 &"\n    [ -z "$server2pid" ] && eval $cmd || echo "hiveserver2服务已启动"\n}\n\nfunction hive_stop()\n{\n    metapid=$(check_process hivemetastore 9083)\n    [ "$metapid" ] && kill $metapid || echo "metastore服务未启动"\n    server2pid=$(check_process hiveserver2 10000)\n    [ "$server2pid" ] && kill $server2pid || echo "hiveserver2服务未启动"\n}\n\ncase $1 in\n"start")\n    hive_start\n    ;;\n"stop")\n    hive_stop\n    ;;\n"restart")\n    hive_stop\n    sleep 2\n    hive_start\n    ;;\n"status")\n    check_process hivemetastore 9083 >/dev/null && echo "metastore服务运行正常" || echo "metastore服务运行异常"\n    check_process hiveserver2 10000 >/dev/null && echo "hiveserver2服务运行正常" || echo "hiveserver2服务运行异常"\n    ;;\n*)\n    echo invalid args!\n    echo \'usage: \'$(basename $0)\' start|stop|restart|status\'\n    ;;\nesac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n修改日志配置\n\nvim /opt/module/hive/conf/hive-log4j2.properties.template\n#修改以下属性的值\nproperty.hive.log.dir = /opt/module/hive/logs\n\n\n1\n2\n3\n\n\n启动脚本\n\nchmod +x $hive_home/bin/hiveservices.sh\nhiveservices.sh start\n\njps\n#如果看到2个runjar则成功\n\n\n1\n2\n3\n4\n5\n\n\n以官方命令行客户端 beeline 启动hivejdbc\n\n在哪台机器启动服务就以哪台服务器ip为准\n\nbeeline -u jdbc:hive2://hadoop102:10000 -n atguigu\n\n\n1\n\n\nhive 2.x以上版本，要先启动这两个服务，否则会报错\n\n如果不以脚本方式启动必须先启动 hive服务端元数据服务 否则无法打开hive客户端\n\nhive --service metastore &\nhive\n\n\n1\n2\n\n\n\n# 更换tez引擎后，执行任务卡住\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml\n\n\n1\n\n\n<property>\n    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\n    \x3c!-- 更改为1  --\x3e\n    <value>1</value>\n    <description>\n      maximum percent of resources in the cluster which can be used to run \n      application masters i.e. controls number of concurrent running\n      applications.\n    </description>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nsudo xsync /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml #同步\n\n\n1\n\n\n\n# 遍历删除yarn中所有app\n\nfor i in  `yarn application  -list | grep -w  accepted | awk \'{print $1}\' | grep application_`; do yarn  application -kill $i; done\nhadoop job -list #任务列表\nyarn application -list # yarn的app\nyarn node -list all # 查看所有节点\nhdfs dfsadmin -safemode get # 查看是否是安全模式\nhadoop dfsadmin -report  #查看每个节点的具体\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n# tez ui\n\n以下操作均在103节点中操作\n\ntez-site.xml\n\n追加\n\n<property>\n  <description>enable tez to use the timeline server for history logging</description>\n  <name>tez.history.logging.service.class</name>\n  <value>org.apache.tez.dag.history.logging.ats.atshistoryloggingservice</value>\n</property>\n\n<property>\n  <description>url for where the tez ui is hosted</description>\n  <name>tez.tez-ui.history-url.base</name>\n  <value>http://hadoop103/tez-ui/</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nyarn-site.xml\n\n追加\n\n<property>\n  <description>indicate to clients whether timeline service is enabled or not.\n  if enabled, the timelineclient library used by end-users will post entities\n  and events to the timeline server.</description>\n  <name>yarn.timeline-service.enabled</name>\n  <value>true</value>\n</property>\n\n<property>\n  <description>the hostname of the timeline service web application.</description>\n  <name>yarn.timeline-service.hostname</name>\n  <value>hadoop103</value>\n</property>\n\n<property>\n  <description>enables cross-origin support (cors) for web services where\n  cross-origin web response headers are needed. for example, javascript making\n  a web services request to the timeline server.</description>\n  <name>yarn.timeline-service.http-cross-origin.enabled</name>\n  <value>true</value>\n</property>\n\n<property>\n  <description>publish yarn information to timeline server</description>\n  <name> yarn.resourcemanager.system-metrics-publisher.enabled</name>\n  <value>true</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n启用tomcat服务器\n\nsudo yum install -y tomcat\nsudo groupmems -g tomcat -a atguigu # 将atguigu添加到,重新连接shell\n#rpm -qa | grep java- | xargs -n sudo rpm -e --nodeps # 删除tomcat自带的java\n\n\n\n1\n2\n3\n4\n\n\n下载tez版本对应的war包并解压\n\nhttps://repository.apache.org/content/repositories/releases/org/apache/tez/tez-ui/\n\nsudo yum -y install unzip\ncd /var/lib/tomcat/wabapps #上传到此目录\n\nmkdir tez-ui\nuzip tez-ui-0.10.1.war -d tez-ui #解压 到tocat中\nrm tez-ui-0.10.1.war\n\ncd tez-ui/config\nvim configs.env\n\n#将里面的web地址 全部改成hadoop103\nsudo systemctl start tomcat #启动服务\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n重启yarn,并启动timeline server\n\nstop-yarn.sh\nstart-yarn.sh\nyarn --daemon start timelineserver #启动timeline server服务\n\n\n1\n2\n3\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"客户端命令行操作",frontmatter:{title:"客户端命令行操作",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/be648b/",categories:["大数据","Zookeeper"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/02.Zookeeper/03.%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C.html",relativePath:"大数据/02.Zookeeper/03.客户端命令行操作.md",key:"v-051079fa",path:"/pages/be648b/",headersStr:null,content:"# 客户端命令行操作\n\n命令基本语法      功能描述\nhelp        显示所有操作命令\nls path     使用 ls 命令来查看当前znode的子节点 -w 监听子节点变化 -s 附加次级信息\ncreate      普通创建 -s 含有序列 -e 临时（重启或者超时消失） -es 临时有序节点\nget path    获得节点的值 -w 监听节点内容变化 -s 附加次级信息\nset         设置节点的具体值\nstat        查看节点状态\ndelete      删除节点\ndeleteall   递归删除节点\n\n启动客户端\n\nzkCli.sh #启动\n\n\n1\n\n\n命令\n\nls -w / #监听指定路径 发生变化zookeeper会通知  只会通知一次 第二次变化不会通知\nls -s / #查看节点的详细信息\n\n\n1\n2\n",normalizedContent:"# 客户端命令行操作\n\n命令基本语法      功能描述\nhelp        显示所有操作命令\nls path     使用 ls 命令来查看当前znode的子节点 -w 监听子节点变化 -s 附加次级信息\ncreate      普通创建 -s 含有序列 -e 临时（重启或者超时消失） -es 临时有序节点\nget path    获得节点的值 -w 监听节点内容变化 -s 附加次级信息\nset         设置节点的具体值\nstat        查看节点状态\ndelete      删除节点\ndeleteall   递归删除节点\n\n启动客户端\n\nzkcli.sh #启动\n\n\n1\n\n\n命令\n\nls -w / #监听指定路径 发生变化zookeeper会通知  只会通知一次 第二次变化不会通知\nls -s / #查看节点的详细信息\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"介绍",frontmatter:{title:"介绍",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/2315d6/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/01.%E4%BB%8B%E7%BB%8D.html",relativePath:"大数据/03.Hive/01.介绍.md",key:"v-00667d92",path:"/pages/2315d6/",headersStr:null,content:"# 介绍\n\nHive 是由 Facebook 开源用于解决海量结构化日志的数据计工具\n\nHive是基于Hadoop 的一个数据仓库工具 可以将结构化的数据文件映射为一张表 并提供类SQL 查询功能",normalizedContent:"# 介绍\n\nhive 是由 facebook 开源用于解决海量结构化日志的数据计工具\n\nhive是基于hadoop 的一个数据仓库工具 可以将结构化的数据文件映射为一张表 并提供类sql 查询功能",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hive 类型",frontmatter:{title:"Hive 类型",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/f55408/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/04.Hive%20%E7%B1%BB%E5%9E%8B.html",relativePath:"大数据/03.Hive/04.Hive 类型.md",key:"v-752ff4e6",path:"/pages/f55408/",headers:[{level:2,title:"上传文件到表数据中",slug:"上传文件到表数据中",normalizedTitle:"上传文件到表数据中",charIndex:2028},{level:3,title:"从网页端上传",slug:"从网页端上传",normalizedTitle:"从网页端上传",charIndex:2042},{level:3,title:"从系统中上传到表",slug:"从系统中上传到表",normalizedTitle:"从系统中上传到表",charIndex:2234},{level:2,title:"常用类型查询",slug:"常用类型查询",normalizedTitle:"常用类型查询",charIndex:2447},{level:2,title:"类型转化",slug:"类型转化",normalizedTitle:"类型转化",charIndex:2554}],headersStr:"上传文件到表数据中 从网页端上传 从系统中上传到表 常用类型查询 类型转化",content:"# Hive 类型\n\nHIVE数据类型    JAVA数据类型   长度                           例子\nTINYINT     byte       1byte有符号整数                   20\nSMALINT     short      2byte有符号整数                   20\nINT         int        4byte有符号整数                   20\nBIGINT      long       8byte有符号整数                   20\nBOOLEAN     boolean    布尔类型，true或者false             TRUE FALSE\nFLOAT       float      单精度浮点数                       3.14159\nDOUBLE      double     双精度浮点数                       3.14159\nSTRING      string     字符系列。可以指定字符集。可以使用单引号或者双引号。   ‘now is the time’ “for all good men”\nTIMESTAMP              时间类型                         \nBINARY                 字节数组                         \n\n数据类型     描述                                                                                                               语法示例\nSTRUCT   和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first                                                       struct() 例如struct<street:string, city:string>\n         STRING, last STRING},那么第1个元素可以通过字段.first来引用。\nMAP      MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键->值对是’first’->’John’和’last’->’Doe’，那么可以通过字段名[‘last’]获取最后一个元素   map() 例如map<string, int>\nARRAY    数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,                                             Array() 例如array<string>\n         ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。\n\nSTRUCT为结构体 为一个只有成员变量的类\n\nclass Peopel{\n    int age;\n    String name;\n}\nPeopel p \np.age = 10\np.name = 15\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n创建一个表\n\ncreate table test(\nname string,\nfriends array<string>,\nchildren map<string, int>,\naddress struct<street:string, city:string>\n)\nrow format delimited fields terminated by ','   -- 列分隔符\ncollection items terminated by '_'   -- MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)\nmap keys terminated by ':';  -- MAP中的key与value的分隔符\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n * row format delimited fields terminated by ',' -- 列分隔符\n\n * collection items terminated by '_' --MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)\n\n * map keys terminated by ':' -- MAP中的key与value的分隔符\n\n * lines terminated by '\\n'; -- 行分隔符\n\n\n# 上传文件到表数据中\n\n\n# 从网页端上传\n\n\n\n每个文件夹为一个表 直接把文本上传到文件夹(表)中\n\nsongsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing\nyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing\n\n\n1\n2\n\n\n\n# 从系统中上传到表\n\ncd /opt/module/\nmkdir datas\ncd datas\n\n\n1\n2\n3\n\n\nl在hive命令行总 load: data local inpath '本地文本路径' into table 表名;\n\n从本地加载表\n\n#在hive中\nload data local inpath '/opt/module/datas/test.txt' into table test;\n\n\n1\n2\n\n\n\n# 常用类型查询\n\n普通类型 数组 map 结构体查询\n\nselect name,friends[1],children['xiao song'],address.street from test;\n\n\n1\n\n\n\n# 类型转化\n\n隐式类型转换规则如下\n\n 1. 任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。\n\n 2. 所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。如果STRING类型无法转换为DOUBLE 则返回NULL 如想强转为int 则cast('1'as int)+2\n\n 3. TINYINT、SMALLINT、INT都可以转换为FLOAT。\n\n 4. BOOLEAN类型不可以转换为任何其它的类型。\n\n 5. 可以使用CAST操作显示进行数据类型强制转换\n    \n    例如CAST('1' AS INT)将把字符串'1' 转换成整数1；如果强制类型转换失败，如执行CAST('X' AS INT)，表达式返回空值 NULL。",normalizedContent:"# hive 类型\n\nhive数据类型    java数据类型   长度                           例子\ntinyint     byte       1byte有符号整数                   20\nsmalint     short      2byte有符号整数                   20\nint         int        4byte有符号整数                   20\nbigint      long       8byte有符号整数                   20\nboolean     boolean    布尔类型，true或者false             true false\nfloat       float      单精度浮点数                       3.14159\ndouble      double     双精度浮点数                       3.14159\nstring      string     字符系列。可以指定字符集。可以使用单引号或者双引号。   ‘now is the time’ “for all good men”\ntimestamp              时间类型                         \nbinary                 字节数组                         \n\n数据类型     描述                                                                                                               语法示例\nstruct   和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是struct{first                                                       struct() 例如struct<street:string, city:string>\n         string, last string},那么第1个元素可以通过字段.first来引用。\nmap      map是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是map，其中键->值对是’first’->’john’和’last’->’doe’，那么可以通过字段名[‘last’]获取最后一个元素   map() 例如map<string, int>\narray    数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘john’,                                             array() 例如array<string>\n         ‘doe’]，那么第2个元素可以通过数组名[1]进行引用。\n\nstruct为结构体 为一个只有成员变量的类\n\nclass peopel{\n    int age;\n    string name;\n}\npeopel p \np.age = 10\np.name = 15\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n创建一个表\n\ncreate table test(\nname string,\nfriends array<string>,\nchildren map<string, int>,\naddress struct<street:string, city:string>\n)\nrow format delimited fields terminated by ','   -- 列分隔符\ncollection items terminated by '_'   -- map struct 和 array 的分隔符(数据分割符号)\nmap keys terminated by ':';  -- map中的key与value的分隔符\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n * row format delimited fields terminated by ',' -- 列分隔符\n\n * collection items terminated by '_' --map struct 和 array 的分隔符(数据分割符号)\n\n * map keys terminated by ':' -- map中的key与value的分隔符\n\n * lines terminated by '\\n'; -- 行分隔符\n\n\n# 上传文件到表数据中\n\n\n# 从网页端上传\n\n\n\n每个文件夹为一个表 直接把文本上传到文件夹(表)中\n\nsongsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing\nyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing\n\n\n1\n2\n\n\n\n# 从系统中上传到表\n\ncd /opt/module/\nmkdir datas\ncd datas\n\n\n1\n2\n3\n\n\nl在hive命令行总 load: data local inpath '本地文本路径' into table 表名;\n\n从本地加载表\n\n#在hive中\nload data local inpath '/opt/module/datas/test.txt' into table test;\n\n\n1\n2\n\n\n\n# 常用类型查询\n\n普通类型 数组 map 结构体查询\n\nselect name,friends[1],children['xiao song'],address.street from test;\n\n\n1\n\n\n\n# 类型转化\n\n隐式类型转换规则如下\n\n 1. 任何整数类型都可以隐式地转换为一个范围更广的类型，如tinyint可以转换成int，int可以转换成bigint。\n\n 2. 所有整数类型、float和string类型都可以隐式地转换成double。如果string类型无法转换为double 则返回null 如想强转为int 则cast('1'as int)+2\n\n 3. tinyint、smallint、int都可以转换为float。\n\n 4. boolean类型不可以转换为任何其它的类型。\n\n 5. 可以使用cast操作显示进行数据类型强制转换\n    \n    例如cast('1' as int)将把字符串'1' 转换成整数1；如果强制类型转换失败，如执行cast('x' as int)，表达式返回空值 null。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"DBeaver",frontmatter:{title:"DBeaver",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/5a826b/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/03.DBeaver.html",relativePath:"大数据/03.Hive/03.DBeaver.md",key:"v-79c818bb",path:"/pages/5a826b/",headersStr:null,content:"# DBeaver\n\n\n\n\n\n\n\n\n\n添加hive的jdbc驱动",normalizedContent:"# dbeaver\n\n\n\n\n\n\n\n\n\n添加hive的jdbc驱动",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hive 客户端命令",frontmatter:{title:"Hive 客户端命令",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/6652e3/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/05.Hive%20%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4.html",relativePath:"大数据/03.Hive/05.Hive 客户端命令.md",key:"v-3874a2bf",path:"/pages/6652e3/",headersStr:null,content:'# Hive 客户端命令\n\nHive****中变量和属性命名空间\n\n命令空间       使用权限    描述\nhivevar    可读/可写   （hive v0.80以及之后的版本）用户自定义变量\nhiveconf   可读/可写   Hive相关的配置属性\nsystem     可读/可写   Java定义的配置环境\nenv        只可读     Shell环境定义的环境变量\n\n * --define key＝value实际上和--hivevar key=value是等价的\n\nhive --define foo=bar #在shell命令行的根目录下输入，进入hive的cli命令行，并且定义了一个变量\nset foo; #查看foo变量值\nset hivevar:foo; #查询指定空间变量下的信息\nset hivevar:foo=bar2;  #查询赋值\ncreate table toss1(i int,${hivevar:foo} string);  #创建表时也可引用变量\n\n\n1\n2\n3\n4\n5\n\n * hiveconf 用于配置Hive行为的所有属性\n\nhive --hiveconf hive.cli.print.current.db=true  #通过hiveconf将指定属性赋值\nset hive.cli.print.current.db; #查询属性值\nhive --hiveconf  y=5\nset y;   #查询值\nSELECT * FROM whatsit WHERE  i=${hiveconf:y};     #语句中也可以使用属性\n\n\n1\n2\n3\n4\n5\n\n * -e 执行一个或者多个查询（使用分号分隔），执行结束后hive CLI立即退出\n\nhive -e "SELECT * FROM my_table"  #查询后退出hive\n\n\n1\n\n * -S -e 增加-S选项可以开启静默模式，这样可以在输出结果中去掉“OK”和“Time taken…”等行，以及其他一些无关紧要的输出信息 只输出结果\n\nhive -S -e "SELECT * FROM my_table LIMIT 3" >/usr/tmp/my_table_3.txt #只输出结果 并写入到指定文件中\n\n\n1\n\n * 在客户端外 文件中执行sql\n\nvim /usr/hive/hive/withqueries.hql #新建文件里面存放sql语句\nSELECT * FROM my_table;\n\n\n1\n2\n\n\nhive -f /usr/hive/hive/withqueries.hql #启动hive时 加载指定文件中的sql语句\n\n\n1\n\n * 在hive客户端 执行指定的sql文件\n\nsource /usr/hive/hive/withqueries.hql;\n\n\n1\n\n * -i 通过文件中sql语句 批量设置值\n\nvim /usr/hive/hive/.hiverc\n\nset hive.cli.print.current.db=true;\nset hive.exec.mode.local.auto=true;\n\n\n1\n2\n3\n4\n\n\nhive -i /usr/hive/hive/.hiverc #加载指定文件的设定\n\n\n1\n',normalizedContent:'# hive 客户端命令\n\nhive****中变量和属性命名空间\n\n命令空间       使用权限    描述\nhivevar    可读/可写   （hive v0.80以及之后的版本）用户自定义变量\nhiveconf   可读/可写   hive相关的配置属性\nsystem     可读/可写   java定义的配置环境\nenv        只可读     shell环境定义的环境变量\n\n * --define key＝value实际上和--hivevar key=value是等价的\n\nhive --define foo=bar #在shell命令行的根目录下输入，进入hive的cli命令行，并且定义了一个变量\nset foo; #查看foo变量值\nset hivevar:foo; #查询指定空间变量下的信息\nset hivevar:foo=bar2;  #查询赋值\ncreate table toss1(i int,${hivevar:foo} string);  #创建表时也可引用变量\n\n\n1\n2\n3\n4\n5\n\n * hiveconf 用于配置hive行为的所有属性\n\nhive --hiveconf hive.cli.print.current.db=true  #通过hiveconf将指定属性赋值\nset hive.cli.print.current.db; #查询属性值\nhive --hiveconf  y=5\nset y;   #查询值\nselect * from whatsit where  i=${hiveconf:y};     #语句中也可以使用属性\n\n\n1\n2\n3\n4\n5\n\n * -e 执行一个或者多个查询（使用分号分隔），执行结束后hive cli立即退出\n\nhive -e "select * from my_table"  #查询后退出hive\n\n\n1\n\n * -s -e 增加-s选项可以开启静默模式，这样可以在输出结果中去掉“ok”和“time taken…”等行，以及其他一些无关紧要的输出信息 只输出结果\n\nhive -s -e "select * from my_table limit 3" >/usr/tmp/my_table_3.txt #只输出结果 并写入到指定文件中\n\n\n1\n\n * 在客户端外 文件中执行sql\n\nvim /usr/hive/hive/withqueries.hql #新建文件里面存放sql语句\nselect * from my_table;\n\n\n1\n2\n\n\nhive -f /usr/hive/hive/withqueries.hql #启动hive时 加载指定文件中的sql语句\n\n\n1\n\n * 在hive客户端 执行指定的sql文件\n\nsource /usr/hive/hive/withqueries.hql;\n\n\n1\n\n * -i 通过文件中sql语句 批量设置值\n\nvim /usr/hive/hive/.hiverc\n\nset hive.cli.print.current.db=true;\nset hive.exec.mode.local.auto=true;\n\n\n1\n2\n3\n4\n\n\nhive -i /usr/hive/hive/.hiverc #加载指定文件的设定\n\n\n1\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"DDL数据定义",frontmatter:{title:"DDL数据定义",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/675adb/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/06.DDL%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89.html",relativePath:"大数据/03.Hive/06.DDL数据定义.md",key:"v-7688bc37",path:"/pages/675adb/",headers:[{level:2,title:"数据库",slug:"数据库",normalizedTitle:"数据库",charIndex:14},{level:3,title:"创建数据库",slug:"创建数据库",normalizedTitle:"创建数据库",charIndex:22},{level:3,title:"查询数据库",slug:"查询数据库",normalizedTitle:"查询数据库",charIndex:352},{level:3,title:"切换数据库",slug:"切换数据库",normalizedTitle:"切换数据库",charIndex:522},{level:3,title:"删除数据库",slug:"删除数据库",normalizedTitle:"删除数据库",charIndex:561},{level:3,title:"修改数据库",slug:"修改数据库",normalizedTitle:"修改数据库",charIndex:728},{level:2,title:"表",slug:"表",normalizedTitle:"表",charIndex:892},{level:3,title:"创建表",slug:"创建表",normalizedTitle:"创建表",charIndex:898},{level:3,title:"查询结果建表",slug:"查询结果建表",normalizedTitle:"查询结果建表",charIndex:2657},{level:3,title:"查询表",slug:"查询表",normalizedTitle:"查询表",charIndex:2738},{level:3,title:"修改表",slug:"修改表",normalizedTitle:"修改表",charIndex:2834},{level:3,title:"删除表",slug:"删除表",normalizedTitle:"删除表",charIndex:1526},{level:2,title:"内外部表",slug:"内外部表",normalizedTitle:"内外部表",charIndex:3470},{level:3,title:"外部表和内部表转换",slug:"外部表和内部表转换",normalizedTitle:"外部表和内部表转换",charIndex:3784},{level:2,title:"分区表",slug:"分区表",normalizedTitle:"分区表",charIndex:1612},{level:3,title:"分区的作用",slug:"分区的作用",normalizedTitle:"分区的作用",charIndex:4267},{level:3,title:"查询分区表的分区",slug:"查询分区表的分区",normalizedTitle:"查询分区表的分区",charIndex:4413},{level:3,title:"修复分区",slug:"修复分区",normalizedTitle:"修复分区",charIndex:4457},{level:3,title:"二级分区",slug:"二级分区",normalizedTitle:"二级分区",charIndex:4805},{level:3,title:"分区的增删",slug:"分区的增删",normalizedTitle:"分区的增删",charIndex:5174}],headersStr:"数据库 创建数据库 查询数据库 切换数据库 删除数据库 修改数据库 表 创建表 查询结果建表 查询表 修改表 删除表 内外部表 外部表和内部表转换 分区表 分区的作用 查询分区表的分区 修复分区 二级分区 分区的增删",content:'# DDL数据定义\n\n\n# 数据库\n\n\n# 创建数据库\n\n-- 语法\nCREATE DATABASE [IF NOT EXISTS] database_name\n[COMMENT database_comment]\n[LOCATION hdfs_path]\n[WITH DBPROPERTIES (property_name=property_value, ...)];\n\n\n1\n2\n3\n4\n5\n\n\ncreate database test\ncomment "Just for test"\nlocation \'/testdb\'  -- 此库会在hdfs中此路径下映射\nwith dpproperties("aaa"="bbb");   -- 属性 不太常用 记录一个键值对\n\n\n1\n2\n3\n4\n\n\n\n# 查询数据库\n\nshow database; -- 查询所有库\ndesc database test; -- 查询指定数据库信息\ndesc database extended test; -- 查询指定库数据库详细信息\nshow databases like \'db_hive*\'; -- 过滤显示查询的数据库\n\n\n1\n2\n3\n4\n\n\n\n# 切换数据库\n\nuse db_hive; -- 切换到指定库\n\n\n1\n\n\n\n# 删除数据库\n\ndrop database test; -- 删除指定数据库 如果库中有数据不能删除 必须为空\ndrop database cascade test; -- 删除数据库 无论是否库中有数据 慎用\ndrop database if exists db_hive2; -- 删除前判断是否存在此库\n\n\n1\n2\n3\n\n\n\n# 修改数据库\n\n数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。只能修改数据库的DBPROPERTIES设置键-值对属性值\n\nalter database db_hive set dbproperties(\'createtime\'=\'20170830\'); -- 修改指定库的键值对值\n\n\n1\n\n\n\n# 表\n\n\n# 创建表\n\n-- 语法\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name \n[(col_name data_type [COMMENT col_comment], ...)] \n[COMMENT table_comment] \n[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] \n[CLUSTERED BY (col_name, col_name, ...) \n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] \n[ROW FORMAT row_format] \n[STORED AS file_format] \n[LOCATION hdfs_path]\n[TBLPROPERTIES (property_name=property_value, ...)]\n[AS select_statement]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n 1. CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。\n\n 2. EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。\n\n 3. COMMENT：为表和列添加注释。\n\n 4. PARTITIONED BY创建分区表\n\n 5. CLUSTERED BY创建分桶表\n\n 6. SORTED BY不常用，对桶中的一个或多个列另外排序\n\n 7. ROW FORMAT\n    \n    DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]\n\n| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]\n\n用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者 ROWFORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列 的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。\n\nSerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。\n\n 8.  STORED AS指定存储文件类型\n     \n     常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）\n     \n     如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。\n\n 9.  LOCATION ：指定表在HDFS上的存储位置。\n\n 10. AS：后跟查询语句，根据查询结果创建表。\n\n 11. LIKE允许用户复制现有的表结构，但是不复制数据。\n\ncreate table test\n(id int comment "ID", name string comment "Nmae")\ncomment "Test Table"\nrow format delimited fields terminated by \'\\t\' -- 以\\t划分切合\nlocation "/test_table" -- 会映射在当前hdfs指定文件夹\ntblproperties("aaa"="bbb");\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 查询结果建表\n\ncreate table stu_result as select * from stu_par where id=1001;\n\n\n1\n\n\n\n# 查询表\n\nshow tables; -- 查询当前库所有表\ndesc test; -- 查看表信息\ndesc formatted test; -- 查询表的详细信息\n\n\n1\n2\n3\n\n\n\n# 修改表\n\n * 重命名表\n\nALTER TABLE table_name RENAME TO new_table_name  -- 语法\nalter table test rename to new_test;\n\n\n1\n2\n\n * 更新列\n\nALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] -- 语法\nalter table test change id id string; -- 将test表中的id列改为string类型\n\n\n1\n2\n\n * 增加列和替换列\n\nALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)  -- 语法\nalter table test add columns(class string); -- 在表中增加class类 string类型 可以添加多列\nalter table test replace columns(id double, name string); -- 将指定表的结构体换成指定结构体 原先的列不保留\n\n\n1\n2\n3\n\n\n\n# 删除表\n\ndrop table test;\n\n\n1\n\n\n\n# 内外部表\n\n我们默认创建的表是内部表 当建表时加上external关键字 则创建一个外部表\n\ncreate external table test\n(id int, name string)\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n\n\n# 向test表导入数据\nload data local inpath "/opt/module/datas/student.txt" into table test;\n\n\n1\n2\n\n\n外部表，当删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。而表数据存放在hdfs上 元数据存储在mysql中\n\n\n# 外部表和内部表转换\n\nalter table test set tblproperties(\'EXTERNAL\'=\'TRUE\'); -- true为外部表 flase为内部表\n\n\n1\n\n\n\n# 分区表\n\n-- 创建分区表\ncreate table stu_par\n(id int, name string)\npartitioned by(class string) -- 设置为分区表 区名为class 类型为sring\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n4\n5\n\n\n向表插入数据\n\nload data local inpath "/opt/module/datas/student.txt" into table stu_par partition(class=\'01\'); -- 插入指定分区\n\n\n1\n\n\n此时会在表文件夹中以 class=01 为文件名存放数据\n\n\n\n本质上是以文件来划分分区(分组) 但此表整合到一起 并以指定分区名区分\n\n如果数据结构不同 也会整合一起\n\n\n# 分区的作用\n\n在hive没有索引的概念 所有默认查询是全表扫描 如果数据庞大那么查询效率会很低 我们可以通过分区表来查询表中指定分区中的数据\n\n-- 查询表时 指定分区 可以减少数据扫描量\nselect * from stu_par where class="01";\n\n\n1\n2\n\n\n\n# 查询分区表的分区\n\nshow partitions stu_par;\n\n\n1\n\n\n\n# 修复分区\n\n如果提前准备数据, 但是没有元数据 在hive是查询不到 我们可以通过修复 来导入文件数据\n\n 1. 添加分区\n\nalter table stu_par add partition(class="03"); -- 表文件夹中的分区文件夹名称必须与这里一致 否则无法映射\n\n\n1\n\n 2. 直接修复\n\nmsck repair table stu_par; -- 会自动扫描当前表下的文件夹 只要以分区名=xx为关键字的都会被添加到分区当中\n\n\n1\n\n 3. 上传时带分区\n\nload data local inpath "/opt/module/datas/student.txt" into table stu_par partition(class=\'04\');\n\n\n1\n\n\n\n# 二级分区\n\ncreate table stu_par2\n(id int, name string)\npartitioned by(grade string,class string) -- 二级分区\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n4\n\n\n\n\n插入数据\n\nload data local inpath "/opt/module/datas/student.txt" into table stu_par2 partition(grade=\'01\',class=\'03\');\n\n\n1\n\n\n目录结构为 stu_par2 --\x3e grade=01 --\x3e class=03 --\x3e student.txt(数据文件)\n\n多少个分区就多少个键值对文件夹\n\n\n\n\n# 分区的增删\n\n * 增加分区\n\nalter table stu_par add partition(class="04"); -- 增加分区\nalter table stu_par add partition(class="05") partition(class=\'06\'); -- 增加多个分区 注意是空格间隔\n\n\n1\n2\n\n * 删除分区\n\nalter table stu_par drop partition(class="05");\nalter table stu_par drop partition(class="05"),partition(class="07"); -- 删除多个分区 注意是逗号间隔\n\n\n1\n2\n',normalizedContent:'# ddl数据定义\n\n\n# 数据库\n\n\n# 创建数据库\n\n-- 语法\ncreate database [if not exists] database_name\n[comment database_comment]\n[location hdfs_path]\n[with dbproperties (property_name=property_value, ...)];\n\n\n1\n2\n3\n4\n5\n\n\ncreate database test\ncomment "just for test"\nlocation \'/testdb\'  -- 此库会在hdfs中此路径下映射\nwith dpproperties("aaa"="bbb");   -- 属性 不太常用 记录一个键值对\n\n\n1\n2\n3\n4\n\n\n\n# 查询数据库\n\nshow database; -- 查询所有库\ndesc database test; -- 查询指定数据库信息\ndesc database extended test; -- 查询指定库数据库详细信息\nshow databases like \'db_hive*\'; -- 过滤显示查询的数据库\n\n\n1\n2\n3\n4\n\n\n\n# 切换数据库\n\nuse db_hive; -- 切换到指定库\n\n\n1\n\n\n\n# 删除数据库\n\ndrop database test; -- 删除指定数据库 如果库中有数据不能删除 必须为空\ndrop database cascade test; -- 删除数据库 无论是否库中有数据 慎用\ndrop database if exists db_hive2; -- 删除前判断是否存在此库\n\n\n1\n2\n3\n\n\n\n# 修改数据库\n\n数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。只能修改数据库的dbproperties设置键-值对属性值\n\nalter database db_hive set dbproperties(\'createtime\'=\'20170830\'); -- 修改指定库的键值对值\n\n\n1\n\n\n\n# 表\n\n\n# 创建表\n\n-- 语法\ncreate [external] table [if not exists] table_name \n[(col_name data_type [comment col_comment], ...)] \n[comment table_comment] \n[partitioned by (col_name data_type [comment col_comment], ...)] \n[clustered by (col_name, col_name, ...) \n[sorted by (col_name [asc|desc], ...)] into num_buckets buckets] \n[row format row_format] \n[stored as file_format] \n[location hdfs_path]\n[tblproperties (property_name=property_value, ...)]\n[as select_statement]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n 1. create table 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 if not exists 选项来忽略这个异常。\n\n 2. external关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（location），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。\n\n 3. comment：为表和列添加注释。\n\n 4. partitioned by创建分区表\n\n 5. clustered by创建分桶表\n\n 6. sorted by不常用，对桶中的一个或多个列另外排序\n\n 7. row format\n    \n    delimited [fields terminated by char] [collection items terminated by char] [[map keys terminated by char] [lines terminated by char]\n\n| serde serde_name [with serdeproperties (property_name=property_value, property_name=property_value, ...)]\n\n用户在建表的时候可以自定义serde或者使用自带的serde。如果没有指定row format 或者 rowformat delimited，将会使用自带的serde。在建表的时候，用户还需要为表指定列，用户在指定表的列 的同时也会指定自定义的serde，hive通过serde确定表的具体的列的数据。\n\nserde是serialize/deserilize的简称， hive使用serde进行行对象的序列与反序列化。\n\n 8.  stored as指定存储文件类型\n     \n     常用的存储文件类型：sequencefile（二进制序列文件）、textfile（文本）、rcfile（列式存储格式文件）\n     \n     如果文件数据是纯文本，可以使用stored as textfile。如果数据需要压缩，使用 stored as sequencefile。\n\n 9.  location ：指定表在hdfs上的存储位置。\n\n 10. as：后跟查询语句，根据查询结果创建表。\n\n 11. like允许用户复制现有的表结构，但是不复制数据。\n\ncreate table test\n(id int comment "id", name string comment "nmae")\ncomment "test table"\nrow format delimited fields terminated by \'\\t\' -- 以\\t划分切合\nlocation "/test_table" -- 会映射在当前hdfs指定文件夹\ntblproperties("aaa"="bbb");\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 查询结果建表\n\ncreate table stu_result as select * from stu_par where id=1001;\n\n\n1\n\n\n\n# 查询表\n\nshow tables; -- 查询当前库所有表\ndesc test; -- 查看表信息\ndesc formatted test; -- 查询表的详细信息\n\n\n1\n2\n3\n\n\n\n# 修改表\n\n * 重命名表\n\nalter table table_name rename to new_table_name  -- 语法\nalter table test rename to new_test;\n\n\n1\n2\n\n * 更新列\n\nalter table table_name change [column] col_old_name col_new_name column_type [comment col_comment] [first|after column_name] -- 语法\nalter table test change id id string; -- 将test表中的id列改为string类型\n\n\n1\n2\n\n * 增加列和替换列\n\nalter table table_name add|replace columns (col_name data_type [comment col_comment], ...)  -- 语法\nalter table test add columns(class string); -- 在表中增加class类 string类型 可以添加多列\nalter table test replace columns(id double, name string); -- 将指定表的结构体换成指定结构体 原先的列不保留\n\n\n1\n2\n3\n\n\n\n# 删除表\n\ndrop table test;\n\n\n1\n\n\n\n# 内外部表\n\n我们默认创建的表是内部表 当建表时加上external关键字 则创建一个外部表\n\ncreate external table test\n(id int, name string)\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n\n\n# 向test表导入数据\nload data local inpath "/opt/module/datas/student.txt" into table test;\n\n\n1\n2\n\n\n外部表，当删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。而表数据存放在hdfs上 元数据存储在mysql中\n\n\n# 外部表和内部表转换\n\nalter table test set tblproperties(\'external\'=\'true\'); -- true为外部表 flase为内部表\n\n\n1\n\n\n\n# 分区表\n\n-- 创建分区表\ncreate table stu_par\n(id int, name string)\npartitioned by(class string) -- 设置为分区表 区名为class 类型为sring\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n4\n5\n\n\n向表插入数据\n\nload data local inpath "/opt/module/datas/student.txt" into table stu_par partition(class=\'01\'); -- 插入指定分区\n\n\n1\n\n\n此时会在表文件夹中以 class=01 为文件名存放数据\n\n\n\n本质上是以文件来划分分区(分组) 但此表整合到一起 并以指定分区名区分\n\n如果数据结构不同 也会整合一起\n\n\n# 分区的作用\n\n在hive没有索引的概念 所有默认查询是全表扫描 如果数据庞大那么查询效率会很低 我们可以通过分区表来查询表中指定分区中的数据\n\n-- 查询表时 指定分区 可以减少数据扫描量\nselect * from stu_par where class="01";\n\n\n1\n2\n\n\n\n# 查询分区表的分区\n\nshow partitions stu_par;\n\n\n1\n\n\n\n# 修复分区\n\n如果提前准备数据, 但是没有元数据 在hive是查询不到 我们可以通过修复 来导入文件数据\n\n 1. 添加分区\n\nalter table stu_par add partition(class="03"); -- 表文件夹中的分区文件夹名称必须与这里一致 否则无法映射\n\n\n1\n\n 2. 直接修复\n\nmsck repair table stu_par; -- 会自动扫描当前表下的文件夹 只要以分区名=xx为关键字的都会被添加到分区当中\n\n\n1\n\n 3. 上传时带分区\n\nload data local inpath "/opt/module/datas/student.txt" into table stu_par partition(class=\'04\');\n\n\n1\n\n\n\n# 二级分区\n\ncreate table stu_par2\n(id int, name string)\npartitioned by(grade string,class string) -- 二级分区\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n4\n\n\n\n\n插入数据\n\nload data local inpath "/opt/module/datas/student.txt" into table stu_par2 partition(grade=\'01\',class=\'03\');\n\n\n1\n\n\n目录结构为 stu_par2 --\x3e grade=01 --\x3e class=03 --\x3e student.txt(数据文件)\n\n多少个分区就多少个键值对文件夹\n\n\n\n\n# 分区的增删\n\n * 增加分区\n\nalter table stu_par add partition(class="04"); -- 增加分区\nalter table stu_par add partition(class="05") partition(class=\'06\'); -- 增加多个分区 注意是空格间隔\n\n\n1\n2\n\n * 删除分区\n\nalter table stu_par drop partition(class="05");\nalter table stu_par drop partition(class="05"),partition(class="07"); -- 删除多个分区 注意是逗号间隔\n\n\n1\n2\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"DML",frontmatter:{title:"DML",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/ff1fce/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/07.DML.html",relativePath:"大数据/03.Hive/07.DML.md",key:"v-132275ff",path:"/pages/ff1fce/",headers:[{level:2,title:"数据导入",slug:"数据导入",normalizedTitle:"数据导入",charIndex:10},{level:2,title:"数据导出",slug:"数据导出",normalizedTitle:"数据导出",charIndex:1220},{level:3,title:"Insert导出",slug:"insert导出",normalizedTitle:"insert导出",charIndex:1229},{level:3,title:"Hive Shell 命令导出",slug:"hive-shell-命令导出",normalizedTitle:"hive shell 命令导出",charIndex:1525},{level:3,title:"Export导出到HDFS上",slug:"export导出到hdfs上",normalizedTitle:"export导出到hdfs上",charIndex:1672},{level:3,title:"Import数据导入到hive表中",slug:"import数据导入到hive表中",normalizedTitle:"import数据导入到hive表中",charIndex:1790},{level:2,title:"数据删除",slug:"数据删除",normalizedTitle:"数据删除",charIndex:1895}],headersStr:"数据导入 数据导出 Insert导出 Hive Shell 命令导出 Export导出到HDFS上 Import数据导入到hive表中 数据删除",content:"# DML\n\n\n# 数据导入\n\nload data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];\n\n\n1\n\n 1. load data:表示加载数据\n 2. local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表\n 3. inpath:表示加载数据的路径\n 4. overwrite:表示覆盖表中已有数据，否则表示追加\n 5. into table:表示加载到哪张表\n 6. student:表示具体的表\n 7. partition:表示上传到指定分区\n\nload data local inpath \"/opt/module/datas/student.txt\" into table stu_par; -- 从宿机加载数据\nload data local inpath \"/opt/module/datas/student.txt\" overwrite into table stu_par; -- 覆盖导入\nload data inpath \"/opt/module/datas/student.txt\" overwrite into table stu_par; -- 从hdfs中加载指定路径文件  HDFS的导入是移动文件,而本地导入是复制上传\n\n\n1\n2\n3\n\n * insert导入\n\ninsert into table  stu_par partition(month='201709') values(1,'wangwu') -- 插入单条 一个括号对应一条\ninsert into table  stu_par partition(month='201709') values(1,'wangwu'),(2,'zhaoliu'); -- 插入多条指定数据 多行数据用逗号隔开并用括号包裹 要类型一致\ninsert into table  stu_par select id,name from stu_par2 where class=\"01\";  -- 插入查询后的数据\n\n\n1\n2\n3\n\n * 建表时as select导入\n\ncreate table if not exists student3\nas select id, name from student;\n\n\n1\n2\n\n * 建表时通过location加载\n\ncreate external table if not exists student4\n(id int, name string)\nrow format delimited fields terminated by '\\t'\nlocation '/student';\n\n\n1\n2\n3\n4\n\n\n\n# 数据导出\n\n\n# Insert导出\n\n * 将查询的结果导出到本地 默认不带格式\n\ninsert overwrite local directory '/opt/module/datas/export/student' select * from student;\n\n\n1\n\n * 将查询的结果格式化导出到本地\n\ninsert overwrite local directory '/opt/module/datas/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' select * from student;\n\n\n1\n\n\n\n# Hive Shell 命令导出\n\n用shell 命令查询表并写出本地\n\n如果不指定库 默认为default库\n\nhive -e 'select * from default.student;' > /opt/module/datas/export/student4.txt;\n\n\n1\n\n\n\n# Export导出到HDFS上\n\n将表导出到HDFS上 元数据和表数据文件\n\nexport table default.student to '/user/hive/warehouse/export/student';\n\n\n1\n\n\n\n# Import数据导入到hive表中\n\n要先用export导出到hdfs中 必须包含元数据和表数据\n\nimport table student from '/export/student'; \n\n\n1\n\n\n\n# 数据删除\n\n清空表 只删除表数据 不删除表本身 Truncate只能删除管理表，不能删除外部表中数据\n\ntruncate table student;\n\n\n1\n",normalizedContent:"# dml\n\n\n# 数据导入\n\nload data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];\n\n\n1\n\n 1. load data:表示加载数据\n 2. local:表示从本地加载数据到hive表；否则从hdfs加载数据到hive表\n 3. inpath:表示加载数据的路径\n 4. overwrite:表示覆盖表中已有数据，否则表示追加\n 5. into table:表示加载到哪张表\n 6. student:表示具体的表\n 7. partition:表示上传到指定分区\n\nload data local inpath \"/opt/module/datas/student.txt\" into table stu_par; -- 从宿机加载数据\nload data local inpath \"/opt/module/datas/student.txt\" overwrite into table stu_par; -- 覆盖导入\nload data inpath \"/opt/module/datas/student.txt\" overwrite into table stu_par; -- 从hdfs中加载指定路径文件  hdfs的导入是移动文件,而本地导入是复制上传\n\n\n1\n2\n3\n\n * insert导入\n\ninsert into table  stu_par partition(month='201709') values(1,'wangwu') -- 插入单条 一个括号对应一条\ninsert into table  stu_par partition(month='201709') values(1,'wangwu'),(2,'zhaoliu'); -- 插入多条指定数据 多行数据用逗号隔开并用括号包裹 要类型一致\ninsert into table  stu_par select id,name from stu_par2 where class=\"01\";  -- 插入查询后的数据\n\n\n1\n2\n3\n\n * 建表时as select导入\n\ncreate table if not exists student3\nas select id, name from student;\n\n\n1\n2\n\n * 建表时通过location加载\n\ncreate external table if not exists student4\n(id int, name string)\nrow format delimited fields terminated by '\\t'\nlocation '/student';\n\n\n1\n2\n3\n4\n\n\n\n# 数据导出\n\n\n# insert导出\n\n * 将查询的结果导出到本地 默认不带格式\n\ninsert overwrite local directory '/opt/module/datas/export/student' select * from student;\n\n\n1\n\n * 将查询的结果格式化导出到本地\n\ninsert overwrite local directory '/opt/module/datas/export/student1' row format delimited fields terminated by '\\t' select * from student;\n\n\n1\n\n\n\n# hive shell 命令导出\n\n用shell 命令查询表并写出本地\n\n如果不指定库 默认为default库\n\nhive -e 'select * from default.student;' > /opt/module/datas/export/student4.txt;\n\n\n1\n\n\n\n# export导出到hdfs上\n\n将表导出到hdfs上 元数据和表数据文件\n\nexport table default.student to '/user/hive/warehouse/export/student';\n\n\n1\n\n\n\n# import数据导入到hive表中\n\n要先用export导出到hdfs中 必须包含元数据和表数据\n\nimport table student from '/export/student'; \n\n\n1\n\n\n\n# 数据删除\n\n清空表 只删除表数据 不删除表本身 truncate只能删除管理表，不能删除外部表中数据\n\ntruncate table student;\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"函数",frontmatter:{title:"函数",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/8f0a49/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/09.%E5%87%BD%E6%95%B0.html",relativePath:"大数据/03.Hive/09.函数.md",key:"v-37a0dd10",path:"/pages/8f0a49/",headers:[{level:2,title:"系统内置函数",slug:"系统内置函数",normalizedTitle:"系统内置函数",charIndex:9},{level:2,title:"空字段赋值",slug:"空字段赋值",normalizedTitle:"空字段赋值",charIndex:184},{level:2,title:"CASE WHEN",slug:"case-when",normalizedTitle:"case when",charIndex:366},{level:2,title:"行转列",slug:"行转列",normalizedTitle:"行转列",charIndex:609},{level:2,title:"列转行",slug:"列转行",normalizedTitle:"列转行",charIndex:1151},{level:2,title:"窗口函数(开窗函数)",slug:"窗口函数-开窗函数",normalizedTitle:"窗口函数(开窗函数)",charIndex:2190},{level:3,title:"窗口函数聚合",slug:"窗口函数聚合",normalizedTitle:"窗口函数聚合",charIndex:2789},{level:3,title:"其他函数",slug:"其他函数",normalizedTitle:"其他函数",charIndex:4565},{level:2,title:"Rank",slug:"rank",normalizedTitle:"rank",charIndex:5695},{level:2,title:"日期函数",slug:"日期函数",normalizedTitle:"日期函数",charIndex:6509}],headersStr:"系统内置函数 空字段赋值 CASE WHEN 行转列 列转行 窗口函数(开窗函数) 窗口函数聚合 其他函数 Rank 日期函数",content:'# 函数\n\n\n# 系统内置函数\n\nshow functions;  -- 查看系统自带的函数\nshow functions like "*date*"; -- 模糊查询函数名\ndesc function upper; -- 显示自带的函数的用法\ndesc function extended upper; -- 详细显示自带的函数的用法\n\n\n1\n2\n3\n4\n\n\n\n# 空字段赋值\n\nNVL( value，default_value) 如果value为null 则替换为 default_value\n\nselect comm,nvl(comm, -1) from emp;  -- 替换为指定值\nselect comm, nvl(comm,mgr) from emp; -- 如果值为列名 则替换为当前行列的值\n\n\n1\n2\n\n\n\n# CASE WHEN\n\n类似于switch\n\nselect \n  dept_id,\n  sum(case sex when \'男\' then 1 else 0 end) male_count,\n  sum(case sex when \'女\' then 1 else 0 end) female_count\nfrom \n  emp_sex\ngroup by\n  dept_id;\n-- 根据 dept_id 分组 条件判断值 再累加列个数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 行转列\n\n将多个值 / 列的值 聚合为一个值\n\n\n\nselect\n    t1.base,\n    concat_ws(\'|\', collect_set(t1.name)) name\nfrom\n    (select\n        name,\n        concat(constellation, ",", blood_type) base\n    from\n        person_info) t1\ngroup by\n    t1.base;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;\n * CONCAT_WS(separator, str1, str2,...)： 它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。 将array或者string 以分隔符分割 返回 string\n * COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。\n * COLLECT_LIST(col)：产生array类型字段, 包含重复元素 .\n\n\n# 列转行\n\n * EXPLODE(col)：将hive一列中复杂的array拆分一列多行 map结构拆分成两列多行 k v。\n\n * split(str,regex): 将指定字符串以 指定的分割 进行拆分 返回数组\n\n * LATERAL VIEW: 虚拟表 在此基础上可以对拆分后的数据进行聚合。\n\n\n\nvi movie.txt\n\n《疑犯追踪》\t悬疑,动作,科幻,剧情\n《Lie to me》\t悬疑,警匪,动作,心理,剧情\n《战狼2》\t战争,动作,灾难\n\n\n1\n2\n3\n4\n5\n\n\ncreate table movie_info(\n    movie string, \n    category string) \nrow format delimited fields terminated by "\\t";\nload data local inpath "/opt/module/datas/movie.txt" into table movie_info;\n\n\n1\n2\n3\n4\n5\n\n\n转换\n\nselect\n    m.movie,\n    tbl.cate\nfrom\n    movie_info m\nlateral view\n    explode(split(category, ",")) tbl as cate;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n现在根据电影名来进行分组聚合 请实现使用类分组 查看每个类别下的电影名\n\nselect\n    cate,\n    collect_list(movie)\nfrom\n    (select\n    m.movie,\n    tbl,cate\nfrom\n    movie_info m\nlateral view\n    explode(split(category, ",")) tbl as cate;) t1\ngroup by \n    cate;\n\n#两sql语句一致\nselect\n    tbl.cate,\n    collect_list(m.movie)\nfrom\n    movie_info m\nlateral view\n    explode(split(category, ",")) tbl as cate\ngroup by \n    cate;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 窗口函数(开窗函数)\n\n数据准备：name，orderdate，cost\n\nvim /opt/module/datas/business.txt\n\n\n1\n\n\njack,2017-01-01,10\ntony,2017-01-02,15\njack,2017-02-03,23\ntony,2017-01-04,29\njack,2017-01-05,46\njack,2017-04-06,42\ntony,2017-01-07,50\njack,2017-01-08,55\nmart,2017-04-08,62\nmart,2017-04-09,68\nneil,2017-05-10,12\nmart,2017-04-11,75\nneil,2017-06-12,80\nmart,2017-04-13,94\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n创建表\n\ncreate table business(\nname string, \norderdate string,\ncost int\n) ROW FORMAT DELIMITED FIELDS TERMINATED BY \',\';\n\nload data local inpath "/opt/module/datas/business.txt" into table business;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 窗口函数聚合\n\n查询在2017年4月份购买过的顾客及总人数\n\n统计所有2017-04的时间 根据用户名去重 count()默认是一列 而我们的name是多列 如果直接拼接则会报错,我们通过窗口函数 over()进行回写\n\nselect \n    distinct name,count(distinct name) over()  -- distinct去重\nfrom business\nwhere\n    substring(orderdate,1,7) = "2017-04";\n\n\n1\n2\n3\n4\n5\n\n\n查询顾客的购买明细及月购买总额\n\n使用窗口函数 根据时间列 分区 统计每区的sum 进行回写到对应的分区 每一个单元格中\n\nselect name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;\n\n\n1\n\n\n上述的场景, 将每个顾客的cost按照日期进行累加\n\nselect name,orderdate,cost, \nsum(cost) over() as sample1,-- 所有行相加 \nsum(cost) over(partition by name) as sample2,-- 按name分组，组内数据累加 \nsum(cost) over(partition by name order by orderdate) as sample3,-- 按name分组，组内数据累加 \nsum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,-- 和sample3一样,由起点到当前行的聚合 \nsum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, -- 当前行和前面一行做聚合 \nsum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,-- 当前行和前边一行及后面一行 \nsum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 -- 当前行及后面所有行 \nfrom business;\n\n#将上述操作整合到一个窗口函数中\nselect name,orderdate,cost,\nsum(cost) over(partition by month(orderdate)) mc,\nsum(cost) over(partition by name order by orderdate asc rows between unbounded PRECEDING and current row) lc, -- 每人购买金额的累加\nsum(cost) over(partition by name,substring(orderdate,1,7))  -- 每人每月的购买金额 \nfrom business;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nOVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。\n\n * CURRENT ROW 当前行\n * n PRECEDING 往前N行数据\n * n FOLLOWING 往后N行数据\n * UNBOUNDED 起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点\n * rows必须跟在Order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量\n\n\n# 其他函数\n\n * LAG(col,n,default_val)：往前第n行数据\n   \n   * col 列名\n   \n   * n 显示当前行的前几行\n   \n   * default_val 如果前几行中没有数据则以此值为默认值\n   \n   * 查看顾客上次的购买时间\n   \n   * select name,orderdate,cost,\n     lag(orderdate,1,\'1970-01-01\') over(partition by name order by orderdate ) as time1,\n     lag(orderdate,2) over (partition by name order by orderdate) as time2\n     from business;\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * LEAD(col,n, default_val)：往后第n行数据\n   \n   * col 列名\n   * n 显示当前行的后几行\n   * default_val 如果前几行中没有数据则以此值为默认值\n\n * NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。\n   \n   * 查询前20%时间的订单信息\n   \n   * select * from (\n         select name,orderdate,cost, \n         ntile(5) over(order by orderdate) sorted -- 将结果分成5组数据 \n         from business\n     ) t\n     where sorted = 1;  -- 因为平均分为5组了 所有第一组为前百分之20\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * PERCENT_RANK() 求出当前行在结果集中位置的百分百 为double类型 每次计算后面都会有误差\n   \n   * select name,orderdate,cost, \n     PERCENT_RANK() over(order by orderdate) pr -- 返回一个0.00-1.00 的值 为当前行所在结果集中占据位置的百分比\n     from business\n     \n     \n     1\n     2\n     3\n     \n\n\n# Rank\n\n原始数据\n\nNAME   SUBJECT   SCORE\n孙悟空    语文        87\n孙悟空    数学        95\n孙悟空    英语        68\n大海     语文        94\n大海     数学        56\n大海     英语        84\n宋宋     语文        64\n宋宋     数学        86\n宋宋     英语        84\n婷婷     语文        65\n婷婷     数学        85\n婷婷     英语        78\n\ncreate table score(\nname string,\nsubject string, \nscore int) \nrow format delimited fields terminated by "\\t";\nload data local inpath \'/opt/module/datas/score.txt\' into table score;\n\n\n1\n2\n3\n4\n5\n6\n\n\n计算每门学科成绩排名\n\nSELECT\n    *,\n    RANK() OVER(PARTITION by subject ORDER BY score desc) r,\n    DENSE_RANK() OVER(PARTITION by subject ORDER BY score desc) dr,\n    ROW_NUMBER() OVER(PARTITION by subject ORDER BY score desc) rn\nFROM\n    score;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * RANK() 排序相同时会重复，总数不会变\n * DENSE_RANK() 排序相同时会重复，总数会减少\n * ROW_NUMBER() 会根据顺序计算\n\n\n\n\n# 日期函数\n\n * current_date返回当前日期\n   \n   * select current_date();\n     \n     \n     1\n     \n\n * date_add 日期的加\n   \n   * -- 今天开始90天以后的日期\n     select date_add(current_date(), 90);\n     \n     \n     1\n     2\n     \n\n * date_sub 日期的减\n   \n   * -- 今天开始90天以前的日期\n     select date_sub(current_date(), 90);\n     \n     \n     1\n     2\n     \n\n * 两个日期之间的日期差\n   \n   * -- 今天和1990年6月4日的天数差\n     SELECT datediff(CURRENT_DATE(), "1990-06-04"); -- 返回的为天数\n     \n     \n     1\n     2\n     \n\n * 判断哪个顾客连续两天光顾过\n   \n   * SELECT\n     \tname,\n     \tcount(*) c\n     from\n     \t(\n     \tSELECT\n     \t\t*,\n     \t\tdate_sub(orderdate,\n     \t\trn) temp\n     \t\t-- 原始时间减去当前name区时间对应的序号 \n     \tfrom\n     \t\t(\n     \t\tSELECT\n     \t\t\t*,\n     \t\t\tROW_NUMBER() over(PARTITION by name\n     \t\tORDER by\n     \t\t\torderdate) as rn\n     \t\tFROM\n     \t\t\tbusiness \n     ) t1)t2\n     group by\n     \tname,\n     \ttemp\n     HAVING\n     \tc >= 2\n     ;\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     25\n     26\n     ',normalizedContent:'# 函数\n\n\n# 系统内置函数\n\nshow functions;  -- 查看系统自带的函数\nshow functions like "*date*"; -- 模糊查询函数名\ndesc function upper; -- 显示自带的函数的用法\ndesc function extended upper; -- 详细显示自带的函数的用法\n\n\n1\n2\n3\n4\n\n\n\n# 空字段赋值\n\nnvl( value，default_value) 如果value为null 则替换为 default_value\n\nselect comm,nvl(comm, -1) from emp;  -- 替换为指定值\nselect comm, nvl(comm,mgr) from emp; -- 如果值为列名 则替换为当前行列的值\n\n\n1\n2\n\n\n\n# case when\n\n类似于switch\n\nselect \n  dept_id,\n  sum(case sex when \'男\' then 1 else 0 end) male_count,\n  sum(case sex when \'女\' then 1 else 0 end) female_count\nfrom \n  emp_sex\ngroup by\n  dept_id;\n-- 根据 dept_id 分组 条件判断值 再累加列个数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 行转列\n\n将多个值 / 列的值 聚合为一个值\n\n\n\nselect\n    t1.base,\n    concat_ws(\'|\', collect_set(t1.name)) name\nfrom\n    (select\n        name,\n        concat(constellation, ",", blood_type) base\n    from\n        person_info) t1\ngroup by\n    t1.base;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * concat(string a/col, string b/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;\n * concat_ws(separator, str1, str2,...)： 它是一个特殊形式的 concat()。第一个参数剩余参数间的分隔符。 将array或者string 以分隔符分割 返回 string\n * collect_set(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。\n * collect_list(col)：产生array类型字段, 包含重复元素 .\n\n\n# 列转行\n\n * explode(col)：将hive一列中复杂的array拆分一列多行 map结构拆分成两列多行 k v。\n\n * split(str,regex): 将指定字符串以 指定的分割 进行拆分 返回数组\n\n * lateral view: 虚拟表 在此基础上可以对拆分后的数据进行聚合。\n\n\n\nvi movie.txt\n\n《疑犯追踪》\t悬疑,动作,科幻,剧情\n《lie to me》\t悬疑,警匪,动作,心理,剧情\n《战狼2》\t战争,动作,灾难\n\n\n1\n2\n3\n4\n5\n\n\ncreate table movie_info(\n    movie string, \n    category string) \nrow format delimited fields terminated by "\\t";\nload data local inpath "/opt/module/datas/movie.txt" into table movie_info;\n\n\n1\n2\n3\n4\n5\n\n\n转换\n\nselect\n    m.movie,\n    tbl.cate\nfrom\n    movie_info m\nlateral view\n    explode(split(category, ",")) tbl as cate;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n现在根据电影名来进行分组聚合 请实现使用类分组 查看每个类别下的电影名\n\nselect\n    cate,\n    collect_list(movie)\nfrom\n    (select\n    m.movie,\n    tbl,cate\nfrom\n    movie_info m\nlateral view\n    explode(split(category, ",")) tbl as cate;) t1\ngroup by \n    cate;\n\n#两sql语句一致\nselect\n    tbl.cate,\n    collect_list(m.movie)\nfrom\n    movie_info m\nlateral view\n    explode(split(category, ",")) tbl as cate\ngroup by \n    cate;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 窗口函数(开窗函数)\n\n数据准备：name，orderdate，cost\n\nvim /opt/module/datas/business.txt\n\n\n1\n\n\njack,2017-01-01,10\ntony,2017-01-02,15\njack,2017-02-03,23\ntony,2017-01-04,29\njack,2017-01-05,46\njack,2017-04-06,42\ntony,2017-01-07,50\njack,2017-01-08,55\nmart,2017-04-08,62\nmart,2017-04-09,68\nneil,2017-05-10,12\nmart,2017-04-11,75\nneil,2017-06-12,80\nmart,2017-04-13,94\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n创建表\n\ncreate table business(\nname string, \norderdate string,\ncost int\n) row format delimited fields terminated by \',\';\n\nload data local inpath "/opt/module/datas/business.txt" into table business;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 窗口函数聚合\n\n查询在2017年4月份购买过的顾客及总人数\n\n统计所有2017-04的时间 根据用户名去重 count()默认是一列 而我们的name是多列 如果直接拼接则会报错,我们通过窗口函数 over()进行回写\n\nselect \n    distinct name,count(distinct name) over()  -- distinct去重\nfrom business\nwhere\n    substring(orderdate,1,7) = "2017-04";\n\n\n1\n2\n3\n4\n5\n\n\n查询顾客的购买明细及月购买总额\n\n使用窗口函数 根据时间列 分区 统计每区的sum 进行回写到对应的分区 每一个单元格中\n\nselect name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;\n\n\n1\n\n\n上述的场景, 将每个顾客的cost按照日期进行累加\n\nselect name,orderdate,cost, \nsum(cost) over() as sample1,-- 所有行相加 \nsum(cost) over(partition by name) as sample2,-- 按name分组，组内数据累加 \nsum(cost) over(partition by name order by orderdate) as sample3,-- 按name分组，组内数据累加 \nsum(cost) over(partition by name order by orderdate rows between unbounded preceding and current row ) as sample4 ,-- 和sample3一样,由起点到当前行的聚合 \nsum(cost) over(partition by name order by orderdate rows between 1 preceding and current row) as sample5, -- 当前行和前面一行做聚合 \nsum(cost) over(partition by name order by orderdate rows between 1 preceding and 1 following ) as sample6,-- 当前行和前边一行及后面一行 \nsum(cost) over(partition by name order by orderdate rows between current row and unbounded following ) as sample7 -- 当前行及后面所有行 \nfrom business;\n\n#将上述操作整合到一个窗口函数中\nselect name,orderdate,cost,\nsum(cost) over(partition by month(orderdate)) mc,\nsum(cost) over(partition by name order by orderdate asc rows between unbounded preceding and current row) lc, -- 每人购买金额的累加\nsum(cost) over(partition by name,substring(orderdate,1,7))  -- 每人每月的购买金额 \nfrom business;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nover()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。\n\n * current row 当前行\n * n preceding 往前n行数据\n * n following 往后n行数据\n * unbounded 起点，unbounded preceding 表示从前面的起点， unbounded following表示到后面的终点\n * rows必须跟在order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量\n\n\n# 其他函数\n\n * lag(col,n,default_val)：往前第n行数据\n   \n   * col 列名\n   \n   * n 显示当前行的前几行\n   \n   * default_val 如果前几行中没有数据则以此值为默认值\n   \n   * 查看顾客上次的购买时间\n   \n   * select name,orderdate,cost,\n     lag(orderdate,1,\'1970-01-01\') over(partition by name order by orderdate ) as time1,\n     lag(orderdate,2) over (partition by name order by orderdate) as time2\n     from business;\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * lead(col,n, default_val)：往后第n行数据\n   \n   * col 列名\n   * n 显示当前行的后几行\n   * default_val 如果前几行中没有数据则以此值为默认值\n\n * ntile(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，ntile返回此行所属的组的编号。注意：n必须为int类型。\n   \n   * 查询前20%时间的订单信息\n   \n   * select * from (\n         select name,orderdate,cost, \n         ntile(5) over(order by orderdate) sorted -- 将结果分成5组数据 \n         from business\n     ) t\n     where sorted = 1;  -- 因为平均分为5组了 所有第一组为前百分之20\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     \n\n * percent_rank() 求出当前行在结果集中位置的百分百 为double类型 每次计算后面都会有误差\n   \n   * select name,orderdate,cost, \n     percent_rank() over(order by orderdate) pr -- 返回一个0.00-1.00 的值 为当前行所在结果集中占据位置的百分比\n     from business\n     \n     \n     1\n     2\n     3\n     \n\n\n# rank\n\n原始数据\n\nname   subject   score\n孙悟空    语文        87\n孙悟空    数学        95\n孙悟空    英语        68\n大海     语文        94\n大海     数学        56\n大海     英语        84\n宋宋     语文        64\n宋宋     数学        86\n宋宋     英语        84\n婷婷     语文        65\n婷婷     数学        85\n婷婷     英语        78\n\ncreate table score(\nname string,\nsubject string, \nscore int) \nrow format delimited fields terminated by "\\t";\nload data local inpath \'/opt/module/datas/score.txt\' into table score;\n\n\n1\n2\n3\n4\n5\n6\n\n\n计算每门学科成绩排名\n\nselect\n    *,\n    rank() over(partition by subject order by score desc) r,\n    dense_rank() over(partition by subject order by score desc) dr,\n    row_number() over(partition by subject order by score desc) rn\nfrom\n    score;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * rank() 排序相同时会重复，总数不会变\n * dense_rank() 排序相同时会重复，总数会减少\n * row_number() 会根据顺序计算\n\n\n\n\n# 日期函数\n\n * current_date返回当前日期\n   \n   * select current_date();\n     \n     \n     1\n     \n\n * date_add 日期的加\n   \n   * -- 今天开始90天以后的日期\n     select date_add(current_date(), 90);\n     \n     \n     1\n     2\n     \n\n * date_sub 日期的减\n   \n   * -- 今天开始90天以前的日期\n     select date_sub(current_date(), 90);\n     \n     \n     1\n     2\n     \n\n * 两个日期之间的日期差\n   \n   * -- 今天和1990年6月4日的天数差\n     select datediff(current_date(), "1990-06-04"); -- 返回的为天数\n     \n     \n     1\n     2\n     \n\n * 判断哪个顾客连续两天光顾过\n   \n   * select\n     \tname,\n     \tcount(*) c\n     from\n     \t(\n     \tselect\n     \t\t*,\n     \t\tdate_sub(orderdate,\n     \t\trn) temp\n     \t\t-- 原始时间减去当前name区时间对应的序号 \n     \tfrom\n     \t\t(\n     \t\tselect\n     \t\t\t*,\n     \t\t\trow_number() over(partition by name\n     \t\torder by\n     \t\t\torderdate) as rn\n     \t\tfrom\n     \t\t\tbusiness \n     ) t1)t2\n     group by\n     \tname,\n     \ttemp\n     having\n     \tc >= 2\n     ;\n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     14\n     15\n     16\n     17\n     18\n     19\n     20\n     21\n     22\n     23\n     24\n     25\n     26\n     ',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"自定义函数",frontmatter:{title:"自定义函数",date:"2022-03-17T22:07:28.000Z",permalink:"/pages/b1e5c6/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/10.%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0.html",relativePath:"大数据/03.Hive/10.自定义函数.md",key:"v-7abe8cca",path:"/pages/b1e5c6/",headers:[{level:2,title:"已过时都UDF方法",slug:"已过时都udf方法",normalizedTitle:"已过时都udf方法",charIndex:269},{level:2,title:"新api",slug:"新api",normalizedTitle:"新api",charIndex:1219}],headersStr:"已过时都UDF方法 新api",content:'# 自定义函数\n\n 1. UDF（User-Defined-Function）\n\n一进一出\n\n 2. UDAF（User-Defined Aggregation Function）\n\n聚集函数，多进一出\n\n类似于：count/max/min\n\n 3. UDTF（User-Defined Table-Generating Functions）\n\n一进多出\n\n如lateral view explore()\n\nhttps://cwiki.apache.org/confluence/display/Hive/HivePlugins\n\n\n# 已过时都UDF方法\n\n导入依赖\n\n<dependencies>\n\t\t\x3c!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --\x3e\n\t\t<dependency>\n\t\t\t<groupId>org.apache.hive</groupId>\n\t\t\t<artifactId>hive-exec</artifactId>\n\t\t\t<version>3.1.2</version>\n\t\t</dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n继承UDF类 并书写extends方法\n\npackage com.atguigu.hive;\n\nimport org.apache.hadoop.hive.ql.exec.UDF;\n\n\npublic class MyUDF extends UDF {\n    //输入一个字符串 返回字符串长度  必须为evaluate这个方法名\n    public int evaluate(String input) {\n        if (input == null) {\n            return 0;\n        }\n        return input.length();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n打包成jar,再将jar包上传到hive中 /opt/module/hive/lib/\n\ncd /opt/module/hive/lib/\n\n\n1\n\n\n在hive中添加jar包 或者 重启hive 它会自动加载\n\nadd jar /opt/module/hive/lib/hive_function-1.0-SNAPSHOT.jar; -- 添加jar到classpath中\ncreate temporary function my_len as "com.atguigu.hive.MyUDF";\n-- create temporary function 自定义名称 as "自定义函数类路径"\n\n\n1\n2\n3\n\n\n使用时通过自定义的名称来使用\n\n\n# 新api\n\n继承GenericUDF类 并重写抽象方法\n\npackage com.atguigu.hive;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDF;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n\npublic class MyNewUDF extends GenericUDF {\n    /**\n     * 对输入的方法做检查 以及约束输出的类型\n     * @param objectInspectors 输入参数的检查器\n     * @return 输出的参数检查器\n     * @throws UDFArgumentException\n     */\n    @Override\n    public ObjectInspector initialize(ObjectInspector[] objectInspectors) throws UDFArgumentException {\n        //长度检查\n        if (objectInspectors.length !=1){\n            throw  new UDFArgumentLengthException("Wrong arguments count!");\n        }\n        //类型检查\n        if (!objectInspectors[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE)){\n            throw new UDFArgumentTypeException(0,"Wrong arguments type!");\n        }\n        return PrimitiveObjectInspectorFactory.javaIntObjectInspector; //返回java中int类型\n\n    }\n\n    /**\n     * 实现逻辑的方法\n     * @param deferredObjects\n     * @return\n     * @throws HiveException\n     */\n    @Override\n    public Object evaluate(DeferredObject[] deferredObjects) throws HiveException {\n        Object o = deferredObjects[0].get();\n        if (o == null){\n            return 0;\n        }\n\n        return o.toString().length();\n    }\n\n    /**\n     * 函数执行出错 提示什么\n     * @param strings\n     * @return\n     */\n    @Override\n    public String getDisplayString(String[] strings) {\n        return "";\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n\n\n打包上传hive lib中\n\ncreate temporary function my_len as "com.atguigu.hive.MyNewUDF";\n\n\n1\n\n\n使用\n\nselect ename, my_len(ename) from emp;\n\n\n1\n',normalizedContent:'# 自定义函数\n\n 1. udf（user-defined-function）\n\n一进一出\n\n 2. udaf（user-defined aggregation function）\n\n聚集函数，多进一出\n\n类似于：count/max/min\n\n 3. udtf（user-defined table-generating functions）\n\n一进多出\n\n如lateral view explore()\n\nhttps://cwiki.apache.org/confluence/display/hive/hiveplugins\n\n\n# 已过时都udf方法\n\n导入依赖\n\n<dependencies>\n\t\t\x3c!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --\x3e\n\t\t<dependency>\n\t\t\t<groupid>org.apache.hive</groupid>\n\t\t\t<artifactid>hive-exec</artifactid>\n\t\t\t<version>3.1.2</version>\n\t\t</dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n继承udf类 并书写extends方法\n\npackage com.atguigu.hive;\n\nimport org.apache.hadoop.hive.ql.exec.udf;\n\n\npublic class myudf extends udf {\n    //输入一个字符串 返回字符串长度  必须为evaluate这个方法名\n    public int evaluate(string input) {\n        if (input == null) {\n            return 0;\n        }\n        return input.length();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n打包成jar,再将jar包上传到hive中 /opt/module/hive/lib/\n\ncd /opt/module/hive/lib/\n\n\n1\n\n\n在hive中添加jar包 或者 重启hive 它会自动加载\n\nadd jar /opt/module/hive/lib/hive_function-1.0-snapshot.jar; -- 添加jar到classpath中\ncreate temporary function my_len as "com.atguigu.hive.myudf";\n-- create temporary function 自定义名称 as "自定义函数类路径"\n\n\n1\n2\n3\n\n\n使用时通过自定义的名称来使用\n\n\n# 新api\n\n继承genericudf类 并重写抽象方法\n\npackage com.atguigu.hive;\n\nimport org.apache.hadoop.hive.ql.exec.udfargumentexception;\nimport org.apache.hadoop.hive.ql.exec.udfargumentlengthexception;\nimport org.apache.hadoop.hive.ql.exec.udfargumenttypeexception;\nimport org.apache.hadoop.hive.ql.metadata.hiveexception;\nimport org.apache.hadoop.hive.ql.udf.generic.genericudf;\nimport org.apache.hadoop.hive.serde2.objectinspector.objectinspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.primitiveobjectinspectorfactory;\n\npublic class mynewudf extends genericudf {\n    /**\n     * 对输入的方法做检查 以及约束输出的类型\n     * @param objectinspectors 输入参数的检查器\n     * @return 输出的参数检查器\n     * @throws udfargumentexception\n     */\n    @override\n    public objectinspector initialize(objectinspector[] objectinspectors) throws udfargumentexception {\n        //长度检查\n        if (objectinspectors.length !=1){\n            throw  new udfargumentlengthexception("wrong arguments count!");\n        }\n        //类型检查\n        if (!objectinspectors[0].getcategory().equals(objectinspector.category.primitive)){\n            throw new udfargumenttypeexception(0,"wrong arguments type!");\n        }\n        return primitiveobjectinspectorfactory.javaintobjectinspector; //返回java中int类型\n\n    }\n\n    /**\n     * 实现逻辑的方法\n     * @param deferredobjects\n     * @return\n     * @throws hiveexception\n     */\n    @override\n    public object evaluate(deferredobject[] deferredobjects) throws hiveexception {\n        object o = deferredobjects[0].get();\n        if (o == null){\n            return 0;\n        }\n\n        return o.tostring().length();\n    }\n\n    /**\n     * 函数执行出错 提示什么\n     * @param strings\n     * @return\n     */\n    @override\n    public string getdisplaystring(string[] strings) {\n        return "";\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n\n\n打包上传hive lib中\n\ncreate temporary function my_len as "com.atguigu.hive.mynewudf";\n\n\n1\n\n\n使用\n\nselect ename, my_len(ename) from emp;\n\n\n1\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"压缩和存储",frontmatter:{title:"压缩和存储",date:"2022-03-17T22:07:28.000Z",permalink:"/pages/9c39ac/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/11.%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8.html",relativePath:"大数据/03.Hive/11.压缩和存储.md",key:"v-a20663ee",path:"/pages/9c39ac/",headers:[{level:2,title:"压缩参数设置",slug:"压缩参数设置",normalizedTitle:"压缩参数设置",charIndex:12},{level:2,title:"开启map输出阶段压缩(MR引擎)",slug:"开启map输出阶段压缩-mr引擎",normalizedTitle:"开启map输出阶段压缩(mr引擎)",charIndex:1253},{level:2,title:"开启Reduce输出阶段压缩",slug:"开启reduce输出阶段压缩",normalizedTitle:"开启reduce输出阶段压缩",charIndex:1513},{level:2,title:"文件存储格式",slug:"文件存储格式",normalizedTitle:"文件存储格式",charIndex:1863},{level:2,title:"TEXTFILE",slug:"textfile",normalizedTitle:"textfile",charIndex:1895},{level:2,title:"Orc格式",slug:"orc格式",normalizedTitle:"orc格式",charIndex:2052},{level:2,title:"Parquet格式",slug:"parquet格式",normalizedTitle:"parquet格式",charIndex:2603},{level:2,title:"指定文件存储格式",slug:"指定文件存储格式",normalizedTitle:"指定文件存储格式",charIndex:2909},{level:3,title:"textfile",slug:"textfile-2",normalizedTitle:"textfile",charIndex:2922},{level:3,title:"orc",slug:"orc",normalizedTitle:"orc",charIndex:2735},{level:3,title:"parquet",slug:"parquet",normalizedTitle:"parquet",charIndex:3589},{level:3,title:"三种存储格式大小比较",slug:"三种存储格式大小比较",normalizedTitle:"三种存储格式大小比较",charIndex:3903},{level:2,title:"存储和压缩结合",slug:"存储和压缩结合",normalizedTitle:"存储和压缩结合",charIndex:3920},{level:3,title:"orc存储方式的压缩",slug:"orc存储方式的压缩",normalizedTitle:"orc存储方式的压缩",charIndex:3932},{level:3,title:"压缩后文件大小比较",slug:"压缩后文件大小比较",normalizedTitle:"压缩后文件大小比较",charIndex:5483},{level:3,title:"Parquet存储方式的压缩",slug:"parquet存储方式的压缩",normalizedTitle:"parquet存储方式的压缩",charIndex:5499},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:5878}],headersStr:"压缩参数设置 开启map输出阶段压缩(MR引擎) 开启Reduce输出阶段压缩 文件存储格式 TEXTFILE Orc格式 Parquet格式 指定文件存储格式 textfile orc parquet 三种存储格式大小比较 存储和压缩结合 orc存储方式的压缩 压缩后文件大小比较 Parquet存储方式的压缩 总结",content:'# 压缩和存储\n\n\n# 压缩参数设置\n\n要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：\n\n参数                                                 默认值                                           阶段          建议\nio.compression.codecs （在core-site.xml中配置）          org.apache.hadoop.io.compress.DefaultCodec,   输入压缩        Hadoop使用文件扩展名判断是否支持某种编解码器\n                                                   org.apache.hadoop.io.compress.GzipCodec,\n                                                   org.apache.hadoop.io.compress.BZip2Codec,\n                                                   org.apache.hadoop.io.compress.Lz4Codec\nmapreduce.map.output.compress                      false                                         mapper输出    这个参数设为true启用压缩\nmapreduce.map.output.compress.codec                org.apache.hadoop.io.compress.DefaultCodec    mapper输出    使用LZO、LZ4或snappy编解码器在此阶段压缩数据\nmapreduce.output.fileoutputformat.compress         false                                         reducer输出   这个参数设为true启用压缩\nmapreduce.output.fileoutputformat.compress.codec   org.apache.hadoop.io.compress. DefaultCodec   reducer输出   使用标准工具或者编解码器，如gzip和bzip2\nmapreduce.output.fileoutputformat.compress.type    RECORD                                        reducer输出   SequenceFile输出使用的压缩类型：NONE和BLOCK\n\n\n# 开启map输出阶段压缩(MR引擎)\n\n临时开启\n\nset hive.exec.compress.intermediate=true; -- 开启hive中间传送的压缩功能\nset mapreduce.map.output.compress=true; -- 开启mapreduce中map输出压缩功能\nset mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec; -- 设置指定的压缩方式\n\n\n1\n2\n3\n\n\n\n# 开启Reduce输出阶段压缩\n\nset hive.exec.compress.output=true; -- 开启hive最终输出数据压缩\nset mapreduce.output.fileoutputformat.compress=true; -- 开启mapreduce最终输出数据压缩\nset mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec; -- 设置压缩方式\nset mapreduce.output.fileoutputformat.compress.type=BLOCK; -- 设置mapreduce最终输出压缩方式\n\n\n1\n2\n3\n4\n\n\n\n# 文件存储格式\n\nHive支持的存储数据的格式主要有：\n\n行存储 TEXTFILE 、SEQUENCEFILE\n\n列存储 ORC、PARQUET Hive中使用列存储比较好\n\n\n\n\n# TEXTFILE\n\n默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。\n\n\n# Orc格式\n\nOrc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：\n\n\n\n1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。\n\n2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。\n\n3）Stripe Footer：存的是各个Stream的类型，长度等信息。\n\n每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。\n\n\n# Parquet格式\n\nParquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。\n\n（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。\n\n（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。\n\n（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。\n\n\n# 指定文件存储格式\n\n\n# textfile\n\n在建表时通过stored 关键字指定格式\n\ncreate table log_text (\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as textfile;  -- 指定为textfile 默认也是textfile\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# orc\n\ncreate table log_orc(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as orc  -- 指定为orc格式\ntblproperties("orc.compress"="NONE"); -- 去掉压缩\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n只能插入数据 不能直接加数据文件\n\ninsert into table log_orc select * from log_text ;\n\n\n1\n\n\n\n# parquet\n\ncreate table log_parquet(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as parquet ;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n插入数据\n\ninsert into table log_parquet select * from log_text ;\n\n\n1\n\n\n\n# 三种存储格式大小比较\n\n\n\n\n# 存储和压缩结合\n\n\n# orc存储方式的压缩\n\nKEY                        DEFAULT       NOTES\norc.compress               ZLIB          high level compression (one of NONE, ZLIB, SNAPPY)\norc.compress.size          262,144       number of bytes in each compression chunk\norc.stripe.size            268,435,456   number of bytes in each stripe\norc.row.index.stride       10,000        number of rows between index entries (must be >= 1000)\norc.create.index           true          whether to create row indexes\norc.bloom.filter.columns   ""            comma separated list of column names for which bloom filter\n                                         should be created\norc.bloom.filter.fpp       0.05          false positive probability for bloom filter (must >0.0 and\n                                         <1.0)\n\n所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现 在建表时指定\n\n# ZLIB压缩格式\n\ncreate table log_zlib(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as orc  \ntblproperties("orc.compress"="ZLIB"); -- 指定压缩为ZLIB 默认也为ZLIB\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n插入数据\n\ninsert into log_orc_zlib select * from log_text;\n\n\n1\n\n\n# SNAPPY压缩格式\n\ncreate table log_orc_snappy(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as orc\ntblproperties("orc.compress"="SNAPPY");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n插入数据\n\ninsert into log_orc_snappy select * from log_text;\n\n\n1\n\n\n\n# 压缩后文件大小比较\n\n\n\n\n# Parquet存储方式的压缩\n\ncreate table log_par_snappy(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as parquet\ntblproperties("parquet.compression"="SNAPPY");\n\ninsert into log_par_snappy select * from log_text;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n\n未压缩和压缩之后文件大小\n\n\n# 总结\n\n在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。',normalizedContent:'# 压缩和存储\n\n\n# 压缩参数设置\n\n要在hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：\n\n参数                                                 默认值                                           阶段          建议\nio.compression.codecs （在core-site.xml中配置）          org.apache.hadoop.io.compress.defaultcodec,   输入压缩        hadoop使用文件扩展名判断是否支持某种编解码器\n                                                   org.apache.hadoop.io.compress.gzipcodec,\n                                                   org.apache.hadoop.io.compress.bzip2codec,\n                                                   org.apache.hadoop.io.compress.lz4codec\nmapreduce.map.output.compress                      false                                         mapper输出    这个参数设为true启用压缩\nmapreduce.map.output.compress.codec                org.apache.hadoop.io.compress.defaultcodec    mapper输出    使用lzo、lz4或snappy编解码器在此阶段压缩数据\nmapreduce.output.fileoutputformat.compress         false                                         reducer输出   这个参数设为true启用压缩\nmapreduce.output.fileoutputformat.compress.codec   org.apache.hadoop.io.compress. defaultcodec   reducer输出   使用标准工具或者编解码器，如gzip和bzip2\nmapreduce.output.fileoutputformat.compress.type    record                                        reducer输出   sequencefile输出使用的压缩类型：none和block\n\n\n# 开启map输出阶段压缩(mr引擎)\n\n临时开启\n\nset hive.exec.compress.intermediate=true; -- 开启hive中间传送的压缩功能\nset mapreduce.map.output.compress=true; -- 开启mapreduce中map输出压缩功能\nset mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.snappycodec; -- 设置指定的压缩方式\n\n\n1\n2\n3\n\n\n\n# 开启reduce输出阶段压缩\n\nset hive.exec.compress.output=true; -- 开启hive最终输出数据压缩\nset mapreduce.output.fileoutputformat.compress=true; -- 开启mapreduce最终输出数据压缩\nset mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.snappycodec; -- 设置压缩方式\nset mapreduce.output.fileoutputformat.compress.type=block; -- 设置mapreduce最终输出压缩方式\n\n\n1\n2\n3\n4\n\n\n\n# 文件存储格式\n\nhive支持的存储数据的格式主要有：\n\n行存储 textfile 、sequencefile\n\n列存储 orc、parquet hive中使用列存储比较好\n\n\n\n\n# textfile\n\n默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合gzip、bzip2使用，但使用gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。\n\n\n# orc格式\n\norc文件由1个或多个stripe组成，每个stripe一般为hdfs的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到parquet中的row group的概念。每个stripe里有三部分组成，分别是index data，row data，stripe footer：\n\n\n\n1）index data：一个轻量级的index，默认是每隔1w行做一个索引。这里做的索引应该只是记录某行的各字段在row data中的offset。\n\n2）row data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个stream来存储。\n\n3）stripe footer：存的是各个stream的类型，长度等信息。\n\n每个文件有一个file footer，这里面存的是每个stripe的行数，每个column的数据类型信息等；每个文件的尾部是一个postscript，这里面记录了整个文件的压缩类型以及filefooter的长度信息等。在读取文件时，会seek到文件尾部读postscript，从里面解析到file footer长度，再读filefooter，从里面解析到各个stripe信息，再读各个stripe，即从后往前读。\n\n\n# parquet格式\n\nparquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此parquet格式文件是自解析的。\n\n（1）行组(row group)：每一个行组包含一定的行数，在一个hdfs文件中至少存储一个行组，类似于orc的stripe的概念。\n\n（2）列块(column chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。\n\n（3）页(page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。\n\n\n# 指定文件存储格式\n\n\n# textfile\n\n在建表时通过stored 关键字指定格式\n\ncreate table log_text (\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as textfile;  -- 指定为textfile 默认也是textfile\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# orc\n\ncreate table log_orc(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as orc  -- 指定为orc格式\ntblproperties("orc.compress"="none"); -- 去掉压缩\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n只能插入数据 不能直接加数据文件\n\ninsert into table log_orc select * from log_text ;\n\n\n1\n\n\n\n# parquet\n\ncreate table log_parquet(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as parquet ;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n插入数据\n\ninsert into table log_parquet select * from log_text ;\n\n\n1\n\n\n\n# 三种存储格式大小比较\n\n\n\n\n# 存储和压缩结合\n\n\n# orc存储方式的压缩\n\nkey                        default       notes\norc.compress               zlib          high level compression (one of none, zlib, snappy)\norc.compress.size          262,144       number of bytes in each compression chunk\norc.stripe.size            268,435,456   number of bytes in each stripe\norc.row.index.stride       10,000        number of rows between index entries (must be >= 1000)\norc.create.index           true          whether to create row indexes\norc.bloom.filter.columns   ""            comma separated list of column names for which bloom filter\n                                         should be created\norc.bloom.filter.fpp       0.05          false positive probability for bloom filter (must >0.0 and\n                                         <1.0)\n\n所有关于orcfile的参数都是在hql语句的tblproperties字段里面出现 在建表时指定\n\n# zlib压缩格式\n\ncreate table log_zlib(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as orc  \ntblproperties("orc.compress"="zlib"); -- 指定压缩为zlib 默认也为zlib\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n插入数据\n\ninsert into log_orc_zlib select * from log_text;\n\n\n1\n\n\n# snappy压缩格式\n\ncreate table log_orc_snappy(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as orc\ntblproperties("orc.compress"="snappy");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n插入数据\n\ninsert into log_orc_snappy select * from log_text;\n\n\n1\n\n\n\n# 压缩后文件大小比较\n\n\n\n\n# parquet存储方式的压缩\n\ncreate table log_par_snappy(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nrow format delimited fields terminated by \'\\t\'\nstored as parquet\ntblproperties("parquet.compression"="snappy");\n\ninsert into log_par_snappy select * from log_text;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n\n未压缩和压缩之后文件大小\n\n\n# 总结\n\n在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"查询",frontmatter:{title:"查询",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/73e227/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/08.%E6%9F%A5%E8%AF%A2.html",relativePath:"大数据/03.Hive/08.查询.md",key:"v-17274f0b",path:"/pages/73e227/",headers:[{level:2,title:"算术运算符",slug:"算术运算符",normalizedTitle:"算术运算符",charIndex:657},{level:2,title:"常用函数",slug:"常用函数",normalizedTitle:"常用函数",charIndex:822},{level:3,title:"UDF函数",slug:"udf函数",normalizedTitle:"udf函数",charIndex:831},{level:3,title:"UDAF函数",slug:"udaf函数",normalizedTitle:"udaf函数",charIndex:899},{level:3,title:"UDTF函数",slug:"udtf函数",normalizedTitle:"udtf函数",charIndex:1148},{level:2,title:"Limit",slug:"limit",normalizedTitle:"limit",charIndex:1180},{level:2,title:"Where",slug:"where",normalizedTitle:"where",charIndex:1231},{level:3,title:"Like和Rlike",slug:"like和rlike",normalizedTitle:"like和rlike",charIndex:2527},{level:3,title:"逻辑运算符（And/Or/Not）",slug:"逻辑运算符-and-or-not",normalizedTitle:"逻辑运算符（and/or/not）",charIndex:2701},{level:2,title:"分组",slug:"分组",normalizedTitle:"分组",charIndex:2817},{level:3,title:"Group By语句",slug:"group-by语句",normalizedTitle:"group by语句",charIndex:2824},{level:3,title:"Having语句",slug:"having语句",normalizedTitle:"having语句",charIndex:2924},{level:3,title:"mysql执行顺序:",slug:"mysql执行顺序",normalizedTitle:"mysql执行顺序:",charIndex:3058},{level:2,title:"Join",slug:"join",normalizedTitle:"join",charIndex:3151},{level:3,title:"内连接",slug:"内连接",normalizedTitle:"内连接",charIndex:3200},{level:3,title:"左外连接",slug:"左外连接",normalizedTitle:"左外连接",charIndex:3354},{level:3,title:"右外连接",slug:"右外连接",normalizedTitle:"右外连接",charIndex:3522},{level:3,title:"满外连接",slug:"满外连接",normalizedTitle:"满外连接",charIndex:3689},{level:3,title:"多表连接查询",slug:"多表连接查询",normalizedTitle:"多表连接查询",charIndex:3883},{level:3,title:"笛卡尔积",slug:"笛卡尔积",normalizedTitle:"笛卡尔积",charIndex:4042},{level:2,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:4159},{level:3,title:"全局排序（Order By）",slug:"全局排序-order-by",normalizedTitle:"全局排序（order by）",charIndex:4166},{level:3,title:"内部排序(Sort By)",slug:"内部排序-sort-by",normalizedTitle:"内部排序(sort by)",charIndex:4413},{level:3,title:"分区排序(Distribute By)",slug:"分区排序-distribute-by",normalizedTitle:"分区排序(distribute by)",charIndex:4697},{level:3,title:"桶排序(Cluster By)",slug:"桶排序-cluster-by",normalizedTitle:"桶排序(cluster by)",charIndex:4975},{level:2,title:"分桶表和抽样查询",slug:"分桶表和抽样查询",normalizedTitle:"分桶表和抽样查询",charIndex:5230},{level:3,title:"抽样查询",slug:"抽样查询",normalizedTitle:"抽样查询",charIndex:5234}],headersStr:"算术运算符 常用函数 UDF函数 UDAF函数 UDTF函数 Limit Where Like和Rlike 逻辑运算符（And/Or/Not） 分组 Group By语句 Having语句 mysql执行顺序: Join 内连接 左外连接 右外连接 满外连接 多表连接查询 笛卡尔积 排序 全局排序（Order By） 内部排序(Sort By) 分区排序(Distribute By) 桶排序(Cluster By) 分桶表和抽样查询 抽样查询",content:"# 查询\n\n先创建部门表和员工表\n\ncreate table if not exists dept(\ndeptno int,\ndname string,\nloc int\n)\nrow format delimited fields terminated by '\\t';\n\ncreate table if not exists emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string, \nsal double, \ncomm double,\ndeptno int)\nrow format delimited fields terminated by '\\t';\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n导入数据\n\nload data local inpath '/opt/module/datas/dept.txt' into table dept;\nload data local inpath '/opt/module/datas/emp.txt' into table emp;\n\n\n1\n2\n\n\n查询\n\nselect * from emp; -- 全表查询\nselect empno, ename from emp; -- 特定列查询\nselect empno as pno, ename as name from emp; -- 起列别名 as可以省略\n\n\n1\n2\n3\n\n\n\n# 算术运算符\n\n运算符   描述\nA+B   A和B 相加\nA-B   A减去B\nA*B   A和B 相乘\nA/B   A除以B\nA%B   A对B取余\nA&B   A和B按位取与\nA|B   A和B按位取或\nA^B   A和B按位取异或\n~A    A按位取反\n\nselect sal +1 from emp;\n\n\n1\n\n\n\n# 常用函数\n\n\n# UDF函数\n\n给定一个输入 返回一个输出\n\nselect substring(name,1,1) from emp;\n\n\n1\n\n\n\n# UDAF函数\n\n给定多个输入 返回一个输出 聚合函数\n\nselect count(*) from emp; -- 统计个数\nselect max(sal) max_sal from emp; -- 此列最大值\nselect min(sal) min_sal from emp; -- 此列最小值\nselect sum(sal) sum_sal from emp;  -- 此列总和\nselect avg(sal) avg_sal from emp; -- 平均数\n\n\n1\n2\n3\n4\n5\n\n\n\n# UDTF函数\n\n表生成函数 给定一个输入 返回多个输出\n\n\n# Limit\n\nselect * from emp limit 5; -- 取前n行\n\n\n1\n\n\n\n# Where\n\n条件语句\n\nselect * from emp where sal > 1000;\n\n\n1\n\n\n操作符                       支持的数据类型     描述\nA=B                       基本数据类型      如果A等于B则返回TRUE，反之返回FALSE\nA<=>B                     基本数据类型      如果A和B都为NULL，则返回TRUE，如果一边为NULL，返回False\nA<>B, A!=B                基本数据类型      A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE\nA<B                       基本数据类型      A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE\nA<=B                      基本数据类型      A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE\nA>B                       基本数据类型      A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE\nA>=B                      基本数据类型      A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE\nA [NOT] BETWEEN B AND C   基本数据类型      如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。\nA IS NULL                 所有数据类型      如果A等于NULL，则返回TRUE，反之返回FALSE\nA IS NOT NULL             所有数据类型      如果A不等于NULL，则返回TRUE，反之返回FALSE\nIN(数值1, 数值2)              所有数据类型      使用 IN运算显示列表中的值\nA [NOT] LIKE B            STRING 类型   B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。\nA RLIKE B, A REGEXP B     STRING 类型   B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。\n\n\n# Like和Rlike\n\nLike是通配符匹配\n\nselect * from emp where ename like \"A%\"; -- 通配符 %_   %零个或多个字符 _单个字符 \n\n\n1\n\n\nRlike是正则表达式匹配\n\nselect * from emp where ename rlike \"^A\"; -- 正则匹配 \n\n\n1\n\n\n\n# 逻辑运算符（And/Or/Not）\n\n操作符   含义\nAND   逻辑并\nOR    逻辑或\nNOT   逻辑否\n\nselect * from emp where sal>1000 and deptno=30;\n\n\n1\n\n\n\n# 分组\n\n\n# Group By语句\n\nselect t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno; -- 按deptno分组\n\n\n1\n\n\n\n# Having语句\n\nwhere后面不能写分组函数，而having后面可以使用分组函数。\n\nselect deptno, avg(sal) avg_sal from emp group by deptno having avg_sal > 2000;\n\n\n1\n\n\n\n# mysql执行顺序:\n\n 1. from\n 2. where\n 3. group by\n 4. select\n 5. having\n 6. order by\n 7. limit\n\n\n# Join\n\nHive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接\n\n\n# 内连接\n\n内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。\n\n只有查询表和join表同时存在才保存\n\nselect e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 左外连接\n\n左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。\n\n就算了查询表在join没有符合的数据 也会返回查询结果\n\nselect e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 右外连接\n\n右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。\n\n如果join表中有查询表没有的数据 也会被保存下来\n\nselect e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 满外连接\n\n满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。\n\n左右表其中一方没有的都会被保存下 使用null代替\n\nselect e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 多表连接查询\n\nSELECT e.ename, d.dname, l.loc_name\nFROM   emp e \nJOIN   dept d\nON     d.deptno = e.deptno \nJOIN   location l\nON     d.loc = l.loc;\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 笛卡尔积\n\n（1）省略连接条件 (即左表每个数据都会连接右表的每条数据 左表*右边)\n\n（2）连接条件无效\n\n（3）所有表中的所有行互相连接\n\nselect empno, dname from emp, dept;\n\n\n1\n\n\n\n# 排序\n\n\n# 全局排序（Order By）\n\n全局排序，只有一个Reducer\n\nselect *\nfrom emp \norder by sal desc;\n\nselect *  -- 多条件排序\nfrom emp \norder by \ndeptno asc,\nsal desc;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n * ASC（ascend）: 升序（默认）\n * DESC（descend）: 降序\n * 不推荐对所有数据排序 通常通过limit 求前n个 减少MapReduce工作量\n\n\n# 内部排序(Sort By)\n\n对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。\n\nSort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。\n\nset mapreduce.job.reduces=3; -- 设置reduce个数 默认为-1 即一个reduce\nset mapreduce.job.reduces; -- 查询reduce个数\n\n\n1\n2\n\n\nselect * from emp sort by deptno desc;\n\n\n1\n\n\n\n# 分区排序(Distribute By)\n\n对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。\n\nselect * from emp\ndistribute by empno\nsort by sal desc; -- 按empno分区排序 再按 sal内部排序\n\n\n1\n2\n3\n\n * distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。\n * Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前\n\n\n# 桶排序(Cluster By)\n\n当distribute by和sorts by字段相同时，可以使用cluster by方式。\n\ncluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。\n\nselect * from emp cluster by deptno;\n-- 等同于\nselect * from emp distribute by deptno sort by deptno;\n\n\n1\n2\n3\n\n\n\n# 分桶表和抽样查询\n\n创建分桶表\n\ncreate table stu_buck(id int,name string) \nclustered by (id) \ninto 4 buckets\nrow format delimited fields terminated by '\\t';\n\n\n1\n2\n3\n4\n\n\nhive3中支持从本地上传 reduce会自动帮我们分桶\n\nload data local inpath \"/opt/module/datas/student.txt\" into table stu_buck;\n\n\n1\n\n\n如果导入卡住 尝试临时换成mr引擎\n\nset hive.execution.engine = mr;\n\n\n1\n\n * 分桶可以使 无法分区的表进行分桶 (分区无法使用主键分区 因为主键是唯一)\n * 每个桶 对应则一个文件\n * 数据存放位置会根据 列值 + 桶数 来存放\n\n\n# 抽样查询\n\n对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。\n\nselect * from stu_buck tablesample(bucket 1 out of 4 on id); -- 根据查询结果 按id分成4份 从中取出第一份出来\n\n\n1\n\n * TABLESAMPLE(BUCKET x OUT OF y) x的值必须小于等于y的值，否则报错\n * 分桶表可以加快抽样效率\n\ny必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。\n\nx表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。",normalizedContent:"# 查询\n\n先创建部门表和员工表\n\ncreate table if not exists dept(\ndeptno int,\ndname string,\nloc int\n)\nrow format delimited fields terminated by '\\t';\n\ncreate table if not exists emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string, \nsal double, \ncomm double,\ndeptno int)\nrow format delimited fields terminated by '\\t';\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n导入数据\n\nload data local inpath '/opt/module/datas/dept.txt' into table dept;\nload data local inpath '/opt/module/datas/emp.txt' into table emp;\n\n\n1\n2\n\n\n查询\n\nselect * from emp; -- 全表查询\nselect empno, ename from emp; -- 特定列查询\nselect empno as pno, ename as name from emp; -- 起列别名 as可以省略\n\n\n1\n2\n3\n\n\n\n# 算术运算符\n\n运算符   描述\na+b   a和b 相加\na-b   a减去b\na*b   a和b 相乘\na/b   a除以b\na%b   a对b取余\na&b   a和b按位取与\na|b   a和b按位取或\na^b   a和b按位取异或\n~a    a按位取反\n\nselect sal +1 from emp;\n\n\n1\n\n\n\n# 常用函数\n\n\n# udf函数\n\n给定一个输入 返回一个输出\n\nselect substring(name,1,1) from emp;\n\n\n1\n\n\n\n# udaf函数\n\n给定多个输入 返回一个输出 聚合函数\n\nselect count(*) from emp; -- 统计个数\nselect max(sal) max_sal from emp; -- 此列最大值\nselect min(sal) min_sal from emp; -- 此列最小值\nselect sum(sal) sum_sal from emp;  -- 此列总和\nselect avg(sal) avg_sal from emp; -- 平均数\n\n\n1\n2\n3\n4\n5\n\n\n\n# udtf函数\n\n表生成函数 给定一个输入 返回多个输出\n\n\n# limit\n\nselect * from emp limit 5; -- 取前n行\n\n\n1\n\n\n\n# where\n\n条件语句\n\nselect * from emp where sal > 1000;\n\n\n1\n\n\n操作符                       支持的数据类型     描述\na=b                       基本数据类型      如果a等于b则返回true，反之返回false\na<=>b                     基本数据类型      如果a和b都为null，则返回true，如果一边为null，返回false\na<>b, a!=b                基本数据类型      a或者b为null则返回null；如果a不等于b，则返回true，反之返回false\na<b                       基本数据类型      a或者b为null，则返回null；如果a小于b，则返回true，反之返回false\na<=b                      基本数据类型      a或者b为null，则返回null；如果a小于等于b，则返回true，反之返回false\na>b                       基本数据类型      a或者b为null，则返回null；如果a大于b，则返回true，反之返回false\na>=b                      基本数据类型      a或者b为null，则返回null；如果a大于等于b，则返回true，反之返回false\na [not] between b and c   基本数据类型      如果a，b或者c任一为null，则结果为null。如果a的值大于等于b而且小于或等于c，则结果为true，反之为false。如果使用not关键字则可达到相反的效果。\na is null                 所有数据类型      如果a等于null，则返回true，反之返回false\na is not null             所有数据类型      如果a不等于null，则返回true，反之返回false\nin(数值1, 数值2)              所有数据类型      使用 in运算显示列表中的值\na [not] like b            string 类型   b是一个sql下的简单正则表达式，也叫通配符模式，如果a与其匹配的话，则返回true；反之返回false。b的表达式说明如下：‘x%’表示a必须以字母‘x’开头，‘%x’表示a必须以字母’x’结尾，而‘%x%’表示a包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用not关键字则可达到相反的效果。\na rlike b, a regexp b     string 类型   b是基于java的正则表达式，如果a与其匹配，则返回true；反之返回false。匹配使用的是jdk中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串a相匹配，而不是只需与其字符串匹配。\n\n\n# like和rlike\n\nlike是通配符匹配\n\nselect * from emp where ename like \"a%\"; -- 通配符 %_   %零个或多个字符 _单个字符 \n\n\n1\n\n\nrlike是正则表达式匹配\n\nselect * from emp where ename rlike \"^a\"; -- 正则匹配 \n\n\n1\n\n\n\n# 逻辑运算符（and/or/not）\n\n操作符   含义\nand   逻辑并\nor    逻辑或\nnot   逻辑否\n\nselect * from emp where sal>1000 and deptno=30;\n\n\n1\n\n\n\n# 分组\n\n\n# group by语句\n\nselect t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno; -- 按deptno分组\n\n\n1\n\n\n\n# having语句\n\nwhere后面不能写分组函数，而having后面可以使用分组函数。\n\nselect deptno, avg(sal) avg_sal from emp group by deptno having avg_sal > 2000;\n\n\n1\n\n\n\n# mysql执行顺序:\n\n 1. from\n 2. where\n 3. group by\n 4. select\n 5. having\n 6. order by\n 7. limit\n\n\n# join\n\nhive支持通常的sql join语句，但是只支持等值连接，不支持非等值连接\n\n\n# 内连接\n\n内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。\n\n只有查询表和join表同时存在才保存\n\nselect e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 左外连接\n\n左外连接：join操作符左边表中符合where子句的所有记录将会被返回。\n\n就算了查询表在join没有符合的数据 也会返回查询结果\n\nselect e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 右外连接\n\n右外连接：join操作符右边表中符合where子句的所有记录将会被返回。\n\n如果join表中有查询表没有的数据 也会被保存下来\n\nselect e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 满外连接\n\n满外连接：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代。\n\n左右表其中一方没有的都会被保存下 使用null代替\n\nselect e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;\n\n\n1\n\n\n\n# 多表连接查询\n\nselect e.ename, d.dname, l.loc_name\nfrom   emp e \njoin   dept d\non     d.deptno = e.deptno \njoin   location l\non     d.loc = l.loc;\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 笛卡尔积\n\n（1）省略连接条件 (即左表每个数据都会连接右表的每条数据 左表*右边)\n\n（2）连接条件无效\n\n（3）所有表中的所有行互相连接\n\nselect empno, dname from emp, dept;\n\n\n1\n\n\n\n# 排序\n\n\n# 全局排序（order by）\n\n全局排序，只有一个reducer\n\nselect *\nfrom emp \norder by sal desc;\n\nselect *  -- 多条件排序\nfrom emp \norder by \ndeptno asc,\nsal desc;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n * asc（ascend）: 升序（默认）\n * desc（descend）: 降序\n * 不推荐对所有数据排序 通常通过limit 求前n个 减少mapreduce工作量\n\n\n# 内部排序(sort by)\n\n对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。\n\nsort by为每个reducer产生一个排序文件。每个reducer内部进行排序，对全局结果集来说不是排序。\n\nset mapreduce.job.reduces=3; -- 设置reduce个数 默认为-1 即一个reduce\nset mapreduce.job.reduces; -- 查询reduce个数\n\n\n1\n2\n\n\nselect * from emp sort by deptno desc;\n\n\n1\n\n\n\n# 分区排序(distribute by)\n\n对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。\n\nselect * from emp\ndistribute by empno\nsort by sal desc; -- 按empno分区排序 再按 sal内部排序\n\n\n1\n2\n3\n\n * distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。\n * hive要求distribute by语句要写在sort by语句之前\n\n\n# 桶排序(cluster by)\n\n当distribute by和sorts by字段相同时，可以使用cluster by方式。\n\ncluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为asc或者desc。\n\nselect * from emp cluster by deptno;\n-- 等同于\nselect * from emp distribute by deptno sort by deptno;\n\n\n1\n2\n3\n\n\n\n# 分桶表和抽样查询\n\n创建分桶表\n\ncreate table stu_buck(id int,name string) \nclustered by (id) \ninto 4 buckets\nrow format delimited fields terminated by '\\t';\n\n\n1\n2\n3\n4\n\n\nhive3中支持从本地上传 reduce会自动帮我们分桶\n\nload data local inpath \"/opt/module/datas/student.txt\" into table stu_buck;\n\n\n1\n\n\n如果导入卡住 尝试临时换成mr引擎\n\nset hive.execution.engine = mr;\n\n\n1\n\n * 分桶可以使 无法分区的表进行分桶 (分区无法使用主键分区 因为主键是唯一)\n * 每个桶 对应则一个文件\n * 数据存放位置会根据 列值 + 桶数 来存放\n\n\n# 抽样查询\n\n对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。hive可以通过对表进行抽样来满足这个需求。\n\nselect * from stu_buck tablesample(bucket 1 out of 4 on id); -- 根据查询结果 按id分成4份 从中取出第一份出来\n\n\n1\n\n * tablesample(bucket x out of y) x的值必须小于等于y的值，否则报错\n * 分桶表可以加快抽样效率\n\ny必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。\n\nx表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"企业优化",frontmatter:{title:"企业优化",date:"2022-03-17T22:07:28.000Z",permalink:"/pages/447a54/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/12.%E4%BC%81%E4%B8%9A%E4%BC%98%E5%8C%96.html",relativePath:"大数据/03.Hive/12.企业优化.md",key:"v-a7b2b846",path:"/pages/447a54/",headers:[{level:2,title:"Fetch抓取",slug:"fetch抓取",normalizedTitle:"fetch抓取",charIndex:11},{level:2,title:"表优化",slug:"表优化",normalizedTitle:"表优化",charIndex:938},{level:3,title:"小表、大表Join",slug:"小表、大表join",normalizedTitle:"小表、大表join",charIndex:946},{level:3,title:"大表join大表",slug:"大表join大表",normalizedTitle:"大表join大表",charIndex:2249},{level:3,title:"MapJoin（MR引擎）",slug:"mapjoin-mr引擎",normalizedTitle:"mapjoin（mr引擎）",charIndex:3181},{level:3,title:"Group By",slug:"group-by",normalizedTitle:"group by",charIndex:3462},{level:3,title:"笛卡尔积",slug:"笛卡尔积",normalizedTitle:"笛卡尔积",charIndex:4068},{level:3,title:"行列过滤",slug:"行列过滤",normalizedTitle:"行列过滤",charIndex:4137},{level:3,title:"动态分区",slug:"动态分区",normalizedTitle:"动态分区",charIndex:4465},{level:2,title:"严格模式",slug:"严格模式",normalizedTitle:"严格模式",charIndex:4479},{level:2,title:"JVM重用",slug:"jvm重用",normalizedTitle:"jvm重用",charIndex:5142},{level:2,title:"执行计划（Explain）",slug:"执行计划-explain",normalizedTitle:"执行计划（explain）",charIndex:5553}],headersStr:"Fetch抓取 表优化 小表、大表Join 大表join大表 MapJoin（MR引擎） Group By 笛卡尔积 行列过滤 动态分区 严格模式 JVM重用 执行计划（Explain）",content:"# 企业优化\n\n\n# Fetch抓取\n\nFetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。\n\n在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。\n\n<property>\n    <name>hive.fetch.task.conversion</name>\n    <value>more</value> \n    <description>\n      Expects one of [none, minimal, more].\n      Some select queries can be converted to single FETCH task minimizing latency.\n      Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.\n      0. none : disable hive.fetch.task.conversion\n      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n      2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)\n    </description>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 表优化\n\n\n# 小表、大表Join\n\n将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。\n\n新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。\n\nset hive.auto.convert.join = false; -- 关闭mapjoin功能（默认是打开的）\n\n\n1\n\n\n-- 建立大表\ncreate table bigtable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/bigtable' into table bigtable;\n\n\n1\n2\n3\n4\n\n\n-- 建立小表\ncreate table smalltable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/smalltable' into table smalltable;\n\n\n1\n2\n3\n4\n\n\n-- 建立结果表格\ncreate table jointable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n\n1\n2\n\n\n-- 连接操作\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom smalltable s\njoin bigtable  b\non b.id = s.id;\n\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable  b\njoin smalltable  s\non s.id = b.id;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 大表join大表\n\n有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。\n\n-- 建没有Null的表\ncreate table ori(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/ori' into table ori;\n\n-- 建立有Null的表\ncreate table nullidtable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/nullid' into table nullidtable;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n-- 空key过滤\ninsert overwrite table jointable select n.* from (select * from nullidtable where id is not null ) n  left join ori o on n.id = o.id;\n\n-- 空key转换\ninsert overwrite table jointable\nselect n.* from nullidtable n full join ori o on \nnvl(n.id,rand()) = o.id;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# MapJoin（MR引擎）\n\n如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。\n\nset hive.auto.convert.join = true; -- 默认为true\nset hive.mapjoin.smalltable.filesize=25000000; -- 大表小表的阈值设置（默认25M一下认为是小表）\n\n\n1\n2\n\n\n\n\n\n# Group By\n\n默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n\n\n\n并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。\n\nset hive.map.aggr = true; -- 是否在Map端进行聚合，默认为True\nset hive.groupby.mapaggr.checkinterval = 100000; -- 在Map端进行聚合操作的条目数目\nset hive.groupby.skewindata = true; -- 有数据倾斜的时候进行负载均衡（默认是false）\n\n\n1\n2\n3\n\n\n当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。\n\n测试\n\nselect deptno from emp group by deptno;\n\n\n1\n\n\n\n# 笛卡尔积\n\n尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。\n\n\n# 行列过滤\n\n列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。\n\n行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤\n\n-- 测试先关联两张表，再用where条件过滤\nselect o.id from bigtable b\njoin ori o on o.id = b.id\nwhere o.id <= 10;\n-- 通过子查询后，再关联表\nselect b.id from bigtable b\njoin (select id from ori where id <= 10 ) o on b.id = o.id;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 动态分区\n\n-- 首先设置非严格模式\nset hive.exec.dynamic.partition.mode=nonstrict; -- 默认为strict\n\n\n1\n2\n\n\n-- 创建分区表\ncreate table dept_partition(id int, name string) partitioned\nby (location int) row format delimited fields terminated by '\\t';\n\n\n1\n2\n3\n\n\n-- 从原表中向分区表插入数据\ninsert into table dept_partition partition(location)\nselect deptno, dname, loc from dept;\n\n\n1\n2\n3\n\n\n\n# 严格模式\n\n设置为严格模式\n\nset hive.exec.dynamic.partition.mode=strict; -- 默认为strict严格模式\n\n\n1\n\n 1. 将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。\n 2. 将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。\n 3. 将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。\n\n\n# JVM重用\n\nHadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。\n\n<property>\n  <name>mapreduce.job.jvm.numtasks</name>\n  <value>10</value>\n  <description>How many tasks to run per jvm. If set to -1, there is\n  no limit. \n  </description>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\ntez引擎默认开启JVM重用\n\n\n# 执行计划（Explain）\n\n-- EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query\nexplain select * from emp;\n\n\n1\n2\n",normalizedContent:"# 企业优化\n\n\n# fetch抓取\n\nfetch抓取是指，hive中对某些情况的查询可以不必使用mapreduce计算。例如：select * from employees;在这种情况下，hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。\n\n在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。\n\n<property>\n    <name>hive.fetch.task.conversion</name>\n    <value>more</value> \n    <description>\n      expects one of [none, minimal, more].\n      some select queries can be converted to single fetch task minimizing latency.\n      currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs rs), lateral views and joins.\n      0. none : disable hive.fetch.task.conversion\n      1. minimal : select star, filter on partition columns, limit only\n      2. more  : select, filter, limit only (support tablesample and virtual columns)\n    </description>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 表优化\n\n\n# 小表、大表join\n\n将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。\n\n新版的hive已经对小表join大表和大表join小表进行了优化。小表放在左边和右边已经没有明显区别。\n\nset hive.auto.convert.join = false; -- 关闭mapjoin功能（默认是打开的）\n\n\n1\n\n\n-- 建立大表\ncreate table bigtable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/bigtable' into table bigtable;\n\n\n1\n2\n3\n4\n\n\n-- 建立小表\ncreate table smalltable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/smalltable' into table smalltable;\n\n\n1\n2\n3\n4\n\n\n-- 建立结果表格\ncreate table jointable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n\n1\n2\n\n\n-- 连接操作\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom smalltable s\njoin bigtable  b\non b.id = s.id;\n\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable  b\njoin smalltable  s\non s.id = b.id;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 大表join大表\n\n有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在sql语句中进行过滤。\n\n-- 建没有null的表\ncreate table ori(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/ori' into table ori;\n\n-- 建立有null的表\ncreate table nullidtable(id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/opt/module/datas/nullid' into table nullidtable;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n-- 空key过滤\ninsert overwrite table jointable select n.* from (select * from nullidtable where id is not null ) n  left join ori o on n.id = o.id;\n\n-- 空key转换\ninsert overwrite table jointable\nselect n.* from nullidtable n full join ori o on \nnvl(n.id,rand()) = o.id;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# mapjoin（mr引擎）\n\n如果不指定mapjoin或者不符合mapjoin的条件，那么hive解析器会将join操作转换成common join，即：在reduce阶段完成join。容易发生数据倾斜。可以用mapjoin把小表全部加载到内存在map端进行join，避免reducer处理。\n\nset hive.auto.convert.join = true; -- 默认为true\nset hive.mapjoin.smalltable.filesize=25000000; -- 大表小表的阈值设置（默认25m一下认为是小表）\n\n\n1\n2\n\n\n\n\n\n# group by\n\n默认情况下，map阶段同一key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n\n\n\n并不是所有的聚合操作都需要在reduce端完成，很多聚合操作都可以先在map端进行部分聚合，最后在reduce端得出最终结果。\n\nset hive.map.aggr = true; -- 是否在map端进行聚合，默认为true\nset hive.groupby.mapaggr.checkinterval = 100000; -- 在map端进行聚合操作的条目数目\nset hive.groupby.skewindata = true; -- 有数据倾斜的时候进行负载均衡（默认是false）\n\n\n1\n2\n3\n\n\n当选项设定为 true，生成的查询计划会有两个mr job。第一个mr job中，map的输出结果会随机分布到reduce中，每个reduce做部分聚合操作，并输出结果，这样处理的结果是相同的group by key有可能被分发到不同的reduce中，从而达到负载均衡的目的；第二个mr job再根据预处理的数据结果按照group by key分布到reduce中（这个过程可以保证相同的group by key被分布到同一个reduce中），最后完成最终的聚合操作。\n\n测试\n\nselect deptno from emp group by deptno;\n\n\n1\n\n\n\n# 笛卡尔积\n\n尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，hive只能使用1个reducer来完成笛卡尔积。\n\n\n# 行列过滤\n\n列处理：在select中，只拿需要的列，如果有，尽量使用分区过滤，少用select *。\n\n行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在where后面，那么就会先全表关联，之后再过滤\n\n-- 测试先关联两张表，再用where条件过滤\nselect o.id from bigtable b\njoin ori o on o.id = b.id\nwhere o.id <= 10;\n-- 通过子查询后，再关联表\nselect b.id from bigtable b\njoin (select id from ori where id <= 10 ) o on b.id = o.id;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 动态分区\n\n-- 首先设置非严格模式\nset hive.exec.dynamic.partition.mode=nonstrict; -- 默认为strict\n\n\n1\n2\n\n\n-- 创建分区表\ncreate table dept_partition(id int, name string) partitioned\nby (location int) row format delimited fields terminated by '\\t';\n\n\n1\n2\n3\n\n\n-- 从原表中向分区表插入数据\ninsert into table dept_partition partition(location)\nselect deptno, dname, loc from dept;\n\n\n1\n2\n3\n\n\n\n# 严格模式\n\n设置为严格模式\n\nset hive.exec.dynamic.partition.mode=strict; -- 默认为strict严格模式\n\n\n1\n\n 1. 将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。\n 2. 将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。\n 3. 将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。\n\n\n# jvm重用\n\nhadoop的默认配置通常是使用派生jvm来执行map和reduce任务的。这时jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。jvm重用可以使得jvm实例在同一个job中重新使用n次。n的值可以在hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。\n\n<property>\n  <name>mapreduce.job.jvm.numtasks</name>\n  <value>10</value>\n  <description>how many tasks to run per jvm. if set to -1, there is\n  no limit. \n  </description>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\ntez引擎默认开启jvm重用\n\n\n# 执行计划（explain）\n\n-- explain [extended | dependency | authorization] query\nexplain select * from emp;\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Flume",frontmatter:{title:"Flume",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/b084db/",categories:["大数据","Flume"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/04.Flume/01.Flume.html",relativePath:"大数据/04.Flume/01.Flume.md",key:"v-07e506ef",path:"/pages/b084db/",headersStr:null,content:"# Flume\n\nFlume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。\n\n",normalizedContent:"# flume\n\nflume是cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。flume基于流式架构，灵活简单。\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hive实战",frontmatter:{title:"Hive实战",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/1aa014/",categories:["大数据","Hive"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/03.Hive/13.Hive%E5%AE%9E%E6%88%98.html",relativePath:"大数据/03.Hive/13.Hive实战.md",key:"v-7aff2c77",path:"/pages/1aa014/",headers:[{level:2,title:"文件字段说明",slug:"文件字段说明",normalizedTitle:"文件字段说明",charIndex:13},{level:2,title:"ETL Mapper 处理",slug:"etl-mapper-处理",normalizedTitle:"etl mapper 处理",charIndex:666},{level:2,title:"创建表",slug:"创建表",normalizedTitle:"创建表",charIndex:5326},{level:2,title:"需求实现",slug:"需求实现",normalizedTitle:"需求实现",charIndex:6620},{level:3,title:"统计视频观看数top10",slug:"统计视频观看数top10",normalizedTitle:"统计视频观看数top10",charIndex:6629},{level:3,title:"统计视频类别热度Top10",slug:"统计视频类别热度top10",normalizedTitle:"统计视频类别热度top10",charIndex:6794},{level:3,title:"统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数",slug:"统计出视频观看数最高的20个视频的所属类别以及类别包含top20视频的个数",normalizedTitle:"统计出视频观看数最高的20个视频的所属类别以及类别包含top20视频的个数",charIndex:7472},{level:3,title:"统计视频观看数Top50所关联视频的所属类别排序",slug:"统计视频观看数top50所关联视频的所属类别排序",normalizedTitle:"统计视频观看数top50所关联视频的所属类别排序",charIndex:8327},{level:3,title:"统计每个类别中的视频热度Top10，以Music为例",slug:"统计每个类别中的视频热度top10-以music为例",normalizedTitle:"统计每个类别中的视频热度top10，以music为例",charIndex:10274},{level:3,title:"统计每个类别中视频流量Top10，以Music为例",slug:"统计每个类别中视频流量top10-以music为例",normalizedTitle:"统计每个类别中视频流量top10，以music为例",charIndex:10937},{level:3,title:"统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频",slug:"统计上传视频最多的用户top10以及他们上传的观看次数在前20的视频",normalizedTitle:"统计上传视频最多的用户top10以及他们上传的观看次数在前20的视频",charIndex:11147},{level:2,title:"统计每个类别视频观看数Top10",slug:"统计每个类别视频观看数top10",normalizedTitle:"统计每个类别视频观看数top10",charIndex:12886}],headersStr:"文件字段说明 ETL Mapper 处理 创建表 需求实现 统计视频观看数top10 统计视频类别热度Top10 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数 统计视频观看数Top50所关联视频的所属类别排序 统计每个类别中的视频热度Top10，以Music为例 统计每个类别中视频流量Top10，以Music为例 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 统计每个类别视频观看数Top10",content:'# Hive实战\n\n\n# 文件字段说明\n\n视频表\n\n字段            备注                      详细描述\nvideo id      视频唯一id（String）          11位字符串\nuploader      视频上传者（String）           上传视频的用户名String\nage           视频年龄（int）               视频在平台上的整数天\ncategory      视频类别（Array<String>）     上传视频指定的视频分类\nlength        视频长度（Int）               整形数字标识的视频长度\nviews         观看次数（Int）               视频被浏览的次数\nrate          视频评分（Double）            满分5分\nRatings       流量（Int）                 视频的流量，整型数字\nconments      评论数（Int）                一个视频的整数评论数\nrelated ids   相关视频id（Array<String>）   相关视频的id，最多20个\n\n用户表\n\n字段         备注       字段类型\nuploader   上传者用户名   string\nvideos     上传视频数    int\nfriends    朋友数量     int\n\n\n# ETL Mapper 处理\n\n书写ETL Mapper编码\n\n导入坐标\n\n<dependencies>\n    <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>4.12</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.logging.log4j</groupId>\n        <artifactId>log4j-slf4j-impl</artifactId>\n        <version>2.12.0</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-client</artifactId>\n        <version>3.1.3</version>\n    </dependency>\n    \x3c!--        <dependency>--\x3e\n    \x3c!--            <groupId>org.apache.hadoop</groupId>--\x3e\n    \x3c!--            <artifactId>hadoop-client-runtime</artifactId>--\x3e\n    \x3c!--            <version>3.1.3</version>--\x3e\n    \x3c!--        </dependency>--\x3e\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nMapper类\n\npackage com.atguigu.etl;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Counter;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class ETLMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n\n\n    private Counter pass;\n    private Counter fail;\n\n    private StringBuffer sb = new StringBuffer();\n\n    private Text result = new Text();\n\n\n    @Override\n    protected void setup(Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {\n        pass = context.getCounter("ETL", "Pass"); //计数器\n        fail = context.getCounter("ETL", "Fail");\n    }\n\n    /**\n     * 将一行日志进行处理 字段不够的抛弃 第四个字段中的空格去掉 将最后相关视频的分割符改为 &\n     *\n     * @param key     行号\n     * @param value   一行日志\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {\n        String line = value.toString();\n\n        String[] field = line.split("\\t");\n\n        //判断字数是否足够\n        if (field.length >= 9) {\n            //处理数据\n            //去掉第四个字段的空格\n            field[3] = field[3].replace(" ", "");  //原本 a & b ==> 变成 a&b\n\n            //拼接成一行\n            sb.setLength(0); //清空\n            for (int i = 0; i < field.length; i++) {\n                //如果当前拼接的字段是我们这一行的最后一个字段 则直接追加\n                if (i == field.length - 1) {\n                    sb.append(field[i]);\n                } else if (i <= 8) { //前面的字段都是用 \\t 隔开\n                    //如果拼的是前9个字段\n                    sb.append(field[i]).append("\\t");\n\n                } else {\n                    //剩下的分割符为&\n                    sb.append(field[i]).append("&");  //最后一个字段为一个数组 元素之间用&隔开\n                }\n\n\n            }\n            result.set(sb.toString());\n            context.write(result, NullWritable.get()); //写入上下文\n            pass.increment(1);\n        } else {\n            //丢弃数据 此数据不足9个字段\n            fail.increment(1);\n        }\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n\n\nDriver\n\npackage com.atguigu.etl;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class ETLDriver {\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n//        args = new String[]{"d:/input","d:/output"}; //本地测试\n\n        Configuration configuration = new Configuration();\n        configuration.set("hive.execution.engine","yarn-tez"); //改为tez引擎 本地模式不要修改请使用默认引擎\n        Job job = Job.getInstance();\n\n        job.setJarByClass(ETLDriver.class);\n\n        job.setMapperClass(ETLMapper.class);\n        job.setNumReduceTasks(0);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullPointerException.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n打包成jar包上传到集群中 运行自定义mapreduce\n\nyarn jar etltool20211110-1.0-SNAPSHOT.jar com.atguigu.etl.ETLDriver /gulivideo/video /gulivideo/video_etl\n\n\n1\n\n\n\n# 创建表\n\n * 创库\n\ncreate database gulivideo;\nuse gulivideo;\n\n\n1\n2\n\n * 外部表\n\n-- video表\ncreate external table video_ori(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nrow format delimited fields terminated by "\\t"\ncollection items terminated by "&"\nlocation \'/gulivideo/video_etl\';\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n-- user表\ncreate external table user_ori(\n    uploader string,\n    videos int,\n    friends int)\nrow format delimited fields terminated by "\\t" \nlocation \'/gulivideo/user\';\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 内部表\n\n-- video_orc表\ncreate table video_orc(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nstored as orc\ntblproperties("orc.compress"="SNAPPY");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n-- user_orc表\ncreate table user_orc(\n    uploader string,\n    videos int,\n    friends int)\nstored as orc\ntblproperties("orc.compress"="SNAPPY");\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 插入数据\n\n-- 从外部表中插入数据\ninsert into table video_orc select * from video_ori;\ninsert into table user_orc select * from user_ori;\n\n\n1\n2\n3\n\n\n\n# 需求实现\n\n\n# 统计视频观看数top10\n\n使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。\n\nSELECT\n    videoid,\n    views\nFROM\n    video_orc\nORDER BY\n    views DESC\nLIMIT 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 统计视频类别热度Top10\n\n 1. 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别\n 2. 我们需要按照类别group by聚合，然后count组内的videoId个数即可。\n 3. 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。\n\n-- category列转行\nSELECT\n    videoid,\n    cate\nFROM\n    video_orc LATERAL VIEW explode(category) tbl as cate;\n\n\n1\n2\n3\n4\n5\n6\n\n\n-- 在上表基础上，统计各个类别有多少视频，并排序取前十\nSELECT\n    cate,\n    COUNT(videoid) n\nFROM\n    t1\nGROUP BY\n    cate\nORDER BY\n    n desc limit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n完整语句\n\nSELECT\n    cate,\n    COUNT(videoid) n\nFROM\n    (SELECT\n    videoid,\n    cate\nFROM\n    video_orc LATERAL VIEW explode(category) tbl as cate)t1\nGROUP BY\n    cate\nORDER BY\n    n desc limit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数\n\n 1. 统计前20视频和类别\n\nSELECT\n    videoid,\n    views,\n    category\nFROM\n    video_orc\nORDER BY\n    views DESC\nLIMIT 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 2. 打散类别 列转行\n\nSELECT\n    videoid,\n    cate\nFROM\n    t1 LATERAL VIEW explode(category) tbl as cate;\n\n\n1\n2\n3\n4\n5\n\n 3. 按照类别统计个数\n\nSELECT\n    cate,\n    COUNT(videoid) n\nFROM\n    t2\nGROUP BY\n    cate\nORDER BY\n    n DESC;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 4. 完整语句\n\nSELECT\n    cate,\n    COUNT(videoid) n\nFROM\n    (\n    SELECT\n        videoid,\n        cate\n    FROM\n        (\n        SELECT\n            videoid,\n            views,\n            category\n        FROM\n            video_orc\n        ORDER BY\n            views DESC\n        LIMIT 20 ) t1 LATERAL VIEW explode(category) tbl as cate ) t2\nGROUP BY\n    cate\nORDER BY\n    n DESC;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 统计视频观看数Top50所关联视频的所属类别排序\n\n 1. 统计观看数前50的视频的关联视频\n\nSELECT\n    videoid,\n    views,\n    relatedid\nFROM\n    video_orc\nORDER BY\n    views DESC\nLIMIT 50;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 2. 打散关联视频 列转行\n\nSELECT\n    explode(relatedid) videoid\nFROM\n    t1;\n\n\n1\n2\n3\n4\n\n 3. 和原表join获取关联视频的类别\n\nSELECT\n    DISTINCT t2.videoid,\n    v.category\nFROM\n    t2\nJOIN video_orc v on\n    t2.videoid = v.videoid;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 4. 打散类别\n\nSELECT\n    explode(category) cate\nFROM\n    t3;\n\n\n1\n2\n3\n4\n\n 5. 类别热度表 每个类别出现次数\n\nSELECT\n        cate,\n        COUNT(videoid) n\n    FROM\n        (\n        SELECT\n            videoid,\n            cate\n        FROM\n            video_orc LATERAL VIEW explode(category) tbl as cate) g1\n    GROUP BY\n        cate\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n 6. 和类别热度表 join并排序\n\nSELECT\n    DISTINCT t4.cate,\n    t5.n\nFROM\n    t4\nJOIN t5 ON\n    t4.cate = t5.cate\nORDER BY\n    t5.n DESC;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 7. 完整语句\n\nSELECT \n    DISTINCT t4.cate,\n    t5.n\nFROM\n    (\n    SELECT\n        explode(category) cate\n    FROM\n        (\n        SELECT\n            DISTINCT t2.videoid,\n            v.category\n        FROM\n            (\n            SELECT\n                explode(relatedid) videoid\n            FROM\n                (\n                SELECT\n                    videoid,\n                    views,\n                    relatedid\n                FROM\n                    video_orc\n                ORDER BY\n                    views DESC\n                LIMIT 50 ) t1 ) t2\n        JOIN video_orc v on\n            t2.videoid = v.videoid ) t3 ) t4\nJOIN (\n    SELECT\n        cate,\n        COUNT(videoid) n\n    FROM\n        (\n        SELECT\n            videoid,\n            cate\n        FROM\n            video_orc LATERAL VIEW explode(category) tbl as cate) g1\n    GROUP BY\n        cate ) t5 ON\n    t4.cate = t5.cate\nORDER BY\n    t5.n DESC;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# 统计每个类别中的视频热度Top10，以Music为例\n\n 1. 把视频表的类别炸开，生成中间表格video_category\n\nCREATE\n    TABLE\n        video_category STORED AS orc TBLPROPERTIES("orc.compress"="SNAPPY") AS SELECT\n            videoid,\n            uploader,\n            age,\n            cate,\n            length,\n            views,\n            rate,\n            ratings,\n            comments,\n            relatedid\n        FROM\n            video_orc LATERAL VIEW explode(category) tbl as cate;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n 2. 从video_category 直接查询Music类的前10视频\n\nSELECT\n    videoid,\n    views\nFROM\n    video_category\nWHERE\n    cate ="Music"\nORDER BY\n    views DESC\nLIMIT 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 统计每个类别中视频流量Top10，以Music为例\n\n 1. 从video_category直接查询Music类的流量前10视频\n\nSELECT\n    videoid,\n    ratings\nFROM\n    video_category\nWHERE\n    cate ="Music"\nORDER BY\n    ratings DESC\nLIMIT 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频\n\n# 理解1.上传视频观看数最多前十用户每人前20条视频\n\n 1. 统计上传视频中 观看数量最大的Top10上传用户\n\nSELECT\n    uploader,\n    videos\nFROM\n    user_orc\nORDER BY\n    videos DESC\nLIMIT 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 2. 和video_orc联立，找出这些用户上传的视频，并按照热度排名\n\nSELECT\n    t1.uploader,\n    v.videoid,\n    RANK() OVER(PARTITION BY t1.uploader ORDER BY v.views DESC) hot\nFROM\n    t1\nLEFT JOIN video_orc v ON\n    t1.uploader = v.uploader;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 求前20\n\nSELECT\n    t2.uploader,\n    t2.videoid,\n    t2.hot\nFROM\n    t2\nWHERE\n    hot <= 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 4. 完整语句\n\nSELECT\n    t2.uploader,\n    t2.videoid,\n    t2.hot\nFROM\n    (SELECT\n    t1.uploader,\n    v.videoid,\n    RANK() OVER(PARTITION BY t1.uploader ORDER BY v.views DESC) hot\nFROM\n    (SELECT\n    uploader,\n    videos\nFROM\n    user_orc\nORDER BY\n    videos DESC\nLIMIT 10)t1\nLEFT JOIN video_orc v ON\n    t1.uploader = v.uploader)t2\nWHERE\n    hot <= 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 理解2.上传视频数前十的用户 是否存在视频播放数总榜前20\n\n 1. 统计视频上传最多的用户Top10\n\nSELECT\n    uploader,\n    videos\nFROM\n    user_orc\nORDER BY\n    videos DESC\nLIMIT 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 2. 观看数前20的视频\n\nSELECT\n    videoid,\n    uploader,\n    views\nFROM\n    video_orc\nORDER BY\n    views DESC\nLIMIT 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 3. 联立两表，看看有没有他们上传的\n\nSELECT\n    t1.uploader,\n    t2.videoid\nFROM\n    t1\nLEFT JOIN t2 ON\n    t1.uploader = t2.uploader;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 4. 完整语句\n\nSELECT\n    t1.uploader,\n    t2.videoid\nFROM\n    (SELECT\n    uploader,\n    videos\nFROM\n    user_orc\nORDER BY\n    videos DESC\nLIMIT 10)t1\nLEFT JOIN (SELECT\n    videoid,\n    uploader,\n    views\nFROM\n    video_orc\nORDER BY\n    views DESC\nLIMIT 20)t2 ON\n    t1.uploader = t2.uploader;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 统计每个类别视频观看数Top10\n\n 1. 从video_category表查出每个类别视频观看数排名\n\nSELECT\n    cate,\n    videoid,\n    views,\n    RANK() OVER(PARTITION BY cate ORDER BY views DESC) hot\nFROM\n    video_category;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 2. 取每个类别的Top10\n\nSELECT\n    cate,\n    videoid,\n    views\nFROM\n    t1\nWHERE\n    hot <= 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 完整语句\n\nSELECT\n    cate,\n    videoid,\n    views\nFROM\n    (SELECT\n    cate,\n    videoid,\n    views,\n    RANK() OVER(PARTITION BY cate ORDER BY views DESC) hot\nFROM\n    video_category)t1\nWHERE\n    hot <= 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n',normalizedContent:'# hive实战\n\n\n# 文件字段说明\n\n视频表\n\n字段            备注                      详细描述\nvideo id      视频唯一id（string）          11位字符串\nuploader      视频上传者（string）           上传视频的用户名string\nage           视频年龄（int）               视频在平台上的整数天\ncategory      视频类别（array<string>）     上传视频指定的视频分类\nlength        视频长度（int）               整形数字标识的视频长度\nviews         观看次数（int）               视频被浏览的次数\nrate          视频评分（double）            满分5分\nratings       流量（int）                 视频的流量，整型数字\nconments      评论数（int）                一个视频的整数评论数\nrelated ids   相关视频id（array<string>）   相关视频的id，最多20个\n\n用户表\n\n字段         备注       字段类型\nuploader   上传者用户名   string\nvideos     上传视频数    int\nfriends    朋友数量     int\n\n\n# etl mapper 处理\n\n书写etl mapper编码\n\n导入坐标\n\n<dependencies>\n    <dependency>\n        <groupid>junit</groupid>\n        <artifactid>junit</artifactid>\n        <version>4.12</version>\n    </dependency>\n    <dependency>\n        <groupid>org.apache.logging.log4j</groupid>\n        <artifactid>log4j-slf4j-impl</artifactid>\n        <version>2.12.0</version>\n    </dependency>\n    <dependency>\n        <groupid>org.apache.hadoop</groupid>\n        <artifactid>hadoop-client</artifactid>\n        <version>3.1.3</version>\n    </dependency>\n    \x3c!--        <dependency>--\x3e\n    \x3c!--            <groupid>org.apache.hadoop</groupid>--\x3e\n    \x3c!--            <artifactid>hadoop-client-runtime</artifactid>--\x3e\n    \x3c!--            <version>3.1.3</version>--\x3e\n    \x3c!--        </dependency>--\x3e\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nmapper类\n\npackage com.atguigu.etl;\n\nimport org.apache.hadoop.io.longwritable;\nimport org.apache.hadoop.io.nullwritable;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.counter;\nimport org.apache.hadoop.mapreduce.mapper;\n\nimport java.io.ioexception;\n\npublic class etlmapper extends mapper<longwritable, text, text, nullwritable> {\n\n\n    private counter pass;\n    private counter fail;\n\n    private stringbuffer sb = new stringbuffer();\n\n    private text result = new text();\n\n\n    @override\n    protected void setup(mapper<longwritable, text, text, nullwritable>.context context) throws ioexception, interruptedexception {\n        pass = context.getcounter("etl", "pass"); //计数器\n        fail = context.getcounter("etl", "fail");\n    }\n\n    /**\n     * 将一行日志进行处理 字段不够的抛弃 第四个字段中的空格去掉 将最后相关视频的分割符改为 &\n     *\n     * @param key     行号\n     * @param value   一行日志\n     * @param context\n     * @throws ioexception\n     * @throws interruptedexception\n     */\n    @override\n    protected void map(longwritable key, text value, mapper<longwritable, text, text, nullwritable>.context context) throws ioexception, interruptedexception {\n        string line = value.tostring();\n\n        string[] field = line.split("\\t");\n\n        //判断字数是否足够\n        if (field.length >= 9) {\n            //处理数据\n            //去掉第四个字段的空格\n            field[3] = field[3].replace(" ", "");  //原本 a & b ==> 变成 a&b\n\n            //拼接成一行\n            sb.setlength(0); //清空\n            for (int i = 0; i < field.length; i++) {\n                //如果当前拼接的字段是我们这一行的最后一个字段 则直接追加\n                if (i == field.length - 1) {\n                    sb.append(field[i]);\n                } else if (i <= 8) { //前面的字段都是用 \\t 隔开\n                    //如果拼的是前9个字段\n                    sb.append(field[i]).append("\\t");\n\n                } else {\n                    //剩下的分割符为&\n                    sb.append(field[i]).append("&");  //最后一个字段为一个数组 元素之间用&隔开\n                }\n\n\n            }\n            result.set(sb.tostring());\n            context.write(result, nullwritable.get()); //写入上下文\n            pass.increment(1);\n        } else {\n            //丢弃数据 此数据不足9个字段\n            fail.increment(1);\n        }\n\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n\n\ndriver\n\npackage com.atguigu.etl;\n\nimport org.apache.hadoop.conf.configuration;\nimport org.apache.hadoop.fs.path;\nimport org.apache.hadoop.io.text;\nimport org.apache.hadoop.mapreduce.job;\nimport org.apache.hadoop.mapreduce.lib.input.fileinputformat;\nimport org.apache.hadoop.mapreduce.lib.output.fileoutputformat;\n\nimport java.io.ioexception;\n\npublic class etldriver {\n    public static void main(string[] args) throws ioexception, interruptedexception, classnotfoundexception {\n//        args = new string[]{"d:/input","d:/output"}; //本地测试\n\n        configuration configuration = new configuration();\n        configuration.set("hive.execution.engine","yarn-tez"); //改为tez引擎 本地模式不要修改请使用默认引擎\n        job job = job.getinstance();\n\n        job.setjarbyclass(etldriver.class);\n\n        job.setmapperclass(etlmapper.class);\n        job.setnumreducetasks(0);\n\n        job.setmapoutputkeyclass(text.class);\n        job.setoutputvalueclass(nullpointerexception.class);\n\n        fileinputformat.setinputpaths(job, new path(args[0]));\n        fileoutputformat.setoutputpath(job, new path(args[1]));\n\n        boolean b = job.waitforcompletion(true);\n        system.exit(b ? 0 : 1);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n打包成jar包上传到集群中 运行自定义mapreduce\n\nyarn jar etltool20211110-1.0-snapshot.jar com.atguigu.etl.etldriver /gulivideo/video /gulivideo/video_etl\n\n\n1\n\n\n\n# 创建表\n\n * 创库\n\ncreate database gulivideo;\nuse gulivideo;\n\n\n1\n2\n\n * 外部表\n\n-- video表\ncreate external table video_ori(\n    videoid string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedid array<string>)\nrow format delimited fields terminated by "\\t"\ncollection items terminated by "&"\nlocation \'/gulivideo/video_etl\';\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n-- user表\ncreate external table user_ori(\n    uploader string,\n    videos int,\n    friends int)\nrow format delimited fields terminated by "\\t" \nlocation \'/gulivideo/user\';\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 内部表\n\n-- video_orc表\ncreate table video_orc(\n    videoid string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedid array<string>)\nstored as orc\ntblproperties("orc.compress"="snappy");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n-- user_orc表\ncreate table user_orc(\n    uploader string,\n    videos int,\n    friends int)\nstored as orc\ntblproperties("orc.compress"="snappy");\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 插入数据\n\n-- 从外部表中插入数据\ninsert into table video_orc select * from video_ori;\ninsert into table user_orc select * from user_ori;\n\n\n1\n2\n3\n\n\n\n# 需求实现\n\n\n# 统计视频观看数top10\n\n使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。\n\nselect\n    videoid,\n    views\nfrom\n    video_orc\norder by\n    views desc\nlimit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 统计视频类别热度top10\n\n 1. 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别\n 2. 我们需要按照类别group by聚合，然后count组内的videoid个数即可。\n 3. 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。\n\n-- category列转行\nselect\n    videoid,\n    cate\nfrom\n    video_orc lateral view explode(category) tbl as cate;\n\n\n1\n2\n3\n4\n5\n6\n\n\n-- 在上表基础上，统计各个类别有多少视频，并排序取前十\nselect\n    cate,\n    count(videoid) n\nfrom\n    t1\ngroup by\n    cate\norder by\n    n desc limit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n完整语句\n\nselect\n    cate,\n    count(videoid) n\nfrom\n    (select\n    videoid,\n    cate\nfrom\n    video_orc lateral view explode(category) tbl as cate)t1\ngroup by\n    cate\norder by\n    n desc limit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 统计出视频观看数最高的20个视频的所属类别以及类别包含top20视频的个数\n\n 1. 统计前20视频和类别\n\nselect\n    videoid,\n    views,\n    category\nfrom\n    video_orc\norder by\n    views desc\nlimit 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 2. 打散类别 列转行\n\nselect\n    videoid,\n    cate\nfrom\n    t1 lateral view explode(category) tbl as cate;\n\n\n1\n2\n3\n4\n5\n\n 3. 按照类别统计个数\n\nselect\n    cate,\n    count(videoid) n\nfrom\n    t2\ngroup by\n    cate\norder by\n    n desc;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 4. 完整语句\n\nselect\n    cate,\n    count(videoid) n\nfrom\n    (\n    select\n        videoid,\n        cate\n    from\n        (\n        select\n            videoid,\n            views,\n            category\n        from\n            video_orc\n        order by\n            views desc\n        limit 20 ) t1 lateral view explode(category) tbl as cate ) t2\ngroup by\n    cate\norder by\n    n desc;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 统计视频观看数top50所关联视频的所属类别排序\n\n 1. 统计观看数前50的视频的关联视频\n\nselect\n    videoid,\n    views,\n    relatedid\nfrom\n    video_orc\norder by\n    views desc\nlimit 50;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 2. 打散关联视频 列转行\n\nselect\n    explode(relatedid) videoid\nfrom\n    t1;\n\n\n1\n2\n3\n4\n\n 3. 和原表join获取关联视频的类别\n\nselect\n    distinct t2.videoid,\n    v.category\nfrom\n    t2\njoin video_orc v on\n    t2.videoid = v.videoid;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 4. 打散类别\n\nselect\n    explode(category) cate\nfrom\n    t3;\n\n\n1\n2\n3\n4\n\n 5. 类别热度表 每个类别出现次数\n\nselect\n        cate,\n        count(videoid) n\n    from\n        (\n        select\n            videoid,\n            cate\n        from\n            video_orc lateral view explode(category) tbl as cate) g1\n    group by\n        cate\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n 6. 和类别热度表 join并排序\n\nselect\n    distinct t4.cate,\n    t5.n\nfrom\n    t4\njoin t5 on\n    t4.cate = t5.cate\norder by\n    t5.n desc;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 7. 完整语句\n\nselect \n    distinct t4.cate,\n    t5.n\nfrom\n    (\n    select\n        explode(category) cate\n    from\n        (\n        select\n            distinct t2.videoid,\n            v.category\n        from\n            (\n            select\n                explode(relatedid) videoid\n            from\n                (\n                select\n                    videoid,\n                    views,\n                    relatedid\n                from\n                    video_orc\n                order by\n                    views desc\n                limit 50 ) t1 ) t2\n        join video_orc v on\n            t2.videoid = v.videoid ) t3 ) t4\njoin (\n    select\n        cate,\n        count(videoid) n\n    from\n        (\n        select\n            videoid,\n            cate\n        from\n            video_orc lateral view explode(category) tbl as cate) g1\n    group by\n        cate ) t5 on\n    t4.cate = t5.cate\norder by\n    t5.n desc;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n\n# 统计每个类别中的视频热度top10，以music为例\n\n 1. 把视频表的类别炸开，生成中间表格video_category\n\ncreate\n    table\n        video_category stored as orc tblproperties("orc.compress"="snappy") as select\n            videoid,\n            uploader,\n            age,\n            cate,\n            length,\n            views,\n            rate,\n            ratings,\n            comments,\n            relatedid\n        from\n            video_orc lateral view explode(category) tbl as cate;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n 2. 从video_category 直接查询music类的前10视频\n\nselect\n    videoid,\n    views\nfrom\n    video_category\nwhere\n    cate ="music"\norder by\n    views desc\nlimit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 统计每个类别中视频流量top10，以music为例\n\n 1. 从video_category直接查询music类的流量前10视频\n\nselect\n    videoid,\n    ratings\nfrom\n    video_category\nwhere\n    cate ="music"\norder by\n    ratings desc\nlimit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 统计上传视频最多的用户top10以及他们上传的观看次数在前20的视频\n\n# 理解1.上传视频观看数最多前十用户每人前20条视频\n\n 1. 统计上传视频中 观看数量最大的top10上传用户\n\nselect\n    uploader,\n    videos\nfrom\n    user_orc\norder by\n    videos desc\nlimit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 2. 和video_orc联立，找出这些用户上传的视频，并按照热度排名\n\nselect\n    t1.uploader,\n    v.videoid,\n    rank() over(partition by t1.uploader order by v.views desc) hot\nfrom\n    t1\nleft join video_orc v on\n    t1.uploader = v.uploader;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 求前20\n\nselect\n    t2.uploader,\n    t2.videoid,\n    t2.hot\nfrom\n    t2\nwhere\n    hot <= 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 4. 完整语句\n\nselect\n    t2.uploader,\n    t2.videoid,\n    t2.hot\nfrom\n    (select\n    t1.uploader,\n    v.videoid,\n    rank() over(partition by t1.uploader order by v.views desc) hot\nfrom\n    (select\n    uploader,\n    videos\nfrom\n    user_orc\norder by\n    videos desc\nlimit 10)t1\nleft join video_orc v on\n    t1.uploader = v.uploader)t2\nwhere\n    hot <= 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 理解2.上传视频数前十的用户 是否存在视频播放数总榜前20\n\n 1. 统计视频上传最多的用户top10\n\nselect\n    uploader,\n    videos\nfrom\n    user_orc\norder by\n    videos desc\nlimit 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 2. 观看数前20的视频\n\nselect\n    videoid,\n    uploader,\n    views\nfrom\n    video_orc\norder by\n    views desc\nlimit 20;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n 3. 联立两表，看看有没有他们上传的\n\nselect\n    t1.uploader,\n    t2.videoid\nfrom\n    t1\nleft join t2 on\n    t1.uploader = t2.uploader;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 4. 完整语句\n\nselect\n    t1.uploader,\n    t2.videoid\nfrom\n    (select\n    uploader,\n    videos\nfrom\n    user_orc\norder by\n    videos desc\nlimit 10)t1\nleft join (select\n    videoid,\n    uploader,\n    views\nfrom\n    video_orc\norder by\n    views desc\nlimit 20)t2 on\n    t1.uploader = t2.uploader;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 统计每个类别视频观看数top10\n\n 1. 从video_category表查出每个类别视频观看数排名\n\nselect\n    cate,\n    videoid,\n    views,\n    rank() over(partition by cate order by views desc) hot\nfrom\n    video_category;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n 2. 取每个类别的top10\n\nselect\n    cate,\n    videoid,\n    views\nfrom\n    t1\nwhere\n    hot <= 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 3. 完整语句\n\nselect\n    cate,\n    videoid,\n    views\nfrom\n    (select\n    cate,\n    videoid,\n    views,\n    rank() over(partition by cate order by views desc) hot\nfrom\n    video_category)t1\nwhere\n    hot <= 10;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 09:00:59",lastUpdatedTimestamp:1647565259e3},{title:"Flume安装",frontmatter:{title:"Flume安装",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/e462b7/",categories:["大数据","Flume"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/04.Flume/03.Flume%E5%AE%89%E8%A3%85.html",relativePath:"大数据/04.Flume/03.Flume安装.md",key:"v-9ed6a6cc",path:"/pages/e462b7/",headers:[{level:2,title:"agent配置文件规则",slug:"agent配置文件规则",normalizedTitle:"agent配置文件规则",charIndex:686}],headersStr:"agent配置文件规则",content:"# Flume安装\n\nFlume官网地址：http://flume.apache.org/\n\n文档查看地址：http://flume.apache.org/FlumeUserGuide.html\n\n下载地址：http://archive.apache.org/dist/flume/\n\nFlume分两个个大版本 分别是 0.9之前和0.9之后\n\n0.9之前 称为 flume og\n\n0.9之后 称为 flume ng\n\ntar -zxvf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/\nmv /opt/module/apache-flume-1.9.0-bin /opt/module/flume\n#将lib文件夹下的guava-11.0.2.jar删除以兼容Hadoop 3.1.3\nrm /opt/module/flume/lib/guava-11.0.2.jar\nsudo vim /etc/profile.d/my_env.sh \n\n#FLUME_HOME\nexport FLUME_HOME=/opt/module/flume\nexport PATH=$PATH:$FLUME_HOME/bin\n\nsource /etc/profile.d/my_env.sh \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n同步\n\ncd /opt/module/\nxsync flume/\nsudo xsync /etc/profile.d/my_env.sh \n\n\n1\n2\n3\n\n\n\n# agent配置文件规则\n\n官方手册http://flume.apache.org/FlumeUserGuide.html\n\n 1. 需要定agent的名字,并且对source channel sink 定义好名字\n 2. 对source channel sink 指明具体的类型和配置\n 3. 指明source channel sink 三者之间的一个关系",normalizedContent:"# flume安装\n\nflume官网地址：http://flume.apache.org/\n\n文档查看地址：http://flume.apache.org/flumeuserguide.html\n\n下载地址：http://archive.apache.org/dist/flume/\n\nflume分两个个大版本 分别是 0.9之前和0.9之后\n\n0.9之前 称为 flume og\n\n0.9之后 称为 flume ng\n\ntar -zxvf /opt/software/apache-flume-1.9.0-bin.tar.gz -c /opt/module/\nmv /opt/module/apache-flume-1.9.0-bin /opt/module/flume\n#将lib文件夹下的guava-11.0.2.jar删除以兼容hadoop 3.1.3\nrm /opt/module/flume/lib/guava-11.0.2.jar\nsudo vim /etc/profile.d/my_env.sh \n\n#flume_home\nexport flume_home=/opt/module/flume\nexport path=$path:$flume_home/bin\n\nsource /etc/profile.d/my_env.sh \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n同步\n\ncd /opt/module/\nxsync flume/\nsudo xsync /etc/profile.d/my_env.sh \n\n\n1\n2\n3\n\n\n\n# agent配置文件规则\n\n官方手册http://flume.apache.org/flumeuserguide.html\n\n 1. 需要定agent的名字,并且对source channel sink 定义好名字\n 2. 对source channel sink 指明具体的类型和配置\n 3. 指明source channel sink 三者之间的一个关系",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"入门案例",frontmatter:{title:"入门案例",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/c12064/",categories:["大数据","Flume"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/04.Flume/04.%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B.html",relativePath:"大数据/04.Flume/04.入门案例.md",key:"v-55b2d0b0",path:"/pages/c12064/",headers:[{level:2,title:"netcat的发送和监听",slug:"netcat的发送和监听",normalizedTitle:"netcat的发送和监听",charIndex:11},{level:2,title:"监控端口数据在控制台输出 官方案例 netcat",slug:"监控端口数据在控制台输出-官方案例-netcat",normalizedTitle:"监控端口数据在控制台输出 官方案例 netcat",charIndex:280},{level:2,title:"配置log4j.properties",slug:"配置log4j-properties",normalizedTitle:"配置log4j.properties",charIndex:1597},{level:2,title:"实时监控单个追加文件到HDFS案例 exec",slug:"实时监控单个追加文件到hdfs案例-exec",normalizedTitle:"实时监控单个追加文件到hdfs案例 exec",charIndex:1825},{level:2,title:"实时监控目录下多个新文件到HDFS spooldir",slug:"实时监控目录下多个新文件到hdfs-spooldir",normalizedTitle:"实时监控目录下多个新文件到hdfs spooldir",charIndex:3445},{level:2,title:"实时监控目录下的多个追加文件 taildir",slug:"实时监控目录下的多个追加文件-taildir",normalizedTitle:"实时监控目录下的多个追加文件 taildir",charIndex:5278}],headersStr:"netcat的发送和监听 监控端口数据在控制台输出 官方案例 netcat 配置log4j.properties 实时监控单个追加文件到HDFS案例 exec 实时监控目录下多个新文件到HDFS spooldir 实时监控目录下的多个追加文件 taildir",content:"# 入门案例\n\n\n# netcat的发送和监听\n\n安装\n\nsudo yum install -y nc #安装netcat工具\nsudo netstat -tunlp | grep 44444 #判断44444端口是否被占用\n\n\n1\n2\n\n\n使用nc发送和接受消息 此操作在一个机器上 2个ssh窗口中操作\n\n接受\n\nnc -l 44444  # 监听44444端口\n\n\n1\n\n\n发送\n\nnc hadoop102 44444 #发送\n输入内容\n\n\n1\n2\n\n\n也可以在其他机器向指定ip 端口 发送消息\n\n监听和发送的端口必须一致\n\n默认为tcp协议\n\n\n# 监控端口数据在控制台输出 官方案例 netcat\n\n#r1:表示a1的Source的名称 多个名称之前空格隔开\na1.sources = r1  \n#k1:表示a1的Sink名称\na1.sinks = k1\n#c1:表示channels的名称\na1.channels = c1 \n\n# a1的输入源类型为netcat端口类型\na1.sources.r1.type = netcat \n# a1的监听地址\na1.sources.r1.bind = localhost \n# a1的监听端口\na1.sources.r1.port = 44444 \n\n# a1的输出目的地是控制台logger类型\na1.sinks.k1.type = logger  \n\n# a1的channel类型是memory内存型\na1.channels.c1.type = memory \n# a1的channel总容量1000个event 默认为100\na1.channels.c1.capacity = 1000 \n# a1的channel传输时收集到了100条event以后再去提交事务\na1.channels.c1.transactionCapacity = 100 \n\n\n#声明source sink 和 channel 之间的关系\n#将r1和c1连接起来\na1.sources.r1.channels = c1 \n#将k1和c1连接起来 注意一个sink只能对应一个channel\na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n\n在/opt/module/flume下启动\n\nflume-ng agent --conf conf/ --name a1 --conf-file datas/netcatsource_loggersink.conf -Dflume.root.logger=INFO,console\n#或\nflume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console\n\n\n1\n2\n3\n\n * --conf/-c：表示配置文件存储在conf/目录\n * --name/-n：表示给agent起名为a1\n * --conf-file/-f：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。\n * -Dflume.root.logger=INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。\n\n在另外一个ssh窗口中使用netcat发送消息\n\nnc localhost 44444 #地址为配置文件中配置的地址\n#输入文字\n\n\n1\n2\n\n\n\n# 配置log4j.properties\n\nvim /opt/module/flume/conf/log4j.properties \n\n#flume.root.logger=DEBUG,console\nflume.root.logger=INFO,LOGFILE #日志级别 和 输出位置\nflume.log.dir=./logs #log文件存放路径\nflume.log.file=flume.log  #log文件名\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 实时监控单个追加文件到HDFS案例 exec\n\n\n\n创建案例配置文件\n\ncd /opt/module/flume/datas\ntouch 123.log\nvim execsource_hdfssink.conf\n\n\n1\n2\n3\n\n\na2.sources = r2\na2.sinks = k2\na2.channels = c2\n\n# 监听文件类型\na2.sources.r2.type = exec\n# 监听命令\na2.sources.r2.command = tail -F /opt/module/flume/datas/123.log\n\n#HDFS Sink\na2.sinks.k2.type = hdfs\n#如果要使用时间转义序列 则需要开启本地时间戳 或者 在event中headers中带时间戳\na2.sinks.k2.hdfs.path = hdfs://hadoop102:8020/flume/%Y%m%d/%H\n#上传文件的前缀\na2.sinks.k2.hdfs.filePrefix = logs-\n\n#下面3个配置定义了多久时间单位创建一个新的文件夹\n#是否按照时间滚动文件夹\na2.sinks.k2.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na2.sinks.k2.hdfs.roundValue = 1\n#重新定义时间单位\na2.sinks.k2.hdfs.roundUnit = hour\n\n\n#是否使用本地时间戳\na2.sinks.k2.hdfs.useLocalTimeStamp = true\n\n\n#积攒多少个Event才flush到HDFS一次\na2.sinks.k2.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na2.sinks.k2.hdfs.fileType = DataStream\n#多久生成一个新的文件 秒为单位\na2.sinks.k2.hdfs.rollInterval = 60\n#设置每个文件的滚动大小 字节单位\na2.sinks.k2.hdfs.rollSize = 134217700\n#文件的滚动与Event数量无关 0为禁用 也可以设置为指定数 当Event到达一定数量则文件滚动\na2.sinks.k2.hdfs.rollCount = 0\n\n# Use a channel which buffers events in memory\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactionCapacity = 100\n\n\na2.sources.r2.channels = c2\na2.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n启动\n\ncd ..\nflume-ng agent -n a2 -c conf/ -f datas/execsource_hdfssink.conf -Dflume.root.logger=INFO,console\necho 123 >> /opt/module/flume/datas/123.log #写入内容测试 hdfs上是否创建文件\n\necho ccc >> /opt/module/flume/datas/123.log #等待60s 追内容 是否滚动文件生成新的采集文件\n\n\n1\n2\n3\n4\n5\n\n\n\n\n当前正在写入的文件为tmp 只有滚动文件后次缓存文件会修改为普通文件\n\n\n# 实时监控目录下多个新文件到HDFS spooldir\n\n\n\ncd /opt/module/flume/\nmkdir upload\nvim datas/flume-dir-hdfs.conf\n\n\n1\n2\n3\n\n\n配置文件\n\na3.sources = r3\na3.sinks = k3\na3.channels = c3\n\n# Describe/configure the source\n# 监听一个指定的文件夹 自动收集目录中的内容\na3.sources.r3.type = spooldir\n#目录 目录下的文件名不能相同 否则抛异常\na3.sources.r3.spoolDir = /opt/module/flume/upload\n#如果目录下的文件名读取完后 文件名相同 可以采取读取完后删除文件  默认为加后缀名\n#设置为 immediate 读取完后删除 默认值为 never 保留\n#a3.sources.r3.deletePolicy = immediate\n#被读取完毕后的文件 后缀名被更改为    默认值为.COMPLETED\na3.sources.r3.fileSuffix = .COMPLETED\n\na3.sources.r3.fileHeader = true\n#忽略所有以.tmp结尾的文件，不上传\na3.sources.r3.ignorePattern = ([^ ]*\\.tmp)\n\n# Describe the sink\na3.sinks.k3.type = hdfs\na3.sinks.k3.hdfs.path = hdfs://hadoop102:8020/flume2/upload/%Y%m%d/%H\n#上传文件的前缀\na3.sinks.k3.hdfs.filePrefix = upload-\n#是否按照时间滚动文件夹\na3.sinks.k3.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na3.sinks.k3.hdfs.roundValue = 1\n#重新定义时间单位\na3.sinks.k3.hdfs.roundUnit = hour\n#是否使用本地时间戳\na3.sinks.k3.hdfs.useLocalTimeStamp = true\n#积攒多少个Event才flush到HDFS一次\na3.sinks.k3.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na3.sinks.k3.hdfs.fileType = DataStream\n#多久生成一个新的文件\na3.sinks.k3.hdfs.rollInterval = 60\n#设置每个文件的滚动大小大概是128M\na3.sinks.k3.hdfs.rollSize = 134217700\n#文件的滚动与Event数量无关\na3.sinks.k3.hdfs.rollCount = 0\n\n# Use a channel which buffers events in memory\na3.channels.c3.type = memory\na3.channels.c3.capacity = 1000\na3.channels.c3.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na3.sources.r3.channels = c3\na3.sinks.k3.channel = c3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n启动\n\nflume-ng agent -n a3 -c conf/ -f datas/flume-dir-hdfs.conf -Dflume.root.logger=INFO,console\n\n\n1\n\n\n创建文本文件写入内容 移动到upload目录下\n\ncd /opt/module/flume/\ntouch 123.log\necho eee >> 123.log \ncp 123.log upload/\n\n\n1\n2\n3\n4\n\n\n\n# 实时监控目录下的多个追加文件 taildir\n\n\n\n我们使用exec 读取单个文件追加内容时 如果flume挂掉后重新启动 会把文件从头到尾重新读取一遍 这是我们不希望的 而taildir读取多个文件追加内容时 会在positionFile 指定的json文件下 保留读取过的文件位置 当flume挂掉重新上线后 读取json文件记录的位置 继续监控读取内容\n\ncd /opt/module/flume/\nmkdir files\nvim datas/flume-taildir-hdfs.conf\n\n\n1\n2\n3\n\n\n配置文件\n\na3.sources = r3\na3.sinks = k3\na3.channels = c3\n\n# Describe/configure the source\n#sources类型\na3.sources.r3.type = TAILDIR\n#taildir保存读取文件内容的位置 如果发生宕机会从该文件中读取上次读取到位置  默认为~/.flume/taildir_position.json\na3.sources.r3.positionFile = /opt/module/flume/tail_dir.json\n#定义组 不同组 指向不同文件 多个组之间空格隔开\na3.sources.r3.filegroups = f1 f2\n#f1组 监控指定目录下名字包含file的文件 正则表达式\na3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*\n#f2组 监控指定目录下名字包含的log的文件\na3.sources.r3.filegroups.f2 = /opt/module/flume/files/.*log.*\n\n# Describe the sink\na3.sinks.k3.type = hdfs\na3.sinks.k3.hdfs.path = hdfs://hadoop102:8020/flume/upload2/%Y%m%d/%H\n#上传文件的前缀\na3.sinks.k3.hdfs.filePrefix = upload-\n#是否按照时间滚动文件夹\na3.sinks.k3.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na3.sinks.k3.hdfs.roundValue = 1\n#重新定义时间单位\na3.sinks.k3.hdfs.roundUnit = hour\n#是否使用本地时间戳\na3.sinks.k3.hdfs.useLocalTimeStamp = true\n#积攒多少个Event才flush到HDFS一次\na3.sinks.k3.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na3.sinks.k3.hdfs.fileType = DataStream\n#多久生成一个新的文件\na3.sinks.k3.hdfs.rollInterval = 60\n#设置每个文件的滚动大小大概是128M\na3.sinks.k3.hdfs.rollSize = 134217700\n#文件的滚动与Event数量无关\na3.sinks.k3.hdfs.rollCount = 0\n\n# Use a channel which buffers events in memory\na3.channels.c3.type = memory\na3.channels.c3.capacity = 1000\na3.channels.c3.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na3.sources.r3.channels = c3\na3.sinks.k3.channel = c3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n启动\n\nflume-ng agent -n a3 -c conf/ -f datas/flume-taildir-hdfs.conf -Dflume.root.logger=INFO,console\n\n\n1\n\n\n测试\n\ncd files\necho hello >> file1.txt\necho atguigu >> log.txt\n\n#关闭flume 重新启动 查看是否还读取 aaa 和 bbb 字段\n\n\n1\n2\n3\n4\n5\n",normalizedContent:"# 入门案例\n\n\n# netcat的发送和监听\n\n安装\n\nsudo yum install -y nc #安装netcat工具\nsudo netstat -tunlp | grep 44444 #判断44444端口是否被占用\n\n\n1\n2\n\n\n使用nc发送和接受消息 此操作在一个机器上 2个ssh窗口中操作\n\n接受\n\nnc -l 44444  # 监听44444端口\n\n\n1\n\n\n发送\n\nnc hadoop102 44444 #发送\n输入内容\n\n\n1\n2\n\n\n也可以在其他机器向指定ip 端口 发送消息\n\n监听和发送的端口必须一致\n\n默认为tcp协议\n\n\n# 监控端口数据在控制台输出 官方案例 netcat\n\n#r1:表示a1的source的名称 多个名称之前空格隔开\na1.sources = r1  \n#k1:表示a1的sink名称\na1.sinks = k1\n#c1:表示channels的名称\na1.channels = c1 \n\n# a1的输入源类型为netcat端口类型\na1.sources.r1.type = netcat \n# a1的监听地址\na1.sources.r1.bind = localhost \n# a1的监听端口\na1.sources.r1.port = 44444 \n\n# a1的输出目的地是控制台logger类型\na1.sinks.k1.type = logger  \n\n# a1的channel类型是memory内存型\na1.channels.c1.type = memory \n# a1的channel总容量1000个event 默认为100\na1.channels.c1.capacity = 1000 \n# a1的channel传输时收集到了100条event以后再去提交事务\na1.channels.c1.transactioncapacity = 100 \n\n\n#声明source sink 和 channel 之间的关系\n#将r1和c1连接起来\na1.sources.r1.channels = c1 \n#将k1和c1连接起来 注意一个sink只能对应一个channel\na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n\n在/opt/module/flume下启动\n\nflume-ng agent --conf conf/ --name a1 --conf-file datas/netcatsource_loggersink.conf -dflume.root.logger=info,console\n#或\nflume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -dflume.root.logger=info,console\n\n\n1\n2\n3\n\n * --conf/-c：表示配置文件存储在conf/目录\n * --name/-n：表示给agent起名为a1\n * --conf-file/-f：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。\n * -dflume.root.logger=info,console ：-d表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为info级别。日志级别包括:log、info、warn、error。\n\n在另外一个ssh窗口中使用netcat发送消息\n\nnc localhost 44444 #地址为配置文件中配置的地址\n#输入文字\n\n\n1\n2\n\n\n\n# 配置log4j.properties\n\nvim /opt/module/flume/conf/log4j.properties \n\n#flume.root.logger=debug,console\nflume.root.logger=info,logfile #日志级别 和 输出位置\nflume.log.dir=./logs #log文件存放路径\nflume.log.file=flume.log  #log文件名\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 实时监控单个追加文件到hdfs案例 exec\n\n\n\n创建案例配置文件\n\ncd /opt/module/flume/datas\ntouch 123.log\nvim execsource_hdfssink.conf\n\n\n1\n2\n3\n\n\na2.sources = r2\na2.sinks = k2\na2.channels = c2\n\n# 监听文件类型\na2.sources.r2.type = exec\n# 监听命令\na2.sources.r2.command = tail -f /opt/module/flume/datas/123.log\n\n#hdfs sink\na2.sinks.k2.type = hdfs\n#如果要使用时间转义序列 则需要开启本地时间戳 或者 在event中headers中带时间戳\na2.sinks.k2.hdfs.path = hdfs://hadoop102:8020/flume/%y%m%d/%h\n#上传文件的前缀\na2.sinks.k2.hdfs.fileprefix = logs-\n\n#下面3个配置定义了多久时间单位创建一个新的文件夹\n#是否按照时间滚动文件夹\na2.sinks.k2.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na2.sinks.k2.hdfs.roundvalue = 1\n#重新定义时间单位\na2.sinks.k2.hdfs.roundunit = hour\n\n\n#是否使用本地时间戳\na2.sinks.k2.hdfs.uselocaltimestamp = true\n\n\n#积攒多少个event才flush到hdfs一次\na2.sinks.k2.hdfs.batchsize = 100\n#设置文件类型，可支持压缩\na2.sinks.k2.hdfs.filetype = datastream\n#多久生成一个新的文件 秒为单位\na2.sinks.k2.hdfs.rollinterval = 60\n#设置每个文件的滚动大小 字节单位\na2.sinks.k2.hdfs.rollsize = 134217700\n#文件的滚动与event数量无关 0为禁用 也可以设置为指定数 当event到达一定数量则文件滚动\na2.sinks.k2.hdfs.rollcount = 0\n\n# use a channel which buffers events in memory\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactioncapacity = 100\n\n\na2.sources.r2.channels = c2\na2.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n启动\n\ncd ..\nflume-ng agent -n a2 -c conf/ -f datas/execsource_hdfssink.conf -dflume.root.logger=info,console\necho 123 >> /opt/module/flume/datas/123.log #写入内容测试 hdfs上是否创建文件\n\necho ccc >> /opt/module/flume/datas/123.log #等待60s 追内容 是否滚动文件生成新的采集文件\n\n\n1\n2\n3\n4\n5\n\n\n\n\n当前正在写入的文件为tmp 只有滚动文件后次缓存文件会修改为普通文件\n\n\n# 实时监控目录下多个新文件到hdfs spooldir\n\n\n\ncd /opt/module/flume/\nmkdir upload\nvim datas/flume-dir-hdfs.conf\n\n\n1\n2\n3\n\n\n配置文件\n\na3.sources = r3\na3.sinks = k3\na3.channels = c3\n\n# describe/configure the source\n# 监听一个指定的文件夹 自动收集目录中的内容\na3.sources.r3.type = spooldir\n#目录 目录下的文件名不能相同 否则抛异常\na3.sources.r3.spooldir = /opt/module/flume/upload\n#如果目录下的文件名读取完后 文件名相同 可以采取读取完后删除文件  默认为加后缀名\n#设置为 immediate 读取完后删除 默认值为 never 保留\n#a3.sources.r3.deletepolicy = immediate\n#被读取完毕后的文件 后缀名被更改为    默认值为.completed\na3.sources.r3.filesuffix = .completed\n\na3.sources.r3.fileheader = true\n#忽略所有以.tmp结尾的文件，不上传\na3.sources.r3.ignorepattern = ([^ ]*\\.tmp)\n\n# describe the sink\na3.sinks.k3.type = hdfs\na3.sinks.k3.hdfs.path = hdfs://hadoop102:8020/flume2/upload/%y%m%d/%h\n#上传文件的前缀\na3.sinks.k3.hdfs.fileprefix = upload-\n#是否按照时间滚动文件夹\na3.sinks.k3.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na3.sinks.k3.hdfs.roundvalue = 1\n#重新定义时间单位\na3.sinks.k3.hdfs.roundunit = hour\n#是否使用本地时间戳\na3.sinks.k3.hdfs.uselocaltimestamp = true\n#积攒多少个event才flush到hdfs一次\na3.sinks.k3.hdfs.batchsize = 100\n#设置文件类型，可支持压缩\na3.sinks.k3.hdfs.filetype = datastream\n#多久生成一个新的文件\na3.sinks.k3.hdfs.rollinterval = 60\n#设置每个文件的滚动大小大概是128m\na3.sinks.k3.hdfs.rollsize = 134217700\n#文件的滚动与event数量无关\na3.sinks.k3.hdfs.rollcount = 0\n\n# use a channel which buffers events in memory\na3.channels.c3.type = memory\na3.channels.c3.capacity = 1000\na3.channels.c3.transactioncapacity = 100\n\n# bind the source and sink to the channel\na3.sources.r3.channels = c3\na3.sinks.k3.channel = c3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n启动\n\nflume-ng agent -n a3 -c conf/ -f datas/flume-dir-hdfs.conf -dflume.root.logger=info,console\n\n\n1\n\n\n创建文本文件写入内容 移动到upload目录下\n\ncd /opt/module/flume/\ntouch 123.log\necho eee >> 123.log \ncp 123.log upload/\n\n\n1\n2\n3\n4\n\n\n\n# 实时监控目录下的多个追加文件 taildir\n\n\n\n我们使用exec 读取单个文件追加内容时 如果flume挂掉后重新启动 会把文件从头到尾重新读取一遍 这是我们不希望的 而taildir读取多个文件追加内容时 会在positionfile 指定的json文件下 保留读取过的文件位置 当flume挂掉重新上线后 读取json文件记录的位置 继续监控读取内容\n\ncd /opt/module/flume/\nmkdir files\nvim datas/flume-taildir-hdfs.conf\n\n\n1\n2\n3\n\n\n配置文件\n\na3.sources = r3\na3.sinks = k3\na3.channels = c3\n\n# describe/configure the source\n#sources类型\na3.sources.r3.type = taildir\n#taildir保存读取文件内容的位置 如果发生宕机会从该文件中读取上次读取到位置  默认为~/.flume/taildir_position.json\na3.sources.r3.positionfile = /opt/module/flume/tail_dir.json\n#定义组 不同组 指向不同文件 多个组之间空格隔开\na3.sources.r3.filegroups = f1 f2\n#f1组 监控指定目录下名字包含file的文件 正则表达式\na3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*\n#f2组 监控指定目录下名字包含的log的文件\na3.sources.r3.filegroups.f2 = /opt/module/flume/files/.*log.*\n\n# describe the sink\na3.sinks.k3.type = hdfs\na3.sinks.k3.hdfs.path = hdfs://hadoop102:8020/flume/upload2/%y%m%d/%h\n#上传文件的前缀\na3.sinks.k3.hdfs.fileprefix = upload-\n#是否按照时间滚动文件夹\na3.sinks.k3.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na3.sinks.k3.hdfs.roundvalue = 1\n#重新定义时间单位\na3.sinks.k3.hdfs.roundunit = hour\n#是否使用本地时间戳\na3.sinks.k3.hdfs.uselocaltimestamp = true\n#积攒多少个event才flush到hdfs一次\na3.sinks.k3.hdfs.batchsize = 100\n#设置文件类型，可支持压缩\na3.sinks.k3.hdfs.filetype = datastream\n#多久生成一个新的文件\na3.sinks.k3.hdfs.rollinterval = 60\n#设置每个文件的滚动大小大概是128m\na3.sinks.k3.hdfs.rollsize = 134217700\n#文件的滚动与event数量无关\na3.sinks.k3.hdfs.rollcount = 0\n\n# use a channel which buffers events in memory\na3.channels.c3.type = memory\na3.channels.c3.capacity = 1000\na3.channels.c3.transactioncapacity = 100\n\n# bind the source and sink to the channel\na3.sources.r3.channels = c3\na3.sinks.k3.channel = c3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n启动\n\nflume-ng agent -n a3 -c conf/ -f datas/flume-taildir-hdfs.conf -dflume.root.logger=info,console\n\n\n1\n\n\n测试\n\ncd files\necho hello >> file1.txt\necho atguigu >> log.txt\n\n#关闭flume 重新启动 查看是否还读取 aaa 和 bbb 字段\n\n\n1\n2\n3\n4\n5\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Flume 进阶",frontmatter:{title:"Flume 进阶",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/6a0c06/",categories:["大数据","Flume"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/04.Flume/05.Flume%20%E8%BF%9B%E9%98%B6.html",relativePath:"大数据/04.Flume/05.Flume 进阶.md",key:"v-73eab2aa",path:"/pages/6a0c06/",headers:[{level:2,title:"Flume 事务",slug:"flume-事务",normalizedTitle:"flume 事务",charIndex:15},{level:2,title:"Agent 内部原理",slug:"agent-内部原理",normalizedTitle:"agent 内部原理",charIndex:30},{level:2,title:"Flume 拓扑结构",slug:"flume-拓扑结构",normalizedTitle:"flume 拓扑结构",charIndex:591},{level:3,title:"简单串联",slug:"简单串联",normalizedTitle:"简单串联",charIndex:606},{level:3,title:"复制和多路复用",slug:"复制和多路复用",normalizedTitle:"复制和多路复用",charIndex:2159},{level:3,title:"负载均衡和故障转移",slug:"负载均衡和故障转移",normalizedTitle:"负载均衡和故障转移",charIndex:6658},{level:3,title:"聚合",slug:"聚合",normalizedTitle:"聚合",charIndex:8752},{level:2,title:"拦截器 Interceptor",slug:"拦截器-interceptor",normalizedTitle:"拦截器 interceptor",charIndex:8973}],headersStr:"Flume 事务 Agent 内部原理 Flume 拓扑结构 简单串联 复制和多路复用 负载均衡和故障转移 聚合 拦截器 Interceptor",content:"# Flume 进阶\n\n\n# Flume 事务\n\n\n\n\n# Agent 内部原理\n\n\n\n 1. ChannelSelector\n    \n    ChannelSelector的作用就是选出Event将要被发往哪个Channel。其共有两种类型，分别是Replicating（复制）和Multiplexing（多路复用）。 默认为Replicating（复制） ReplicatingSelector会将同一个Event发往所有的Channel，Multiplexing会根据相应的原则，将不同的Event发往不同的Channel。\n\n 2. SinkProcessor\n    \n    SinkProcessor共有三种类型，分别是DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessor\n    \n    1. DefaultSinkProcessor对应的是单个的Sink\n    2. LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group，\n    3. LoadBalancingSinkProcessor可以实现负载均衡的功能\n    4. FailoverSinkProcessor可以错误恢复的功能。\n\n\n# Flume 拓扑结构\n\n\n# 简单串联\n\n\n\n这种模式是将多个flume顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。\n\n# 实现串联 输出到arvo\n\nflume1 配置文件\n\n#agent1   netcatsource --\x3e memorychannel --\x3e arvosink\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = hadoop102\na1.sources.r1.port = 22222 \n\n#设置为arvosink 向指定地址:端口输出数据\na1.sinks.k1.type = arvo \n#输出数据的地址 \na1.sinks.k1.hostname = hadoop103\n#输出数据的地址\na1.sinks.k1.port = 33333\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\nflume2 配置文件\n\n#agent2   netcatsource --\x3e memorychannel --\x3e loggersink\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n#输入数据类型改为 arvo\na1.sources.r1.type = arvo \n#输入地址\na1.sources.r1.bind = hadoop102\n#输入端口\na1.sources.r1.port = 33333\n\n#设置为logger 写入到log文件 持久化\na1.sinks.k1.type = logger\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n测试\n\n#先启动103 否则102发送数据无人接收  在hadoop103操作 \nflume-ng agent --conf conf/ --name a1 --conf-file datas/avrosource_loggersink.conf -Dflume.root.logger=INFO,console\n\n\n1\n2\n\n\n#在hadoop102操作\nflume-ng agent --conf conf/ --name a1 --conf-file datas/netcatsource_avrosink.conf -Dflume.root.logger=INFO,console\n#在另外个ssh窗口中操作\nnc hadoop102 22222\n\n\n1\n2\n3\n4\n\n\n\n# 复制和多路复用\n\n\n\nFlume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。\n\n# 实现复制 selector=replicating\n\n\n\n从指定文件中读取日志 复制转发到个channel中 channel再转发给指定的sink方\n\n#agent 1  hadoop102\na1.sources = r1\n\na1.sinks = k1 k2\na1.channels = c1 c2\n\na1.sources.r1.type = exec \n#读取hive日志文件\na1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log\n#selector频道选择器 默认为replicating 为复制 不配置type也是这个方案\na1.sources.r1.selector.type = replicating \n#可选的channel\n#a1.sources.r1.selector.optional = c3\n\n#设置为arvosink 向指定地址:端口输出数据\na1.sinks.k1.type = arvo \n#输出数据的地址 \na1.sinks.k1.hostname = hadoop103\n#输出数据的地址\na1.sinks.k1.port = 33333\n\n#第二个sinks\na1.sinks.k2.type = arvo \na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n#第二个channel\na1.channels.c2.type = memory \na1.channels.c2.capacity = 1000 \na1.channels.c2.transactionCapacity = 100 \n\n#一个sources 对接两个channels\na1.sources.r1.channels = c1 c2\n#每个sinks对应一个channel\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nsink方1 从hadoop102接收数据 再存储到hdfs中\n\n#agent2 hadoop103\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = avro\na1.sources.r1.bind = hadoop103\na1.sources.r1.port = 33333\n\n# Describe the sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = hdfs://hadoop102:8020/flume2/%Y%m%d/%H\n#上传文件的前缀\na1.sinks.k1.hdfs.filePrefix = flume2-\n#是否按照时间滚动文件夹\na1.sinks.k1.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na1.sinks.k1.hdfs.roundValue = 1\n#重新定义时间单位\na1.sinks.k1.hdfs.roundUnit = hour\n#是否使用本地时间戳\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#积攒多少个Event才flush到HDFS一次\na1.sinks.k1.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na1.sinks.k1.hdfs.fileType = DataStream\n#多久生成一个新的文件\na1.sinks.k1.hdfs.rollInterval = 600\n#设置每个文件的滚动大小大概是128M\na1.sinks.k1.hdfs.rollSize = 134217700\n#文件的滚动与Event数量无关\na1.sinks.k1.hdfs.rollCount = 0\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nsink2 从hadoop102接收数据 再通过File_roll sink存储到本地目录中\n\n#agent3 hadoop104\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = avro\na1.sources.r1.bind = hadoop104\na1.sources.r1.port = 44444\n\n#将event数据存储在指定sink为file_roll 本地存储模式\na1.sinks.k1.type = file_roll\n#存放目录 输出的本地目录必须是已经存在的目录\na1.sinks.k1.sink.directory = /opt/module/flume/demo\n#默认为30s 滚动文件 设置为0将不再滚动\na1.sinks.k1.sink.rollInterval = 30\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n启动 先启动103和104的监听 再启动102的监听\n\n# 实现多路复用 selector=multiplexing\n\nagent1\n\n#agent 1  hadoop102\na1.sources = r1\n\na1.sinks = k1 k2\na1.channels = c1 c2\n\na1.sources.r1.type = exec \n#读取hive日志文件\na1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log\n\n\n#复用配置\n#selector频道选择器 默认为replicating  multiplexing为复用  需要配合拦截器使用\na1.sources.r1.selector.type = multiplexing \n# header的key 根据event的header里面指定key 判断值 分发给哪个channel \na1.sources.r1.selector.header = state\n#CZ为自定义value值 为上面指定key中对应值 如key中值为CZ 则分发给 c1 channel\na1.sources.r1.selector.mapping.CZ = c1\n#值为US 则分发到 c2 channel\na1.sources.r1.selector.mapping.US = c2\n\n\n# 设置拦截器 (用于向headers中添加指定键值对)\n#拦截器名称\na1.sources.r1.interceptors = i1\n#拦截器类型 static 向header添加 自定义键值对\na1.sources.r1.interceptors.i1.type = static\na1.sources.r1.interceptors.i1.key = state\n#多个值只能通过自定义拦截器定义 此处是写死为CZ\na1.sources.r1.interceptors.i1.value = CZ\n\n\n#设置为arvosink 向指定地址:端口输出数据\na1.sinks.k1.type = arvo \n#输出数据的地址 \na1.sinks.k1.hostname = hadoop103\n#输出数据的地址\na1.sinks.k1.port = 33333\n\n#第二个sinks\na1.sinks.k2.type = arvo \na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n#第二个channel\na1.channels.c2.type = memory \na1.channels.c2.capacity = 1000 \na1.channels.c2.transactionCapacity = 100 \n\n#一个sources 对接两个channels\na1.sources.r1.channels = c1 c2\n#每个sinks对应一个channel\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\nagent2和agent3 与上面复制一样 或 自定义\n\n\n# 负载均衡和故障转移\n\n\n\nFlume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。\n\n# 实现故障转移 processor=failover\n\n\n\n#agent1 hadoop102\na1.sources = r1  \na1.sinks = k1 k2\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = hadoop102\na1.sources.r1.port = 22222\n\n#sinks1\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = hadoop103\na1.sinks.k1.port = 33333\n#sinks2\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\n#定义sinkgroups\na1.sinkgroups = g1\n#该组下面有哪些sink实例\na1.sinkgroups.g1.sinks = k1 k2\n#failover为故障转移  默认为一对一\na1.sinkgroups.g1.processor.type = failover\n#优先级 值越大优先级越大\na1.sinkgroups.g1.processor.priority.k1 = 5\na1.sinkgroups.g1.processor.priority.k2 = 10\n#sink连接超时时间 默认为30000毫秒\na1.sinkgroups.g1.processor.maxpenalty = 10000\n\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\n\na1.sources.r1.channels = c1 \n#sinks绑定的channel 应为一个\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n# 实现负载均衡 processor=load_balance\n\n#agent1 hadoop102\na1.sources = r1  \na1.sinks = k1 k2\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = hadoop102\na1.sources.r1.port = 22222\n\n#sinks1\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = hadoop103\na1.sinks.k1.port = 33333\n#sinks2\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\n#定义sinkgroups\na1.sinkgroups = g1\n#该组下面有哪些sink实例\na1.sinkgroups.g1.sinks = k1 k2\n#load_balance 为负载均衡\na1.sinkgroups.g1.processor.type = load_balance\n# 默认为round_robin轮询sink    random为随机发给某个sink\na1.sinkgroups.g1.processor.selector = random\n#连接超时时间 30000毫秒\na1.sinkgroups.g1.processor.maxpenalty = 10000\n\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\n\na1.sources.r1.channels = c1 \n#sinks绑定的channel 应为一个\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 聚合\n\n\n\n这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等，进行日志分析。\n\n# 实现聚合\n\n\n\n将分开的agent的sink全部汇总到一个agent上 再进行持久化\n\n\n# 拦截器 Interceptor\n\n更多类型拦截器查看官方文档\n\n通过配置文件 配置拦截器 agent名称.sources.r1.interceptors\n\na1.sources = r1\na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# 设置拦截器 (用于向headers中添加时间戳)\n#拦截器名称\na1.sources.r1.interceptors = i1\n#拦截器类型 timestamp 向header添加时间戳\na1.sources.r1.interceptors.i1.type = timestamp\n\na1.sinks.k1.type = logger\n\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n",normalizedContent:"# flume 进阶\n\n\n# flume 事务\n\n\n\n\n# agent 内部原理\n\n\n\n 1. channelselector\n    \n    channelselector的作用就是选出event将要被发往哪个channel。其共有两种类型，分别是replicating（复制）和multiplexing（多路复用）。 默认为replicating（复制） replicatingselector会将同一个event发往所有的channel，multiplexing会根据相应的原则，将不同的event发往不同的channel。\n\n 2. sinkprocessor\n    \n    sinkprocessor共有三种类型，分别是defaultsinkprocessor、loadbalancingsinkprocessor和failoversinkprocessor\n    \n    1. defaultsinkprocessor对应的是单个的sink\n    2. loadbalancingsinkprocessor和failoversinkprocessor对应的是sink group，\n    3. loadbalancingsinkprocessor可以实现负载均衡的功能\n    4. failoversinkprocessor可以错误恢复的功能。\n\n\n# flume 拓扑结构\n\n\n# 简单串联\n\n\n\n这种模式是将多个flume顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。\n\n# 实现串联 输出到arvo\n\nflume1 配置文件\n\n#agent1   netcatsource --\x3e memorychannel --\x3e arvosink\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = hadoop102\na1.sources.r1.port = 22222 \n\n#设置为arvosink 向指定地址:端口输出数据\na1.sinks.k1.type = arvo \n#输出数据的地址 \na1.sinks.k1.hostname = hadoop103\n#输出数据的地址\na1.sinks.k1.port = 33333\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\nflume2 配置文件\n\n#agent2   netcatsource --\x3e memorychannel --\x3e loggersink\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n#输入数据类型改为 arvo\na1.sources.r1.type = arvo \n#输入地址\na1.sources.r1.bind = hadoop102\n#输入端口\na1.sources.r1.port = 33333\n\n#设置为logger 写入到log文件 持久化\na1.sinks.k1.type = logger\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n测试\n\n#先启动103 否则102发送数据无人接收  在hadoop103操作 \nflume-ng agent --conf conf/ --name a1 --conf-file datas/avrosource_loggersink.conf -dflume.root.logger=info,console\n\n\n1\n2\n\n\n#在hadoop102操作\nflume-ng agent --conf conf/ --name a1 --conf-file datas/netcatsource_avrosink.conf -dflume.root.logger=info,console\n#在另外个ssh窗口中操作\nnc hadoop102 22222\n\n\n1\n2\n3\n4\n\n\n\n# 复制和多路复用\n\n\n\nflume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。\n\n# 实现复制 selector=replicating\n\n\n\n从指定文件中读取日志 复制转发到个channel中 channel再转发给指定的sink方\n\n#agent 1  hadoop102\na1.sources = r1\n\na1.sinks = k1 k2\na1.channels = c1 c2\n\na1.sources.r1.type = exec \n#读取hive日志文件\na1.sources.r1.command = tail -f /opt/module/hive/logs/hive.log\n#selector频道选择器 默认为replicating 为复制 不配置type也是这个方案\na1.sources.r1.selector.type = replicating \n#可选的channel\n#a1.sources.r1.selector.optional = c3\n\n#设置为arvosink 向指定地址:端口输出数据\na1.sinks.k1.type = arvo \n#输出数据的地址 \na1.sinks.k1.hostname = hadoop103\n#输出数据的地址\na1.sinks.k1.port = 33333\n\n#第二个sinks\na1.sinks.k2.type = arvo \na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n#第二个channel\na1.channels.c2.type = memory \na1.channels.c2.capacity = 1000 \na1.channels.c2.transactioncapacity = 100 \n\n#一个sources 对接两个channels\na1.sources.r1.channels = c1 c2\n#每个sinks对应一个channel\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nsink方1 从hadoop102接收数据 再存储到hdfs中\n\n#agent2 hadoop103\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = avro\na1.sources.r1.bind = hadoop103\na1.sources.r1.port = 33333\n\n# describe the sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = hdfs://hadoop102:8020/flume2/%y%m%d/%h\n#上传文件的前缀\na1.sinks.k1.hdfs.fileprefix = flume2-\n#是否按照时间滚动文件夹\na1.sinks.k1.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na1.sinks.k1.hdfs.roundvalue = 1\n#重新定义时间单位\na1.sinks.k1.hdfs.roundunit = hour\n#是否使用本地时间戳\na1.sinks.k1.hdfs.uselocaltimestamp = true\n#积攒多少个event才flush到hdfs一次\na1.sinks.k1.hdfs.batchsize = 100\n#设置文件类型，可支持压缩\na1.sinks.k1.hdfs.filetype = datastream\n#多久生成一个新的文件\na1.sinks.k1.hdfs.rollinterval = 600\n#设置每个文件的滚动大小大概是128m\na1.sinks.k1.hdfs.rollsize = 134217700\n#文件的滚动与event数量无关\na1.sinks.k1.hdfs.rollcount = 0\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nsink2 从hadoop102接收数据 再通过file_roll sink存储到本地目录中\n\n#agent3 hadoop104\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = avro\na1.sources.r1.bind = hadoop104\na1.sources.r1.port = 44444\n\n#将event数据存储在指定sink为file_roll 本地存储模式\na1.sinks.k1.type = file_roll\n#存放目录 输出的本地目录必须是已经存在的目录\na1.sinks.k1.sink.directory = /opt/module/flume/demo\n#默认为30s 滚动文件 设置为0将不再滚动\na1.sinks.k1.sink.rollinterval = 30\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n启动 先启动103和104的监听 再启动102的监听\n\n# 实现多路复用 selector=multiplexing\n\nagent1\n\n#agent 1  hadoop102\na1.sources = r1\n\na1.sinks = k1 k2\na1.channels = c1 c2\n\na1.sources.r1.type = exec \n#读取hive日志文件\na1.sources.r1.command = tail -f /opt/module/hive/logs/hive.log\n\n\n#复用配置\n#selector频道选择器 默认为replicating  multiplexing为复用  需要配合拦截器使用\na1.sources.r1.selector.type = multiplexing \n# header的key 根据event的header里面指定key 判断值 分发给哪个channel \na1.sources.r1.selector.header = state\n#cz为自定义value值 为上面指定key中对应值 如key中值为cz 则分发给 c1 channel\na1.sources.r1.selector.mapping.cz = c1\n#值为us 则分发到 c2 channel\na1.sources.r1.selector.mapping.us = c2\n\n\n# 设置拦截器 (用于向headers中添加指定键值对)\n#拦截器名称\na1.sources.r1.interceptors = i1\n#拦截器类型 static 向header添加 自定义键值对\na1.sources.r1.interceptors.i1.type = static\na1.sources.r1.interceptors.i1.key = state\n#多个值只能通过自定义拦截器定义 此处是写死为cz\na1.sources.r1.interceptors.i1.value = cz\n\n\n#设置为arvosink 向指定地址:端口输出数据\na1.sinks.k1.type = arvo \n#输出数据的地址 \na1.sinks.k1.hostname = hadoop103\n#输出数据的地址\na1.sinks.k1.port = 33333\n\n#第二个sinks\na1.sinks.k2.type = arvo \na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n#第二个channel\na1.channels.c2.type = memory \na1.channels.c2.capacity = 1000 \na1.channels.c2.transactioncapacity = 100 \n\n#一个sources 对接两个channels\na1.sources.r1.channels = c1 c2\n#每个sinks对应一个channel\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\nagent2和agent3 与上面复制一样 或 自定义\n\n\n# 负载均衡和故障转移\n\n\n\nflume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的sinkprocessor可以实现负载均衡和错误恢复的功能。\n\n# 实现故障转移 processor=failover\n\n\n\n#agent1 hadoop102\na1.sources = r1  \na1.sinks = k1 k2\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = hadoop102\na1.sources.r1.port = 22222\n\n#sinks1\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = hadoop103\na1.sinks.k1.port = 33333\n#sinks2\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\n#定义sinkgroups\na1.sinkgroups = g1\n#该组下面有哪些sink实例\na1.sinkgroups.g1.sinks = k1 k2\n#failover为故障转移  默认为一对一\na1.sinkgroups.g1.processor.type = failover\n#优先级 值越大优先级越大\na1.sinkgroups.g1.processor.priority.k1 = 5\na1.sinkgroups.g1.processor.priority.k2 = 10\n#sink连接超时时间 默认为30000毫秒\na1.sinkgroups.g1.processor.maxpenalty = 10000\n\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\n\na1.sources.r1.channels = c1 \n#sinks绑定的channel 应为一个\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n# 实现负载均衡 processor=load_balance\n\n#agent1 hadoop102\na1.sources = r1  \na1.sinks = k1 k2\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = hadoop102\na1.sources.r1.port = 22222\n\n#sinks1\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = hadoop103\na1.sinks.k1.port = 33333\n#sinks2\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\n#定义sinkgroups\na1.sinkgroups = g1\n#该组下面有哪些sink实例\na1.sinkgroups.g1.sinks = k1 k2\n#load_balance 为负载均衡\na1.sinkgroups.g1.processor.type = load_balance\n# 默认为round_robin轮询sink    random为随机发给某个sink\na1.sinkgroups.g1.processor.selector = random\n#连接超时时间 30000毫秒\na1.sinkgroups.g1.processor.maxpenalty = 10000\n\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\n\na1.sources.r1.channels = c1 \n#sinks绑定的channel 应为一个\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 聚合\n\n\n\n这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等，进行日志分析。\n\n# 实现聚合\n\n\n\n将分开的agent的sink全部汇总到一个agent上 再进行持久化\n\n\n# 拦截器 interceptor\n\n更多类型拦截器查看官方文档\n\n通过配置文件 配置拦截器 agent名称.sources.r1.interceptors\n\na1.sources = r1\na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# 设置拦截器 (用于向headers中添加时间戳)\n#拦截器名称\na1.sources.r1.interceptors = i1\n#拦截器类型 timestamp 向header添加时间戳\na1.sources.r1.interceptors.i1.type = timestamp\n\na1.sinks.k1.type = logger\n\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactioncapacity = 100\n\n\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Flime基础架构",frontmatter:{title:"Flime基础架构",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/f8bd8e/",categories:["大数据","Flume"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/04.Flume/02.Flime%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.html",relativePath:"大数据/04.Flume/02.Flime基础架构.md",key:"v-6e080d03",path:"/pages/f8bd8e/",headers:[{level:2,title:"Agent",slug:"agent",normalizedTitle:"agent",charIndex:18},{level:2,title:"Source",slug:"source",normalizedTitle:"source",charIndex:74},{level:2,title:"Sink",slug:"sink",normalizedTitle:"sink",charIndex:89},{level:2,title:"Channel",slug:"channel",normalizedTitle:"channel",charIndex:81},{level:2,title:"Event",slug:"event",normalizedTitle:"event",charIndex:724}],headersStr:"Agent Source Sink Channel Event",content:"# Flime基础架构\n\n\n\n\n# Agent\n\nAgent是一个JVM进程，它以事件的形式将数据从源头送至目的。\n\nAgent主要有3个部分组成，Source、Channel、Sink。\n\n\n# Source\n\nSource是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。\n\n\n# Sink\n\nSink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。\n\nSink组件目的地包括hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。\n\n\n# Channel\n\nChannel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。\n\nFlume自带两种Channel：Memory Channel和File Channel。\n\nMemory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。\n\nFile Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。\n\n\n# Event\n\n传输单元，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。Event由Header和Body两部分组成，Header用来存放该event的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组。\n\n",normalizedContent:"# flime基础架构\n\n\n\n\n# agent\n\nagent是一个jvm进程，它以事件的形式将数据从源头送至目的。\n\nagent主要有3个部分组成，source、channel、sink。\n\n\n# source\n\nsource是负责接收数据到flume agent的组件。source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。\n\n\n# sink\n\nsink不断地轮询channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个flume agent。\n\nsink组件目的地包括hdfs、logger、avro、thrift、ipc、file、hbase、solr、自定义。\n\n\n# channel\n\nchannel是位于source和sink之间的缓冲区。因此，channel允许source和sink运作在不同的速率上。channel是线程安全的，可以同时处理几个source的写入操作和几个sink的读取操作。\n\nflume自带两种channel：memory channel和file channel。\n\nmemory channel是内存中的队列。memory channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么memory channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。\n\nfile channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。\n\n\n# event\n\n传输单元，flume数据传输的基本单元，以event的形式将数据从源头送至目的地。event由header和body两部分组成，header用来存放该event的一些属性，为k-v结构，body用来存放该条数据，形式为字节数组。\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Kafka",frontmatter:{title:"Kafka",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/872cdc/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/01.Kafka.html",relativePath:"大数据/05.Kafka/01.Kafka.md",key:"v-1391410f",path:"/pages/872cdc/",headersStr:null,content:"# Kafka\n\nKafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。\n\n主要应用场景是：日志收集系统和消息系统。\n\nKafka主要设计目标如下：\n\n * 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。\n * 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。\n * 支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。\n * 同时支持离线数据处理和实时数据处理。\n * Scale out:支持在线水平扩展",normalizedContent:"# kafka\n\nkafka是最初由linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做mq系统），常见可以用于web/nginx日志、访问日志，消息服务等等，linkedin于2010年贡献给了apache基金会并成为顶级开源项目。\n\n主要应用场景是：日志收集系统和消息系统。\n\nkafka主要设计目标如下：\n\n * 以时间复杂度为o(1)的方式提供消息持久化能力，即使对tb级以上数据也能保证常数时间的访问性能。\n * 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100k条消息的传输。\n * 支持kafka server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。\n * 同时支持离线数据处理和实时数据处理。\n * scale out:支持在线水平扩展",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"自定义组件",frontmatter:{title:"自定义组件",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/8e2fbd/",categories:["大数据","Flume"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/04.Flume/06.%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BB%84%E4%BB%B6.html",relativePath:"大数据/04.Flume/06.自定义组件.md",key:"v-210176a7",path:"/pages/8e2fbd/",headers:[{level:2,title:"自定义Interceptor 拦截器",slug:"自定义interceptor-拦截器",normalizedTitle:"自定义interceptor 拦截器",charIndex:85},{level:2,title:"自定义 Source",slug:"自定义-source",normalizedTitle:"自定义 source",charIndex:3260},{level:2,title:"自定义 Sink",slug:"自定义-sink",normalizedTitle:"自定义 sink",charIndex:6185},{level:2,title:"Flume 数据流监控",slug:"flume-数据流监控",normalizedTitle:"flume 数据流监控",charIndex:8562},{level:3,title:"Ganglia的安装与部署",slug:"ganglia的安装与部署",normalizedTitle:"ganglia的安装与部署",charIndex:8578},{level:3,title:"修改配置",slug:"修改配置",normalizedTitle:"修改配置",charIndex:9200},{level:3,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:11172},{level:2,title:"操作Flume 测试监控",slug:"操作flume-测试监控",normalizedTitle:"操作flume 测试监控",charIndex:11387}],headersStr:"自定义Interceptor 拦截器 自定义 Source 自定义 Sink Flume 数据流监控 Ganglia的安装与部署 修改配置 启动 操作Flume 测试监控",content:'# 自定义组件\n\nhttp://flume.apache.org/releases/content/1.9.0/FlumeDeveloperGuide.html\n\n\n# 自定义Interceptor 拦截器\n\n\n\n#agent 1  hadoop102\na1.sources = r1\n\na1.sinks = k1 k2\na1.channels = c1 c2\n\na1.sources.r1.type = netcat \na1.sources.r1.bing = hadoop102\na1.sources.r1.port = 22222 \n\n#复用配置\na1.sources.r1.selector.type = multiplexing \na1.sources.r1.selector.header = type\na1.sources.r1.selector.mapping.letter = c1\na1.sources.r1.selector.mapping.number = c2\n\n\na1.sources.r1.interceptors = i1\n#自定义拦截器 引入类路径 中 Builder 内部类\na1.sources.r1.interceptors.i1.type = comm.atguigu.demo.MyInterceptor$Builder\n\n\na1.sinks.k1.type = arvo \na1.sinks.k1.hostname = hadoop103\na1.sinks.k1.port = 33333\n\n#第二个sinks\na1.sinks.k2.type = arvo \na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n#第二个channel\na1.channels.c2.type = memory \na1.channels.c2.capacity = 1000 \na1.channels.c2.transactionCapacity = 100 \n\n#一个sources 对接两个channels\na1.sources.r1.channels = c1 c2\n#每个sinks对应一个channel\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\nagent2和agent3与之前无差\n\n导入pom依赖\n\n<dependency>\n    <groupId>org.apache.flume</groupId>\n    <artifactId>flume-ng-core</artifactId>\n    <version>1.9.0</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n实现Interceptor接口 重写抽象方法 并书写一个内部类实现Interceptor.Builder接口 实现抽象方法\n\npackage comm.atguigu.demo;\n\nimport org.apache.flume.Context;\nimport org.apache.flume.Event;\nimport org.apache.flume.interceptor.Interceptor;\n\nimport java.util.List;\n\n/**\n * 自定义拦截器\n */\npublic class MyInterceptor implements Interceptor {\n    //初始化\n    @Override\n    public void initialize() {\n\n    }\n\n    //为每个event中的header添加键值对  channelProcessor调用拦截器时调用此方法并将event传过来\n    @Override\n    public Event intercept(Event event) {\n        //获取event body中的内容\n        byte[] body = event.getBody();\n        //判断内容是否是字母\n        if ((body[0] >= \'A\' && body[0] <= \'Z\') || (body[0] >= \'a\' && body[0] <= \'z\')) {\n            //向header添加 type = letter\n            event.getHeaders().put("type", "letter");\n        } else if (body[0] >= \'0\' && body[0] <= \'9\') {\n            //否则添加 type = number\n            event.getHeaders().put("type", "number");\n        }\n\n        return event;\n    }\n\n    @Override\n    public List<Event> intercept(List<Event> list) {\n        //遍历\n        for (Event event : list) {\n            intercept(event);\n        }\n        return list;\n    }\n\n    //关闭资源\n    @Override\n    public void close() {\n\n    }\n\n    /**\n     * 返回MyInterceptor的实例\n     * 1.静态内部类 公开权限\n     */\n    public static class Builder implements Interceptor.Builder{\n\n        //返回自定义拦截器类\n        @Override\n        public Interceptor build() {\n            return new MyInterceptor();\n        }\n\n        @Override\n        public void configure(Context context) {\n\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\nmaven打包 上传到flume中lib文件夹\n\nflume-ng agent -n a1 -c conf/ -f datas/flume_interceptor.conf -Dflume.root.logger=INFO,console\n\n\n1\n\n\n\n# 自定义 Source\n\n使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。\n\n\n\n继承 AbstractSource 实现 Configurable, PollableSource\n\npackage comm.atguigu.demo;\n\nimport org.apache.flume.Context;\nimport org.apache.flume.Event;\nimport org.apache.flume.EventDeliveryException;\nimport org.apache.flume.PollableSource;\nimport org.apache.flume.channel.ChannelProcessor;\nimport org.apache.flume.conf.Configurable;\nimport org.apache.flume.event.SimpleEvent;\nimport org.apache.flume.source.AbstractSource;\n\nimport java.nio.charset.StandardCharsets;\nimport java.util.ArrayList;\nimport java.util.List;\n\n//自定义source\n//使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。\npublic class MySource extends AbstractSource implements Configurable, PollableSource {\n    private String prefix;\n\n    /**\n     * 获取数据封装成event并写入channel，这个方法将被循环调用。\n     *\n     * @return status 枚举类 1.READY 添加event成功  2.BACKOFF 添加event失败\n     * @throws EventDeliveryException\n     */\n    @Override\n    public Status process() throws EventDeliveryException {\n\n        try {\n            List<Event> list = new ArrayList<>();\n            for (int i = 0; i < 5; i++) {\n                //封装event\n                SimpleEvent event = new SimpleEvent();\n                //event设置数据 加上前缀\n                event.setBody((prefix + "hello" + i).getBytes(StandardCharsets.UTF_8));\n                //放入集合中\n                list.add(event);\n            }\n            //获取channelProcessor\n            ChannelProcessor channelProcessor = getChannelProcessor();\n            //将数据放入到channel中(channelProcessor)\n            // channelProcessor.processEvent(event); //单个数据\n            channelProcessor.processEventBatch(list); //集合放入\n        } catch (Exception e) {\n            e.printStackTrace();\n            return Status.BACKOFF;\n        }\n        return Status.READY;\n    }\n\n    //暂不用  当source没数据可封装时 会让source所在的线程休息会\n    @Override\n    public long getBackOffSleepIncrement() {\n        return 2000L; //休息2000毫秒\n    }\n\n    //暂不用  当source没数据可封装时 会让source所在的线程休息的最大时间 如果前面休息的时间大于设置的max值 则后面都不休息(休息0毫秒)\n    @Override\n    public long getMaxBackOffSleepInterval() {\n        return 5000L;\n    }\n\n    //初始化context（读取配置文件内容）\n    @Override\n    public void configure(Context context) {\n        //前缀 默认值为默认值test=\n        prefix = context.getString("prefix", "test=");\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n\n\n打包并上传到 flume的lib中\n\n配置文件\n\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\n#自定义sources 引用类路径\na1.sources.r1.type = comm.atguigu.demo.MySource\n#自定义设置前缀 如果为空 则使用自定义类中默认值test=\na1.sources.r1.prefix = qaq\n\na1.sinks.k1.type = logger  \n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 自定义 Sink\n\n\n\n继承 AbstractSink 实现 Configurable\n\npackage comm.atguigu.demo;\n\nimport org.apache.flume.*;\nimport org.apache.flume.conf.Configurable;\nimport org.apache.flume.sink.AbstractSink;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class MySink extends AbstractSink implements Configurable {\n\n    //从配置文件读取suffix的值\n    private String suffix;\n    //获取logger对象 可以将数据以日志的方式输出  或以 写入数据本地等持久化数据\n    Logger logger = LoggerFactory.getLogger(MySink.class);\n\n    /**\n     * 用来处理sink逻辑 将channel中的内容写出去 会被不停的循环调用\n     *\n     * @return\n     * @throws EventDeliveryException\n     */\n    @Override\n    public Status process() throws EventDeliveryException {\n        //获取channel\n        Channel channel = getChannel();\n        //获取事务\n        Transaction transaction = channel.getTransaction();\n        try {\n            Event event = null;\n            transaction.begin();//开启事务\n\n            while (true) {\n                event = channel.take();//获取数据\n                if (event != null) {\n                    //保证event中是有数据的\n                    break;\n                }\n            }\n\n            //将数据写出 此处以日志形式输出  以某种持久化形式将数据输出\n            logger.info(new String(event.getBody()) + suffix);\n            //提交事务\n            transaction.commit();\n        } catch (ChannelException e) {\n            e.printStackTrace();\n            transaction.rollback(); //事务回滚\n            return Status.BACKOFF; //获取数据失败\n\n        } finally {\n            transaction.close(); //关闭资源\n        }\n\n        return Status.READY;\n    }\n\n\n    /**\n     * 获取上下文 读取配置文件中的内容\n     *\n     * @param context\n     */\n    @Override\n    public void configure(Context context) {\n        suffix = context.getString("suffix", "test");\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n\n\n打包上传到flume的lib中\n\n配置文件\n\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = localhost \na1.sources.r1.port = 44444 \n\n#自定义sink\na1.sinks.k1.type = comm.atguigu.demo.MySink\n#给数据设置后缀\na1.sinks.k1.suffix = atguigu\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactionCapacity = 100 \n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# Flume 数据流监控\n\n\n# Ganglia的安装与部署\n\nsudo yum -y install httpd php # 安装 httpd服务 与 php\nsudo yum -y install rrdtool perl-rrdtool rrdtool-devel # 依赖\nsudo yum -y install apr-devel #依赖\n\n#安装ganglia\nsudo yum install epel-release\nsudo yum -y install ganglia-gmetad\nsudo yum -y install ganglia-web\nsudo yum install -y ganglia-gmond\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nGanglia由gmond、gmetad和gweb三部分组成。\n\ngmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用gmond，你可以很容易收集很多系统指标数据，如CPU、内存、磁盘、网络和活跃进程的数据等。\n\ngmetad（Ganglia Meta Daemon）整合所有信息，并将其以RRD格式存储至磁盘的服务。\n\ngweb（Ganglia Web）Ganglia可视化工具，gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。在Web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。\n\n\n# 修改配置\n\nsudo vim /etc/httpd/conf.d/ganglia.conf\n\n\n1\n\n\n# Ganglia monitoring system php web frontend\nAlias /ganglia /usr/share/ganglia\n<Location /ganglia>\n  Require all granted\n  #Deny from all\n  # Allow from 127.0.0.1\n  # Allow from ::1\n  # Allow from .example.com\n</Location>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nsudo vim /etc/ganglia/gmetad.conf\n\n\n1\n\n\n#更改此字段的ip地址\ndata_source "my_cluster" 192.168.130.102 \n\n\n1\n2\n\n\nsudo vim /etc/ganglia/gmond.conf\n\n\n1\n\n\ncluster {\n  name = "my_cluster"\n  owner = "unspecified"\n  latlong = "unspecified"\n  url = "unspecified"\n}\nudp_send_channel {\n  #bind_hostname = yes # Highly recommended, soon to be default.\n                       # This option tells gmond to use a source address\n                       # that resolves to the machine\'s hostname.  Without\n                       # this, the metrics may appear to come from any\n                       # interface and the DNS names associated with\n                       # those IPs will be used to create the RRDs.\n  # mcast_join = 239.2.11.71\n  host = 192.168.130.102\n  port = 8649\n  ttl = 1\n}\nudp_recv_channel {\n  # mcast_join = 239.2.11.71\n  port = 8649\n  bind = 192.168.130.102\n  retry_bind = true\n  # Size of the UDP buffer. If you are handling lots of metrics you really\n  # should bump it up to e.g. 10MB or even higher.\n  # buffer = 10485760\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nsudo vim /etc/selinux/config\n\n\n1\n\n\n# This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these two values:\n#     targeted - Targeted processes are protected,\n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nselinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之\n\nsudo setenforce 0\n\n\n1\n\n\n\n# 启动\n\nsudo service httpd start\nsudo service gmetad start\nsudo service gmond start\n\n\n1\n2\n3\n\n\n访问 http://192.168.130.102/ganglia\n\n如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：\n\nsudo chmod -R 777 /var/lib/ganglia\n\n\n1\n\n\n\n# 操作Flume 测试监控\n\n进入flume下的conf目录\n\ncd /opt/module/flume/conf/\nmv flume-env.sh.template flume-env.sh #重命名\nvim flume-env.sh \n\n\n1\n2\n3\n\n\n追加以下配置 ip地址为ganglia主地址\n\nJAVA_OPTS="-Dflume.monitoring.type=ganglia\n-Dflume.monitoring.hosts=192.168.130.102:8649\n-Xms100m\n-Xmx200m"\n\n\n1\n2\n3\n4\n\n\n启动flume任务\n\nflume-ng agent \\\n--conf conf/ \\\n--name a1 \\\n--conf-file datas/netcatsource_loggersink.conf \\\n-Dflume.root.logger==INFO,console \\\n-Dflume.monitoring.type=ganglia \\\n-Dflume.monitoring.hosts=192.168.130.102:8649\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n发送信息\n\nnc localhost 44444\n\n\n1\n\n\n\n\n图例说明\n\n字段（图表名称）                字段含义\nEventPutAttemptCount    source尝试写入channel的事件总数量\nEventPutSuccessCount    成功写入channel且提交的事件总数量\nEventTakeAttemptCount   sink尝试从channel拉取事件的总数量。\nEventTakeSuccessCount   sink成功读取的事件的总数量\nStartTime               channel启动的时间（毫秒）\nStopTime                channel停止的时间（毫秒）\nChannelSize             目前channel中事件的总数量\nChannelFillPercentage   channel占用百分比\nChannelCapacity         channel的容量',normalizedContent:'# 自定义组件\n\nhttp://flume.apache.org/releases/content/1.9.0/flumedeveloperguide.html\n\n\n# 自定义interceptor 拦截器\n\n\n\n#agent 1  hadoop102\na1.sources = r1\n\na1.sinks = k1 k2\na1.channels = c1 c2\n\na1.sources.r1.type = netcat \na1.sources.r1.bing = hadoop102\na1.sources.r1.port = 22222 \n\n#复用配置\na1.sources.r1.selector.type = multiplexing \na1.sources.r1.selector.header = type\na1.sources.r1.selector.mapping.letter = c1\na1.sources.r1.selector.mapping.number = c2\n\n\na1.sources.r1.interceptors = i1\n#自定义拦截器 引入类路径 中 builder 内部类\na1.sources.r1.interceptors.i1.type = comm.atguigu.demo.myinterceptor$builder\n\n\na1.sinks.k1.type = arvo \na1.sinks.k1.hostname = hadoop103\na1.sinks.k1.port = 33333\n\n#第二个sinks\na1.sinks.k2.type = arvo \na1.sinks.k2.hostname = hadoop104\na1.sinks.k2.port = 44444\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n#第二个channel\na1.channels.c2.type = memory \na1.channels.c2.capacity = 1000 \na1.channels.c2.transactioncapacity = 100 \n\n#一个sources 对接两个channels\na1.sources.r1.channels = c1 c2\n#每个sinks对应一个channel\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\nagent2和agent3与之前无差\n\n导入pom依赖\n\n<dependency>\n    <groupid>org.apache.flume</groupid>\n    <artifactid>flume-ng-core</artifactid>\n    <version>1.9.0</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n实现interceptor接口 重写抽象方法 并书写一个内部类实现interceptor.builder接口 实现抽象方法\n\npackage comm.atguigu.demo;\n\nimport org.apache.flume.context;\nimport org.apache.flume.event;\nimport org.apache.flume.interceptor.interceptor;\n\nimport java.util.list;\n\n/**\n * 自定义拦截器\n */\npublic class myinterceptor implements interceptor {\n    //初始化\n    @override\n    public void initialize() {\n\n    }\n\n    //为每个event中的header添加键值对  channelprocessor调用拦截器时调用此方法并将event传过来\n    @override\n    public event intercept(event event) {\n        //获取event body中的内容\n        byte[] body = event.getbody();\n        //判断内容是否是字母\n        if ((body[0] >= \'a\' && body[0] <= \'z\') || (body[0] >= \'a\' && body[0] <= \'z\')) {\n            //向header添加 type = letter\n            event.getheaders().put("type", "letter");\n        } else if (body[0] >= \'0\' && body[0] <= \'9\') {\n            //否则添加 type = number\n            event.getheaders().put("type", "number");\n        }\n\n        return event;\n    }\n\n    @override\n    public list<event> intercept(list<event> list) {\n        //遍历\n        for (event event : list) {\n            intercept(event);\n        }\n        return list;\n    }\n\n    //关闭资源\n    @override\n    public void close() {\n\n    }\n\n    /**\n     * 返回myinterceptor的实例\n     * 1.静态内部类 公开权限\n     */\n    public static class builder implements interceptor.builder{\n\n        //返回自定义拦截器类\n        @override\n        public interceptor build() {\n            return new myinterceptor();\n        }\n\n        @override\n        public void configure(context context) {\n\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\nmaven打包 上传到flume中lib文件夹\n\nflume-ng agent -n a1 -c conf/ -f datas/flume_interceptor.conf -dflume.root.logger=info,console\n\n\n1\n\n\n\n# 自定义 source\n\n使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。\n\n\n\n继承 abstractsource 实现 configurable, pollablesource\n\npackage comm.atguigu.demo;\n\nimport org.apache.flume.context;\nimport org.apache.flume.event;\nimport org.apache.flume.eventdeliveryexception;\nimport org.apache.flume.pollablesource;\nimport org.apache.flume.channel.channelprocessor;\nimport org.apache.flume.conf.configurable;\nimport org.apache.flume.event.simpleevent;\nimport org.apache.flume.source.abstractsource;\n\nimport java.nio.charset.standardcharsets;\nimport java.util.arraylist;\nimport java.util.list;\n\n//自定义source\n//使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。\npublic class mysource extends abstractsource implements configurable, pollablesource {\n    private string prefix;\n\n    /**\n     * 获取数据封装成event并写入channel，这个方法将被循环调用。\n     *\n     * @return status 枚举类 1.ready 添加event成功  2.backoff 添加event失败\n     * @throws eventdeliveryexception\n     */\n    @override\n    public status process() throws eventdeliveryexception {\n\n        try {\n            list<event> list = new arraylist<>();\n            for (int i = 0; i < 5; i++) {\n                //封装event\n                simpleevent event = new simpleevent();\n                //event设置数据 加上前缀\n                event.setbody((prefix + "hello" + i).getbytes(standardcharsets.utf_8));\n                //放入集合中\n                list.add(event);\n            }\n            //获取channelprocessor\n            channelprocessor channelprocessor = getchannelprocessor();\n            //将数据放入到channel中(channelprocessor)\n            // channelprocessor.processevent(event); //单个数据\n            channelprocessor.processeventbatch(list); //集合放入\n        } catch (exception e) {\n            e.printstacktrace();\n            return status.backoff;\n        }\n        return status.ready;\n    }\n\n    //暂不用  当source没数据可封装时 会让source所在的线程休息会\n    @override\n    public long getbackoffsleepincrement() {\n        return 2000l; //休息2000毫秒\n    }\n\n    //暂不用  当source没数据可封装时 会让source所在的线程休息的最大时间 如果前面休息的时间大于设置的max值 则后面都不休息(休息0毫秒)\n    @override\n    public long getmaxbackoffsleepinterval() {\n        return 5000l;\n    }\n\n    //初始化context（读取配置文件内容）\n    @override\n    public void configure(context context) {\n        //前缀 默认值为默认值test=\n        prefix = context.getstring("prefix", "test=");\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n\n\n打包并上传到 flume的lib中\n\n配置文件\n\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\n#自定义sources 引用类路径\na1.sources.r1.type = comm.atguigu.demo.mysource\n#自定义设置前缀 如果为空 则使用自定义类中默认值test=\na1.sources.r1.prefix = qaq\n\na1.sinks.k1.type = logger  \n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 自定义 sink\n\n\n\n继承 abstractsink 实现 configurable\n\npackage comm.atguigu.demo;\n\nimport org.apache.flume.*;\nimport org.apache.flume.conf.configurable;\nimport org.apache.flume.sink.abstractsink;\nimport org.slf4j.logger;\nimport org.slf4j.loggerfactory;\n\npublic class mysink extends abstractsink implements configurable {\n\n    //从配置文件读取suffix的值\n    private string suffix;\n    //获取logger对象 可以将数据以日志的方式输出  或以 写入数据本地等持久化数据\n    logger logger = loggerfactory.getlogger(mysink.class);\n\n    /**\n     * 用来处理sink逻辑 将channel中的内容写出去 会被不停的循环调用\n     *\n     * @return\n     * @throws eventdeliveryexception\n     */\n    @override\n    public status process() throws eventdeliveryexception {\n        //获取channel\n        channel channel = getchannel();\n        //获取事务\n        transaction transaction = channel.gettransaction();\n        try {\n            event event = null;\n            transaction.begin();//开启事务\n\n            while (true) {\n                event = channel.take();//获取数据\n                if (event != null) {\n                    //保证event中是有数据的\n                    break;\n                }\n            }\n\n            //将数据写出 此处以日志形式输出  以某种持久化形式将数据输出\n            logger.info(new string(event.getbody()) + suffix);\n            //提交事务\n            transaction.commit();\n        } catch (channelexception e) {\n            e.printstacktrace();\n            transaction.rollback(); //事务回滚\n            return status.backoff; //获取数据失败\n\n        } finally {\n            transaction.close(); //关闭资源\n        }\n\n        return status.ready;\n    }\n\n\n    /**\n     * 获取上下文 读取配置文件中的内容\n     *\n     * @param context\n     */\n    @override\n    public void configure(context context) {\n        suffix = context.getstring("suffix", "test");\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n\n\n打包上传到flume的lib中\n\n配置文件\n\na1.sources = r1  \na1.sinks = k1\na1.channels = c1 \n\na1.sources.r1.type = netcat \na1.sources.r1.bind = localhost \na1.sources.r1.port = 44444 \n\n#自定义sink\na1.sinks.k1.type = comm.atguigu.demo.mysink\n#给数据设置后缀\na1.sinks.k1.suffix = atguigu\n\na1.channels.c1.type = memory \na1.channels.c1.capacity = 1000 \na1.channels.c1.transactioncapacity = 100 \n\na1.sources.r1.channels = c1 \na1.sinks.k1.channel = c1 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# flume 数据流监控\n\n\n# ganglia的安装与部署\n\nsudo yum -y install httpd php # 安装 httpd服务 与 php\nsudo yum -y install rrdtool perl-rrdtool rrdtool-devel # 依赖\nsudo yum -y install apr-devel #依赖\n\n#安装ganglia\nsudo yum install epel-release\nsudo yum -y install ganglia-gmetad\nsudo yum -y install ganglia-web\nsudo yum install -y ganglia-gmond\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nganglia由gmond、gmetad和gweb三部分组成。\n\ngmond（ganglia monitoring daemon）是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用gmond，你可以很容易收集很多系统指标数据，如cpu、内存、磁盘、网络和活跃进程的数据等。\n\ngmetad（ganglia meta daemon）整合所有信息，并将其以rrd格式存储至磁盘的服务。\n\ngweb（ganglia web）ganglia可视化工具，gweb是一种利用浏览器显示gmetad所存储数据的php前端。在web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。\n\n\n# 修改配置\n\nsudo vim /etc/httpd/conf.d/ganglia.conf\n\n\n1\n\n\n# ganglia monitoring system php web frontend\nalias /ganglia /usr/share/ganglia\n<location /ganglia>\n  require all granted\n  #deny from all\n  # allow from 127.0.0.1\n  # allow from ::1\n  # allow from .example.com\n</location>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nsudo vim /etc/ganglia/gmetad.conf\n\n\n1\n\n\n#更改此字段的ip地址\ndata_source "my_cluster" 192.168.130.102 \n\n\n1\n2\n\n\nsudo vim /etc/ganglia/gmond.conf\n\n\n1\n\n\ncluster {\n  name = "my_cluster"\n  owner = "unspecified"\n  latlong = "unspecified"\n  url = "unspecified"\n}\nudp_send_channel {\n  #bind_hostname = yes # highly recommended, soon to be default.\n                       # this option tells gmond to use a source address\n                       # that resolves to the machine\'s hostname.  without\n                       # this, the metrics may appear to come from any\n                       # interface and the dns names associated with\n                       # those ips will be used to create the rrds.\n  # mcast_join = 239.2.11.71\n  host = 192.168.130.102\n  port = 8649\n  ttl = 1\n}\nudp_recv_channel {\n  # mcast_join = 239.2.11.71\n  port = 8649\n  bind = 192.168.130.102\n  retry_bind = true\n  # size of the udp buffer. if you are handling lots of metrics you really\n  # should bump it up to e.g. 10mb or even higher.\n  # buffer = 10485760\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nsudo vim /etc/selinux/config\n\n\n1\n\n\n# this file controls the state of selinux on the system.\n# selinux= can take one of these three values:\n#     enforcing - selinux security policy is enforced.\n#     permissive - selinux prints warnings instead of enforcing.\n#     disabled - no selinux policy is loaded.\nselinux=disabled\n# selinuxtype= can take one of these two values:\n#     targeted - targeted processes are protected,\n#     mls - multi level security protection.\nselinuxtype=targeted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nselinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之\n\nsudo setenforce 0\n\n\n1\n\n\n\n# 启动\n\nsudo service httpd start\nsudo service gmetad start\nsudo service gmond start\n\n\n1\n2\n3\n\n\n访问 http://192.168.130.102/ganglia\n\n如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：\n\nsudo chmod -r 777 /var/lib/ganglia\n\n\n1\n\n\n\n# 操作flume 测试监控\n\n进入flume下的conf目录\n\ncd /opt/module/flume/conf/\nmv flume-env.sh.template flume-env.sh #重命名\nvim flume-env.sh \n\n\n1\n2\n3\n\n\n追加以下配置 ip地址为ganglia主地址\n\njava_opts="-dflume.monitoring.type=ganglia\n-dflume.monitoring.hosts=192.168.130.102:8649\n-xms100m\n-xmx200m"\n\n\n1\n2\n3\n4\n\n\n启动flume任务\n\nflume-ng agent \\\n--conf conf/ \\\n--name a1 \\\n--conf-file datas/netcatsource_loggersink.conf \\\n-dflume.root.logger==info,console \\\n-dflume.monitoring.type=ganglia \\\n-dflume.monitoring.hosts=192.168.130.102:8649\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n发送信息\n\nnc localhost 44444\n\n\n1\n\n\n\n\n图例说明\n\n字段（图表名称）                字段含义\neventputattemptcount    source尝试写入channel的事件总数量\neventputsuccesscount    成功写入channel且提交的事件总数量\neventtakeattemptcount   sink尝试从channel拉取事件的总数量。\neventtakesuccesscount   sink成功读取的事件的总数量\nstarttime               channel启动的时间（毫秒）\nstoptime                channel停止的时间（毫秒）\nchannelsize             目前channel中事件的总数量\nchannelfillpercentage   channel占用百分比\nchannelcapacity         channel的容量',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"面试题",frontmatter:{title:"面试题",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/800f9f/",categories:["大数据","Flume"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/04.Flume/07.%E9%9D%A2%E8%AF%95%E9%A2%98.html",relativePath:"大数据/04.Flume/07.面试题.md",key:"v-598b6c9d",path:"/pages/800f9f/",headers:[{level:2,title:"你是如何实现Flume数据传输的监控的",slug:"你是如何实现flume数据传输的监控的",normalizedTitle:"你是如何实现flume数据传输的监控的",charIndex:10},{level:2,title:"Flume的Source，Sink，Channel的作用？你们Source是什么类型？",slug:"flume的source-sink-channel的作用-你们source是什么类型",normalizedTitle:"flume的source，sink，channel的作用？你们source是什么类型？",charIndex:60},{level:2,title:"Flume的Channel Selectors",slug:"flume的channel-selectors",normalizedTitle:"flume的channel selectors",charIndex:441},{level:2,title:"Flume参数调优",slug:"flume参数调优",normalizedTitle:"flume参数调优",charIndex:471},{level:2,title:"Flume采集数据会丢失吗?",slug:"flume采集数据会丢失吗",normalizedTitle:"flume采集数据会丢失吗?",charIndex:1195},{level:2,title:"Flume的事务机制",slug:"flume的事务机制",normalizedTitle:"flume的事务机制",charIndex:1476}],headersStr:"你是如何实现Flume数据传输的监控的 Flume的Source，Sink，Channel的作用？你们Source是什么类型？ Flume的Channel Selectors Flume参数调优 Flume采集数据会丢失吗? Flume的事务机制",content:"# 面试题\n\n\n# 你是如何实现Flume数据传输的监控的\n\n使用第三方框架Ganglia实时监控Flume。\n\n\n# Flume的Source，Sink，Channel的作用？你们Source是什么类型？\n\n1）作用\n\n（1）Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy\n\n（2）Channel组件对采集到的数据进行缓存，可以存放在Memory或File中。\n\n（3）Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。\n\n2）我公司采用的Source类型为：\n\n（1）监控后台日志：exec\n\n（2）监控后台产生日志的端口：netcat\n\nExec spooldir\n\n\n# Flume的Channel Selectors\n\n\n\n\n# Flume参数调优\n\n1）Source\n\n增加Source个（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。\n\nbatchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。\n\n2）Channel\n\ntype 选择memory的Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。\n\n使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。\n\nCapacity 参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。\n\n3）Sink\n\n增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。\n\nbatchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。\n\n\n# Flume采集数据会丢失吗?\n\n根据Flume的架构原理，Flume是不可能丢失数据的，其内部有完善的事务机制，Source到Channel是事务性的，Channel到Sink是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是Channel采用memoryChannel，agent宕机导致数据丢失，或者Channel存储数据已满，导致Source不再写入，未写入的数据丢失。\n\nFlume不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由Sink发出，但是没有接收到响应，Sink会再次发送数据，此时可能会导致数据的重复。\n\n\n# Flume的事务机制\n\nFlume的事务机制（类似数据库的事务机制）：Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递。",normalizedContent:"# 面试题\n\n\n# 你是如何实现flume数据传输的监控的\n\n使用第三方框架ganglia实时监控flume。\n\n\n# flume的source，sink，channel的作用？你们source是什么类型？\n\n1）作用\n\n（1）source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy\n\n（2）channel组件对采集到的数据进行缓存，可以存放在memory或file中。\n\n（3）sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、hbase、solr、自定义。\n\n2）我公司采用的source类型为：\n\n（1）监控后台日志：exec\n\n（2）监控后台产生日志的端口：netcat\n\nexec spooldir\n\n\n# flume的channel selectors\n\n\n\n\n# flume参数调优\n\n1）source\n\n增加source个（使用tair dir source时可增加filegroups个数）可以增大source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个source 以保证source有足够的能力获取到新产生的数据。\n\nbatchsize参数决定source一次批量运输到channel的event条数，适当调大这个参数可以提高source搬运event到channel时的性能。\n\n2）channel\n\ntype 选择memory的channel的性能最好，但是如果flume进程意外挂掉可能会丢失数据。type选择file时channel的容错性更好，但是性能上会比memory channel差。\n\n使用file channel时datadirs配置多个不同盘下的目录可以提高性能。\n\ncapacity 参数决定channel可容纳最大的event条数。transactioncapacity 参数决定每次source往channel里面写的最大event条数和每次sink从channel里面读的最大event条数。transactioncapacity需要大于source和sink的batchsize参数。\n\n3）sink\n\n增加sink的个数可以增加sink消费event的能力。sink也不是越多越好够用就行，过多的sink会占用系统资源，造成系统资源不必要的浪费。\n\nbatchsize参数决定sink一次批量从channel读取的event条数，适当调大这个参数可以提高sink从channel搬出event的性能。\n\n\n# flume采集数据会丢失吗?\n\n根据flume的架构原理，flume是不可能丢失数据的，其内部有完善的事务机制，source到channel是事务性的，channel到sink是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是channel采用memorychannel，agent宕机导致数据丢失，或者channel存储数据已满，导致source不再写入，未写入的数据丢失。\n\nflume不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由sink发出，但是没有接收到响应，sink会再次发送数据，此时可能会导致数据的重复。\n\n\n# flume的事务机制\n\nflume的事务机制（类似数据库的事务机制）：flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到channel且提交成功，那么soucrce就将该文件标记为完成。同理，事务以类似的方式处理从channel到sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到channel中，等待重新传递。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"架构",frontmatter:{title:"架构",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/3c04a9/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/02.%E6%9E%B6%E6%9E%84.html",relativePath:"大数据/05.Kafka/02.架构.md",key:"v-8c34a962",path:"/pages/3c04a9/",headers:[{level:2,title:"传统消息队列应用场景",slug:"传统消息队列应用场景",normalizedTitle:"传统消息队列应用场景",charIndex:9},{level:2,title:"消息队列两种模式",slug:"消息队列两种模式",normalizedTitle:"消息队列两种模式",charIndex:26},{level:2,title:"基础架构",slug:"基础架构",normalizedTitle:"基础架构",charIndex:353}],headersStr:"传统消息队列应用场景 消息队列两种模式 基础架构",content:"# 架构\n\n\n# 传统消息队列应用场景\n\n\n\n\n# 消息队列两种模式\n\n 1. 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）\n    \n    消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。\n    \n    消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。\n    \n    \n\n 2. 发布/订阅模式（一对多，消费者消费数据之后不会清除消息）\n    \n    消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。\n    \n    \n\n\n# 基础架构\n\nKafka是一个分布式的基于发布/订阅模式的**消息队列，**主要应用于大数据实时处理领域。\n\n\n\n 1. Producer ：消息生产者，就是向kafka broker发消息的客户端；\n 2. Consumer ：消息消费者，向kafka broker取消息的客户端；\n 3. Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。\n 4. Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。\n 5. Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；\n 6. Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；\n 7. Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。\n 8. leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。\n 9. follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。",normalizedContent:"# 架构\n\n\n# 传统消息队列应用场景\n\n\n\n\n# 消息队列两种模式\n\n 1. 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）\n    \n    消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。\n    \n    消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。\n    \n    \n\n 2. 发布/订阅模式（一对多，消费者消费数据之后不会清除消息）\n    \n    消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。\n    \n    \n\n\n# 基础架构\n\nkafka是一个分布式的基于发布/订阅模式的**消息队列，**主要应用于大数据实时处理领域。\n\n\n\n 1. producer ：消息生产者，就是向kafka broker发消息的客户端；\n 2. consumer ：消息消费者，向kafka broker取消息的客户端；\n 3. consumer group （cg）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。\n 4. broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。\n 5. topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；\n 6. partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；\n 7. replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。\n 8. leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。\n 9. follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Kafka 安装",frontmatter:{title:"Kafka 安装",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/6bd0d7/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/03.Kafka%20%E5%AE%89%E8%A3%85.html",relativePath:"大数据/05.Kafka/03.Kafka 安装.md",key:"v-4254887a",path:"/pages/6bd0d7/",headersStr:null,content:"# Kafka 安装\n\nKafka 不依赖于hadoop运作 只依赖zookeeper运行\n\n要先事先安装好zookeeper 并运行\n\nhttp://kafka.apache.org/downloads\n\ntar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/\ncd /opt/module/\nmv kafka_2.11-2.4.1/ kafka\ncd kafka\nmkdir logs\n\n#环境变量\nsudo vim /etc/profile.d/my_env.sh\n\n#KAFKA_HOME\nexport KAFKA_HOME=/opt/module/kafka\nexport PATH=$PATH:$KAFKA_HOME/bin\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n配置kafka\n\nvim /opt/module/kafka/config/server.properties\n\n\n1\n\n\n#broker的全局唯一编号，不能重复\nbroker.id=0\n#删除topic功能使能\ndelete.topic.enable=true\n\n#处理网络请求的线程数量\nnum.network.threads=3\n#用来处理磁盘IO的现成数量\nnum.io.threads=8\n#发送套接字的缓冲区大小\nsocket.send.buffer.bytes=102400\n#接收套接字的缓冲区大小\nsocket.receive.buffer.bytes=102400\n#请求套接字的缓冲区大小\nsocket.request.max.bytes=104857600\n\n#kafka运行日志存放的路径\nlog.dirs=/opt/module/kafka/logs\n\n#topic在当前broker上的分区个数\nnum.partitions=1\n#用来恢复和清理data下数据的线程数量\nnum.recovery.threads.per.data.dir=1\n#segment文件保留的最长时间，超时将被删除\nlog.retention.hours=168\n\n#配置连接Zookeeper集群地址\nzookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n分发环境变量和kafka\n\nsudo xsync /opt/module/kafka \nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n2\n\n\n修改每个kafka中 broker.id\n\nvim /opt/module/kafka/config/server.properties\n#102 为 2, 103为3  , 104为4\n\n\n1\n2\n\n\n启动\n\nkafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties #每台机器单独起\n\nkafka-server-stop.sh #关闭\n\n\n1\n2\n3\n\n\n群起脚本 记得先启动zookeeper\n\nsudo vim /bin/kafkalist.sh\n\nfor i in `cat /opt/module/hadoop-3.1.3/etc/hadoop/workers`\ndo\necho \"========== $i ==========\" \nssh $i 'kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties'\necho $?\ndone\n\n\nsudo chmod +x /bin/kafkalist.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n如果遇kafka闪退 可以尝试删除logs文件夹下的内容\n\n查看zookeeper中是否有kafka id已经记录\n\nzkCli.sh\nls /kafka/brokers/ids\n\n\n1\n2\n",normalizedContent:"# kafka 安装\n\nkafka 不依赖于hadoop运作 只依赖zookeeper运行\n\n要先事先安装好zookeeper 并运行\n\nhttp://kafka.apache.org/downloads\n\ntar -zxvf kafka_2.11-2.4.1.tgz -c /opt/module/\ncd /opt/module/\nmv kafka_2.11-2.4.1/ kafka\ncd kafka\nmkdir logs\n\n#环境变量\nsudo vim /etc/profile.d/my_env.sh\n\n#kafka_home\nexport kafka_home=/opt/module/kafka\nexport path=$path:$kafka_home/bin\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n配置kafka\n\nvim /opt/module/kafka/config/server.properties\n\n\n1\n\n\n#broker的全局唯一编号，不能重复\nbroker.id=0\n#删除topic功能使能\ndelete.topic.enable=true\n\n#处理网络请求的线程数量\nnum.network.threads=3\n#用来处理磁盘io的现成数量\nnum.io.threads=8\n#发送套接字的缓冲区大小\nsocket.send.buffer.bytes=102400\n#接收套接字的缓冲区大小\nsocket.receive.buffer.bytes=102400\n#请求套接字的缓冲区大小\nsocket.request.max.bytes=104857600\n\n#kafka运行日志存放的路径\nlog.dirs=/opt/module/kafka/logs\n\n#topic在当前broker上的分区个数\nnum.partitions=1\n#用来恢复和清理data下数据的线程数量\nnum.recovery.threads.per.data.dir=1\n#segment文件保留的最长时间，超时将被删除\nlog.retention.hours=168\n\n#配置连接zookeeper集群地址\nzookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n分发环境变量和kafka\n\nsudo xsync /opt/module/kafka \nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n2\n\n\n修改每个kafka中 broker.id\n\nvim /opt/module/kafka/config/server.properties\n#102 为 2, 103为3  , 104为4\n\n\n1\n2\n\n\n启动\n\nkafka-server-start.sh -daemon $kafka_home/config/server.properties #每台机器单独起\n\nkafka-server-stop.sh #关闭\n\n\n1\n2\n3\n\n\n群起脚本 记得先启动zookeeper\n\nsudo vim /bin/kafkalist.sh\n\nfor i in `cat /opt/module/hadoop-3.1.3/etc/hadoop/workers`\ndo\necho \"========== $i ==========\" \nssh $i 'kafka-server-start.sh -daemon $kafka_home/config/server.properties'\necho $?\ndone\n\n\nsudo chmod +x /bin/kafkalist.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n如果遇kafka闪退 可以尝试删除logs文件夹下的内容\n\n查看zookeeper中是否有kafka id已经记录\n\nzkcli.sh\nls /kafka/brokers/ids\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"命令操作",frontmatter:{title:"命令操作",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/ab6220/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/04.%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C.html",relativePath:"大数据/05.Kafka/04.命令操作.md",key:"v-43f57a12",path:"/pages/ab6220/",headersStr:null,content:"# 命令操作\n\n * 查看当前服务器中的所有topic\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --list\n\n\n1\n\n\n * 创建topic\n   \n   * kafka-topics.sh --zookeeper hadoop102:2181/kafka --create --replication-factor 3 --partitions 1 --topic first\n     \n     \n     1\n     \n   \n   * --topic 定义topic名\n   \n   * --replication-factor 定义副本数\n   \n   * --partitions 定义分区数\n\n * 删除topic\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --delete --topic first\n#需要server.properties中设置delete.topic.enable=true否则只是标记逻辑删除。\n\n\n1\n2\n\n * 发送消息\n\nkafka-console-producer.sh --broker-list hadoop102:9092 --topic first\n#消息会存储在/opt/module/kafka/logs/first-xxx 中\n\n\n1\n2\n\n * 消费消息\n\nkafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first # 启动后只有发送消息才会消费消息\nkafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first  #会把topic中以往所有的数据都读取出来 读取是以分区进行顺序读取的 读完一个分区再读另外一个分区\nkafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first --partition 1 #partition 读取指定分区 如不指定所有的分区都会消费\n\n\n1\n2\n3\n\n * 查看某个Topic的详情\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --describe --topic first\n\n\n1\n\n * 修改分区\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --alter --topic first --partitions 6\n\n\n1\n",normalizedContent:"# 命令操作\n\n * 查看当前服务器中的所有topic\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --list\n\n\n1\n\n\n * 创建topic\n   \n   * kafka-topics.sh --zookeeper hadoop102:2181/kafka --create --replication-factor 3 --partitions 1 --topic first\n     \n     \n     1\n     \n   \n   * --topic 定义topic名\n   \n   * --replication-factor 定义副本数\n   \n   * --partitions 定义分区数\n\n * 删除topic\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --delete --topic first\n#需要server.properties中设置delete.topic.enable=true否则只是标记逻辑删除。\n\n\n1\n2\n\n * 发送消息\n\nkafka-console-producer.sh --broker-list hadoop102:9092 --topic first\n#消息会存储在/opt/module/kafka/logs/first-xxx 中\n\n\n1\n2\n\n * 消费消息\n\nkafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first # 启动后只有发送消息才会消费消息\nkafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first  #会把topic中以往所有的数据都读取出来 读取是以分区进行顺序读取的 读完一个分区再读另外一个分区\nkafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first --partition 1 #partition 读取指定分区 如不指定所有的分区都会消费\n\n\n1\n2\n3\n\n * 查看某个topic的详情\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --describe --topic first\n\n\n1\n\n * 修改分区\n\nkafka-topics.sh --zookeeper hadoop102:2181/kafka --alter --topic first --partitions 6\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Kafka API",frontmatter:{title:"Kafka API",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/939509/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/06.Kafka%20API.html",relativePath:"大数据/05.Kafka/06.Kafka API.md",key:"v-3f9e4bb2",path:"/pages/939509/",headers:[{level:2,title:"Producer API",slug:"producer-api",normalizedTitle:"producer api",charIndex:16},{level:3,title:"异步发送API",slug:"异步发送api",normalizedTitle:"异步发送api",charIndex:212},{level:3,title:"同步发送 API",slug:"同步发送-api",normalizedTitle:"同步发送 api",charIndex:2741},{level:2,title:"Consumer API",slug:"consumer-api",normalizedTitle:"consumer api",charIndex:5099},{level:3,title:"自动提交offset",slug:"自动提交offset",normalizedTitle:"自动提交offset",charIndex:5116},{level:3,title:"手动提交offset",slug:"手动提交offset",normalizedTitle:"手动提交offset",charIndex:6728},{level:3,title:"自定义存储offset",slug:"自定义存储offset",normalizedTitle:"自定义存储offset",charIndex:9397},{level:2,title:"自定义Interceptor(拦截器)",slug:"自定义interceptor-拦截器",normalizedTitle:"自定义interceptor(拦截器)",charIndex:14263}],headersStr:"Producer API 异步发送API 同步发送 API Consumer API 自动提交offset 手动提交offset 自定义存储offset 自定义Interceptor(拦截器)",content:'# Kafka API\n\n\n# Producer API\n\nKafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。\n\n\n\n\n# 异步发送API\n\n导入依赖\n\n<dependency>\n<groupId>org.apache.kafka</groupId>\n<artifactId>kafka-clients</artifactId>\n<version>2.4.1</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\napi使用\n\npackage com.atguigu.producer;\n\nimport org.apache.kafka.clients.producer.Callback;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\n\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\n\npublic class Producer {\n    public static void main(String[] args) throws ExecutionException, InterruptedException {\n        //实例化kafka集群\n        Properties properties = new Properties();\n        properties.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //key的序列化类\n        properties.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //value的序列化类\n        properties.setProperty("acks", "all"); //ack级别\n        properties.setProperty("bootstrap.servers", "hadoop102:9092");\n        properties.setProperty("buffer.memory", "33554432");//RecordAccumulator缓冲区大小\n        properties.setProperty("retries", "1"); //重试次数\n        properties.setProperty("batch.size", "16384");//打包大小\n        properties.setProperty("linger.ms", "1");//等待时间\n        //当缓冲区大小达到16384时就向broker发送一次 如果没有达到但时间已经等待了1毫秒也会发送\n        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);\n        //用集群对象发送数据\n        for (int i = 0; i < 100; i++) {\n            Future<RecordMetadata> fist = producer.send(\n                    //封装ProducerRecord\n                    new ProducerRecord<>("first", Integer.toString(i), "Value" + i), new Callback() {\n                        //回调函数\n                        @Override\n                        public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n                            if (e == null) {\n                                System.out.println(recordMetadata);\n                            }\n                        }\n                    });\n            System.out.println("发完了" + i + "条数据");\n        }\n        //关闭资源\n        producer.close();\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n回调函数不是必须的 也可以不传递回调函数\n\n回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。\n\n注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。\n\n\n# 同步发送 API\n\n同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。\n\n由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方发即可。\n\npackage com.atguigu.producer;\n\nimport org.apache.kafka.clients.producer.*;\n\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\n\npublic class Producer {\n    public static void main(String[] args) throws ExecutionException, InterruptedException {\n        //实例化kafka集群\n        Properties properties = new Properties();\n        properties.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //key的序列化类\n        properties.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //value的序列化类\n        properties.setProperty("acks", "all"); //ack级别\n//        properties.put(ProducerConfig.ACKS_CONFIG,1); //ProducerConfig封装配置所有key\n        properties.setProperty("bootstrap.servers", "hadoop102:9092");\n        properties.setProperty("buffer.memory", "33554432");//RecordAccumulator缓冲区大小\n        properties.setProperty("retries", "1"); //重试次数\n        properties.setProperty("batch.size", "16384");//打包大小\n        properties.setProperty("linger.ms", "1");//等待时间\n        //当缓冲区大小达到16384时就向broker发送一次 如果没有达到但时间已经等待了1毫秒也会发送\n        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);\n//        producer.beginTransaction(); //获取事务对象\n        //用集群对象发送数据\n        for (int i = 0; i < 100; i++) {\n            Future<RecordMetadata> fist = producer.send(\n                    //封装ProducerRecord\n                    new ProducerRecord<>("first", Integer.toString(i), "Value" + i), new Callback() {\n                        //回调函数\n                        @Override\n                        public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n                            if (e == null) {\n                                System.out.println(recordMetadata);\n                            }\n                        }\n                    });\n            RecordMetadata recordMetadata = fist.get();  //直到返回ack后 RecordMetadata 有数据了 才发下一条数据\n            System.out.println("发完了" + i + "条数据");\n        }\n        //关闭资源\n        producer.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# Consumer API\n\n\n# 自动提交offset\n\n读取properties文件\n\nbootstrap.servers=hadoop102:9092\ngroup.id=test\nenable.auto.commit=true\nauto.commit.interval.ms=1000\nkey.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nvalue.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nauto.offset.reset=earliest\n# 默认为latest从最后一条数据后拉取 earliest从开头拉取\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nconsumer类\n\npackage com.atguigu.consumer;\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Properties;\n\npublic class Consumer {\n    public static void main(String[] args) throws IOException, InterruptedException {\n        //实例化一个Consumer对象\n        Properties properties = new Properties();\n        properties.load(Consumer.class.getClassLoader().getResourceAsStream("conusumer1.properties"));\n        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(properties);\n        //接受消息\n        consumer.subscribe(Collections.singleton("first")); //定义话题\n        while (true) {\n            ConsumerRecords<String, String> poll = consumer.poll(2000); //从话题中拉取数据 2000毫秒\n            if (poll.count() == 0){\n               Thread.sleep(100);\n            }\n            for (ConsumerRecord<String, String> record : poll) {\n                System.out.println(record);\n            }\n        }\n        //关闭Consumer\n//        consumer.close();\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 手动提交offset\n\n虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。\n\nproperties文件\n\nbootstrap.servers=hadoop102:9092\ngroup.id=test\nenable.auto.commit=flase\n#自动提交offset 默认为true 如果自动提交offset由broker来进行保存\nauto.commit.interval.ms=1000\n#多久提交一次offset\nkey.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nvalue.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nauto.offset.reset=earliest\n# 默认为latest从最后一条数据后拉取 earliest从开头拉取\nauto.commit.interval.ms=5000\n#自动提交offset的时间 默认为5000毫秒\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nconsumer类\n\npackage com.atguigu.consumer;\n\nimport org.apache.kafka.clients.consumer.*;\nimport org.apache.kafka.common.TopicPartition;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.Map;\nimport java.util.Properties;\n\npublic class Consumer {\n    public static void main(String[] args) throws IOException, InterruptedException {\n        //实例化一个Consumer对象\n        Properties properties = new Properties();\n        properties.load(Consumer.class.getClassLoader().getResourceAsStream("conusumer1.properties"));\n        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(properties);\n        //接受消息\n        consumer.subscribe(Collections.singleton("first")); //定义话题\n        while (true) {\n            ConsumerRecords<String, String> poll = consumer.poll(2000); //从话题中拉取数据 2000毫秒\n            //ConsumerRecords<String, String> poll = consumer.poll(Duration.ofMillis(2000)); //从话题中拉取数据 2000毫秒\n            if (poll.count() == 0) {\n                Thread.sleep(100);\n            }\n            for (ConsumerRecord<String, String> record : poll) {\n                System.out.println(record);\n            }\n//            consumer.commitSync(); //手动提交offset 同步提交\n            consumer.commitAsync(new OffsetCommitCallback() {\n                //回调函数\n                @Override\n                public void onComplete(Map<TopicPartition, OffsetAndMetadata> map, Exception e) {\n                    if (e !=null){\n                        System.out.println("Commit failed for " + map);\n                    }\n                }\n            }); //手动提交offset 异步提交\n        }\n        //关闭Consumer\n//        consumer.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。\n\n# 数据漏消费和重复消费分析\n\n无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。\n\n\n\n解决方案: 只有将消费和提交offset进行一个原子绑定才能解决\n\n\n# 自定义存储offset\n\npackage com.atguigu.consumer;\n\nimport org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.common.TopicPartition;\n\nimport java.io.*;\nimport java.util.*;\n\n/*\n自定义保存\n */\npublic class ConsumerManual {\n    //用于记录top 分区\n    private static Map<TopicPartition, Long> offset = new HashMap<>();\n    private static String file = "d:/offset";\n\n    public static void main(String[] args) throws IOException {\n        //实例化一个Consumer对象\n        Properties properties = new Properties();\n        properties.load(Consumer.class.getClassLoader().getResourceAsStream("conusumer1.properties"));\n        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(properties);\n        //订阅话题 拉取消息\n        consumer.subscribe(Collections.singleton("first"), new ConsumerRebalanceListener() {\n            //分区分配之前做的操作\n            @Override\n            public void onPartitionsRevoked(Collection<TopicPartition> collection) {\n                //提交旧的offset\n                commit();\n            }\n\n            //分区分配之后做的操作\n            @Override\n            public void onPartitionsAssigned(Collection<TopicPartition> collection) {\n                //获取新的offset\n                readOffset(collection);\n                for (TopicPartition partition : collection) {\n                    Long os = offset.get(partition);\n                    if (os == null) {\n                        consumer.seek(partition, 0);\n                    } else {\n                        consumer.seek(partition, os);\n                    }\n                }\n            }\n        });\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(2000);\n            //原子绑定\n            for (ConsumerRecord<String, String> record : records) {\n                //消费\n                System.out.println(record);\n                //消费完后 写入map中\n                offset.put(\n                        new TopicPartition(record.topic(), record.partition()), record.offset());\n            }\n            commit();\n        }\n    }\n\n    /**\n     * 从自定义介质读取offset到缓存\n     *\n     * @param collection\n     */\n    private static void readOffset(Collection<TopicPartition> collection) {\n        ObjectInputStream objectInputStream = null;\n        Map<TopicPartition, Long> temp;\n        try {\n            objectInputStream = new ObjectInputStream(new FileInputStream(file));\n            temp = (Map<TopicPartition, Long>) objectInputStream.readObject();\n        } catch (Exception e) {\n            temp = new HashMap<>();\n        } finally {\n            if (objectInputStream != null) {\n                try {\n                    objectInputStream.close();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n        //从全部分区offset中读取我们分配到的分区的offset\n        for (TopicPartition partition : collection) {\n            offset.put(partition, temp.get(partition));\n        }\n    }\n\n    /**\n     * 将缓存中的offset提交到自定义介质中\n     */\n    private static void commit() {\n        //先从文件中读取旧的所有的offset\n        ObjectInputStream objectInputStream = null;\n        Map<TopicPartition, Long> temp;\n        try {\n            objectInputStream = new ObjectInputStream(new FileInputStream(file));\n            temp = (Map<TopicPartition, Long>) objectInputStream.readObject();\n        } catch (Exception e) {\n            temp = new HashMap<>();\n        } finally {\n            if (objectInputStream != null) {\n                try {\n                    objectInputStream.close();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n        //合并offset\n        temp.putAll(offset);\n        //将新的offset写出去\n        ObjectOutputStream objectOutputStream = null;\n        try {\n            objectOutputStream = new ObjectOutputStream(new FileOutputStream(file));\n            objectOutputStream.writeObject(temp);\n        } catch (IOException e) {\n            e.printStackTrace();\n        } finally {\n            if (objectInputStream != null) {\n                try {\n                    objectInputStream.close();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n\n\n\n# 自定义Interceptor(拦截器)\n\n拦截器实现的接口是ProducerInterceptor\n\n# 使用拦截器统计消息发送成功和失败的数量\n\npackage com.atguigu.interceptor;\n\nimport org.apache.kafka.clients.producer.ProducerInterceptor;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\n\nimport java.util.Map;\n\n/**\n * 统计消息发送成功和失败的数量\n */\npublic class CountInterceptor implements ProducerInterceptor<String, String> {\n\n    private long success = 0;\n    private long fail = 0;\n\n    @Override\n    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> producerRecord) {\n        return producerRecord;\n    }\n\n    /**\n     * 收到ACK后做计数\n     *\n     * @param recordMetadata\n     * @param e\n     */\n    @Override\n    public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) {\n        if (e == null) {\n            success++;\n        } else {\n            fail++;\n        }\n    }\n\n    @Override\n    public void close() {\n        System.out.println("成功了" + success + "条");\n        System.out.println("失败了" + fail + "条");\n    }\n\n    @Override\n    public void configure(Map<String, ?> map) {\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# 使用拦截器 将值改为 自定义前缀 + 时间戳 + 值\n\npackage com.atguigu.interceptor;\n\nimport org.apache.kafka.clients.producer.ProducerInterceptor;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\n\nimport java.util.Map;\n\npublic class TimeInterceptor implements ProducerInterceptor<String, String> {\n\n    //前缀\n    private String prefix;\n\n    /**\n     * 自定义Record 修改时间戳可以在此方法中修改\n     *\n     * @param producerRecord 原始Record\n     * @return 修改后的Record\n     */\n    @Override\n    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> producerRecord) {\n        Long timestamp = producerRecord.timestamp();\n        //Record只能获取 不能修改 所有我们只重新创建一个Record 并把对应的值赋上去\n        return new ProducerRecord<String, String>(\n                producerRecord.topic(),\n                producerRecord.partition(),\n                producerRecord.timestamp(),\n                producerRecord.key(),\n                prefix + System.currentTimeMillis() + producerRecord.value(),\n                producerRecord.headers()\n        );\n    }\n\n    /**\n     * 收到 ACK以后调用\n     *\n     * @param recordMetadata\n     * @param e\n     */\n    @Override\n    public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) {\n\n    }\n\n    /**\n     * 关闭Producer时调用\n     */\n    @Override\n    public void close() {\n\n    }\n\n    /**\n     * 定义拦截器的方法\n     *\n     * @param map\n     */\n    @Override\n    public void configure(Map<String, ?> map) {\n        //定义前缀\n        //获取配置文件中配置值\n        prefix = (String) map.get("prefix");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n# 生产者调用自定义拦截器\n\npackage com.atguigu.interceptor;\n\nimport org.apache.kafka.clients.producer.*;\n\nimport java.util.ArrayList;\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\n\npublic class Producer {\n    public static void main(String[] args) throws ExecutionException, InterruptedException {\n        //实例化kafka集群\n        Properties properties = new Properties();\n        properties.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //key的序列化类\n        properties.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //value的序列化类\n        properties.setProperty("acks", "all"); //ack级别\n        properties.setProperty("bootstrap.servers", "hadoop102:9092");\n        properties.setProperty("buffer.memory", "33554432");//RecordAccumulator缓冲区大小\n        properties.setProperty("retries", "1"); //重试次数\n        properties.setProperty("batch.size", "16384");//打包大小\n        properties.setProperty("linger.ms", "1");//等待时间\n\n        //自定义拦截器 列表\n        ArrayList<String> interceptors = new ArrayList<>();\n        interceptors.add("com.atguigu.interceptor.TimeInterceptor"); //执行顺序为添加顺序\n        interceptors.add("com.atguigu.interceptor.CountInterceptor"); //值为类引用路径\n        properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); //添加到properties中\n        //自定义前缀\n        properties.setProperty("prefix","自定义前缀测试");\n\n        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(properties);\n        //用集群对象发送数据\n        for (int i = 0; i < 10; i++) {\n            Future<RecordMetadata> fist = producer.send(\n                    //封装ProducerRecord\n                    new ProducerRecord<>("first", Integer.toString(i), "Value" + i), new Callback() {\n                        //回调函数\n                        @Override\n                        public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n                            if (e == null) {\n                                System.out.println(recordMetadata);\n                            }\n                        }\n                    });\n            RecordMetadata recordMetadata = fist.get();  //直到返回ack后 RecordMetadata 有数据了 才发下一条数据\n            System.out.println("发完了" + i + "条数据");\n        }\n        //关闭资源\n        producer.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n',normalizedContent:'# kafka api\n\n\n# producer api\n\nkafka的producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和sender线程，以及一个线程共享变量——recordaccumulator。main线程将消息发送给recordaccumulator，sender线程不断从recordaccumulator中拉取消息发送到kafka broker。\n\n\n\n\n# 异步发送api\n\n导入依赖\n\n<dependency>\n<groupid>org.apache.kafka</groupid>\n<artifactid>kafka-clients</artifactid>\n<version>2.4.1</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\napi使用\n\npackage com.atguigu.producer;\n\nimport org.apache.kafka.clients.producer.callback;\nimport org.apache.kafka.clients.producer.kafkaproducer;\nimport org.apache.kafka.clients.producer.producerrecord;\nimport org.apache.kafka.clients.producer.recordmetadata;\n\nimport java.util.properties;\nimport java.util.concurrent.executionexception;\nimport java.util.concurrent.future;\n\npublic class producer {\n    public static void main(string[] args) throws executionexception, interruptedexception {\n        //实例化kafka集群\n        properties properties = new properties();\n        properties.setproperty("key.serializer", "org.apache.kafka.common.serialization.stringserializer"); //key的序列化类\n        properties.setproperty("value.serializer", "org.apache.kafka.common.serialization.stringserializer"); //value的序列化类\n        properties.setproperty("acks", "all"); //ack级别\n        properties.setproperty("bootstrap.servers", "hadoop102:9092");\n        properties.setproperty("buffer.memory", "33554432");//recordaccumulator缓冲区大小\n        properties.setproperty("retries", "1"); //重试次数\n        properties.setproperty("batch.size", "16384");//打包大小\n        properties.setproperty("linger.ms", "1");//等待时间\n        //当缓冲区大小达到16384时就向broker发送一次 如果没有达到但时间已经等待了1毫秒也会发送\n        kafkaproducer<string, string> producer = new kafkaproducer<string, string>(properties);\n        //用集群对象发送数据\n        for (int i = 0; i < 100; i++) {\n            future<recordmetadata> fist = producer.send(\n                    //封装producerrecord\n                    new producerrecord<>("first", integer.tostring(i), "value" + i), new callback() {\n                        //回调函数\n                        @override\n                        public void oncompletion(recordmetadata recordmetadata, exception e) {\n                            if (e == null) {\n                                system.out.println(recordmetadata);\n                            }\n                        }\n                    });\n            system.out.println("发完了" + i + "条数据");\n        }\n        //关闭资源\n        producer.close();\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n回调函数不是必须的 也可以不传递回调函数\n\n回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是recordmetadata和exception，如果exception为null，说明消息发送成功，如果exception不为null，说明消息发送失败。\n\n注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。\n\n\n# 同步发送 api\n\n同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。\n\n由于send方法返回的是一个future对象，根据futrue对象的特点，我们也可以实现同步发送的效果，只需在调用future对象的get方发即可。\n\npackage com.atguigu.producer;\n\nimport org.apache.kafka.clients.producer.*;\n\nimport java.util.properties;\nimport java.util.concurrent.executionexception;\nimport java.util.concurrent.future;\n\npublic class producer {\n    public static void main(string[] args) throws executionexception, interruptedexception {\n        //实例化kafka集群\n        properties properties = new properties();\n        properties.setproperty("key.serializer", "org.apache.kafka.common.serialization.stringserializer"); //key的序列化类\n        properties.setproperty("value.serializer", "org.apache.kafka.common.serialization.stringserializer"); //value的序列化类\n        properties.setproperty("acks", "all"); //ack级别\n//        properties.put(producerconfig.acks_config,1); //producerconfig封装配置所有key\n        properties.setproperty("bootstrap.servers", "hadoop102:9092");\n        properties.setproperty("buffer.memory", "33554432");//recordaccumulator缓冲区大小\n        properties.setproperty("retries", "1"); //重试次数\n        properties.setproperty("batch.size", "16384");//打包大小\n        properties.setproperty("linger.ms", "1");//等待时间\n        //当缓冲区大小达到16384时就向broker发送一次 如果没有达到但时间已经等待了1毫秒也会发送\n        kafkaproducer<string, string> producer = new kafkaproducer<string, string>(properties);\n//        producer.begintransaction(); //获取事务对象\n        //用集群对象发送数据\n        for (int i = 0; i < 100; i++) {\n            future<recordmetadata> fist = producer.send(\n                    //封装producerrecord\n                    new producerrecord<>("first", integer.tostring(i), "value" + i), new callback() {\n                        //回调函数\n                        @override\n                        public void oncompletion(recordmetadata recordmetadata, exception e) {\n                            if (e == null) {\n                                system.out.println(recordmetadata);\n                            }\n                        }\n                    });\n            recordmetadata recordmetadata = fist.get();  //直到返回ack后 recordmetadata 有数据了 才发下一条数据\n            system.out.println("发完了" + i + "条数据");\n        }\n        //关闭资源\n        producer.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\n\n# consumer api\n\n\n# 自动提交offset\n\n读取properties文件\n\nbootstrap.servers=hadoop102:9092\ngroup.id=test\nenable.auto.commit=true\nauto.commit.interval.ms=1000\nkey.deserializer=org.apache.kafka.common.serialization.stringdeserializer\nvalue.deserializer=org.apache.kafka.common.serialization.stringdeserializer\nauto.offset.reset=earliest\n# 默认为latest从最后一条数据后拉取 earliest从开头拉取\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nconsumer类\n\npackage com.atguigu.consumer;\n\nimport org.apache.kafka.clients.consumer.consumerrecord;\nimport org.apache.kafka.clients.consumer.consumerrecords;\nimport org.apache.kafka.clients.consumer.kafkaconsumer;\n\nimport java.io.ioexception;\nimport java.util.collections;\nimport java.util.properties;\n\npublic class consumer {\n    public static void main(string[] args) throws ioexception, interruptedexception {\n        //实例化一个consumer对象\n        properties properties = new properties();\n        properties.load(consumer.class.getclassloader().getresourceasstream("conusumer1.properties"));\n        kafkaconsumer<string, string> consumer = new kafkaconsumer<string, string>(properties);\n        //接受消息\n        consumer.subscribe(collections.singleton("first")); //定义话题\n        while (true) {\n            consumerrecords<string, string> poll = consumer.poll(2000); //从话题中拉取数据 2000毫秒\n            if (poll.count() == 0){\n               thread.sleep(100);\n            }\n            for (consumerrecord<string, string> record : poll) {\n                system.out.println(record);\n            }\n        }\n        //关闭consumer\n//        consumer.close();\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 手动提交offset\n\n虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此kafka还提供了手动提交offset的api。\n\nproperties文件\n\nbootstrap.servers=hadoop102:9092\ngroup.id=test\nenable.auto.commit=flase\n#自动提交offset 默认为true 如果自动提交offset由broker来进行保存\nauto.commit.interval.ms=1000\n#多久提交一次offset\nkey.deserializer=org.apache.kafka.common.serialization.stringdeserializer\nvalue.deserializer=org.apache.kafka.common.serialization.stringdeserializer\nauto.offset.reset=earliest\n# 默认为latest从最后一条数据后拉取 earliest从开头拉取\nauto.commit.interval.ms=5000\n#自动提交offset的时间 默认为5000毫秒\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nconsumer类\n\npackage com.atguigu.consumer;\n\nimport org.apache.kafka.clients.consumer.*;\nimport org.apache.kafka.common.topicpartition;\n\nimport java.io.ioexception;\nimport java.util.collections;\nimport java.util.map;\nimport java.util.properties;\n\npublic class consumer {\n    public static void main(string[] args) throws ioexception, interruptedexception {\n        //实例化一个consumer对象\n        properties properties = new properties();\n        properties.load(consumer.class.getclassloader().getresourceasstream("conusumer1.properties"));\n        kafkaconsumer<string, string> consumer = new kafkaconsumer<string, string>(properties);\n        //接受消息\n        consumer.subscribe(collections.singleton("first")); //定义话题\n        while (true) {\n            consumerrecords<string, string> poll = consumer.poll(2000); //从话题中拉取数据 2000毫秒\n            //consumerrecords<string, string> poll = consumer.poll(duration.ofmillis(2000)); //从话题中拉取数据 2000毫秒\n            if (poll.count() == 0) {\n                thread.sleep(100);\n            }\n            for (consumerrecord<string, string> record : poll) {\n                system.out.println(record);\n            }\n//            consumer.commitsync(); //手动提交offset 同步提交\n            consumer.commitasync(new offsetcommitcallback() {\n                //回调函数\n                @override\n                public void oncomplete(map<topicpartition, offsetandmetadata> map, exception e) {\n                    if (e !=null){\n                        system.out.println("commit failed for " + map);\n                    }\n                }\n            }); //手动提交offset 异步提交\n        }\n        //关闭consumer\n//        consumer.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n手动提交offset的方法有两种：分别是commitsync（同步提交）和commitasync（异步提交）。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitsync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitasync则没有失败重试机制，故有可能提交失败。\n\n# 数据漏消费和重复消费分析\n\n无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。\n\n\n\n解决方案: 只有将消费和提交offset进行一个原子绑定才能解决\n\n\n# 自定义存储offset\n\npackage com.atguigu.consumer;\n\nimport org.apache.kafka.clients.consumer.consumerrebalancelistener;\nimport org.apache.kafka.clients.consumer.consumerrecord;\nimport org.apache.kafka.clients.consumer.consumerrecords;\nimport org.apache.kafka.clients.consumer.kafkaconsumer;\nimport org.apache.kafka.common.topicpartition;\n\nimport java.io.*;\nimport java.util.*;\n\n/*\n自定义保存\n */\npublic class consumermanual {\n    //用于记录top 分区\n    private static map<topicpartition, long> offset = new hashmap<>();\n    private static string file = "d:/offset";\n\n    public static void main(string[] args) throws ioexception {\n        //实例化一个consumer对象\n        properties properties = new properties();\n        properties.load(consumer.class.getclassloader().getresourceasstream("conusumer1.properties"));\n        kafkaconsumer<string, string> consumer = new kafkaconsumer<string, string>(properties);\n        //订阅话题 拉取消息\n        consumer.subscribe(collections.singleton("first"), new consumerrebalancelistener() {\n            //分区分配之前做的操作\n            @override\n            public void onpartitionsrevoked(collection<topicpartition> collection) {\n                //提交旧的offset\n                commit();\n            }\n\n            //分区分配之后做的操作\n            @override\n            public void onpartitionsassigned(collection<topicpartition> collection) {\n                //获取新的offset\n                readoffset(collection);\n                for (topicpartition partition : collection) {\n                    long os = offset.get(partition);\n                    if (os == null) {\n                        consumer.seek(partition, 0);\n                    } else {\n                        consumer.seek(partition, os);\n                    }\n                }\n            }\n        });\n        while (true) {\n            consumerrecords<string, string> records = consumer.poll(2000);\n            //原子绑定\n            for (consumerrecord<string, string> record : records) {\n                //消费\n                system.out.println(record);\n                //消费完后 写入map中\n                offset.put(\n                        new topicpartition(record.topic(), record.partition()), record.offset());\n            }\n            commit();\n        }\n    }\n\n    /**\n     * 从自定义介质读取offset到缓存\n     *\n     * @param collection\n     */\n    private static void readoffset(collection<topicpartition> collection) {\n        objectinputstream objectinputstream = null;\n        map<topicpartition, long> temp;\n        try {\n            objectinputstream = new objectinputstream(new fileinputstream(file));\n            temp = (map<topicpartition, long>) objectinputstream.readobject();\n        } catch (exception e) {\n            temp = new hashmap<>();\n        } finally {\n            if (objectinputstream != null) {\n                try {\n                    objectinputstream.close();\n                } catch (ioexception e) {\n                    e.printstacktrace();\n                }\n            }\n        }\n        //从全部分区offset中读取我们分配到的分区的offset\n        for (topicpartition partition : collection) {\n            offset.put(partition, temp.get(partition));\n        }\n    }\n\n    /**\n     * 将缓存中的offset提交到自定义介质中\n     */\n    private static void commit() {\n        //先从文件中读取旧的所有的offset\n        objectinputstream objectinputstream = null;\n        map<topicpartition, long> temp;\n        try {\n            objectinputstream = new objectinputstream(new fileinputstream(file));\n            temp = (map<topicpartition, long>) objectinputstream.readobject();\n        } catch (exception e) {\n            temp = new hashmap<>();\n        } finally {\n            if (objectinputstream != null) {\n                try {\n                    objectinputstream.close();\n                } catch (ioexception e) {\n                    e.printstacktrace();\n                }\n            }\n        }\n        //合并offset\n        temp.putall(offset);\n        //将新的offset写出去\n        objectoutputstream objectoutputstream = null;\n        try {\n            objectoutputstream = new objectoutputstream(new fileoutputstream(file));\n            objectoutputstream.writeobject(temp);\n        } catch (ioexception e) {\n            e.printstacktrace();\n        } finally {\n            if (objectinputstream != null) {\n                try {\n                    objectinputstream.close();\n                } catch (ioexception e) {\n                    e.printstacktrace();\n                }\n            }\n        }\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n\n\n\n# 自定义interceptor(拦截器)\n\n拦截器实现的接口是producerinterceptor\n\n# 使用拦截器统计消息发送成功和失败的数量\n\npackage com.atguigu.interceptor;\n\nimport org.apache.kafka.clients.producer.producerinterceptor;\nimport org.apache.kafka.clients.producer.producerrecord;\nimport org.apache.kafka.clients.producer.recordmetadata;\n\nimport java.util.map;\n\n/**\n * 统计消息发送成功和失败的数量\n */\npublic class countinterceptor implements producerinterceptor<string, string> {\n\n    private long success = 0;\n    private long fail = 0;\n\n    @override\n    public producerrecord<string, string> onsend(producerrecord<string, string> producerrecord) {\n        return producerrecord;\n    }\n\n    /**\n     * 收到ack后做计数\n     *\n     * @param recordmetadata\n     * @param e\n     */\n    @override\n    public void onacknowledgement(recordmetadata recordmetadata, exception e) {\n        if (e == null) {\n            success++;\n        } else {\n            fail++;\n        }\n    }\n\n    @override\n    public void close() {\n        system.out.println("成功了" + success + "条");\n        system.out.println("失败了" + fail + "条");\n    }\n\n    @override\n    public void configure(map<string, ?> map) {\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# 使用拦截器 将值改为 自定义前缀 + 时间戳 + 值\n\npackage com.atguigu.interceptor;\n\nimport org.apache.kafka.clients.producer.producerinterceptor;\nimport org.apache.kafka.clients.producer.producerrecord;\nimport org.apache.kafka.clients.producer.recordmetadata;\n\nimport java.util.map;\n\npublic class timeinterceptor implements producerinterceptor<string, string> {\n\n    //前缀\n    private string prefix;\n\n    /**\n     * 自定义record 修改时间戳可以在此方法中修改\n     *\n     * @param producerrecord 原始record\n     * @return 修改后的record\n     */\n    @override\n    public producerrecord<string, string> onsend(producerrecord<string, string> producerrecord) {\n        long timestamp = producerrecord.timestamp();\n        //record只能获取 不能修改 所有我们只重新创建一个record 并把对应的值赋上去\n        return new producerrecord<string, string>(\n                producerrecord.topic(),\n                producerrecord.partition(),\n                producerrecord.timestamp(),\n                producerrecord.key(),\n                prefix + system.currenttimemillis() + producerrecord.value(),\n                producerrecord.headers()\n        );\n    }\n\n    /**\n     * 收到 ack以后调用\n     *\n     * @param recordmetadata\n     * @param e\n     */\n    @override\n    public void onacknowledgement(recordmetadata recordmetadata, exception e) {\n\n    }\n\n    /**\n     * 关闭producer时调用\n     */\n    @override\n    public void close() {\n\n    }\n\n    /**\n     * 定义拦截器的方法\n     *\n     * @param map\n     */\n    @override\n    public void configure(map<string, ?> map) {\n        //定义前缀\n        //获取配置文件中配置值\n        prefix = (string) map.get("prefix");\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n# 生产者调用自定义拦截器\n\npackage com.atguigu.interceptor;\n\nimport org.apache.kafka.clients.producer.*;\n\nimport java.util.arraylist;\nimport java.util.properties;\nimport java.util.concurrent.executionexception;\nimport java.util.concurrent.future;\n\npublic class producer {\n    public static void main(string[] args) throws executionexception, interruptedexception {\n        //实例化kafka集群\n        properties properties = new properties();\n        properties.setproperty("key.serializer", "org.apache.kafka.common.serialization.stringserializer"); //key的序列化类\n        properties.setproperty("value.serializer", "org.apache.kafka.common.serialization.stringserializer"); //value的序列化类\n        properties.setproperty("acks", "all"); //ack级别\n        properties.setproperty("bootstrap.servers", "hadoop102:9092");\n        properties.setproperty("buffer.memory", "33554432");//recordaccumulator缓冲区大小\n        properties.setproperty("retries", "1"); //重试次数\n        properties.setproperty("batch.size", "16384");//打包大小\n        properties.setproperty("linger.ms", "1");//等待时间\n\n        //自定义拦截器 列表\n        arraylist<string> interceptors = new arraylist<>();\n        interceptors.add("com.atguigu.interceptor.timeinterceptor"); //执行顺序为添加顺序\n        interceptors.add("com.atguigu.interceptor.countinterceptor"); //值为类引用路径\n        properties.put(producerconfig.interceptor_classes_config, interceptors); //添加到properties中\n        //自定义前缀\n        properties.setproperty("prefix","自定义前缀测试");\n\n        kafkaproducer<string, string> producer = new kafkaproducer<string, string>(properties);\n        //用集群对象发送数据\n        for (int i = 0; i < 10; i++) {\n            future<recordmetadata> fist = producer.send(\n                    //封装producerrecord\n                    new producerrecord<>("first", integer.tostring(i), "value" + i), new callback() {\n                        //回调函数\n                        @override\n                        public void oncompletion(recordmetadata recordmetadata, exception e) {\n                            if (e == null) {\n                                system.out.println(recordmetadata);\n                            }\n                        }\n                    });\n            recordmetadata recordmetadata = fist.get();  //直到返回ack后 recordmetadata 有数据了 才发下一条数据\n            system.out.println("发完了" + i + "条数据");\n        }\n        //关闭资源\n        producer.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Kafka原理",frontmatter:{title:"Kafka原理",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/6c830a/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/05.Kafka%E5%8E%9F%E7%90%86.html",relativePath:"大数据/05.Kafka/05.Kafka原理.md",key:"v-3cd13c66",path:"/pages/6c830a/",headers:[{level:2,title:"Kafka 工作流程和文件存储机制",slug:"kafka-工作流程和文件存储机制",normalizedTitle:"kafka 工作流程和文件存储机制",charIndex:14},{level:2,title:"Kafka 生产者",slug:"kafka-生产者",normalizedTitle:"kafka 生产者",charIndex:521},{level:3,title:"分区策略",slug:"分区策略",normalizedTitle:"分区策略",charIndex:535},{level:3,title:"数据可靠性保证",slug:"数据可靠性保证",normalizedTitle:"数据可靠性保证",charIndex:964},{level:3,title:"Exactly Once语义 (幂等性)",slug:"exactly-once语义-幂等性",normalizedTitle:"exactly once语义 (幂等性)",charIndex:2440},{level:2,title:"Kafka 消费者",slug:"kafka-消费者",normalizedTitle:"kafka 消费者",charIndex:3275},{level:3,title:"消费方式",slug:"消费方式",normalizedTitle:"消费方式",charIndex:3289},{level:3,title:"分区分配策略",slug:"分区分配策略",normalizedTitle:"分区分配策略",charIndex:3611},{level:3,title:"offset的维护",slug:"offset的维护",normalizedTitle:"offset的维护",charIndex:3776},{level:2,title:"Kafka高效读写数据",slug:"kafka高效读写数据",normalizedTitle:"kafka高效读写数据",charIndex:4014},{level:3,title:"顺序写磁盘",slug:"顺序写磁盘",normalizedTitle:"顺序写磁盘",charIndex:4030},{level:3,title:"应用Pagecache",slug:"应用pagecache",normalizedTitle:"应用pagecache",charIndex:4129},{level:3,title:"零拷贝",slug:"零拷贝",normalizedTitle:"零拷贝",charIndex:4571},{level:2,title:"Zookeeper在Kafka的作用",slug:"zookeeper在kafka的作用",normalizedTitle:"zookeeper在kafka的作用",charIndex:5211},{level:2,title:"Kafka事务",slug:"kafka事务",normalizedTitle:"kafka事务",charIndex:5348},{level:3,title:"Producer事务",slug:"producer事务",normalizedTitle:"producer事务",charIndex:5443},{level:3,title:"Consumer事务（精准一次性消费）",slug:"consumer事务-精准一次性消费",normalizedTitle:"consumer事务（精准一次性消费）",charIndex:5792}],headersStr:"Kafka 工作流程和文件存储机制 Kafka 生产者 分区策略 数据可靠性保证 Exactly Once语义 (幂等性) Kafka 消费者 消费方式 分区分配策略 offset的维护 Kafka高效读写数据 顺序写磁盘 应用Pagecache 零拷贝 Zookeeper在Kafka的作用 Kafka事务 Producer事务 Consumer事务（精准一次性消费）",content:"# Kafka原理\n\n\n# Kafka 工作流程和文件存储机制\n\n\n\nKafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。\n\ntopic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。\n\n\n\n配置文件中 1个星期清除 之前的被切割的文件 1GB切割为1个文件\n\n由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。\n\n\n\n\n# Kafka 生产者\n\n\n# 分区策略\n\n 1. 分区的原因\n    1. 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；\n    2. 可以提高并发，因为可以以Partition为单位读写了\n 2. 分区的原则\n    1. 指明 partition 的情况下，直接将指明的值直接作为 partiton 值；\n    2. 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；\n    3. 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。\n\n\n# 数据可靠性保证\n\n为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。\n\n\n\n\n\n方案                优点                               缺点\n半数以上完成同步，就发送ack   延迟低                              选举新的leader时，容忍n台节点的故障，需要2n+1个副本\n全部完成同步，才发送ack     选举新的leader时，容忍n台节点的故障，需要n+1个副本   延迟高\n\nKafka选择了第二种方案 虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小\n\n同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余\n\n# ISR\n\nLeader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。\n\n# 故障处理细节\n\n\n\n 1. follower故障 follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。\n 2. leader故障 leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。\n\n注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。\n\n# ack应答机制\n\n对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。\n\n所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。\n\n-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。\n\n\n\n0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；\n\n1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；\n\n\n\n\n# Exactly Once语义 (幂等性)\n\n将服务器的ACK级别设置为**-1**，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。\n\nAt Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。\n\n在0.11版本以前的Kafka，对于数据重复是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。\n\n0.11版本的Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是**指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。**幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：\n\nAt Least Once + 幂等性 = Exactly Once\n\n要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对**<PID, Partition, SeqNumber>做缓存，当具有相同主键的消息提交时，Broker只会持久化一条**。\n\n但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。\n\n\n# Kafka 消费者\n\n\n# 消费方式\n\nconsumer采用pull（拉）模式从broker中读取数据。\n\npush（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。\n\n而pull模式则可以根据consumer的消费能力以适当的速率消费消息。\n\npull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。\n\n针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。\n\n\n# 分区分配策略\n\n一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。\n\nKafka有两种分配策略\n\n 1. roundrobin 轮询策略\n 2. range 随机分配\n\n\n# offset的维护\n\n由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。\n\nKafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为**__consumer_offsets**。\n\n\n# Kafka高效读写数据\n\n\n# 顺序写磁盘\n\nKafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。\n\n\n# 应用Pagecache\n\n磁盘高速缓存（PageCache） Kafka数据持久化是直接持久化到Pagecache中\n\n * I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能\n * I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间\n * 充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担\n * 读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据\n * 如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用\n\n尽管持久化到Pagecache上可能会造成宕机丢失数据的情况，但这可以被Kafka的Replication机制解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。\n\n\n# 零拷贝\n\n\n\n从Page Cache直接 写到NIC中 不经过应用层\n\n\n\n零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。\n\n零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。\n\n所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。\n\n事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。\n\n如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 transferTo 方法：\n\n@Overridepublic \nlong transferFrom(FileChannel fileChannel, long position, long count) throws IOException { \n    return fileChannel.transferTo(position, count, socketChannel);\n}\n\n\n1\n2\n3\n4\n\n\n当然，要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本。\n\n\n# Zookeeper在Kafka的作用\n\nKafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。\n\nController的管理工作都是依赖于Zookeeper的。\n\n\n\n\n# Kafka事务\n\nKafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。\n\n\n# Producer事务\n\n为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。\n\n为了管理Transaction，Kafka引入了一个新的组件Transaction Coordinator。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。\n\n\n# Consumer事务（精准一次性消费）\n\n上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。\n\n如果想完成Consumer端的精准一次性消费，那么需要kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将kafka的offset保存到支持事务的自定义介质中（比如mysql）。",normalizedContent:"# kafka原理\n\n\n# kafka 工作流程和文件存储机制\n\n\n\nkafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。\n\ntopic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。\n\n\n\n配置文件中 1个星期清除 之前的被切割的文件 1gb切割为1个文件\n\n由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。\n\n\n\n\n# kafka 生产者\n\n\n# 分区策略\n\n 1. 分区的原因\n    1. 方便在集群中扩展，每个partition可以通过调整以适应它所在的机器，而一个topic又可以有多个partition组成，因此整个集群就可以适应任意大小的数据了；\n    2. 可以提高并发，因为可以以partition为单位读写了\n 2. 分区的原则\n    1. 指明 partition 的情况下，直接将指明的值直接作为 partiton 值；\n    2. 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；\n    3. 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。\n\n\n# 数据可靠性保证\n\n为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。\n\n\n\n\n\n方案                优点                               缺点\n半数以上完成同步，就发送ack   延迟低                              选举新的leader时，容忍n台节点的故障，需要2n+1个副本\n全部完成同步，才发送ack     选举新的leader时，容忍n台节点的故障，需要n+1个副本   延迟高\n\nkafka选择了第二种方案 虽然第二种方案的网络延迟会比较高，但网络延迟对kafka的影响较小\n\n同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余\n\n# isr\n\nleader维护了一个动态的in-sync replica set (isr)，意为和leader保持同步的follower集合。当isr中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出isr，该时间阈值由replica.lag.time.max.ms参数设定。leader发生故障之后，就会从isr中选举新的leader。\n\n# 故障处理细节\n\n\n\n 1. follower故障 follower发生故障后会被临时踢出isr，待该follower恢复后，follower会读取本地磁盘记录的上次的hw，并将log文件高于hw的部分截取掉，从hw开始向leader进行同步。等该follower的leo大于等于该partition的hw，即follower追上leader之后，就可以重新加入isr了。\n 2. leader故障 leader发生故障之后，会从isr中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于hw的部分截掉，然后从新的leader同步数据。\n\n注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。\n\n# ack应答机制\n\n对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等isr中的follower全部接收成功。\n\n所以kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。\n\n-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。\n\n\n\n0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；\n\n1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；\n\n\n\n\n# exactly once语义 (幂等性)\n\n将服务器的ack级别设置为**-1**，可以保证producer到server之间不会丢失数据，即at least once语义。相对的，将服务器ack级别设置为0，可以保证生产者每条消息只会被发送一次，即at most once语义。\n\nat least once可以保证数据不丢失，但是不能保证数据不重复；相对的，at least once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即exactly once语义。\n\n在0.11版本以前的kafka，对于数据重复是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。\n\n0.11版本的kafka，引入了一项重大特性：幂等性。所谓的幂等性就是**指producer不论向server发送多少次重复数据，server端都只会持久化一条。**幂等性结合at least once语义，就构成了kafka的exactly once语义。即：\n\nat least once + 幂等性 = exactly once\n\n要启用幂等性，只需要将producer的参数中enable.idompotence设置为true即可。kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的producer在初始化的时候会被分配一个pid，发往同一partition的消息会附带sequence number。而broker端会对**<pid, partition, seqnumber>做缓存，当具有相同主键的消息提交时，broker只会持久化一条**。\n\n但是pid重启就会变化，同时不同的partition也具有不同主键，所以幂等性无法保证跨分区跨会话的exactly once。\n\n\n# kafka 消费者\n\n\n# 消费方式\n\nconsumer采用pull（拉）模式从broker中读取数据。\n\npush（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。\n\n而pull模式则可以根据consumer的消费能力以适当的速率消费消息。\n\npull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。\n\n针对这一点，kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。\n\n\n# 分区分配策略\n\n一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。\n\nkafka有两种分配策略\n\n 1. roundrobin 轮询策略\n 2. range 随机分配\n\n\n# offset的维护\n\n由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。\n\nkafka 0.9版本之前，consumer默认将offset保存在zookeeper中，从0.9版本开始，consumer默认将offset保存在kafka一个内置的topic中，该topic为**__consumer_offsets**。\n\n\n# kafka高效读写数据\n\n\n# 顺序写磁盘\n\nkafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。\n\n\n# 应用pagecache\n\n磁盘高速缓存（pagecache） kafka数据持久化是直接持久化到pagecache中\n\n * i/o scheduler 会将连续的小块写组装成大块的物理写从而提高性能\n * i/o scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间\n * 充分利用所有空闲内存（非 jvm 内存）。如果使用应用层 cache（即 jvm 堆内存），会增加 gc 负担\n * 读操作可直接在 page cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 page cache）交换数据\n * 如果进程重启，jvm 内的 cache 会失效，但 page cache 仍然可用\n\n尽管持久化到pagecache上可能会造成宕机丢失数据的情况，但这可以被kafka的replication机制解决。如果为了保证这种情况下数据不丢失而强制将 page cache 中的数据 flush 到磁盘，反而会降低性能。\n\n\n# 零拷贝\n\n\n\n从page cache直接 写到nic中 不经过应用层\n\n\n\n零拷贝（zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 cpu 来搬运数据，所有的数据都是通过 dma 来进行传输的。\n\n零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 cpu，2 次都是由 dma 来搬运。\n\n所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。\n\n事实上，kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 i/o 的吞吐率，这也是 kafka 在处理海量数据为什么这么快的原因之一。\n\n如果你追溯 kafka 文件传输的代码，你会发现，最终它调用了 java nio 库里的 transferto 方法：\n\n@overridepublic \nlong transferfrom(filechannel filechannel, long position, long count) throws ioexception { \n    return filechannel.transferto(position, count, socketchannel);\n}\n\n\n1\n2\n3\n4\n\n\n当然，要使用 sendfile，linux 内核版本必须要 2.1 以上的版本。\n\n\n# zookeeper在kafka的作用\n\nkafka集群中有一个broker会被选举为controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。\n\ncontroller的管理工作都是依赖于zookeeper的。\n\n\n\n\n# kafka事务\n\nkafka从0.11版本开始引入了事务支持。事务可以保证kafka在exactly once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。\n\n\n# producer事务\n\n为了实现跨分区跨会话的事务，需要引入一个全局唯一的transaction id，并将producer获得的pid和transaction id绑定。这样当producer重启后就可以通过正在进行的transaction id获得原来的pid。\n\n为了管理transaction，kafka引入了一个新的组件transaction coordinator。producer就是通过和transaction coordinator交互获得transaction id对应的任务状态。transaction coordinator还负责将事务所有写入kafka的一个内部topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。\n\n\n# consumer事务（精准一次性消费）\n\n上述事务机制主要是从producer方面考虑，对于consumer而言，事务的保证就会相对较弱，尤其时无法保证commit的信息被精确消费。这是由于consumer可以通过offset访问任意信息，而且不同的segment file生命周期不同，同一事务的消息可能会出现重启后被删除的情况。\n\n如果想完成consumer端的精准一次性消费，那么需要kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将kafka的offset保存到支持事务的自定义介质中（比如mysql）。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Flume 对接 Kafka",frontmatter:{title:"Flume 对接 Kafka",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/cf0e70/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/07.Flume%20%E5%AF%B9%E6%8E%A5%20Kafka.html",relativePath:"大数据/05.Kafka/07.Flume 对接 Kafka.md",key:"v-1e6e69af",path:"/pages/cf0e70/",headersStr:null,content:"# Flume 对接 Kafka\n\nFlume输出到Kafka中\n\nFlume配置文件\n\na2.sources = r2\na2.sinks = k2\na2.channels = c2\n\na2.sources.r2.type = exec\na2.sources.r2.command = tail -F /opt/module/flume/datas/123.log\na2.sources.r2.shell = /bin/bash -C\n\n#kafka Sink\na2.sinks.k2.type = org.apache.flume.source.kafka.KafkaSource\n#发送topic\na2.sinks.k2.kafka.topic = first\n#kafka地址\na2.sinks.k2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092\na2.sinks.k2.kafka.flumeBatchSize = 20\n#ack模式\na2.sinks.k2.kafka.producer.acks = all\na2.sinks.k2.kafka.producer.linger.ms = 1\n\n\n\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactionCapacity = 100\n\n\na2.sources.r2.channels = c2\na2.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n启动flume\n\nflume-ng agent -n a2 -c conf/ -f datas/flume-taildir-hdfs.conf -Dflume.root.logger=INFO,console\n\n\n1\n\n\n同时flume给我们提供了 kafka source 可以从kafka指定话题中读取数据到channel中",normalizedContent:"# flume 对接 kafka\n\nflume输出到kafka中\n\nflume配置文件\n\na2.sources = r2\na2.sinks = k2\na2.channels = c2\n\na2.sources.r2.type = exec\na2.sources.r2.command = tail -f /opt/module/flume/datas/123.log\na2.sources.r2.shell = /bin/bash -c\n\n#kafka sink\na2.sinks.k2.type = org.apache.flume.source.kafka.kafkasource\n#发送topic\na2.sinks.k2.kafka.topic = first\n#kafka地址\na2.sinks.k2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092\na2.sinks.k2.kafka.flumebatchsize = 20\n#ack模式\na2.sinks.k2.kafka.producer.acks = all\na2.sinks.k2.kafka.producer.linger.ms = 1\n\n\n\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactioncapacity = 100\n\n\na2.sources.r2.channels = c2\na2.sinks.k2.channel = c2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n启动flume\n\nflume-ng agent -n a2 -c conf/ -f datas/flume-taildir-hdfs.conf -dflume.root.logger=info,console\n\n\n1\n\n\n同时flume给我们提供了 kafka source 可以从kafka指定话题中读取数据到channel中",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Kafka监控",frontmatter:{title:"Kafka监控",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/f5b30c/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/08.Kafka%E7%9B%91%E6%8E%A7.html",relativePath:"大数据/05.Kafka/08.Kafka监控.md",key:"v-6a373828",path:"/pages/f5b30c/",headers:[{level:2,title:"Kafka Eagle",slug:"kafka-eagle",normalizedTitle:"kafka eagle",charIndex:14},{level:2,title:"Kafka Monitor",slug:"kafka-monitor",normalizedTitle:"kafka monitor",charIndex:1895},{level:2,title:"Kafka Manager(CMAK)",slug:"kafka-manager-cmak",normalizedTitle:"kafka manager(cmak)",charIndex:2683}],headersStr:"Kafka Eagle Kafka Monitor Kafka Manager(CMAK)",content:'# Kafka监控\n\n\n# Kafka Eagle\n\n先关闭所有kafka\n\n修改kafka启动命令\n\ncd /opt/module/kafka/bin\nvim kafka-server-start.sh\n\n\n1\n2\n\n\n修改以下内容\n\nif [ "x$KAFKA_HEAP_OPTS" = "x" ]; then\n    export KAFKA_HEAP_OPTS="-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"\n    export JMX_PORT="9999"\n    #export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"\nfi\n\n\n1\n2\n3\n4\n5\n\n\nxsync /opt/module/kafka/bin/kafka-server-start.sh\n\n\n1\n\n\n再启动kafka\n\n安装eagle\n\ntar -zxvf kafka-eagle-bin-1.3.7.tar.gz\ncd kafka-eagle-bin-1.3.7\ntar -zxvf kafka-eagle-web-1.3.7-bin.tar.gz -C /opt/module/\ncd /opt/module/\nmv kafka-eagle-web-1.3.7/ eagle\ncd eagle/bin/\nchmod +x ke.sh\nvim ke.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n修改 ke.sh\n\n######################################\n# multi zookeeper&kafka cluster list\n######################################\nkafka.eagle.zk.cluster.alias=cluster1\ncluster1.zk.list=hadoop102:2181,hadoop103:2181,hadoop104:2181\n\n######################################\n# kafka offset storage\n######################################\ncluster1.kafka.eagle.offset.storage=kafka\n\n######################################\n# enable kafka metrics\n######################################\nkafka.eagle.metrics.charts=true\nkafka.eagle.sql.fix.error=false\n\n######################################\n# kafka jdbc driver address\n######################################\nkafka.eagle.driver=com.mysql.jdbc.Driver\nkafka.eagle.url=jdbc:mysql://hadoop102:3306/ke?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull\nkafka.eagle.username=root\nkafka.eagle.password=000000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n添加环境变量\n\nvim /etc/profile.d/my_env.sh\n\n\n1\n\n\nexport KE_HOME=/opt/module/eagle\nexport PATH=$PATH:$KE_HOME/bin\n\n\n1\n2\n\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n\n\n启动\n\nke.sh start\n\n\n1\n\n\n访问http://192.168.130.102:8048/ke 用户admin 密码123456\n\n\n# Kafka Monitor\n\n目前已经停止更新了 不推荐使用\n\nhttps://github.com/Morningstar/kafka-offset-monitor/releases\n\nmkdir -p /opt/module/kafka-offset-console\ncd /opt/module/kafka-offset-console\n\n\n1\n2\n\n\n将KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar上传到集群中的 /opt/module/kafka-offset-console\n\n创建脚本文件 start.sh\n\nvim start.sh\n\n\n1\n\n\n#!/bin/bash\nnohup java -cp KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar \\\ncom.quantifind.kafka.offsetapp.OffsetGetterWeb \\\n--offsetStorage kafka \\\n--kafkaBrokers hadoop102:9092,hadoop103:9092,hadoop104:9092 \\\n--kafkaSecurityProtocol PLAINTEXT \\\n--zk hadoop102:9092,hadoop103:9092,hadoop104:9092 \\\n--port 8086 \\\n--refresh 10.seconds \\\n--retain 2.days \\\n--dbName offsetapp_kafka >/dev/null 2>&1 &\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nchmod +x start.sh\n\n\n1\n\n\n访问http://hadoop102:8086/查看web\n\n\n# Kafka Manager(CMAK)\n\nhttps://github.com/yahoo/CMAK\n\nCMAK v3.0.0.5需要JDK11以上，如果你想用JDK1.8去支持此版本是行不通的，会报错提示找不到有些class文件\n\nyum -y install java-11-openjdk-devel.x86_64\n\n\n1\n\n\n修改配置文件\n\nsuod vim /etc/profile\n\n\n1\n\n\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.12.0.7-0.el7_9.x86_64\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\n\n\n1\n2\n3\n\n\nsource /etc/profile\njava -version\n\n\n1\n2\n\n\n上传解压\n\nmkdir -p /opt/module/kafkamanager\ncd /opt/module/kafkamanager\n\n\n1\n2\n\n\n修改配置\n\nvim conf/application.conf \n\n\n1\n\n\nkafka-manager.zkhosts="hadoop102:2181,hadoop103:2181,hadoop104:2181"\nkafka-manager.zkhosts=${?ZK_HOSTS}\ncmak.zkhosts="hadoop102:2181,hadoop103:2181,hadoop104:2181"\ncmak.zkhosts=${?ZK_HOSTS}\n\n\n1\n2\n3\n4\n\n\n启动\n\nchmod +x bin/cmak\nbin/cmak\n\n\n1\n2\n\n\n访问http://hadoop102:9000/',normalizedContent:'# kafka监控\n\n\n# kafka eagle\n\n先关闭所有kafka\n\n修改kafka启动命令\n\ncd /opt/module/kafka/bin\nvim kafka-server-start.sh\n\n\n1\n2\n\n\n修改以下内容\n\nif [ "x$kafka_heap_opts" = "x" ]; then\n    export kafka_heap_opts="-server -xms2g -xmx2g -xx:permsize=128m -xx:+useg1gc -xx:maxgcpausemillis=200 -xx:parallelgcthreads=8 -xx:concgcthreads=5 -xx:initiatingheapoccupancypercent=70"\n    export jmx_port="9999"\n    #export kafka_heap_opts="-xmx1g -xms1g"\nfi\n\n\n1\n2\n3\n4\n5\n\n\nxsync /opt/module/kafka/bin/kafka-server-start.sh\n\n\n1\n\n\n再启动kafka\n\n安装eagle\n\ntar -zxvf kafka-eagle-bin-1.3.7.tar.gz\ncd kafka-eagle-bin-1.3.7\ntar -zxvf kafka-eagle-web-1.3.7-bin.tar.gz -c /opt/module/\ncd /opt/module/\nmv kafka-eagle-web-1.3.7/ eagle\ncd eagle/bin/\nchmod +x ke.sh\nvim ke.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n修改 ke.sh\n\n######################################\n# multi zookeeper&kafka cluster list\n######################################\nkafka.eagle.zk.cluster.alias=cluster1\ncluster1.zk.list=hadoop102:2181,hadoop103:2181,hadoop104:2181\n\n######################################\n# kafka offset storage\n######################################\ncluster1.kafka.eagle.offset.storage=kafka\n\n######################################\n# enable kafka metrics\n######################################\nkafka.eagle.metrics.charts=true\nkafka.eagle.sql.fix.error=false\n\n######################################\n# kafka jdbc driver address\n######################################\nkafka.eagle.driver=com.mysql.jdbc.driver\nkafka.eagle.url=jdbc:mysql://hadoop102:3306/ke?useunicode=true&characterencoding=utf-8&zerodatetimebehavior=converttonull\nkafka.eagle.username=root\nkafka.eagle.password=000000\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n添加环境变量\n\nvim /etc/profile.d/my_env.sh\n\n\n1\n\n\nexport ke_home=/opt/module/eagle\nexport path=$path:$ke_home/bin\n\n\n1\n2\n\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n\n\n启动\n\nke.sh start\n\n\n1\n\n\n访问http://192.168.130.102:8048/ke 用户admin 密码123456\n\n\n# kafka monitor\n\n目前已经停止更新了 不推荐使用\n\nhttps://github.com/morningstar/kafka-offset-monitor/releases\n\nmkdir -p /opt/module/kafka-offset-console\ncd /opt/module/kafka-offset-console\n\n\n1\n2\n\n\n将kafkaoffsetmonitor-assembly-0.4.6-snapshot.jar上传到集群中的 /opt/module/kafka-offset-console\n\n创建脚本文件 start.sh\n\nvim start.sh\n\n\n1\n\n\n#!/bin/bash\nnohup java -cp kafkaoffsetmonitor-assembly-0.4.6-snapshot.jar \\\ncom.quantifind.kafka.offsetapp.offsetgetterweb \\\n--offsetstorage kafka \\\n--kafkabrokers hadoop102:9092,hadoop103:9092,hadoop104:9092 \\\n--kafkasecurityprotocol plaintext \\\n--zk hadoop102:9092,hadoop103:9092,hadoop104:9092 \\\n--port 8086 \\\n--refresh 10.seconds \\\n--retain 2.days \\\n--dbname offsetapp_kafka >/dev/null 2>&1 &\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nchmod +x start.sh\n\n\n1\n\n\n访问http://hadoop102:8086/查看web\n\n\n# kafka manager(cmak)\n\nhttps://github.com/yahoo/cmak\n\ncmak v3.0.0.5需要jdk11以上，如果你想用jdk1.8去支持此版本是行不通的，会报错提示找不到有些class文件\n\nyum -y install java-11-openjdk-devel.x86_64\n\n\n1\n\n\n修改配置文件\n\nsuod vim /etc/profile\n\n\n1\n\n\nexport java_home=/usr/lib/jvm/java-11-openjdk-11.0.12.0.7-0.el7_9.x86_64\nexport path=$java_home/bin:$path\nexport classpath=.:$java_home/lib/dt.jar:$java_home/lib/tools.jar\n\n\n1\n2\n3\n\n\nsource /etc/profile\njava -version\n\n\n1\n2\n\n\n上传解压\n\nmkdir -p /opt/module/kafkamanager\ncd /opt/module/kafkamanager\n\n\n1\n2\n\n\n修改配置\n\nvim conf/application.conf \n\n\n1\n\n\nkafka-manager.zkhosts="hadoop102:2181,hadoop103:2181,hadoop104:2181"\nkafka-manager.zkhosts=${?zk_hosts}\ncmak.zkhosts="hadoop102:2181,hadoop103:2181,hadoop104:2181"\ncmak.zkhosts=${?zk_hosts}\n\n\n1\n2\n3\n4\n\n\n启动\n\nchmod +x bin/cmak\nbin/cmak\n\n\n1\n2\n\n\n访问http://hadoop102:9000/',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"任务调度",frontmatter:{title:"任务调度",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/c7cfc3/",categories:["大数据","Azkaban"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/06.Azkaban/02.%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6.html",relativePath:"大数据/06.Azkaban/02.任务调度.md",key:"v-64817218",path:"/pages/c7cfc3/",headers:[{level:2,title:"常见工作流调度系统",slug:"常见工作流调度系统",normalizedTitle:"常见工作流调度系统",charIndex:149}],headersStr:"常见工作流调度系统",content:"# 任务调度\n\n 1. 一个完整的数据分析系统通常都是由大量任务单元组成： Shell 脚本程序，Java 程序，MapReduce 程序、Hive 脚本等\n 2. 各任务单元之间存在时间先后及前后依赖关系\n 3. 为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行\n\n\n\n\n# 常见工作流调度系统\n\n 1. 简单的任务调度：直接使用 Linux 的 Crontab 来定义；\n 2. 复杂的任务调度：开发调度平台或使用现成的开源调度系统，比如 Ooize、Azkaban、 Airflow、DolphinScheduler 等。",normalizedContent:"# 任务调度\n\n 1. 一个完整的数据分析系统通常都是由大量任务单元组成： shell 脚本程序，java 程序，mapreduce 程序、hive 脚本等\n 2. 各任务单元之间存在时间先后及前后依赖关系\n 3. 为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行\n\n\n\n\n# 常见工作流调度系统\n\n 1. 简单的任务调度：直接使用 linux 的 crontab 来定义；\n 2. 复杂的任务调度：开发调度平台或使用现成的开源调度系统，比如 ooize、azkaban、 airflow、dolphinscheduler 等。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"安装",frontmatter:{title:"安装",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/394c5d/",categories:["大数据","Azkaban"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/06.Azkaban/03.%E5%AE%89%E8%A3%85.html",relativePath:"大数据/06.Azkaban/03.安装.md",key:"v-55a5d0e4",path:"/pages/394c5d/",headers:[{level:2,title:"导入数据库",slug:"导入数据库",normalizedTitle:"导入数据库",charIndex:367},{level:2,title:"创建SSL配置",slug:"创建ssl配置",normalizedTitle:"创建ssl配置",charIndex:520},{level:2,title:"配置文件修改",slug:"配置文件修改",normalizedTitle:"配置文件修改",charIndex:681}],headersStr:"导入数据库 创建SSL配置 配置文件修改",content:'# 安装\n\nmkdir -p /opt/module/azkaban\ncd /opt/software\ntar -zxvf azkaban-web-server-2.5.0.tar.gz -C /opt/module/azkaban/\ntar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /opt/module/azkaban/\ntar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /opt/module/azkaban/\ncd /opt/module/azkaban/\nmv azkaban-web-2.5.0/ server\nmv azkaban-executor-2.5.0/ executor\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 导入数据库\n\nmysql -uroot -pA373213257s\ncreate database azkaban;\nuse azkaban;\nsource /opt/module/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql\n\n\n1\n2\n3\n4\n\n\n\n# 创建SSL配置\n\ncd server/\nkeytool -keystore keystore -alias jetty -genkey -keyalg RSA\n#密码设置为123456 其他全部回车 只有否那里输入y确认\n\n\n1\n2\n3\n\n\ndate 确保linux为东八区(CST-8)时间 并集群之间时间同步\n\n\n# 配置文件修改\n\ncd /opt/module/azkaban/server/conf/\nvim azkaban.properties\n\n\n1\n2\n\n\n修改地区 mysql地址 账号密码 和ssl密码\n\n#Azkaban Personalization Settings\nazkaban.name=Test\n#服务器UI名称,用于服务器上方显示的名字\nazkaban.label=My Local Azkaban\n#描述\nazkaban.color=#FF3601\n#UI颜色\nazkaban.default.servlet.path=/index\nweb.resource.dir=web/\n#默认根web目录\ndefault.timezone.id=Asia/Shanghai\n#默认时区,已改为亚洲/上海 默认为美国\n\n#Azkaban UserManager class\nuser.manager.class=azkaban.user.XmlUserManager\n#用户权限管理默认类\nuser.manager.xml.file=conf/azkaban-users.xml\n#用户配置,具体配置参加下文\n\n#Loader for projects\nexecutor.global.properties=conf/global.properties\n# global配置文件所在位置\nazkaban.project.dir=projects\n\ndatabase.type=mysql\n#数据库类型\nmysql.port=3306\n#端口号\nmysql.host=hadoop102\n#数据库连接IP\nmysql.database=azkaban\n#数据库实例名\nmysql.user=root\n#数据库用户名\nmysql.password=A373213257s\n#数据库密码\nmysql.numconnections=100\n#最大连接数\n\n# Velocity dev mode\nvelocity.dev.mode=false\n# Jetty服务器属性.\njetty.maxThreads=25\n#最大线程数\njetty.ssl.port=8443\n#Jetty SSL端口\njetty.port=8081\n#Jetty端口\njetty.keystore=keystore\n#SSL文件名\njetty.password=123456\n#SSL文件密码\njetty.keypassword=123456\n#Jetty主密码 与 keystore文件相同\njetty.truststore=keystore\n#SSL文件名\njetty.trustpassword=123456\n# SSL文件密码\n\n# 执行服务器属性\nexecutor.port=12321\n#执行服务器端口\n\n# 邮件设置\nmail.sender=xxxxxxxx@163.com\n#发送邮箱\nmail.host=smtp.163.com\n#发送邮箱smtp地址\nmail.user=xxxxxxxx\n#发送邮件时显示的名称\nmail.password=**********\n#邮箱密码\njob.failure.email=xxxxxxxx@163.com\n#任务失败时发送邮件的地址\njob.success.email=xxxxxxxx@163.com\n#任务成功时发送邮件的地址\nlockdown.create.projects=false\ncache.directory=cache\n#缓存目录\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n\n\n修改 azkaban-users.xml 添加管理员用户\n\nvim azkaban-users.xml\n\n\n1\n\n\n<azkaban-users>\n        <user username="azkaban" password="azkaban" roles="admin" groups="azkaban" />\n        <user username="metrics" password="metrics" roles="metrics"/>\n        \x3c!-- 添加以下这行 --\x3e\n        <user username="admin" password="admin" roles="admin,metrics" />\n        <role name="admin" permissions="ADMIN" />\n        <role name="metrics" permissions="METRICS"/>\n</azkaban-users>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n修改 执行服务器配置\n\ncd /opt/module/azkaban/executor/conf\nvim azkaban.properties\n\n\n1\n2\n\n\n修改时区 mysql 地址 账号密码\n\n#Azkaban\ndefault.timezone.id=Asia/Shanghai                                              \n# 时区\n\n# Azkaban JobTypes 插件配置\nazkaban.jobtype.plugin.dir=plugins/jobtypes                   \n# jobtype 插件所在位置\n\n#Loader for projects\nexecutor.global.properties=conf/global.properties\nazkaban.project.dir=projects\n\n#数据库设置\ndatabase.type=mysql                                                                       \n# 数据库类型(目前只支持mysql)\nmysql.port=3306                                                                                \n# 数据库端口号\nmysql.host=hadoop102                                                           \n# 数据库IP地址\nmysql.database=azkaban                                                                \n# 数据库实例名\nmysql.user=root                                                                         \n# 数据库用户名\nmysql.password=A373213257s                                                                   \n# 数据库密码\nmysql.numconnections=100 \n# 最大连接数\n\n# 执行服务器配置\nexecutor.maxThreads=50 \n# 最大线程数\nexecutor.port=12321\n# 端口号(如修改,请与web服务中一致)\nexecutor.flow.threads=30 \n# 线程数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n启动 执行服务器实战\n\ncd /opt/module/azkaban/executor\n/bin/azkaban-executor-start.sh\ncd /opt/module/azkaban/server/\nbin/azkaban-web-start.sh\n\n\n1\n2\n3\n4\n\n\n访问 https://hadoop102:8443/ 不要用http连接 账号密码admin',normalizedContent:'# 安装\n\nmkdir -p /opt/module/azkaban\ncd /opt/software\ntar -zxvf azkaban-web-server-2.5.0.tar.gz -c /opt/module/azkaban/\ntar -zxvf azkaban-executor-server-2.5.0.tar.gz -c /opt/module/azkaban/\ntar -zxvf azkaban-sql-script-2.5.0.tar.gz -c /opt/module/azkaban/\ncd /opt/module/azkaban/\nmv azkaban-web-2.5.0/ server\nmv azkaban-executor-2.5.0/ executor\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 导入数据库\n\nmysql -uroot -pa373213257s\ncreate database azkaban;\nuse azkaban;\nsource /opt/module/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql\n\n\n1\n2\n3\n4\n\n\n\n# 创建ssl配置\n\ncd server/\nkeytool -keystore keystore -alias jetty -genkey -keyalg rsa\n#密码设置为123456 其他全部回车 只有否那里输入y确认\n\n\n1\n2\n3\n\n\ndate 确保linux为东八区(cst-8)时间 并集群之间时间同步\n\n\n# 配置文件修改\n\ncd /opt/module/azkaban/server/conf/\nvim azkaban.properties\n\n\n1\n2\n\n\n修改地区 mysql地址 账号密码 和ssl密码\n\n#azkaban personalization settings\nazkaban.name=test\n#服务器ui名称,用于服务器上方显示的名字\nazkaban.label=my local azkaban\n#描述\nazkaban.color=#ff3601\n#ui颜色\nazkaban.default.servlet.path=/index\nweb.resource.dir=web/\n#默认根web目录\ndefault.timezone.id=asia/shanghai\n#默认时区,已改为亚洲/上海 默认为美国\n\n#azkaban usermanager class\nuser.manager.class=azkaban.user.xmlusermanager\n#用户权限管理默认类\nuser.manager.xml.file=conf/azkaban-users.xml\n#用户配置,具体配置参加下文\n\n#loader for projects\nexecutor.global.properties=conf/global.properties\n# global配置文件所在位置\nazkaban.project.dir=projects\n\ndatabase.type=mysql\n#数据库类型\nmysql.port=3306\n#端口号\nmysql.host=hadoop102\n#数据库连接ip\nmysql.database=azkaban\n#数据库实例名\nmysql.user=root\n#数据库用户名\nmysql.password=a373213257s\n#数据库密码\nmysql.numconnections=100\n#最大连接数\n\n# velocity dev mode\nvelocity.dev.mode=false\n# jetty服务器属性.\njetty.maxthreads=25\n#最大线程数\njetty.ssl.port=8443\n#jetty ssl端口\njetty.port=8081\n#jetty端口\njetty.keystore=keystore\n#ssl文件名\njetty.password=123456\n#ssl文件密码\njetty.keypassword=123456\n#jetty主密码 与 keystore文件相同\njetty.truststore=keystore\n#ssl文件名\njetty.trustpassword=123456\n# ssl文件密码\n\n# 执行服务器属性\nexecutor.port=12321\n#执行服务器端口\n\n# 邮件设置\nmail.sender=xxxxxxxx@163.com\n#发送邮箱\nmail.host=smtp.163.com\n#发送邮箱smtp地址\nmail.user=xxxxxxxx\n#发送邮件时显示的名称\nmail.password=**********\n#邮箱密码\njob.failure.email=xxxxxxxx@163.com\n#任务失败时发送邮件的地址\njob.success.email=xxxxxxxx@163.com\n#任务成功时发送邮件的地址\nlockdown.create.projects=false\ncache.directory=cache\n#缓存目录\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n\n\n修改 azkaban-users.xml 添加管理员用户\n\nvim azkaban-users.xml\n\n\n1\n\n\n<azkaban-users>\n        <user username="azkaban" password="azkaban" roles="admin" groups="azkaban" />\n        <user username="metrics" password="metrics" roles="metrics"/>\n        \x3c!-- 添加以下这行 --\x3e\n        <user username="admin" password="admin" roles="admin,metrics" />\n        <role name="admin" permissions="admin" />\n        <role name="metrics" permissions="metrics"/>\n</azkaban-users>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n修改 执行服务器配置\n\ncd /opt/module/azkaban/executor/conf\nvim azkaban.properties\n\n\n1\n2\n\n\n修改时区 mysql 地址 账号密码\n\n#azkaban\ndefault.timezone.id=asia/shanghai                                              \n# 时区\n\n# azkaban jobtypes 插件配置\nazkaban.jobtype.plugin.dir=plugins/jobtypes                   \n# jobtype 插件所在位置\n\n#loader for projects\nexecutor.global.properties=conf/global.properties\nazkaban.project.dir=projects\n\n#数据库设置\ndatabase.type=mysql                                                                       \n# 数据库类型(目前只支持mysql)\nmysql.port=3306                                                                                \n# 数据库端口号\nmysql.host=hadoop102                                                           \n# 数据库ip地址\nmysql.database=azkaban                                                                \n# 数据库实例名\nmysql.user=root                                                                         \n# 数据库用户名\nmysql.password=a373213257s                                                                   \n# 数据库密码\nmysql.numconnections=100 \n# 最大连接数\n\n# 执行服务器配置\nexecutor.maxthreads=50 \n# 最大线程数\nexecutor.port=12321\n# 端口号(如修改,请与web服务中一致)\nexecutor.flow.threads=30 \n# 线程数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n启动 执行服务器实战\n\ncd /opt/module/azkaban/executor\n/bin/azkaban-executor-start.sh\ncd /opt/module/azkaban/server/\nbin/azkaban-web-start.sh\n\n\n1\n2\n3\n4\n\n\n访问 https://hadoop102:8443/ 不要用http连接 账号密码admin',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Azkaban",frontmatter:{title:"Azkaban",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/bac882/",categories:["大数据","Azkaban"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/06.Azkaban/01.Azkaban.html",relativePath:"大数据/06.Azkaban/01.Azkaban.md",key:"v-266c9e22",path:"/pages/bac882/",headersStr:null,content:"# Azkaban\n\nOoize 相比于 Azkaban 是一个重量级的任务调度系统，功能全面，但配置使用 也更复杂。如果可以不在意某些功能的缺失，轻量级调度器 Azkaban 是很不错的候选对象。",normalizedContent:"# azkaban\n\nooize 相比于 azkaban 是一个重量级的任务调度系统，功能全面，但配置使用 也更复杂。如果可以不在意某些功能的缺失，轻量级调度器 azkaban 是很不错的候选对象。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hbase",frontmatter:{title:"Hbase",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/310c39/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/01.Hbase.html",relativePath:"大数据/07.Hbase/01.Hbase.md",key:"v-3c82fd62",path:"/pages/310c39/",headersStr:null,content:"# Hbase\n\nHase是 一种分布式 可扩展 支持海量数据存储的 NoSQL 数据库",normalizedContent:"# hbase\n\nhase是 一种分布式 可扩展 支持海量数据存储的 nosql 数据库",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Kafka面试题",frontmatter:{title:"Kafka面试题",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/cf96cc/",categories:["大数据","Kafka"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/05.Kafka/09.Kafka%E9%9D%A2%E8%AF%95%E9%A2%98.html",relativePath:"大数据/05.Kafka/09.Kafka面试题.md",key:"v-c86974ba",path:"/pages/cf96cc/",headers:[{level:2,title:"Kafka中的ISR、AR又代表什么？",slug:"kafka中的isr、ar又代表什么",normalizedTitle:"kafka中的isr、ar又代表什么？",charIndex:15},{level:2,title:"Kafka中的HW、LEO等分别代表什么？",slug:"kafka中的hw、leo等分别代表什么",normalizedTitle:"kafka中的hw、leo等分别代表什么？",charIndex:170},{level:2,title:"Kafka中是怎么体现消息顺序性的？",slug:"kafka中是怎么体现消息顺序性的",normalizedTitle:"kafka中是怎么体现消息顺序性的？",charIndex:242},{level:2,title:"Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？",slug:"kafka中的分区器、序列化器、拦截器是否了解-它们之间的处理顺序是什么",normalizedTitle:"kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？",charIndex:299},{level:2,title:"Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？",slug:"kafka生产者客户端的整体结构是什么样子的-使用了几个线程来处理-分别是什么",normalizedTitle:"kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？",charIndex:361},{level:2,title:"“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？",slug:"消费组中的消费者个数如果超过topic的分区-那么就会有消费者消费不到数据-这句话是否正确",normalizedTitle:"“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？",charIndex:408},{level:2,title:"消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？",slug:"消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1",normalizedTitle:"消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？",charIndex:464},{level:2,title:"有哪些情形会造成重复消费？",slug:"有哪些情形会造成重复消费",normalizedTitle:"有哪些情形会造成重复消费？",charIndex:521},{level:2,title:"那些情景会造成消息漏消费？",slug:"那些情景会造成消息漏消费",normalizedTitle:"那些情景会造成消息漏消费？",charIndex:541},{level:2,title:"当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？",slug:"当你使用kafka-topics-sh创建-删除-了一个topic之后-kafka背后会执行什么逻辑",normalizedTitle:"当你使用kafka-topics.sh创建（删除）了一个topic之后，kafka背后会执行什么逻辑？",charIndex:585},{level:2,title:"topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？",slug:"topic的分区数可不可以增加-如果可以怎么增加-如果不可以-那又是为什么",normalizedTitle:"topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？",charIndex:785},{level:2,title:"topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？",slug:"topic的分区数可不可以减少-如果可以怎么减少-如果不可以-那又是为什么",normalizedTitle:"topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？",charIndex:933},{level:2,title:"Kafka有内部的topic吗？如果有是什么？有什么所用？",slug:"kafka有内部的topic吗-如果有是什么-有什么所用",normalizedTitle:"kafka有内部的topic吗？如果有是什么？有什么所用？",charIndex:997},{level:2,title:"Kafka分区分配的概念？",slug:"kafka分区分配的概念",normalizedTitle:"kafka分区分配的概念？",charIndex:1063},{level:2,title:"简述Kafka的日志目录结构？",slug:"简述kafka的日志目录结构",normalizedTitle:"简述kafka的日志目录结构？",charIndex:1137},{level:2,title:"如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？",slug:"如果我指定了一个offset-kafka-controller怎么查找到对应的消息",normalizedTitle:"如果我指定了一个offset，kafka controller怎么查找到对应的消息？",charIndex:1210},{level:2,title:"聊一聊Kafka Controller的作用？",slug:"聊一聊kafka-controller的作用",normalizedTitle:"聊一聊kafka controller的作用？",charIndex:1259},{level:2,title:"Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？",slug:"kafka中有那些地方需要选举-这些地方的选举策略又有哪些",normalizedTitle:"kafka中有那些地方需要选举？这些地方的选举策略又有哪些？",charIndex:1333},{level:2,title:"失效副本是指什么？有那些应对措施？",slug:"失效副本是指什么-有那些应对措施",normalizedTitle:"失效副本是指什么？有那些应对措施？",charIndex:1408},{level:2,title:"Kafka的那些设计让它有如此高的性能？",slug:"kafka的那些设计让它有如此高的性能",normalizedTitle:"kafka的那些设计让它有如此高的性能？",charIndex:1471}],headersStr:"Kafka中的ISR、AR又代表什么？ Kafka中的HW、LEO等分别代表什么？ Kafka中是怎么体现消息顺序性的？ Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？ Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？ “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？ 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？ 有哪些情形会造成重复消费？ 那些情景会造成消息漏消费？ 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？ topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？ topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？ Kafka有内部的topic吗？如果有是什么？有什么所用？ Kafka分区分配的概念？ 简述Kafka的日志目录结构？ 如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？ 聊一聊Kafka Controller的作用？ Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？ 失效副本是指什么？有那些应对措施？ Kafka的那些设计让它有如此高的性能？",content:"# Kafka面试题\n\n\n# Kafka中的ISR、AR又代表什么？\n\nISR：与leader保持同步的follower集合\n\nAR：分区的所有副本\n\nOSR: 被暂时踢出ISR的副本 Out-of-Sync Replicas 脱离同步副本。 数据同步严重滞后的副本组成OSR（网络原因造成的等等）\n\nAR = ISR + OSR\n\n\n# Kafka中的HW、LEO等分别代表什么？\n\nLEO：每个副本的最后条消息的offset\n\nHW：一个分区中所有副本最小的offset\n\n\n# Kafka中是怎么体现消息顺序性的？\n\n每个分区内，每条消息都有一个offset，故只能保证分区内有序。\n\n\n# Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\n\n拦截器 -> 序列化器 -> 分区器\n\n\n# Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？\n\n\n\n\n# “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？\n\n正确\n\n\n# 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？\n\noffset+1\n\n\n# 有哪些情形会造成重复消费？\n\n\n\n\n# 那些情景会造成消息漏消费？\n\n先提交offset，后消费，有可能造成数据的重复\n\n\n# 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\n\n1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first\n\n2）触发Controller的监听程序\n\n3）kafka Controller 负责topic的创建工作，并更新metadata cache\n\n\n# topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\n\n可以增加\n\nkafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-config --partitions 3\n\n\n1\n\n\n\n# topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\n\n不可以减少，被删除的分区数据难以处理。\n\n\n# Kafka有内部的topic吗？如果有是什么？有什么所用？\n\n__consumer_offsets,保存消费者offset\n\n\n# Kafka分区分配的概念？\n\n一个topic多个分区，一个消费者组多个消费者，故需要将分区分配个消费者(roundrobin、range)\n\n\n# 简述Kafka的日志目录结构？\n\n每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件\n\n\n# 如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？\n\n\n\n\n# 聊一聊Kafka Controller的作用？\n\n负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。\n\n\n# Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\n\npartition leader（ISR），controller（先到先得）\n\n\n# 失效副本是指什么？有那些应对措施？\n\n不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入\n\n\n# Kafka的那些设计让它有如此高的性能？\n\n分区，顺序写磁盘，0-copy",normalizedContent:"# kafka面试题\n\n\n# kafka中的isr、ar又代表什么？\n\nisr：与leader保持同步的follower集合\n\nar：分区的所有副本\n\nosr: 被暂时踢出isr的副本 out-of-sync replicas 脱离同步副本。 数据同步严重滞后的副本组成osr（网络原因造成的等等）\n\nar = isr + osr\n\n\n# kafka中的hw、leo等分别代表什么？\n\nleo：每个副本的最后条消息的offset\n\nhw：一个分区中所有副本最小的offset\n\n\n# kafka中是怎么体现消息顺序性的？\n\n每个分区内，每条消息都有一个offset，故只能保证分区内有序。\n\n\n# kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\n\n拦截器 -> 序列化器 -> 分区器\n\n\n# kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？\n\n\n\n\n# “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？\n\n正确\n\n\n# 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？\n\noffset+1\n\n\n# 有哪些情形会造成重复消费？\n\n\n\n\n# 那些情景会造成消息漏消费？\n\n先提交offset，后消费，有可能造成数据的重复\n\n\n# 当你使用kafka-topics.sh创建（删除）了一个topic之后，kafka背后会执行什么逻辑？\n\n1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first\n\n2）触发controller的监听程序\n\n3）kafka controller 负责topic的创建工作，并更新metadata cache\n\n\n# topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\n\n可以增加\n\nkafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-config --partitions 3\n\n\n1\n\n\n\n# topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\n\n不可以减少，被删除的分区数据难以处理。\n\n\n# kafka有内部的topic吗？如果有是什么？有什么所用？\n\n__consumer_offsets,保存消费者offset\n\n\n# kafka分区分配的概念？\n\n一个topic多个分区，一个消费者组多个消费者，故需要将分区分配个消费者(roundrobin、range)\n\n\n# 简述kafka的日志目录结构？\n\n每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件\n\n\n# 如果我指定了一个offset，kafka controller怎么查找到对应的消息？\n\n\n\n\n# 聊一聊kafka controller的作用？\n\n负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。\n\n\n# kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\n\npartition leader（isr），controller（先到先得）\n\n\n# 失效副本是指什么？有那些应对措施？\n\n不能及时与leader同步，暂时踢出isr，等其追上leader之后再重新加入\n\n\n# kafka的那些设计让它有如此高的性能？\n\n分区，顺序写磁盘，0-copy",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Azkaban实战",frontmatter:{title:"Azkaban实战",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/4ff79d/",categories:["大数据","Azkaban"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/06.Azkaban/04.Azkaban%E5%AE%9E%E6%88%98.html",relativePath:"大数据/06.Azkaban/04.Azkaban实战.md",key:"v-3f1fc75e",path:"/pages/4ff79d/",headers:[{level:2,title:"Command 单一job案例",slug:"command-单一job案例",normalizedTitle:"command 单一job案例",charIndex:16},{level:2,title:"多job工作流",slug:"多job工作流",normalizedTitle:"多job工作流",charIndex:188},{level:2,title:"MapReduce",slug:"mapreduce",normalizedTitle:"mapreduce",charIndex:422},{level:2,title:"hive",slug:"hive",normalizedTitle:"hive",charIndex:589}],headersStr:"Command 单一job案例 多job工作流 MapReduce hive",content:"# Azkaban实战\n\n\n# Command 单一job案例\n\n在 windows 环境，创建job描述文件，编辑内容如下\n\n#command.job\ntype=command\ncommand=mkdir /opt/module/test_azkaban\n\n\n1\n2\n3\n\n\n将此文件打包成zip 不能带中文\n\n\n\n创建项目\n\n\n\n上传zip包\n\n\n\n执行\n\n\n\n\n\n\n# 多job工作流\n\n创建有依赖关系的多个job描述\n\n第一个job: foo.job\n\n#foo.job\ntype=command\ncommand=mkdir /opt/module/az\n\n\n1\n2\n3\n\n\n第二个job:bar.job\n\n#bar.job\ntype=command\ndependencies=foo\ncommand=touch /opt/module/az/test.txt\n\n\n1\n2\n3\n4\n\n\n打包成zip包提交到azkaban中\n\n\n\n\n# MapReduce\n\n#foo.job\ntype=command\ncommand=yarn jar hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\n\n\n1\n2\n3\n\n\n注意jar包也要打包进zip中 一般我们使用自定义的mapreduce\n\n上传执行\n\n\n# hive\n\nhive.sql\n\nuse default;\ndrop table if exists aztest;\ndrop table if exists azres;\ncreate external table aztest(id int,name string) row format delimited fields terminated by '\\t'location '/student';\nload data inpath '/aztest/hiveinput' into table aztest;\ncreate table azres as select * from aztest;\ninsert overwrite directory '/aztest/hiveoutput' select count(1) from aztest;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nhive.job\n\n#hive.job\ntype=command\ncommand=hive -f 'hive.sql'\n\n\n1\n2\n3\n\n\n打包上传并执行",normalizedContent:"# azkaban实战\n\n\n# command 单一job案例\n\n在 windows 环境，创建job描述文件，编辑内容如下\n\n#command.job\ntype=command\ncommand=mkdir /opt/module/test_azkaban\n\n\n1\n2\n3\n\n\n将此文件打包成zip 不能带中文\n\n\n\n创建项目\n\n\n\n上传zip包\n\n\n\n执行\n\n\n\n\n\n\n# 多job工作流\n\n创建有依赖关系的多个job描述\n\n第一个job: foo.job\n\n#foo.job\ntype=command\ncommand=mkdir /opt/module/az\n\n\n1\n2\n3\n\n\n第二个job:bar.job\n\n#bar.job\ntype=command\ndependencies=foo\ncommand=touch /opt/module/az/test.txt\n\n\n1\n2\n3\n4\n\n\n打包成zip包提交到azkaban中\n\n\n\n\n# mapreduce\n\n#foo.job\ntype=command\ncommand=yarn jar hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\n\n\n1\n2\n3\n\n\n注意jar包也要打包进zip中 一般我们使用自定义的mapreduce\n\n上传执行\n\n\n# hive\n\nhive.sql\n\nuse default;\ndrop table if exists aztest;\ndrop table if exists azres;\ncreate external table aztest(id int,name string) row format delimited fields terminated by '\\t'location '/student';\nload data inpath '/aztest/hiveinput' into table aztest;\ncreate table azres as select * from aztest;\ninsert overwrite directory '/aztest/hiveoutput' select count(1) from aztest;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nhive.job\n\n#hive.job\ntype=command\ncommand=hive -f 'hive.sql'\n\n\n1\n2\n3\n\n\n打包上传并执行",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hbase 安装",frontmatter:{title:"Hbase 安装",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/a8d091/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/03.Hbase%20%E5%AE%89%E8%A3%85.html",relativePath:"大数据/07.Hbase/03.Hbase 安装.md",key:"v-c73f1002",path:"/pages/a8d091/",headers:[{level:2,title:"Hbase配置文件",slug:"hbase配置文件",normalizedTitle:"hbase配置文件",charIndex:414},{level:2,title:"高可用",slug:"高可用",normalizedTitle:"高可用",charIndex:1910}],headersStr:"Hbase配置文件 高可用",content:"# Hbase 安装\n\n先保证Zookeeper集群的正常部署和Hadoop集群正常 并启动\n\nzkServer.sh start\nstart-dfs.sh\nstart-yarn.sh #yarn可以不启动\n\n\n1\n2\n3\n\n\n安装\n\ncd /opt/software\ntar zxvf hbase-2.0.5-bin.tar.gz -C /opt/module/\ncd /opt/module/\nmv hbase-2.0.5/ hbase\n\n\n1\n2\n3\n4\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n#HBASE_HOME\nexport HBASE_HOME=/opt/module/hbase\nexport PATH=$PATH:$HBASE_HOME/bin\n\n\n1\n2\n3\n\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n\n\n\n# Hbase配置文件\n\ncd /opt/module/hbase/conf/\nvim hbase-env.sh\n\n\n1\n2\n\n\n第125行 关闭HBASE自带的zookeeper 默认会使用\n\nexport HBASE_MANAGES_ZK=false\n\n\n1\n\n\n修改hbase-site.xml\n\nvim hbase-site.xml\n\n\n1\n\n\n\x3c!-- hbase数据存放在hdfs上哪个目录下 --\x3e\n<property>\n    <name>hbase.rootdir</name>\n    <value>hdfs://hadoop102:8020/hbase</value>\n  </property>\n\x3c!-- 是否使用完全分布式hbase --\x3e\n  <property>\n    <name>hbase.cluster.distributed</name>\n    <value>true</value>\n  </property>\n\x3c!--  zookeeper连接地址 --\x3e\n  <property>\n    <name>hbase.zookeeper.quorum</name>\n    <value>hadoop102,hadoop103,hadoop104</value>\n  </property>\n\x3c!--  兼容性配置 --\x3e\n  <property>\n    <name>hbase.unsafe.stream.capability.enforce</name>\n    <value>false</value>\n  </property>\n  \x3c!--  兼容性配置 --\x3e\n<property>\n<name>hbase.wal.provider</name>\n<value>filesystem</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n修改 regionservers 有几台集群机器\n\nvim regionservers \n\n\n1\n\n\nhadoop102\nhadoop103\nhadoop104\n\n\n1\n2\n3\n\n\n软连接hadoop配置文件到Hbase 也可不配置 只要环境变量配置正确即可\n\nln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xml\nln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml\n\n\n1\n2\n\n\n删除日志冲突\n\nrm /opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar\n\n\n1\n\n\n分发hbase\n\nxsync /opt/module/hbase/\nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n2\n\n\n群启hbase 如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。\n\nstart-hbase.sh\n\n\n1\n\n\n访问web页面 http://hadoop102:16010/master-status\n\n\n# 高可用\n\n关闭HBase集群\n\nstop-hbase.sh\n\n\n1\n\n\n在conf目录下创建backup-masters文件\n\ncd /opt/module/hbase/conf/\ntouch backup-masters\n\n\n1\n2\n\n\n在backup-masters文件中配置高可用HMaster节点\n\necho hadoop103 > backup-masters\n\n\n1\n\n\n同步文件\n\nxsync /opt/module/hbase/conf/backup-masters\nstart-hbase.sh\n\n\n1\n2\n\n\n查看web页面 http://hadoop102:16010/",normalizedContent:"# hbase 安装\n\n先保证zookeeper集群的正常部署和hadoop集群正常 并启动\n\nzkserver.sh start\nstart-dfs.sh\nstart-yarn.sh #yarn可以不启动\n\n\n1\n2\n3\n\n\n安装\n\ncd /opt/software\ntar zxvf hbase-2.0.5-bin.tar.gz -c /opt/module/\ncd /opt/module/\nmv hbase-2.0.5/ hbase\n\n\n1\n2\n3\n4\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n#hbase_home\nexport hbase_home=/opt/module/hbase\nexport path=$path:$hbase_home/bin\n\n\n1\n2\n3\n\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n\n\n\n# hbase配置文件\n\ncd /opt/module/hbase/conf/\nvim hbase-env.sh\n\n\n1\n2\n\n\n第125行 关闭hbase自带的zookeeper 默认会使用\n\nexport hbase_manages_zk=false\n\n\n1\n\n\n修改hbase-site.xml\n\nvim hbase-site.xml\n\n\n1\n\n\n\x3c!-- hbase数据存放在hdfs上哪个目录下 --\x3e\n<property>\n    <name>hbase.rootdir</name>\n    <value>hdfs://hadoop102:8020/hbase</value>\n  </property>\n\x3c!-- 是否使用完全分布式hbase --\x3e\n  <property>\n    <name>hbase.cluster.distributed</name>\n    <value>true</value>\n  </property>\n\x3c!--  zookeeper连接地址 --\x3e\n  <property>\n    <name>hbase.zookeeper.quorum</name>\n    <value>hadoop102,hadoop103,hadoop104</value>\n  </property>\n\x3c!--  兼容性配置 --\x3e\n  <property>\n    <name>hbase.unsafe.stream.capability.enforce</name>\n    <value>false</value>\n  </property>\n  \x3c!--  兼容性配置 --\x3e\n<property>\n<name>hbase.wal.provider</name>\n<value>filesystem</value>\n</property>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n修改 regionservers 有几台集群机器\n\nvim regionservers \n\n\n1\n\n\nhadoop102\nhadoop103\nhadoop104\n\n\n1\n2\n3\n\n\n软连接hadoop配置文件到hbase 也可不配置 只要环境变量配置正确即可\n\nln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xml\nln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml\n\n\n1\n2\n\n\n删除日志冲突\n\nrm /opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar\n\n\n1\n\n\n分发hbase\n\nxsync /opt/module/hbase/\nsudo xsync /etc/profile.d/my_env.sh\n\n\n1\n2\n\n\n群启hbase 如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出clockoutofsyncexception异常。\n\nstart-hbase.sh\n\n\n1\n\n\n访问web页面 http://hadoop102:16010/master-status\n\n\n# 高可用\n\n关闭hbase集群\n\nstop-hbase.sh\n\n\n1\n\n\n在conf目录下创建backup-masters文件\n\ncd /opt/module/hbase/conf/\ntouch backup-masters\n\n\n1\n2\n\n\n在backup-masters文件中配置高可用hmaster节点\n\necho hadoop103 > backup-masters\n\n\n1\n\n\n同步文件\n\nxsync /opt/module/hbase/conf/backup-masters\nstart-hbase.sh\n\n\n1\n2\n\n\n查看web页面 http://hadoop102:16010/",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hbase数据模型",frontmatter:{title:"Hbase数据模型",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/817c32/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/02.Hbase%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B.html",relativePath:"大数据/07.Hbase/02.Hbase数据模型.md",key:"v-d5889de6",path:"/pages/817c32/",headers:[{level:2,title:"逻辑结构",slug:"逻辑结构",normalizedTitle:"逻辑结构",charIndex:114},{level:2,title:"物理结构",slug:"物理结构",normalizedTitle:"物理结构",charIndex:125},{level:2,title:"数据模型",slug:"数据模型",normalizedTitle:"数据模型",charIndex:7},{level:2,title:"基本架构",slug:"基本架构",normalizedTitle:"基本架构",charIndex:762}],headersStr:"逻辑结构 物理结构 数据模型 基本架构",content:"# Hbase数据模型\n\n逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个multi-dimensional map。\n\n\n# 逻辑结构\n\n\n\n\n# 物理结构\n\n一个store存储模型\n\n\n\n它是一个多维度的映射模型,一个rowkey对应着多个列名 映射到一个value上 如有多相同rowkey和列映射多值 则以timestamp最新的为准\n\n\n# 数据模型\n\n 1. Name Space\n    \n    命名空间，类似于关系型数据库的DatabBase概念\n\n 2. Region\n    \n    类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，不需要声明具体的列。往HBase写入数据时，字段可以动态、按需指定\n\n 3. Row\n    \n    HBase表中的每行数据都由一个RowKey和多个Column（列）组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。\n\n 4. Column\n    \n    HBase中的每个列都由**Column Family(列族)和Column Qualifier（列限定符）**进行限定\n\n 5. Time Stamp 用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间。\n\n 6. Cell 由**{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元**。cell中的数据是没有类型的，全部是字节数组形式存贮。\n\n\n# 基本架构\n\n\n\n架构角色\n\n 1. Region Server Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下: 对于数据的操作：get, put, delete； 对于Region的操作：splitRegion、compactRegion。\n 2. Master Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下： 对于表的操作：create, delete, alter 对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。\n 3. Zookeeper HBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。\n 4. HDFS HDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。",normalizedContent:"# hbase数据模型\n\n逻辑上，hbase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从hbase的底层物理存储结构（k-v）来看，hbase更像是一个multi-dimensional map。\n\n\n# 逻辑结构\n\n\n\n\n# 物理结构\n\n一个store存储模型\n\n\n\n它是一个多维度的映射模型,一个rowkey对应着多个列名 映射到一个value上 如有多相同rowkey和列映射多值 则以timestamp最新的为准\n\n\n# 数据模型\n\n 1. name space\n    \n    命名空间，类似于关系型数据库的databbase概念\n\n 2. region\n    \n    类似于关系型数据库的表概念。不同的是，hbase定义表时只需要声明列族即可，不需要声明具体的列。往hbase写入数据时，字段可以动态、按需指定\n\n 3. row\n    \n    hbase表中的每行数据都由一个rowkey和多个column（列）组成，数据是按照rowkey的字典顺序存储的，并且查询数据时只能根据rowkey进行检索，所以rowkey的设计十分重要。\n\n 4. column\n    \n    hbase中的每个列都由**column family(列族)和column qualifier（列限定符）**进行限定\n\n 5. time stamp 用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入hbase的时间。\n\n 6. cell 由**{rowkey, column family：column qualifier, time stamp} 唯一确定的单元**。cell中的数据是没有类型的，全部是字节数组形式存贮。\n\n\n# 基本架构\n\n\n\n架构角色\n\n 1. region server region server为 region的管理者，其实现类为hregionserver，主要作用如下: 对于数据的操作：get, put, delete； 对于region的操作：splitregion、compactregion。\n 2. master master是所有region server的管理者，其实现类为hmaster，主要作用如下： 对于表的操作：create, delete, alter 对于regionserver的操作：分配regions到每个regionserver，监控每个regionserver的状态，负载均衡和故障转移。\n 3. zookeeper hbase通过zookeeper来做master的高可用、regionserver的监控、元数据的入口以及集群配置的维护等工作。\n 4. hdfs hdfs为hbase提供最终的底层数据存储服务，同时为hbase提供高可用的支持。",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hbase原理",frontmatter:{title:"Hbase原理",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/ec28f2/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/05.Hbase%E5%8E%9F%E7%90%86.html",relativePath:"大数据/07.Hbase/05.Hbase原理.md",key:"v-13acc28d",path:"/pages/ec28f2/",headers:[{level:2,title:"架构原理",slug:"架构原理",normalizedTitle:"架构原理",charIndex:14},{level:2,title:"写流程",slug:"写流程",normalizedTitle:"写流程",charIndex:389},{level:2,title:"MemStore Flush(刷新时机)",slug:"memstore-flush-刷新时机",normalizedTitle:"memstore flush(刷新时机)",charIndex:748},{level:2,title:"读流程",slug:"读流程",normalizedTitle:"读流程",charIndex:1780},{level:2,title:"StoreFile Compaction",slug:"storefile-compaction",normalizedTitle:"storefile compaction",charIndex:2268},{level:2,title:"Region Split",slug:"region-split",normalizedTitle:"region split",charIndex:2637}],headersStr:"架构原理 写流程 MemStore Flush(刷新时机) 读流程 StoreFile Compaction Region Split",content:'# Hbase原理\n\n\n# 架构原理\n\n\n\n 1. StoreFile 保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。\n 2. MemStore 写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。\n 3. WAL 由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。Hlog默认存储在HDFS上\n\n\n# 写流程\n\n\n\n 1. Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。\n 2. 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。\n 3. 与目标Region Server进行通讯；\n 4. 将数据顺序写入（追加）到WAL；\n 5. 将数据写入对应的MemStore，数据会在MemStore进行排序；\n 6. 向客户端发送ack；\n 7. 等达到MemStore的刷写时机后，将数据刷写到HFile。\n\n\n# MemStore Flush(刷新时机)\n\n\n\nMemStore刷写时机：\n\n 1. 当某个memstroe的大小达到了hbase.hregion.memstore.flush.size（默认值128M），其所在region的所有memstore都会刷写。 当memstore的大小达到了 hbase.hregion.memstore.flush.size（默认值128M）* hbase.hregion.memstore.block.multiplier（默认值4倍）时(即默认为128M*4=512MB)，会阻止继续往该memstore写数据。\n    Hbase中不推荐创建太多了列族 由于刷写整个memstore都会刷写 而每次刷写都是在hdfs中建立新的文件 可能有store很小就被刷写了 浪费系统资源\n\n 2. 当region server中memstore的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.upper.limit（默认值0.95）， region server 会把其的所有 region 按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到hbase.regionserver.global.memstore.size.lower.limit(默认为空 需要配置)以下。 当region server中memstore的总大小达到java_heapsize*hbase.regionserver.global.memstore.size（默认值0.4）时，会阻止继续往所有的memstore写数据。\n\n 3. 到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置hbase.regionserver.optionalcacheflushinterval（默认1小时）。\n\n 4. 当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log以下（该属性名已经废弃，现无需手动设置，最大值为32）。\n\n\n# 读流程\n\n\n\n读流程\n\n 1. Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。\n 2. 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。\n 3. 与目标Region Server进行通讯；\n 4. 分别在Block Cache（读缓存），MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。 先看缓存 再看MemStore 再看Store file\n 5. 将从文件中查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。\n 6. 将合并后的最终结果返回给客户端。\n\n\n# StoreFile Compaction\n\n由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。\n\n\n\nCompaction分为两种，分别是Minor Compaction和Major Compaction。\n\n 1. Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。\n 2. Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据。\n\n\n# Region Split\n\n默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。\n\n\n\nRegion Split时机：\n\n 1. 当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize(默认为10737418240 十GB大小)，该Region就会进行拆分（0.94版本之前）。\n 2. 当1个region中的某个Store下所有StoreFile的总大小超过Min(R^3 * 2 * "hbase.hregion.memstore.flush.size",hbase.hregion.max.filesize")，该Region就会进行拆分，其中R为当前Region Server中属于该Table的个数（0.94版本之后）。\n 3. Hbase 2.0 引入了新的split策略: 如果当前 RegionServer 上该表只有一个 Regin 按照 2 * "hbase.hregion.memstore.flush.size" 分裂，否则按照 hbase.hregion.max.filesize 分裂',normalizedContent:'# hbase原理\n\n\n# 架构原理\n\n\n\n 1. storefile 保存实际数据的物理文件，storefile以hfile的形式存储在hdfs上。每个store会有一个或多个storefile（hfile），数据在每个storefile中都是有序的。\n 2. memstore 写缓存，由于hfile中的数据要求是有序的，所以数据是先存储在memstore中，排好序后，等到达刷写时机才会刷写到hfile，每次刷写都会形成一个新的hfile。\n 3. wal 由于数据要经memstore排序后才能刷写到hfile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做write-ahead logfile的文件中，然后再写入memstore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。hlog默认存储在hdfs上\n\n\n# 写流程\n\n\n\n 1. client先访问zookeeper，获取hbase:meta表位于哪个region server。\n 2. 访问对应的region server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个region server中的哪个region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。\n 3. 与目标region server进行通讯；\n 4. 将数据顺序写入（追加）到wal；\n 5. 将数据写入对应的memstore，数据会在memstore进行排序；\n 6. 向客户端发送ack；\n 7. 等达到memstore的刷写时机后，将数据刷写到hfile。\n\n\n# memstore flush(刷新时机)\n\n\n\nmemstore刷写时机：\n\n 1. 当某个memstroe的大小达到了hbase.hregion.memstore.flush.size（默认值128m），其所在region的所有memstore都会刷写。 当memstore的大小达到了 hbase.hregion.memstore.flush.size（默认值128m）* hbase.hregion.memstore.block.multiplier（默认值4倍）时(即默认为128m*4=512mb)，会阻止继续往该memstore写数据。\n    hbase中不推荐创建太多了列族 由于刷写整个memstore都会刷写 而每次刷写都是在hdfs中建立新的文件 可能有store很小就被刷写了 浪费系统资源\n\n 2. 当region server中memstore的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.upper.limit（默认值0.95）， region server 会把其的所有 region 按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到hbase.regionserver.global.memstore.size.lower.limit(默认为空 需要配置)以下。 当region server中memstore的总大小达到java_heapsize*hbase.regionserver.global.memstore.size（默认值0.4）时，会阻止继续往所有的memstore写数据。\n\n 3. 到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置hbase.regionserver.optionalcacheflushinterval（默认1小时）。\n\n 4. 当wal文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到wal文件数量减小到hbase.regionserver.max.log以下（该属性名已经废弃，现无需手动设置，最大值为32）。\n\n\n# 读流程\n\n\n\n读流程\n\n 1. client先访问zookeeper，获取hbase:meta表位于哪个region server。\n 2. 访问对应的region server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个region server中的哪个region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。\n 3. 与目标region server进行通讯；\n 4. 分别在block cache（读缓存），memstore和store file（hfile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（put/delete）。 先看缓存 再看memstore 再看store file\n 5. 将从文件中查询到的数据块（block，hfile数据存储单元，默认大小为64kb）缓存到block cache。\n 6. 将合并后的最终结果返回给客户端。\n\n\n# storefile compaction\n\n由于memstore每次刷写都会生成一个新的hfile，且同一个字段的不同版本（timestamp）和不同类型（put/delete）有可能会分布在不同的hfile中，因此查询时需要遍历所有的hfile。为了减少hfile的个数，以及清理掉过期和删除的数据，会进行storefile compaction。\n\n\n\ncompaction分为两种，分别是minor compaction和major compaction。\n\n 1. minor compaction会将临近的若干个较小的hfile合并成一个较大的hfile，但不会清理过期和删除的数据。\n 2. major compaction会将一个store下的所有的hfile合并成一个大hfile，并且会清理掉过期和删除的数据。\n\n\n# region split\n\n默认情况下，每个table起初只有一个region，随着数据的不断写入，region会自动进行拆分。刚拆分时，两个子region都位于当前的region server，但处于负载均衡的考虑，hmaster有可能会将某个region转移给其他的region server。\n\n\n\nregion split时机：\n\n 1. 当1个region中的某个store下所有storefile的总大小超过hbase.hregion.max.filesize(默认为10737418240 十gb大小)，该region就会进行拆分（0.94版本之前）。\n 2. 当1个region中的某个store下所有storefile的总大小超过min(r^3 * 2 * "hbase.hregion.memstore.flush.size",hbase.hregion.max.filesize")，该region就会进行拆分，其中r为当前region server中属于该table的个数（0.94版本之后）。\n 3. hbase 2.0 引入了新的split策略: 如果当前 regionserver 上该表只有一个 regin 按照 2 * "hbase.hregion.memstore.flush.size" 分裂，否则按照 hbase.hregion.max.filesize 分裂',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hbase shell",frontmatter:{title:"Hbase shell",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/73c948/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/04.Hbase%20shell.html",relativePath:"大数据/07.Hbase/04.Hbase shell.md",key:"v-41cba2f2",path:"/pages/73c948/",headers:[{level:2,title:"客户端操作",slug:"客户端操作",normalizedTitle:"客户端操作",charIndex:18},{level:2,title:"表操作",slug:"表操作",normalizedTitle:"表操作",charIndex:114}],headersStr:"客户端操作 表操作",content:"# Hbase shell\n\n\n# 客户端操作\n\n连接客户端\n\n#进入Habse客户端\nhbase shell\n\n\n1\n2\n\n\n内部命令\n\n#帮助命令\nhelp\n#查看当前数据库有哪些表\nlist\n\n\n1\n2\n3\n4\n\n\n\n# 表操作\n\n * 创建表\n\n#create '表名','列族'\ncreate 'student','info'\n\n\n1\n2\n\n * 查看列族\n\n#describe '表名'\ndescribe 'student'\n\n\n1\n2\n\n * 插入数据到表\n\n#put '表名','行键','列族:列名','数据'\nput 'student','1001','info:name','zhangsan'\nput 'student','1001','info:age','18'\nput 'student','1002','info:name','lisi'\nput 'student','1002','info:age','20'\n\n\n1\n2\n3\n4\n5\n\n * 查询指定表数据\n\n#scan '表名'  查询全部数据\nscan 'student'\n\n#scan '表名',{STARTROW => '行键'}  查询指定startrow数据\nscan 'student',{STARTROW => '1001'}\nscan 'student',{STARTROW => '1001', STOPROW  => '1002'}  #查询范围内的数据  查询1002之前的 不包含1002\n\n#get '表名','行键'  查询指定表中指定行键的数据`\nget 'student','1001'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n * 修改数据\n\n#put '表名','行键','列族:列名','数据'  更新数据\nput 'student','1001','info:name','wangwu'\n\n\n1\n2\n\n * 删除数据\n\n#truncate '表名'  删除指定表的所有数据  但表仍然存在\ntruncate 'student'\n\n#deleteall '表名','行键'  删除rowkey的全部数据\ndeleteall 'student','1001'\n\n#delete 'student','行键','列族:列名'  删除rowkey的某列数据\ndelete 'student','1002','info:age'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * 删除表\n\n#先disable 表 再drop表  直接drop是无法操作的\ndisable 'student'\ndrop 'student'\n\n\n1\n2\n3\n\n * 统计表数据行数\n\ncount 'student'\n\n\n1\n\n * 变更表信息\n\nalter 'student',{NAME=>'info',VERSIONS=>3}\nget 'student','1001',{COLUMN=>'info:name',VERSIONS=>3}\n\n\n1\n2\n",normalizedContent:"# hbase shell\n\n\n# 客户端操作\n\n连接客户端\n\n#进入habse客户端\nhbase shell\n\n\n1\n2\n\n\n内部命令\n\n#帮助命令\nhelp\n#查看当前数据库有哪些表\nlist\n\n\n1\n2\n3\n4\n\n\n\n# 表操作\n\n * 创建表\n\n#create '表名','列族'\ncreate 'student','info'\n\n\n1\n2\n\n * 查看列族\n\n#describe '表名'\ndescribe 'student'\n\n\n1\n2\n\n * 插入数据到表\n\n#put '表名','行键','列族:列名','数据'\nput 'student','1001','info:name','zhangsan'\nput 'student','1001','info:age','18'\nput 'student','1002','info:name','lisi'\nput 'student','1002','info:age','20'\n\n\n1\n2\n3\n4\n5\n\n * 查询指定表数据\n\n#scan '表名'  查询全部数据\nscan 'student'\n\n#scan '表名',{startrow => '行键'}  查询指定startrow数据\nscan 'student',{startrow => '1001'}\nscan 'student',{startrow => '1001', stoprow  => '1002'}  #查询范围内的数据  查询1002之前的 不包含1002\n\n#get '表名','行键'  查询指定表中指定行键的数据`\nget 'student','1001'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n * 修改数据\n\n#put '表名','行键','列族:列名','数据'  更新数据\nput 'student','1001','info:name','wangwu'\n\n\n1\n2\n\n * 删除数据\n\n#truncate '表名'  删除指定表的所有数据  但表仍然存在\ntruncate 'student'\n\n#deleteall '表名','行键'  删除rowkey的全部数据\ndeleteall 'student','1001'\n\n#delete 'student','行键','列族:列名'  删除rowkey的某列数据\ndelete 'student','1002','info:age'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * 删除表\n\n#先disable 表 再drop表  直接drop是无法操作的\ndisable 'student'\ndrop 'student'\n\n\n1\n2\n3\n\n * 统计表数据行数\n\ncount 'student'\n\n\n1\n\n * 变更表信息\n\nalter 'student',{name=>'info',versions=>3}\nget 'student','1001',{column=>'info:name',versions=>3}\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Phoenix",frontmatter:{title:"Phoenix",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/6dd88b/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/06.Phoenix.html",relativePath:"大数据/07.Hbase/06.Phoenix.md",key:"v-283892a2",path:"/pages/6dd88b/",headers:[{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:201},{level:2,title:"Phoenix Shell操作",slug:"phoenix-shell操作",normalizedTitle:"phoenix shell操作",charIndex:948},{level:2,title:"Dbeaver连接",slug:"dbeaver连接",normalizedTitle:"dbeaver连接",charIndex:1919},{level:2,title:"表的映射",slug:"表的映射",normalizedTitle:"表的映射",charIndex:2302},{level:3,title:"视图映射",slug:"视图映射",normalizedTitle:"视图映射",charIndex:2399},{level:3,title:"表映射",slug:"表映射",normalizedTitle:"表映射",charIndex:2404},{level:2,title:"JDBC操作",slug:"jdbc操作",normalizedTitle:"jdbc操作",charIndex:2855},{level:3,title:"胖客户端",slug:"胖客户端",normalizedTitle:"胖客户端",charIndex:2866},{level:3,title:"瘦客户端",slug:"瘦客户端",normalizedTitle:"瘦客户端",charIndex:1941},{level:2,title:"Phoenix 二级索引",slug:"phoenix-二级索引",normalizedTitle:"phoenix 二级索引",charIndex:5622},{level:3,title:"二级索引配置文件",slug:"二级索引配置文件",normalizedTitle:"二级索引配置文件",charIndex:5639},{level:3,title:"全局二级索引",slug:"全局二级索引",normalizedTitle:"全局二级索引",charIndex:6920},{level:3,title:"多级索引",slug:"多级索引",normalizedTitle:"多级索引",charIndex:7321},{level:3,title:"删除索引",slug:"删除索引",normalizedTitle:"删除索引",charIndex:7735},{level:3,title:"携带其他字段的全局索引",slug:"携带其他字段的全局索引",normalizedTitle:"携带其他字段的全局索引",charIndex:7807},{level:3,title:"全局索引的缺点",slug:"全局索引的缺点",normalizedTitle:"全局索引的缺点",charIndex:7991},{level:3,title:"Phoenix 本地索引",slug:"phoenix-本地索引",normalizedTitle:"phoenix 本地索引",charIndex:8058},{level:3,title:"协处理器",slug:"协处理器",normalizedTitle:"协处理器",charIndex:8524}],headersStr:"安装 Phoenix Shell操作 Dbeaver连接 表的映射 视图映射 表映射 JDBC操作 胖客户端 瘦客户端 Phoenix 二级索引 二级索引配置文件 全局二级索引 多级索引 删除索引 携带其他字段的全局索引 全局索引的缺点 Phoenix 本地索引 协处理器",content:'# Phoenix\n\nPhoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。\n\nPhoenix特点:\n\n 1. 容易集成：如Spark，Hive，Pig，Flume和Map Reduce；\n 2. 操作简单：DML命令以及通过DDL命令创建和操作表和版本化增量更改；\n 3. 支持HBase二级索引创建。\n\n\n\n\n# 安装\n\ncd /opt/software/\ntar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module/\ncd /opt/module/\nmv apache-phoenix-5.0.0-HBase-2.0-bin phoenix\n\n\n1\n2\n3\n4\n\n\n复制server包并拷贝到各个节点的hbase/lib\n\ncd /opt/module/phoenix/\ncp phoenix-5.0.0-HBase-2.0-server.jar /opt/module/hbase/lib/\n\n\n1\n2\n\n\n复制client包并拷贝到各个节点的hbase/lib\n\ncp phoenix-5.0.0-HBase-2.0-client.jar /opt/module/hbase/lib/\n\n\n1\n\n\n同步hbase lib\n\nxsync /opt/module/hbase/lib/\n\n\n1\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n#phoenix\nexport PHOENIX_HOME=/opt/module/phoenix\nexport PHOENIX_CLASSPATH=$PHOENIX_HOME\nexport PATH=$PATH:$PHOENIX_HOME/bin\n\n\n1\n2\n3\n4\n\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n\n\n集群启动 hadoop zookeeper和hbase\n\n启动Phoenix\n\nsqlline.py hadoop102,hadoop103,hadoop104:2181\n\n\n1\n\n\n\n# Phoenix Shell操作\n\n * 显示所有表\n   \n   * !table\n     -- 或者\n     !tables\n     \n     \n     1\n     2\n     3\n     \n\n * 创建表\n   \n   * -- 直接指定单个列作为RowKey\n     CREATE TABLE IF NOT EXISTS student(\n     id VARCHAR primary key,\n     name VARCHAR);\n     \n     -- 在phoenix中，表名等会自动转换为大写，若要小写，使用双引号，如"us_population"。\n     -- 指定多个列的联合作为RowKey\n     CREATE TABLE IF NOT EXISTS us_population (\n     State CHAR(2) NOT NULL,\n     City VARCHAR NOT NULL,\n     Population BIGINT\n     CONSTRAINT my_pk PRIMARY KEY (state, city));\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     \n\n * 插入数据\n   \n   * upsert into student values(\'1001\',\'zhangsan\');\n     \n     \n     1\n     \n\n * 查询数据\n   \n   * select * from student;\n     select * from student where id=\'1001\';\n     \n     \n     1\n     2\n     \n\n * 删除数据\n   \n   * delete from student where id=\'1001\';\n     \n     \n     1\n     \n\n * 删除表\n   \n   * drop table student;\n     \n     \n     1\n     \n\n\n# Dbeaver连接\n\nHbase推荐我们使用瘦客户端进行连接 我们先启动服务端\n\nqueryserver.py start #服务端启动\nsqlline-thin.py hadoop102:8765 #客户端连接\n\n\n1\n2\n\n\n修改类名 URL模块和端口号\n\n类名: org.apache.phoenix.queryserver.client.Driver\nURL模板: jdbc:phoenix:thin:url=http://{host}:{port};serialization=PROTOBUF\n端口号: 8765\n\n\n1\n2\n3\n\n\n\n\n添加驱动从maven仓库下载jar导入\n\nhttps://mvnrepository.com/artifact/org.apache.phoenix/phoenix-queryserver-client\n\n\n\n\n# 表的映射\n\n默认情况下，直接在HBase中创建的表，通过Phoenix是查看不到的。如果要在Phoenix中操作直接在HBase中创建的表，则需要在Phoenix中进行表的映射。映射方式有两种：视图映射和表映射。\n\n\n# 视图映射\n\nPhoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作。\n\n在hbase shell建立表\n\ncreate \'test\',\'info1\',\'info2\'\n\n\n1\n\n\n在phoenix中创建关联test表的视图\n\ncreate view "test"(id varchar primary key,"info1"."name" varchar, "info2"."address" varchar);\n\n\n1\n\n\n删除视图\n\ndrop view "test"\n\n\n1\n\n\n\n# 表映射\n\n以类似创建视图的方式创建关联表，只需要将create view改为create table即可\n\ncreate table "test"(id varchar primary key,"info1"."name" varchar, "info2"."address" varchar) column_encoded_bytes=0;\n\n\n1\n\n\n\n# JDBC操作\n\n\n# 胖客户端\n\n创建工程 导入依赖\n\n  <dependencies>\n        <dependency>\n            <groupId>org.apache.phoenix</groupId>\n            <artifactId>phoenix-core</artifactId>\n            <version>5.0.0-HBase-2.0</version>\n        </dependency>\n\n        <dependency>\n            <groupId>com.lmax</groupId>\n            <artifactId>disruptor</artifactId>\n            <version>3.3.6</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.8.4</version>\n        </dependency>\n\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n编码\n\npackage com.atguigu.phoenix;\n\n\nimport java.sql.*;\n\npublic class PhoenixClient {\n\n    public static void main(String[] args)  throws  Exception{\n        //1.定义参数\n        String driver = "org.apache.phoenix.jdbc.PhoenixDriver";\n        String url = "jdbc:phoenix:hadoop102,hadoop103,hadoop104:2181";\n\n        //2.加载驱动\n        Class.forName(driver);\n\n        //3.创建连接\n        Connection connection = DriverManager.getConnection(url);\n\n        //4.预编译SQL\n        PreparedStatement preparedStatement = connection.prepareStatement("SELECT * FROM STUDENT");\n\n        //5.查询获取返回值\n        ResultSet resultSet = preparedStatement.executeQuery();\n\n        //6.打印结果\n        while (resultSet.next()) {\n            System.out.println(resultSet.getString(1) + resultSet.getString(2));\n        }\n\n        //7.关闭资源\n        resultSet.close();\n        preparedStatement.close();\n        connection.close();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# 瘦客户端\n\n依赖\n\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.phoenix</groupId>\n            <artifactId>phoenix-queryserver-client</artifactId>\n            <version>5.0.0-HBase-2.0</version>\n        </dependency>\n\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n编码\n\npackage com.atguigu.phoenix;\n\nimport org.apache.phoenix.queryserver.client.ThinClientUtil;\n\nimport java.sql.*;\n\npublic class PhoenixClient2 {\n    public static void main(String[] args) throws SQLException {\n        String url = ThinClientUtil.getConnectionUrl("hadoop102", 8765); //获取jdbc url\n        Connection connection = DriverManager.getConnection(url); //获取连接对象\n        PreparedStatement preparedStatement = connection.prepareStatement("SELECT * FROM STUDENT");\n        ResultSet resultSet = preparedStatement.executeQuery();\n\n        while (resultSet.next()) {\n            System.out.println(resultSet.getString(1) + resultSet.getString(2));\n        }\n\n        resultSet.close();\n        preparedStatement.close();\n        connection.close();\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# Phoenix 二级索引\n\n\n# 二级索引配置文件\n\n添加如下配置到HBase的HRegionserver节点的hbase-site.xml\n\n\x3c!-- phoenix regionserver 配置参数--\x3e\n\t<property>\n\t\t<name>hbase.regionserver.wal.codec</name>\n\t\t<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>\n\t</property>\n\n\t<property>\n\t\t<name>hbase.region.server.rpc.scheduler.factory.class</name>\n\t\t<value>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory</value>\n\t\t<description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>\n\t</property>\n\n\t<property>\n\t\t<name>hbase.rpc.controllerfactory.class</name>\n\t\t<value>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</value>\n\t\t<description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>\n\t</property>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n添加如下配置到HBase中HMaster节点的hbase-site.xml中\n\n\x3c!-- phoenix master 配置参数 --\x3e\n\t<property>\n\t\t<name>hbase.master.loadbalancer.class</name>\n\t\t<value>org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer</value>\n\t</property>\n\n\t<property>\n\t\t<name>hbase.coprocessor.master.classes</name>\n\t\t<value>org.apache.phoenix.hbase.index.master.IndexMasterObserver</value>\n\t</property>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n重新启动hbase和Phoenix\n\n\n# 全局二级索引\n\nGlobal Index是默认的索引格式，创建全局索引时，会在HBase中建立一张新表。也就是说索引数据和数据表是存放在不同的表中的，因此全局索引适用于多读少写的业务场景。\n\n\n\n写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。\n\n在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。\n\n创建单个字段的全局索引\n\nCREATE INDEX student_index ON student (name);\n\n\n1\n\n\n索引表查看\n\nscan \'STUDENT_INDEX\'\n\n\n1\n\n\nvalue全部为x空的 而row变成value+rowkey二进制\n\n\n\n二级索引只能针对 指定列限定符+主键的索引表 如果查询中查询列带上其他列限定符 则无法从此索引表进行索引查询 从而进行全表扫描。\n\n\n\n\n# 多级索引\n\n针对上面二级索引只能进行两个列限定符 进索引扫描 我们可以创建索引表时指定多个列限定符\n\nCREATE INDEX student_name_sex_index ON student (name,sex); -- 创建一个主键 name sex 索引表\n\n\n1\n\n\n创建完成后我们就可以进行多个列限定符来进行索引查询\n\nexplain select id,name,sex from student  where name = \'6\'; -- 从有这三个列限定符的索引表进行索引查询 没有则全表扫描\nexplain select id,name,sex,address from student  where name = \'6\'; -- 因为没有包含id,name,sex,address这四个列限定符的索引表 所有进行全表扫描\n\n\n1\n2\n\n\n注意: 创建索引表顺序要与查询时一致 否则无法进行索引查询\n\n\n# 删除索引\n\n直接删除指定索引表\n\ndrop index STUDENT_NAME_SEX_INDEX on student;\n\n\n1\n\n\n\n# 携带其他字段的全局索引\n\nCREATE INDEX student_name_index_all ON student (name) INCLUDE (age,sex,address);\n\n\n1\n\n\n\n\n通过携带其他字段 根据指定字段创建索引 并携带其他字段中的值 携带字段没有创建索引只是附加为索引字段的结果 但条件查询必须是索引字段 否则也是进行全表扫描\n\n\n# 全局索引的缺点\n\n 1. 占用空间 需要建立多个表\n 2. 维护成本高 无法在更新和添加频繁的表中使用 需要重新全表更新索引\n\n\n# Phoenix 本地索引\n\nLocal Index适用于写操作频繁的场景。\n\n索引数据和数据表的数据是存放在同一张表中（且是同一个Region），避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。\n\nCREATE LOCAL INDEX local_index_student ON student (name);\n\n\n1\n\n\n同样有个新的索引表\n\n\n\n但在hbase shell中 查看list时没有查看到此表 索引数据和数据表的数据是存放在同一张表中\n\n查看student表数据\n\n\n\n直接在原表中插入索引数据 索引值+主键 value为空\n\n\n\n本地索引不局限查询列 我们可以查询所有列数据 但是条件语句必须是索引列限定\n\n 1. 先命中索引列查出主键id\n 2. 根据主键id命中原表的数据\n 3. 全局比本地索引快 全局是一次命中 而本地需要命中之后根据主键查询\n 4. 全局空间牺牲更大 而本地直接在表中增加\n 5. 全局索引更新要更新全部索引表 而本地索引只需要更新一张表(原数据表中的索引行)\n\n\n# 协处理器\n\n编写协处理器，实现在往A表插入数据的同时让HBase自身（协处理器）向B表中插入一条数据。\n\n相当于增强操作 而我们的全局索引就是基于协处理器进行索引查询\n\n创建项目 导入依赖\n\n<dependencies>\n    <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-client</artifactId>\n        <version>1.3.1</version>\n</dependency>\n\n    <dependency>\n        <groupId>org.apache.hbase</groupId>\n        <artifactId>hbase-server</artifactId>\n        <version>1.3.1</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n定义FruitTableCoprocessor类并继承BaseRegionObserver类\n\npackage com.atguigu;\n\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.*;\nimport org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;\nimport org.apache.hadoop.hbase.coprocessor.ObserverContext;\nimport org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\nimport org.apache.hadoop.hbase.regionserver.wal.WALEdit;\n\nimport java.io.IOException;\n\npublic class FruitTableCoprocessor extends BaseRegionObserver {\n\n    @Override\n    public void postPut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit, Durability durability) throws IOException {\n\n        //获取连接\n        Connection connection = ConnectionFactory.createConnection(HBaseConfiguration.create());\n\n        //获取表对象\n        Table table = connection.getTable(TableName.valueOf("fruit"));\n\n        //插入数据\n        table.put(put);\n\n        //关闭资源\n        table.close();\n        connection.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n',normalizedContent:'# phoenix\n\nphoenix是hbase的开源sql皮肤。可以使用标准jdbc api代替hbase客户端api来创建表，插入数据和查询hbase数据。\n\nphoenix特点:\n\n 1. 容易集成：如spark，hive，pig，flume和map reduce；\n 2. 操作简单：dml命令以及通过ddl命令创建和操作表和版本化增量更改；\n 3. 支持hbase二级索引创建。\n\n\n\n\n# 安装\n\ncd /opt/software/\ntar -zxvf apache-phoenix-5.0.0-hbase-2.0-bin.tar.gz -c /opt/module/\ncd /opt/module/\nmv apache-phoenix-5.0.0-hbase-2.0-bin phoenix\n\n\n1\n2\n3\n4\n\n\n复制server包并拷贝到各个节点的hbase/lib\n\ncd /opt/module/phoenix/\ncp phoenix-5.0.0-hbase-2.0-server.jar /opt/module/hbase/lib/\n\n\n1\n2\n\n\n复制client包并拷贝到各个节点的hbase/lib\n\ncp phoenix-5.0.0-hbase-2.0-client.jar /opt/module/hbase/lib/\n\n\n1\n\n\n同步hbase lib\n\nxsync /opt/module/hbase/lib/\n\n\n1\n\n\n配置环境变量\n\nsudo vim /etc/profile.d/my_env.sh\n\n\n1\n\n\n#phoenix\nexport phoenix_home=/opt/module/phoenix\nexport phoenix_classpath=$phoenix_home\nexport path=$path:$phoenix_home/bin\n\n\n1\n2\n3\n4\n\n\nsource /etc/profile.d/my_env.sh\n\n\n1\n\n\n集群启动 hadoop zookeeper和hbase\n\n启动phoenix\n\nsqlline.py hadoop102,hadoop103,hadoop104:2181\n\n\n1\n\n\n\n# phoenix shell操作\n\n * 显示所有表\n   \n   * !table\n     -- 或者\n     !tables\n     \n     \n     1\n     2\n     3\n     \n\n * 创建表\n   \n   * -- 直接指定单个列作为rowkey\n     create table if not exists student(\n     id varchar primary key,\n     name varchar);\n     \n     -- 在phoenix中，表名等会自动转换为大写，若要小写，使用双引号，如"us_population"。\n     -- 指定多个列的联合作为rowkey\n     create table if not exists us_population (\n     state char(2) not null,\n     city varchar not null,\n     population bigint\n     constraint my_pk primary key (state, city));\n     \n     \n     \n     1\n     2\n     3\n     4\n     5\n     6\n     7\n     8\n     9\n     10\n     11\n     12\n     13\n     \n\n * 插入数据\n   \n   * upsert into student values(\'1001\',\'zhangsan\');\n     \n     \n     1\n     \n\n * 查询数据\n   \n   * select * from student;\n     select * from student where id=\'1001\';\n     \n     \n     1\n     2\n     \n\n * 删除数据\n   \n   * delete from student where id=\'1001\';\n     \n     \n     1\n     \n\n * 删除表\n   \n   * drop table student;\n     \n     \n     1\n     \n\n\n# dbeaver连接\n\nhbase推荐我们使用瘦客户端进行连接 我们先启动服务端\n\nqueryserver.py start #服务端启动\nsqlline-thin.py hadoop102:8765 #客户端连接\n\n\n1\n2\n\n\n修改类名 url模块和端口号\n\n类名: org.apache.phoenix.queryserver.client.driver\nurl模板: jdbc:phoenix:thin:url=http://{host}:{port};serialization=protobuf\n端口号: 8765\n\n\n1\n2\n3\n\n\n\n\n添加驱动从maven仓库下载jar导入\n\nhttps://mvnrepository.com/artifact/org.apache.phoenix/phoenix-queryserver-client\n\n\n\n\n# 表的映射\n\n默认情况下，直接在hbase中创建的表，通过phoenix是查看不到的。如果要在phoenix中操作直接在hbase中创建的表，则需要在phoenix中进行表的映射。映射方式有两种：视图映射和表映射。\n\n\n# 视图映射\n\nphoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作。\n\n在hbase shell建立表\n\ncreate \'test\',\'info1\',\'info2\'\n\n\n1\n\n\n在phoenix中创建关联test表的视图\n\ncreate view "test"(id varchar primary key,"info1"."name" varchar, "info2"."address" varchar);\n\n\n1\n\n\n删除视图\n\ndrop view "test"\n\n\n1\n\n\n\n# 表映射\n\n以类似创建视图的方式创建关联表，只需要将create view改为create table即可\n\ncreate table "test"(id varchar primary key,"info1"."name" varchar, "info2"."address" varchar) column_encoded_bytes=0;\n\n\n1\n\n\n\n# jdbc操作\n\n\n# 胖客户端\n\n创建工程 导入依赖\n\n  <dependencies>\n        <dependency>\n            <groupid>org.apache.phoenix</groupid>\n            <artifactid>phoenix-core</artifactid>\n            <version>5.0.0-hbase-2.0</version>\n        </dependency>\n\n        <dependency>\n            <groupid>com.lmax</groupid>\n            <artifactid>disruptor</artifactid>\n            <version>3.3.6</version>\n        </dependency>\n        <dependency>\n            <groupid>org.apache.hadoop</groupid>\n            <artifactid>hadoop-common</artifactid>\n            <version>2.8.4</version>\n        </dependency>\n\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n编码\n\npackage com.atguigu.phoenix;\n\n\nimport java.sql.*;\n\npublic class phoenixclient {\n\n    public static void main(string[] args)  throws  exception{\n        //1.定义参数\n        string driver = "org.apache.phoenix.jdbc.phoenixdriver";\n        string url = "jdbc:phoenix:hadoop102,hadoop103,hadoop104:2181";\n\n        //2.加载驱动\n        class.forname(driver);\n\n        //3.创建连接\n        connection connection = drivermanager.getconnection(url);\n\n        //4.预编译sql\n        preparedstatement preparedstatement = connection.preparestatement("select * from student");\n\n        //5.查询获取返回值\n        resultset resultset = preparedstatement.executequery();\n\n        //6.打印结果\n        while (resultset.next()) {\n            system.out.println(resultset.getstring(1) + resultset.getstring(2));\n        }\n\n        //7.关闭资源\n        resultset.close();\n        preparedstatement.close();\n        connection.close();\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# 瘦客户端\n\n依赖\n\n    <dependencies>\n        <dependency>\n            <groupid>org.apache.phoenix</groupid>\n            <artifactid>phoenix-queryserver-client</artifactid>\n            <version>5.0.0-hbase-2.0</version>\n        </dependency>\n\n    </dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n编码\n\npackage com.atguigu.phoenix;\n\nimport org.apache.phoenix.queryserver.client.thinclientutil;\n\nimport java.sql.*;\n\npublic class phoenixclient2 {\n    public static void main(string[] args) throws sqlexception {\n        string url = thinclientutil.getconnectionurl("hadoop102", 8765); //获取jdbc url\n        connection connection = drivermanager.getconnection(url); //获取连接对象\n        preparedstatement preparedstatement = connection.preparestatement("select * from student");\n        resultset resultset = preparedstatement.executequery();\n\n        while (resultset.next()) {\n            system.out.println(resultset.getstring(1) + resultset.getstring(2));\n        }\n\n        resultset.close();\n        preparedstatement.close();\n        connection.close();\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# phoenix 二级索引\n\n\n# 二级索引配置文件\n\n添加如下配置到hbase的hregionserver节点的hbase-site.xml\n\n\x3c!-- phoenix regionserver 配置参数--\x3e\n\t<property>\n\t\t<name>hbase.regionserver.wal.codec</name>\n\t\t<value>org.apache.hadoop.hbase.regionserver.wal.indexedwaleditcodec</value>\n\t</property>\n\n\t<property>\n\t\t<name>hbase.region.server.rpc.scheduler.factory.class</name>\n\t\t<value>org.apache.hadoop.hbase.ipc.phoenixrpcschedulerfactory</value>\n\t\t<description>factory to create the phoenix rpc scheduler that uses separate queues for index and metadata updates</description>\n\t</property>\n\n\t<property>\n\t\t<name>hbase.rpc.controllerfactory.class</name>\n\t\t<value>org.apache.hadoop.hbase.ipc.controller.serverrpccontrollerfactory</value>\n\t\t<description>factory to create the phoenix rpc scheduler that uses separate queues for index and metadata updates</description>\n\t</property>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n添加如下配置到hbase中hmaster节点的hbase-site.xml中\n\n\x3c!-- phoenix master 配置参数 --\x3e\n\t<property>\n\t\t<name>hbase.master.loadbalancer.class</name>\n\t\t<value>org.apache.phoenix.hbase.index.balancer.indexloadbalancer</value>\n\t</property>\n\n\t<property>\n\t\t<name>hbase.coprocessor.master.classes</name>\n\t\t<value>org.apache.phoenix.hbase.index.master.indexmasterobserver</value>\n\t</property>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n重新启动hbase和phoenix\n\n\n# 全局二级索引\n\nglobal index是默认的索引格式，创建全局索引时，会在hbase中建立一张新表。也就是说索引数据和数据表是存放在不同的表中的，因此全局索引适用于多读少写的业务场景。\n\n\n\n写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。\n\n在读数据的时候phoenix会选择索引表来降低查询消耗的时间。\n\n创建单个字段的全局索引\n\ncreate index student_index on student (name);\n\n\n1\n\n\n索引表查看\n\nscan \'student_index\'\n\n\n1\n\n\nvalue全部为x空的 而row变成value+rowkey二进制\n\n\n\n二级索引只能针对 指定列限定符+主键的索引表 如果查询中查询列带上其他列限定符 则无法从此索引表进行索引查询 从而进行全表扫描。\n\n\n\n\n# 多级索引\n\n针对上面二级索引只能进行两个列限定符 进索引扫描 我们可以创建索引表时指定多个列限定符\n\ncreate index student_name_sex_index on student (name,sex); -- 创建一个主键 name sex 索引表\n\n\n1\n\n\n创建完成后我们就可以进行多个列限定符来进行索引查询\n\nexplain select id,name,sex from student  where name = \'6\'; -- 从有这三个列限定符的索引表进行索引查询 没有则全表扫描\nexplain select id,name,sex,address from student  where name = \'6\'; -- 因为没有包含id,name,sex,address这四个列限定符的索引表 所有进行全表扫描\n\n\n1\n2\n\n\n注意: 创建索引表顺序要与查询时一致 否则无法进行索引查询\n\n\n# 删除索引\n\n直接删除指定索引表\n\ndrop index student_name_sex_index on student;\n\n\n1\n\n\n\n# 携带其他字段的全局索引\n\ncreate index student_name_index_all on student (name) include (age,sex,address);\n\n\n1\n\n\n\n\n通过携带其他字段 根据指定字段创建索引 并携带其他字段中的值 携带字段没有创建索引只是附加为索引字段的结果 但条件查询必须是索引字段 否则也是进行全表扫描\n\n\n# 全局索引的缺点\n\n 1. 占用空间 需要建立多个表\n 2. 维护成本高 无法在更新和添加频繁的表中使用 需要重新全表更新索引\n\n\n# phoenix 本地索引\n\nlocal index适用于写操作频繁的场景。\n\n索引数据和数据表的数据是存放在同一张表中（且是同一个region），避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。\n\ncreate local index local_index_student on student (name);\n\n\n1\n\n\n同样有个新的索引表\n\n\n\n但在hbase shell中 查看list时没有查看到此表 索引数据和数据表的数据是存放在同一张表中\n\n查看student表数据\n\n\n\n直接在原表中插入索引数据 索引值+主键 value为空\n\n\n\n本地索引不局限查询列 我们可以查询所有列数据 但是条件语句必须是索引列限定\n\n 1. 先命中索引列查出主键id\n 2. 根据主键id命中原表的数据\n 3. 全局比本地索引快 全局是一次命中 而本地需要命中之后根据主键查询\n 4. 全局空间牺牲更大 而本地直接在表中增加\n 5. 全局索引更新要更新全部索引表 而本地索引只需要更新一张表(原数据表中的索引行)\n\n\n# 协处理器\n\n编写协处理器，实现在往a表插入数据的同时让hbase自身（协处理器）向b表中插入一条数据。\n\n相当于增强操作 而我们的全局索引就是基于协处理器进行索引查询\n\n创建项目 导入依赖\n\n<dependencies>\n    <dependency>\n        <groupid>org.apache.hbase</groupid>\n        <artifactid>hbase-client</artifactid>\n        <version>1.3.1</version>\n</dependency>\n\n    <dependency>\n        <groupid>org.apache.hbase</groupid>\n        <artifactid>hbase-server</artifactid>\n        <version>1.3.1</version>\n    </dependency>\n</dependencies>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n定义fruittablecoprocessor类并继承baseregionobserver类\n\npackage com.atguigu;\n\nimport org.apache.hadoop.hbase.hbaseconfiguration;\nimport org.apache.hadoop.hbase.tablename;\nimport org.apache.hadoop.hbase.client.*;\nimport org.apache.hadoop.hbase.coprocessor.baseregionobserver;\nimport org.apache.hadoop.hbase.coprocessor.observercontext;\nimport org.apache.hadoop.hbase.coprocessor.regioncoprocessorenvironment;\nimport org.apache.hadoop.hbase.regionserver.wal.waledit;\n\nimport java.io.ioexception;\n\npublic class fruittablecoprocessor extends baseregionobserver {\n\n    @override\n    public void postput(observercontext<regioncoprocessorenvironment> e, put put, waledit edit, durability durability) throws ioexception {\n\n        //获取连接\n        connection connection = connectionfactory.createconnection(hbaseconfiguration.create());\n\n        //获取表对象\n        table table = connection.gettable(tablename.valueof("fruit"));\n\n        //插入数据\n        table.put(put);\n\n        //关闭资源\n        table.close();\n        connection.close();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"HBase优化",frontmatter:{title:"HBase优化",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/ce507c/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/08.HBase%E4%BC%98%E5%8C%96.html",relativePath:"大数据/07.Hbase/08.HBase优化.md",key:"v-5a8af3d8",path:"/pages/ce507c/",headers:[{level:2,title:"预分区",slug:"预分区",normalizedTitle:"预分区",charIndex:14},{level:3,title:"手动设定预分区",slug:"手动设定预分区",normalizedTitle:"手动设定预分区",charIndex:141},{level:3,title:"生成16进制序列预分区",slug:"生成16进制序列预分区",normalizedTitle:"生成16进制序列预分区",charIndex:268},{level:3,title:"按文件设置的规则分区",slug:"按文件设置的规则分区",normalizedTitle:"按文件设置的规则分区",charIndex:387},{level:3,title:"使用JavaAPI创建预分区",slug:"使用javaapi创建预分区",normalizedTitle:"使用javaapi创建预分区",charIndex:534},{level:2,title:"RowKey设计",slug:"rowkey设计",normalizedTitle:"rowkey设计",charIndex:885},{level:3,title:"生成随机数、hash、散列值",slug:"生成随机数、hash、散列值",normalizedTitle:"生成随机数、hash、散列值",charIndex:1005},{level:3,title:"字符串反转",slug:"字符串反转",normalizedTitle:"字符串反转",charIndex:1277},{level:3,title:"字符串拼接",slug:"字符串拼接",normalizedTitle:"字符串拼接",charIndex:1350},{level:2,title:"内存优化",slug:"内存优化",normalizedTitle:"内存优化",charIndex:1401},{level:2,title:"基础优化",slug:"基础优化",normalizedTitle:"基础优化",charIndex:1577},{level:3,title:"允许在HDFS的文件中追加内容",slug:"允许在hdfs的文件中追加内容",normalizedTitle:"允许在hdfs的文件中追加内容",charIndex:1586},{level:3,title:"优化DataNode允许的最大文件打开数",slug:"优化datanode允许的最大文件打开数",normalizedTitle:"优化datanode允许的最大文件打开数",charIndex:1710},{level:3,title:"优化延迟高的数据操作的等待时间",slug:"优化延迟高的数据操作的等待时间",normalizedTitle:"优化延迟高的数据操作的等待时间",charIndex:1851},{level:3,title:"优化数据的写入效率",slug:"优化数据的写入效率",normalizedTitle:"优化数据的写入效率",charIndex:2003},{level:3,title:"设置RPC监听数量",slug:"设置rpc监听数量",normalizedTitle:"设置rpc监听数量",charIndex:2213},{level:3,title:"优化HStore文件大小",slug:"优化hstore文件大小",normalizedTitle:"优化hstore文件大小",charIndex:2334},{level:3,title:"优化HBase客户端缓存",slug:"优化hbase客户端缓存",normalizedTitle:"优化hbase客户端缓存",charIndex:2543},{level:3,title:"指定scan.next扫描HBase所获取的行数",slug:"指定scan-next扫描hbase所获取的行数",normalizedTitle:"指定scan.next扫描hbase所获取的行数",charIndex:2688},{level:3,title:"flush、compact、split机制",slug:"flush、compact、split机制",normalizedTitle:"flush、compact、split机制",charIndex:2805}],headersStr:"预分区 手动设定预分区 生成16进制序列预分区 按文件设置的规则分区 使用JavaAPI创建预分区 RowKey设计 生成随机数、hash、散列值 字符串反转 字符串拼接 内存优化 基础优化 允许在HDFS的文件中追加内容 优化DataNode允许的最大文件打开数 优化延迟高的数据操作的等待时间 优化数据的写入效率 设置RPC监听数量 优化HStore文件大小 优化HBase客户端缓存 指定scan.next扫描HBase所获取的行数 flush、compact、split机制",content:"# HBase优化\n\n\n# 预分区\n\n每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。\n\n\n# 手动设定预分区\n\ncreate 'staff1','info',SPLITS => ['1000','2000','3000','4000']\n\n\n1\n\n\n\n\n分为5个区 0-1000 1000-2000 2000-3000 3000-4000\n\n\n# 生成16进制序列预分区\n\ncreate 'staff2','info','partition2',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}\n\n\n1\n\n\n会分为15个区\n\n\n\n\n# 按文件设置的规则分区\n\n创建splits.txt文件内容如下：\n\naaaa\nbbbb\ncccc\ndddd\n\n\n1\n2\n3\n4\n\n\ncreate 'staff3','partition3',SPLITS_FILE => '/home/atguigu/splits.txt'\n\n\n1\n\n\n\n\n\n# 使用JavaAPI创建预分区\n\n//自定义算法，产生一系列hash散列值存储在二维数组中\nbyte[][] splitKeys = 某个散列值函数\n//创建HbaseAdmin实例\nHBaseAdmin hAdmin = new HBaseAdmin(HbaseConfiguration.create());\n//创建HTableDescriptor实例\nHTableDescriptor tableDesc = new HTableDescriptor(tableName);\n//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表\nhAdmin.createTable(tableDesc, splitKeys);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# RowKey设计\n\n一条数据的唯一标识就是RowKey，那么这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内，设计RowKey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。\n\n\n# 生成随机数、hash、散列值\n\n比如： 原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7 原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd 原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913 在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。\n\n\n# 字符串反转\n\n20170524000001转成10000042507102 20170524000002转成20000042507102\n\n\n# 字符串拼接\n\n20170524000001_a12e 20170524000001_93i7\n\n\n# 内存优化\n\nHBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。\n\n\n# 基础优化\n\n\n# 允许在HDFS的文件中追加内容\n\n在hdfs-site.xml、hbase-site.xml中添加\n\n属性：dfs.support.append\n\n解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。\n\n\n# 优化DataNode允许的最大文件打开数\n\nhdfs-site.xml\n\n属性：dfs.datanode.max.transfer.threads\n\n解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096\n\n\n# 优化延迟高的数据操作的等待时间\n\nhdfs-site.xml\n\n属性：dfs.image.transfer.timeout\n\n解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。\n\n\n# 优化数据的写入效率\n\nmapred-site.xml\n\n属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec\n\n解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。\n\n第一个属性值修改为true\n\n第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。\n\n\n# 设置RPC监听数量\n\nhbase-site.xml\n\n属性：Hbase.regionserver.handler.count\n\n解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。\n\n\n# 优化HStore文件大小\n\nhbase-site.xml\n\n属性：hbase.hregion.max.filesize\n\n解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。\n\n\n# 优化HBase客户端缓存\n\nhbase-site.xml\n\n属性：hbase.client.write.buffer\n\n解释：用于指定Hbase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。\n\n\n# 指定scan.next扫描HBase所获取的行数\n\nhbase-site.xml\n\n属性：hbase.client.scanner.caching\n\n解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。\n\n\n# flush、compact、split机制\n\n当MemStore达到阈值，将Memstore中的数据Flush进Storefile；\n\ncompact机制则是把flush出来的小文件合并成大的Storefile文件。\n\nsplit则是当Region达到阈值，会把过大的Region一分为二。\n\n涉及属性：\n\n即：128M就是Memstore的默认阈值\n\nhbase.hregion.memstore.flush.size：134217728\n\n即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。\n\n**hbase.regionserver.global.memstore.upperLimit：0.4 **\n\nhbase.regionserver.global.memstore.lowerLimit：0.38\n\n即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit",normalizedContent:"# hbase优化\n\n\n# 预分区\n\n每一个region维护着startrow与endrow，如果加入的数据符合某个region维护的rowkey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高hbase性能。\n\n\n# 手动设定预分区\n\ncreate 'staff1','info',splits => ['1000','2000','3000','4000']\n\n\n1\n\n\n\n\n分为5个区 0-1000 1000-2000 2000-3000 3000-4000\n\n\n# 生成16进制序列预分区\n\ncreate 'staff2','info','partition2',{numregions => 15, splitalgo => 'hexstringsplit'}\n\n\n1\n\n\n会分为15个区\n\n\n\n\n# 按文件设置的规则分区\n\n创建splits.txt文件内容如下：\n\naaaa\nbbbb\ncccc\ndddd\n\n\n1\n2\n3\n4\n\n\ncreate 'staff3','partition3',splits_file => '/home/atguigu/splits.txt'\n\n\n1\n\n\n\n\n\n# 使用javaapi创建预分区\n\n//自定义算法，产生一系列hash散列值存储在二维数组中\nbyte[][] splitkeys = 某个散列值函数\n//创建hbaseadmin实例\nhbaseadmin hadmin = new hbaseadmin(hbaseconfiguration.create());\n//创建htabledescriptor实例\nhtabledescriptor tabledesc = new htabledescriptor(tablename);\n//通过htabledescriptor实例和散列值二维数组创建带有预分区的hbase表\nhadmin.createtable(tabledesc, splitkeys);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# rowkey设计\n\n一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。\n\n\n# 生成随机数、hash、散列值\n\n比如： 原本rowkey为1001的，sha1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7 原本rowkey为3001的，sha1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd 原本rowkey为5001的，sha1后变成：7b61dec07e02c188790670af43e717f0f46e8913 在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowkey来hash后作为每个分区的临界值。\n\n\n# 字符串反转\n\n20170524000001转成10000042507102 20170524000002转成20000042507102\n\n\n# 字符串拼接\n\n20170524000001_a12e 20170524000001_93i7\n\n\n# 内存优化\n\nhbase操作过程中需要大量的内存开销，毕竟table是可以缓存在内存中的，一般会分配整个可用内存的70%给hbase的java堆。但是不建议分配非常大的堆内存，因为gc过程持续太久会导致regionserver处于长期不可用状态，一般16~48g内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。\n\n\n# 基础优化\n\n\n# 允许在hdfs的文件中追加内容\n\n在hdfs-site.xml、hbase-site.xml中添加\n\n属性：dfs.support.append\n\n解释：开启hdfs追加同步，可以优秀的配合hbase的数据同步和持久化。默认值为true。\n\n\n# 优化datanode允许的最大文件打开数\n\nhdfs-site.xml\n\n属性：dfs.datanode.max.transfer.threads\n\n解释：hbase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096\n\n\n# 优化延迟高的数据操作的等待时间\n\nhdfs-site.xml\n\n属性：dfs.image.transfer.timeout\n\n解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。\n\n\n# 优化数据的写入效率\n\nmapred-site.xml\n\n属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec\n\n解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。\n\n第一个属性值修改为true\n\n第二个属性值修改为：org.apache.hadoop.io.compress.gzipcodec或者其他压缩方式。\n\n\n# 设置rpc监听数量\n\nhbase-site.xml\n\n属性：hbase.regionserver.handler.count\n\n解释：默认值为30，用于指定rpc监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。\n\n\n# 优化hstore文件大小\n\nhbase-site.xml\n\n属性：hbase.hregion.max.filesize\n\n解释：默认值10737418240（10gb），如果需要运行hbase的mr任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果hfile的大小达到这个数值，则这个region会被切分为两个hfile。\n\n\n# 优化hbase客户端缓存\n\nhbase-site.xml\n\n属性：hbase.client.write.buffer\n\n解释：用于指定hbase客户端缓存，增大该值可以减少rpc调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少rpc次数的目的。\n\n\n# 指定scan.next扫描hbase所获取的行数\n\nhbase-site.xml\n\n属性：hbase.client.scanner.caching\n\n解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。\n\n\n# flush、compact、split机制\n\n当memstore达到阈值，将memstore中的数据flush进storefile；\n\ncompact机制则是把flush出来的小文件合并成大的storefile文件。\n\nsplit则是当region达到阈值，会把过大的region一分为二。\n\n涉及属性：\n\n即：128m就是memstore的默认阈值\n\nhbase.hregion.memstore.flush.size：134217728\n\n即：这个参数的作用是当单个hregion内所有的memstore大小总和超过指定值时，flush该hregion的所有memstore。regionserver的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发oom。\n\n**hbase.regionserver.global.memstore.upperlimit：0.4 **\n\nhbase.regionserver.global.memstore.lowerlimit：0.38\n\n即：当memstore使用内存总量达到hbase.regionserver.global.memstore.upperlimit指定值时，将会有多个memstores flush到文件中，memstore flush 顺序是按照大小降序执行的，直到刷新到memstore使用内存略小于lowerlimit",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Hbase与Hive的集成",frontmatter:{title:"Hbase与Hive的集成",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/e654ab/",categories:["大数据","Hbase"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/07.Hbase/07.Hbase%E4%B8%8EHive%E7%9A%84%E9%9B%86%E6%88%90.html",relativePath:"大数据/07.Hbase/07.Hbase与Hive的集成.md",key:"v-2117d120",path:"/pages/e654ab/",headers:[{level:2,title:"HBase与Hive的对比",slug:"hbase与hive的对比",normalizedTitle:"hbase与hive的对比",charIndex:20},{level:2,title:"HBase与Hive集成使用",slug:"hbase与hive集成使用",normalizedTitle:"hbase与hive集成使用",charIndex:449},{level:3,title:"从Hive映射到HBase上",slug:"从hive映射到hbase上",normalizedTitle:"从hive映射到hbase上",charIndex:582},{level:3,title:"从HBase映射到Hive",slug:"从hbase映射到hive",normalizedTitle:"从hbase映射到hive",charIndex:1737}],headersStr:"HBase与Hive的对比 HBase与Hive集成使用 从Hive映射到HBase上 从HBase映射到Hive",content:'# Hbase与Hive的集成\n\n\n# HBase与Hive的对比\n\n1.Hive (1) 数据分析工具 Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。 (2) 用于数据分析、清洗 Hive适用于离线的数据分析和清洗，延迟较高。 (3) 基于HDFS、MapReduce Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。 2．HBase (1) 数据库 是一种面向列族存储的非关系型数据库。 (2) 用于存储结构化和非结构化的数据 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。 (3) 基于HDFS 数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。 (4) 延迟较低，接入在线业务使用 面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。\n\n\n# HBase与Hive集成使用\n\n启动yarn 和 hive服务\n\nstart-yarn.sh\nhiveservices.sh start\nbeeline -u jdbc:hive2://hadoop102:10000 -n atguigu\n\n\n1\n2\n3\n\n\n\n# 从Hive映射到HBase上\n\n建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表\n\n在Hive中创建表同时关联HBase\n\nCREATE TABLE emp_hbase(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nSTORED BY \'org.apache.hadoop.hive.hbase.HBaseStorageHandler\'\nWITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno")\nTBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n从hbase中查看所有表 自动创建了映射表\n\n\n\nhive中创建emp表并导入文本数据\n\nCREATE TABLE emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n查询工资大于2000的数据插入到emp_hbase表中\n\ninsert into emp_hbase select * from emp where sal > 2000;\n\n\n1\n\n\n使用hbase shell 查看 hbase_emp_table\n\n\n\n将emp_hbase表映射Phoenix上\n\nCREATE VIEW "hbase_emp_table"(empno varchar PRIMARY KEY,\n"info"."ename" varchar,\n"info"."job" varchar,\n"info"."mgr" varchar,\n"info"."hiredate" varchar,\n"info"."sal" varchar,\n"info"."comm" varchar,\n"info"."deptno" varchar);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nselect * from "hbase_emp_table";\n\n\n1\n\n\n\n\n\n# 从HBase映射到Hive\n\n在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。\n\n在Hive中创建外部表\n\nCREATE EXTERNAL TABLE relevance_hbase_emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nSTORED BY \n\'org.apache.hadoop.hive.hbase.HBaseStorageHandler\'\nWITH SERDEPROPERTIES ("hbase.columns.mapping" = \n":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno") \nTBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n关联后就可以使用Hive函数进行一些分析操作了\n\nselect * from relevance_hbase_emp;\n\n\n1\n\n\n',normalizedContent:'# hbase与hive的集成\n\n\n# hbase与hive的对比\n\n1.hive (1) 数据分析工具 hive的本质其实就相当于将hdfs中已经存储的文件在mysql中做了一个双射关系，以方便使用hql去管理查询。 (2) 用于数据分析、清洗 hive适用于离线的数据分析和清洗，延迟较高。 (3) 基于hdfs、mapreduce hive存储的数据依旧在datanode上，编写的hql语句终将是转换为mapreduce代码执行。 2．hbase (1) 数据库 是一种面向列族存储的非关系型数据库。 (2) 用于存储结构化和非结构化的数据 适用于单表非关系型数据的存储，不适合做关联查询，类似join等操作。 (3) 基于hdfs 数据持久化存储的体现形式是hfile，存放于datanode中，被resionserver以region的形式进行管理。 (4) 延迟较低，接入在线业务使用 面对大量的企业数据，hbase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。\n\n\n# hbase与hive集成使用\n\n启动yarn 和 hive服务\n\nstart-yarn.sh\nhiveservices.sh start\nbeeline -u jdbc:hive2://hadoop102:10000 -n atguigu\n\n\n1\n2\n3\n\n\n\n# 从hive映射到hbase上\n\n建立hive表，关联hbase表，插入数据到hive表的同时能够影响hbase表\n\n在hive中创建表同时关联hbase\n\ncreate table emp_hbase(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nstored by \'org.apache.hadoop.hive.hbase.hbasestoragehandler\'\nwith serdeproperties ("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno")\ntblproperties ("hbase.table.name" = "hbase_emp_table");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n从hbase中查看所有表 自动创建了映射表\n\n\n\nhive中创建emp表并导入文本数据\n\ncreate table emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nrow format delimited fields terminated by \'\\t\';\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n查询工资大于2000的数据插入到emp_hbase表中\n\ninsert into emp_hbase select * from emp where sal > 2000;\n\n\n1\n\n\n使用hbase shell 查看 hbase_emp_table\n\n\n\n将emp_hbase表映射phoenix上\n\ncreate view "hbase_emp_table"(empno varchar primary key,\n"info"."ename" varchar,\n"info"."job" varchar,\n"info"."mgr" varchar,\n"info"."hiredate" varchar,\n"info"."sal" varchar,\n"info"."comm" varchar,\n"info"."deptno" varchar);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nselect * from "hbase_emp_table";\n\n\n1\n\n\n\n\n\n# 从hbase映射到hive\n\n在hbase中已经存储了某一张表hbase_emp_table，然后在hive中创建一个外部表来关联hbase中的hbase_emp_table这张表，使之可以借助hive来分析hbase这张表中的数据。\n\n在hive中创建外部表\n\ncreate external table relevance_hbase_emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nstored by \n\'org.apache.hadoop.hive.hbase.hbasestoragehandler\'\nwith serdeproperties ("hbase.columns.mapping" = \n":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno") \ntblproperties ("hbase.table.name" = "hbase_emp_table");\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n关联后就可以使用hive函数进行一些分析操作了\n\nselect * from relevance_hbase_emp;\n\n\n1\n\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Scala入门",frontmatter:{title:"Scala入门",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/a93b77/",categories:["大数据","Scala"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/08.Scala/02.Scala%E5%85%A5%E9%97%A8.html",relativePath:"大数据/08.Scala/02.Scala入门.md",key:"v-d5c2e428",path:"/pages/a93b77/",headers:[{level:2,title:"环境搭建",slug:"环境搭建",normalizedTitle:"环境搭建",charIndex:14},{level:2,title:"Idea创建Scala",slug:"idea创建scala",normalizedTitle:"idea创建scala",charIndex:187},{level:2,title:"伴生对象和伴生类",slug:"伴生对象和伴生类",normalizedTitle:"伴生对象和伴生类",charIndex:569}],headersStr:"环境搭建 Idea创建Scala 伴生对象和伴生类",content:'# Scala入门\n\n\n# 环境搭建\n\n 1. 依赖于jdk 确保jdk安装并配置好环境变量\n\n 2. 从官网中下载\n    \n    https://www.scala-lang.org/download/2.11.8.html 并解压到压缩包\n\n 3. 配置环境变量\n    \n    \n    \n    \n\n 4. 在cmd执行scala查看环境变量是否生效\n\n\n# Idea创建Scala\n\n先在idea中安装scala插件\n\n\n\n 1. 右键项目添加框架支持\n    \n    \n\n 2. 添加scala并选择版本\n    \n    \n\n 3. 在src的main包下创建scala源文件夹 用于编写scala\n    \n    \n\n 4. 创建scala类\n    \n    \n    \n    package com.atguigu.scala.chapter\n    \n    object HelloWorld {\n      def main(args: Array[String]): Unit = {\n        println("Hello World")\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n\n# 伴生对象和伴生类\n\n伴随着类产生一个对象 当我们对源文件进行编译之后 默认会生成两个字节码文件 一个是伴生类 另外一个是伴生对象所属类(带$的class)\n\nObject 名称 Scala是纯面向对象 去除了java中static关键字和void关键字,通过伴生对象模拟static效果\n\n\n\n如果不想默认生成伴生类 则需要在伴生对象中手动声明伴生类 必须要与伴生对象命名一致\n\n如果在scala中要定义类型java中static内容 都应该放到伴生对象中声明\n\n 1. 伴生类定义了非静态内容\n 2. 伴生对象所属类定义了静态内容\n 3. 伴生类通过伴生对象所属类调用mian方法(因为main方法是静态内容)\n 4. 伴生对象所属类会在 无参构造器中 生成一个MODULES$ 为伴生对象所属类\n 5. 而伴生对象通过 MODULES$ 来调用静态内容',normalizedContent:'# scala入门\n\n\n# 环境搭建\n\n 1. 依赖于jdk 确保jdk安装并配置好环境变量\n\n 2. 从官网中下载\n    \n    https://www.scala-lang.org/download/2.11.8.html 并解压到压缩包\n\n 3. 配置环境变量\n    \n    \n    \n    \n\n 4. 在cmd执行scala查看环境变量是否生效\n\n\n# idea创建scala\n\n先在idea中安装scala插件\n\n\n\n 1. 右键项目添加框架支持\n    \n    \n\n 2. 添加scala并选择版本\n    \n    \n\n 3. 在src的main包下创建scala源文件夹 用于编写scala\n    \n    \n\n 4. 创建scala类\n    \n    \n    \n    package com.atguigu.scala.chapter\n    \n    object helloworld {\n      def main(args: array[string]): unit = {\n        println("hello world")\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n\n# 伴生对象和伴生类\n\n伴随着类产生一个对象 当我们对源文件进行编译之后 默认会生成两个字节码文件 一个是伴生类 另外一个是伴生对象所属类(带$的class)\n\nobject 名称 scala是纯面向对象 去除了java中static关键字和void关键字,通过伴生对象模拟static效果\n\n\n\n如果不想默认生成伴生类 则需要在伴生对象中手动声明伴生类 必须要与伴生对象命名一致\n\n如果在scala中要定义类型java中static内容 都应该放到伴生对象中声明\n\n 1. 伴生类定义了非静态内容\n 2. 伴生对象所属类定义了静态内容\n 3. 伴生类通过伴生对象所属类调用mian方法(因为main方法是静态内容)\n 4. 伴生对象所属类会在 无参构造器中 生成一个modules$ 为伴生对象所属类\n 5. 而伴生对象通过 modules$ 来调用静态内容',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"流程控制",frontmatter:{title:"流程控制",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/8b0acc/",categories:["大数据","Scala"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/08.Scala/03.%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html",relativePath:"大数据/08.Scala/03.流程控制.md",key:"v-6a5e0750",path:"/pages/8b0acc/",headers:[{level:2,title:"分支控制",slug:"分支控制",normalizedTitle:"分支控制",charIndex:11},{level:2,title:"三元运算符",slug:"三元运算符",normalizedTitle:"三元运算符",charIndex:430},{level:2,title:"Switch分支结构",slug:"switch分支结构",normalizedTitle:"switch分支结构",charIndex:515},{level:2,title:"For循环",slug:"for循环",normalizedTitle:"for循环",charIndex:560},{level:3,title:"范围数据循环（To）",slug:"范围数据循环-to",normalizedTitle:"范围数据循环（to）",charIndex:570},{level:3,title:"范围数据循环（Until）",slug:"范围数据循环-until",normalizedTitle:"范围数据循环（until）",charIndex:709},{level:3,title:"循环守卫",slug:"循环守卫",normalizedTitle:"循环守卫",charIndex:829},{level:3,title:"循环步长",slug:"循环步长",normalizedTitle:"循环步长",charIndex:1034},{level:3,title:"嵌套循环",slug:"嵌套循环",normalizedTitle:"嵌套循环",charIndex:1207},{level:3,title:"引入变量",slug:"引入变量",normalizedTitle:"引入变量",charIndex:1308},{level:3,title:"循环返回值",slug:"循环返回值",normalizedTitle:"循环返回值",charIndex:1578},{level:3,title:"倒序打印",slug:"倒序打印",normalizedTitle:"倒序打印",charIndex:1786},{level:2,title:"循环中断",slug:"循环中断",normalizedTitle:"循环中断",charIndex:1873}],headersStr:"分支控制 三元运算符 Switch分支结构 For循环 范围数据循环（To） 范围数据循环（Until） 循环守卫 循环步长 嵌套循环 引入变量 循环返回值 倒序打印 循环中断",content:'# 流程控制\n\n\n# 分支控制\n\nScala中if else表达式其实是有返回值的，具体返回值取决于满足条件的代码体的最后一行内容\n\nobject TestIfElse  {\n    def main(args: Array[String]): Unit = {\n\n        println("input age")\n        var age = StdIn.readInt()\n\n        val res :String = if (age < 18){\n            "童年"\n        }else if(age>=18 && age<30){\n            "中年"\n        }else{\n            "老年"\n        }\n\n        println(res)\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 三元运算符\n\n利用条件运算符可以有返回值特性 使用三元运算符\n\nval res:Any = if (age < 18)  "童年" else "成年"\n\n\n1\n\n\n\n# Switch分支结构\n\n在Scala中没有Switch，而是使用模式匹配来处理。\n\n\n# For循环\n\n\n# 范围数据循环（To）\n\n使用 1 to 3 赋值给一个变量\n\nfor(i <- 1 to 3){\n    print(i + " ")\n}\n// 1 2 3 \n\n\n1\n2\n3\n4\n\n 1. i 表示循环的变量，<- 规定to\n 2. i 将会从 1-3 循环，前后闭合\n\n\n# 范围数据循环（Until）\n\nfor(i <- 1 until 3) {\n    print(i + " ")\n}\n// 1 2 \n\n\n1\n2\n3\n4\n\n 1. 这种方式和前面的区别在于i是从1到3-1\n 2. 即前闭合后开的范围\n\n\n# 循环守卫\n\n循环守卫，即循环保护式（也称条件判断式，守卫）。保护式为true则进入循环体内部，为false则跳过，类似于continue。\n\n在scala中没有break 和 continue 关键字,循环守卫其实是通过if判断模拟continue跳出本次循环\n\nfor(i <- 1 to 3 if i != 2) {\n    print(i + " ")\n}\n// 1 3 \n\n\n1\n2\n3\n4\n\n\n\n# 循环步长\n\nfor (i <- 1 to 10 by 2) {\n    println("i=" + i)\n}\n// 2 4 6 8 10\n//负步长\nfor (i <- 10 to 1 by -2) {\n    println("i=" + i)\n}\n//10 8 6 4 2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nby表示步长\n\n\n# 嵌套循环\n\nfor(i <- 1 to 3; j <- 1 to 3) {\n    println(" i =" + i + " j = " + j)\n}\n\n\n1\n2\n3\n\n\n使用；来隔断逻辑\n\n\n# 引入变量\n\nfor(i <- 1 to 3; j = 4 - i) {\n    println("i=" + i + " j=" + j)\n}\n// 1 3\n// 2 2\n// 3 1\n\n\n1\n2\n3\n4\n5\n6\n\n\nfor推导式有一个不成文的约定：当for推导式仅包含单一表达式时使用圆括号，当包含多个表达式时，一般每行一个表达式，并用花括号代替圆括号\n\nfor {\n    i <- 1 to 3\nj = 4 - i\n} {\n    println("i=" + i + " j=" + j)\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 循环返回值\n\n将遍历过程中处理的结果返回到一个新Vector集合中，使用yield关键字。\n\nval res = for(i <- 1 to 10) yield i\n// 1 2 3 4 5 6 7 8 9 10\nvar res = for(i <-1 to 10) yield {i * 2}\n//Vector(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)\n\n\n1\n2\n3\n4\n\n\n\n# 倒序打印\n\n倒序打印一组数据，可以用reverse关键字\n\nfor(i <- 1 to 10 reverse){\n    println(i)\n}\n\n\n1\n2\n3\n\n\n\n# 循环中断\n\nScala内置控制结构特地去掉了break和continue，是为了更好的适应函数式编程，推荐使用函数式的风格解决break和continue的功能，而不是一个关键字。Scala中使用breakable控制结构来实现break和continue功能。\n\nimport scala.util.control.Breaks\n\ndef main(args: Array[String]): Unit = {\n\n    Breaks.breakable(\n        for (elem <- 1 to 10) {\n            println(elem)\n            if (elem == 5) Breaks.break()\n        }\n    )\n\n    println("正常结束循环")\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nBreaks底层是抛出异常来结束循环 break方法捕抓异常\n\n简化写法\n\nimport scala.util.control.Breaks._\n\nobject TestBreak {\n\n    def main(args: Array[String]): Unit = {\n    \n        breakable {\n            for (elem <- 1 to 10) {\n                println(elem)\n                if (elem == 5) break\n            }\n        }\n    \n        println("正常结束循环")\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n',normalizedContent:'# 流程控制\n\n\n# 分支控制\n\nscala中if else表达式其实是有返回值的，具体返回值取决于满足条件的代码体的最后一行内容\n\nobject testifelse  {\n    def main(args: array[string]): unit = {\n\n        println("input age")\n        var age = stdin.readint()\n\n        val res :string = if (age < 18){\n            "童年"\n        }else if(age>=18 && age<30){\n            "中年"\n        }else{\n            "老年"\n        }\n\n        println(res)\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 三元运算符\n\n利用条件运算符可以有返回值特性 使用三元运算符\n\nval res:any = if (age < 18)  "童年" else "成年"\n\n\n1\n\n\n\n# switch分支结构\n\n在scala中没有switch，而是使用模式匹配来处理。\n\n\n# for循环\n\n\n# 范围数据循环（to）\n\n使用 1 to 3 赋值给一个变量\n\nfor(i <- 1 to 3){\n    print(i + " ")\n}\n// 1 2 3 \n\n\n1\n2\n3\n4\n\n 1. i 表示循环的变量，<- 规定to\n 2. i 将会从 1-3 循环，前后闭合\n\n\n# 范围数据循环（until）\n\nfor(i <- 1 until 3) {\n    print(i + " ")\n}\n// 1 2 \n\n\n1\n2\n3\n4\n\n 1. 这种方式和前面的区别在于i是从1到3-1\n 2. 即前闭合后开的范围\n\n\n# 循环守卫\n\n循环守卫，即循环保护式（也称条件判断式，守卫）。保护式为true则进入循环体内部，为false则跳过，类似于continue。\n\n在scala中没有break 和 continue 关键字,循环守卫其实是通过if判断模拟continue跳出本次循环\n\nfor(i <- 1 to 3 if i != 2) {\n    print(i + " ")\n}\n// 1 3 \n\n\n1\n2\n3\n4\n\n\n\n# 循环步长\n\nfor (i <- 1 to 10 by 2) {\n    println("i=" + i)\n}\n// 2 4 6 8 10\n//负步长\nfor (i <- 10 to 1 by -2) {\n    println("i=" + i)\n}\n//10 8 6 4 2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nby表示步长\n\n\n# 嵌套循环\n\nfor(i <- 1 to 3; j <- 1 to 3) {\n    println(" i =" + i + " j = " + j)\n}\n\n\n1\n2\n3\n\n\n使用；来隔断逻辑\n\n\n# 引入变量\n\nfor(i <- 1 to 3; j = 4 - i) {\n    println("i=" + i + " j=" + j)\n}\n// 1 3\n// 2 2\n// 3 1\n\n\n1\n2\n3\n4\n5\n6\n\n\nfor推导式有一个不成文的约定：当for推导式仅包含单一表达式时使用圆括号，当包含多个表达式时，一般每行一个表达式，并用花括号代替圆括号\n\nfor {\n    i <- 1 to 3\nj = 4 - i\n} {\n    println("i=" + i + " j=" + j)\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 循环返回值\n\n将遍历过程中处理的结果返回到一个新vector集合中，使用yield关键字。\n\nval res = for(i <- 1 to 10) yield i\n// 1 2 3 4 5 6 7 8 9 10\nvar res = for(i <-1 to 10) yield {i * 2}\n//vector(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)\n\n\n1\n2\n3\n4\n\n\n\n# 倒序打印\n\n倒序打印一组数据，可以用reverse关键字\n\nfor(i <- 1 to 10 reverse){\n    println(i)\n}\n\n\n1\n2\n3\n\n\n\n# 循环中断\n\nscala内置控制结构特地去掉了break和continue，是为了更好的适应函数式编程，推荐使用函数式的风格解决break和continue的功能，而不是一个关键字。scala中使用breakable控制结构来实现break和continue功能。\n\nimport scala.util.control.breaks\n\ndef main(args: array[string]): unit = {\n\n    breaks.breakable(\n        for (elem <- 1 to 10) {\n            println(elem)\n            if (elem == 5) breaks.break()\n        }\n    )\n\n    println("正常结束循环")\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nbreaks底层是抛出异常来结束循环 break方法捕抓异常\n\n简化写法\n\nimport scala.util.control.breaks._\n\nobject testbreak {\n\n    def main(args: array[string]): unit = {\n    \n        breakable {\n            for (elem <- 1 to 10) {\n                println(elem)\n                if (elem == 5) break\n            }\n        }\n    \n        println("正常结束循环")\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Spark",frontmatter:{title:"Spark",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/cc783e/",categories:["大数据","Spark"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/09.Spark/01.Spark.html",relativePath:"大数据/09.Spark/01.Spark.md",key:"v-762d1d8f",path:"/pages/cc783e/",headers:[{level:2,title:"Spark内置模块",slug:"spark内置模块",normalizedTitle:"spark内置模块",charIndex:49},{level:2,title:"Spark 特点",slug:"spark-特点",normalizedTitle:"spark 特点",charIndex:853}],headersStr:"Spark内置模块 Spark 特点",content:"# Spark\n\nSpark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。\n\n\n\n\n# Spark内置模块\n\n\n\nSpark Core：实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义。\n\nSpark SQL：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的HQL来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等。\n\nSpark Streaming：是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应。\n\nSpark MLlib：提供常见的机器学习功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。\n\nSpark GraphX：主要用于图形并行计算和图挖掘系统的组件。\n\n集群管理器：Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度器，叫作独立调度器。\n\nSpark得到了众多大数据公司的支持，这些公司包括Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、阿里、腾讯、京东、携程、优酷土豆。当前百度的Spark已应用于大搜索、直达号、百度大数据等业务；阿里利用GraphX构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯Spark集群达到8000台的规模，是当前已知的世界上最大的Spark集群。\n\n\n# Spark 特点\n\n",normalizedContent:"# spark\n\nspark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。\n\n\n\n\n# spark内置模块\n\n\n\nspark core：实现了spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。spark core中还包含了对弹性分布式数据集(resilient distributed dataset，简称rdd)的api定义。\n\nspark sql：是spark用来操作结构化数据的程序包。通过spark sql，我们可以使用 sql或者apache hive版本的hql来查询数据。spark sql支持多种数据源，比如hive表、parquet以及json等。\n\nspark streaming：是spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的api，并且与spark core中的 rdd api高度对应。\n\nspark mllib：提供常见的机器学习功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。\n\nspark graphx：主要用于图形并行计算和图挖掘系统的组件。\n\n集群管理器：spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，spark支持在各种集群管理器(cluster manager)上运行，包括hadoop yarn、apache mesos，以及spark自带的一个简易调度器，叫作独立调度器。\n\nspark得到了众多大数据公司的支持，这些公司包括hortonworks、ibm、intel、cloudera、mapr、pivotal、百度、阿里、腾讯、京东、携程、优酷土豆。当前百度的spark已应用于大搜索、直达号、百度大数据等业务；阿里利用graphx构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯spark集群达到8000台的规模，是当前已知的世界上最大的spark集群。\n\n\n# spark 特点\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Spark 入门",frontmatter:{title:"Spark 入门",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/3223e3/",categories:["大数据","Spark"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/09.Spark/02.Spark%20%E5%85%A5%E9%97%A8.html",relativePath:"大数据/09.Spark/02.Spark 入门.md",key:"v-a108e506",path:"/pages/3223e3/",headers:[{level:2,title:"Local模式",slug:"local模式",normalizedTitle:"local模式",charIndex:143},{level:3,title:"官方求PI案例",slug:"官方求pi案例",normalizedTitle:"官方求pi案例",charIndex:606},{level:3,title:"官方WordCount案例",slug:"官方wordcount案例",normalizedTitle:"官方wordcount案例",charIndex:1085},{level:2,title:"集群角色",slug:"集群角色",normalizedTitle:"集群角色",charIndex:1504},{level:3,title:"Master 和 Worker",slug:"master-和-worker",normalizedTitle:"master 和 worker",charIndex:1513},{level:3,title:"Driver 和 Executor",slug:"driver-和-executor",normalizedTitle:"driver 和 executor",charIndex:1535},{level:2,title:"Standalone模式",slug:"standalone模式",normalizedTitle:"standalone模式",charIndex:170},{level:3,title:"配置历史服务",slug:"配置历史服务",normalizedTitle:"配置历史服务",charIndex:4078},{level:3,title:"配置高可用（HA）",slug:"配置高可用-ha",normalizedTitle:"配置高可用（ha）",charIndex:5273},{level:3,title:"运行流程",slug:"运行流程",normalizedTitle:"运行流程",charIndex:6451},{level:2,title:"Yarn模式",slug:"yarn模式",normalizedTitle:"yarn模式",charIndex:7179},{level:3,title:"配置历史服务",slug:"配置历史服务-2",normalizedTitle:"配置历史服务",charIndex:4078},{level:3,title:"配置查看历史日志",slug:"配置查看历史日志",normalizedTitle:"配置查看历史日志",charIndex:9719},{level:2,title:"运行流程",slug:"运行流程-2",normalizedTitle:"运行流程",charIndex:6451},{level:2,title:"Mesos模式",slug:"mesos模式",normalizedTitle:"mesos模式",charIndex:257},{level:2,title:"几种模式对比",slug:"几种模式对比",normalizedTitle:"几种模式对比",charIndex:11336},{level:2,title:"端口号总结",slug:"端口号总结",normalizedTitle:"端口号总结",charIndex:11539},{level:2,title:"WordCount案例",slug:"wordcount案例",normalizedTitle:"wordcount案例",charIndex:1087}],headersStr:"Local模式 官方求PI案例 官方WordCount案例 集群角色 Master 和 Worker Driver 和 Executor Standalone模式 配置历史服务 配置高可用（HA） 运行流程 Yarn模式 配置历史服务 配置查看历史日志 运行流程 Mesos模式 几种模式对比 端口号总结 WordCount案例",content:'# Spark 入门\n\n部署Spark集群大体上分为两种模式：单机模式与集群模式 大多数分布式框架都支持单机模式，方便开发者调试框架的运行环境。但是在生产环境中，并不会使用单机模式。因此，后续直接按照集群模式部署Spark集群。 下面详细列举了Spark目前支持的部署模式。\n\n 1. Local模式：在本地部署单个Spark服务\n 2. Standalone模式：Spark自带的任务调度模式。（国内常用）\n 3. YARN模式：Spark使用Hadoop的YARN组件进行资源与任务调度。（国内常用）\n 4. Mesos模式：Spark使用Mesos平台进行资源与任务的调度。\n\n1）官网地址：http://spark.apache.org/\n\n2）文档查看地址：https://spark.apache.org/docs/2.1.1/\n\n3）下载地址：https://spark.apache.org/downloads.html\n\n\n# Local模式\n\n上传并解压安装包\n\ncd /opt/software/\ntar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/\ncd /opt/module/\nmv spark-2.1.1-bin-hadoop2.7/ spark-local\ncd spark-local\n\n\n1\n2\n3\n4\n5\n\n\n\n# 官方求PI案例\n\n利用蒙特·卡罗算法求PI\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master local[2] \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n * --class：表示要执行程序的主类；\n\n * --master local[2]\n   \n   * local: 没有指定线程数，则所有计算都运行在一个线程当中，没有任何并行计算\n   \n   * local[K]:指定使用K个Core来运行计算，比如local[2]就是运行2个Core来执行\n   \n   * local[*]: 自动帮你按照CPU最多核来设置线程数。比如CPU有4核，Spark帮你自动设置4个线程计算\n\n * spark-examples_2.11-2.1.1.jar：要运行的程序；\n\n * 10：要运行程序的输入参数（计算圆周率π的次数，计算次数越多，准确率越高）；\n\n\n\n\n# 官方WordCount案例\n\n读取多个输入文件，统计每个单词出现的总次数\n\n\n\n准备数据\n\nmkdir input\necho hello world > input/1.txt\necho hello spark > input/2.txt\n\n\n1\n2\n3\n\n\n启动spark-shell\n\nbin/spark-shell\n\n\n1\n\n\n\n\nsc是SparkCore程序的入口；spark是SparkSQL程序入口；master = local[*]表示本地模式运行。\n\nsc.textFile("/opt/module/spark-local/input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect\n\n\n1\n\n\n\n\n查看web页面\n\n\n\nspark-shell窗口关闭掉，则hadoop102:4040页面关闭\n\n本地模式下，默认的调度器为FIFO。\n\n\n\n\n# 集群角色\n\n\n# Master 和 Worker\n\n\n\n\n# Driver 和 Executor\n\n\n\nMaster和Worker是Spark的守护进程，即Spark在特定模式下正常运行所必须的进程。Driver和Executor是临时程序，当有具体任务提交到Spark集群才会开启的程序\n\n\n# Standalone模式\n\nStandalone模式是Spark自带的资源调动引擎，构建一个由Master + Slave构成的Spark集群，Spark运行在集群中。\n\n这个要和Hadoop中的Standalone区别开来。这里的Standalone是指只用Spark来搭建一个集群，不需要借助其他的框架。是相对于Yarn和Mesos来说的。\n\n集群规划\n\n        HADOOP102       HADOOP103   HADOOP104\nSpark   Master Worker   Worker      Worker\n\n解压安装\n\ncd /opt/software/\ntar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/\ncd /opt/module/\nmv spark-2.1.1-bin-hadoop2.7/ spark-standalone\ncd spark-standalone\n\n\n1\n2\n3\n4\n5\n\n\n配置spark\n\ncd conf\nmv slaves.template slaves\nvim slaves\n\nhadoop102\nhadoop103\nhadoop104\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n修改spark-env.sh文件，添加master节点\n\nmv spark-env.sh.template spark-env.sh\nvim spark-env.sh\n\nSPARK_MASTER_HOST=hadoop102\nSPARK_MASTER_PORT=7077\n\n\n1\n2\n3\n4\n5\n\n\n分发\n\nxsync /opt/module/spark-standalone/\n\n\n1\n\n\n启动集群\n\ncd ..\nsbin/start-all.sh\n\n\n1\n2\n\n\n查看进程\n\njps\n\n================atguigu@hadoop102================\n3330 Jps\n3238 Worker\n3163 Master\n================atguigu@hadoop103================\n2966 Jps\n2908 Worker\n================atguigu@hadoop104================\n2978 Worker\n3036 Jps\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置\n\nexport JAVA_HOME=/opt/module/jdk1.8.0_212\n\n\n1\n\n\n网页查看 访问hadoop102:8080\n\n官方求PI案例\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://hadoop102:7077 \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n查看web页面 hadoop102:8080\n\n\n\n一共12个cores 12核 每个核1024内存\n\n\n\n我们提交任务时也可以通过属性来控制 核数和内存\n\n配置Executor可用内存为2G，使用CPU核数为2个\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://hadoop102:7077 \\\n--executor-memory 2G \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n参数                         解释                                                          可选值举例\n--class                    Spark程序中包含主函数的类                                             \n--master                   Spark程序运行的模式                                                本地模式：local[*]、spark://hadoop102:7077、 Yarn\n--executor-memory 1G       指定每个executor可用内存为1G                                         符合集群内存配置即可，具体情况具体分析。\n--total-executor-cores 2   指定所有executor使用的cpu核数为2个                                     \napplication-jar            打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file://   \n                           path，那么所有的节点的path都包含同样的jar\napplication-arguments      传给main()方法的参数                                               \n\n\n# 配置历史服务\n\n由于spark-shell停止掉后，hadoop102:4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况。\n\n先停止spark\n\nsbin/stop-all.sh\n\n\n1\n\n\n修改spark-default.conf文件\n\ncd /opt/module/spark-standalone/conf/\nmv spark-defaults.conf.template spark-defaults.conf\nvi spark-defaults.conf\n\n\n1\n2\n3\n\n\nspark.eventLog.enabled          true\nspark.eventLog.dir               hdfs://hadoop102:8020/directory\n\n\n1\n2\n\n\n分发\n\nxsync spark-defaults.conf\n\n\n1\n\n\n启动hadoop集群并且保证logdir的目录提前存在\n\nstart-dfs.sh\nhadoop fs -mkdir /directory\n\n\n1\n2\n\n\n修改spark-env.sh文件\n\nvi spark-env.sh\n\nexport SPARK_HISTORY_OPTS="\n-Dspark.history.ui.port=18080 \n-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory \n-Dspark.history.retainedApplications=30"\n\n\n1\n2\n3\n4\n5\n6\n\n\n参数1含义：WEBUI访问的端口号为18080\n\n参数2含义：指定历史服务器日志存储路径\n\n参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。\n\n分发配置文件\n\nxsync spark-env.sh\n\n\n1\n\n\n启动spark\n\nsbin/start-all.sh\n\n\n1\n\n\n启动历史服务\n\nsbin/start-history-server.sh\n\n\n1\n\n\n\n\n再执行任务\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://hadoop102:7077 \\\n--executor-memory 1G \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n访问历史服务器\n\nhttp://hadoop102:18080/\n\n\n\n\n# 配置高可用（HA）\n\n\n\n停止spark集群\n\nsbin/stop-all.sh\n\n\n1\n\n\n启动zookeeper\n\nzk.sh start\n\n\n1\n\n\n修改spark-env.sh文件添加如下配置\n\nvim conf/spark-env.sh\n\n#注释掉如下内容：\n#SPARK_MASTER_HOST=hadoop102\n#SPARK_MASTER_PORT=7077\n\n#添加上如下内容。配置由Zookeeper管理Master，在Zookeeper节点中自动创建/spark目录，用于管理：\nexport SPARK_DAEMON_JAVA_OPTS="\n-Dspark.deploy.recoveryMode=ZOOKEEPER \n-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 \n-Dspark.deploy.zookeeper.dir=/spark"\n\n#Zookeeper3.5的AdminServer默认端口是8080，和Spark的WebUI冲突 所以要把spark默认8080端口改为8989\nexport SPARK_MASTER_WEBUI_PORT=8989\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n分发配置\n\nxsync conf/spark-env.sh\n\n\n1\n\n\n启动\n\nsbin/start-all.sh\n\n\n1\n\n\n在103上单独启动master节点 一共有两个master节点\n\nsbin/start-master.sh\n\n\n1\n\n\n\n\nhadoop103上的master处于待命状态\n\n在102将sprak-local/input 数据上传到hadoop集群的/input目录\n\nhadoop fs -put /opt/module/spark-local/input/ /input\n\n\n1\n\n\nspark HA集群访问 注意master 为两个spark master\n\nbin/spark-shell \\\n--master spark://hadoop102:7077,hadoop103:7077 \\\n--executor-memory 2g \\\n--total-executor-cores 2\n\n\n1\n2\n3\n4\n\n\n执行Wordcount程序\n\nsc.textFile("hdfs://hadoop102:8020/input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect\n\n\n1\n\n\n测试高可用 在102上kill掉Master进程\n\n查看hadoop103:8989 是否从STANDBY 变为Alive\n\n\n# 运行流程\n\nSpark有standalone-client和standalone-cluster两种模式，主要区别在于：Driver程序的运行节点。\n\n客户端模式\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://hadoop102:7077,hadoop103:7077 \\\n--executor-memory 2G \\\n--total-executor-cores 2 \\\n--deploy-mode client \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n--deploy-mode client，表示Driver程序运行在本地客户端 默认为client\n\n\n\n集群模式模式\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://hadoop102:7077,hadoop103:7077 \\\n--executor-memory 2G \\\n--total-executor-cores 2 \\\n--deploy-mode cluster \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n查看http://hadoop102:8989/页面，点击Completed Drivers里面的Worker的结果\n\n\n\n\n\n\n\n\n# Yarn模式\n\nSpark客户端直接连接Yarn，不需要额外构建Spark集群。\n\n停止Standalone模式下的spark集群\n\nsbin/stop-all.sh\nsbin/stop-master.sh  #103的master\nzk.sh stop\n\n\n1\n2\n3\n\n\n解压安装\n\ncd /opt/software\ntar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/\ncd /opt/module/\nmv spark-2.1.1-bin-hadoop2.7/ spark-yarn\n\n\n1\n2\n3\n4\n\n\n修改hadoop配置文件/opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml 添加如下内容\n\n因为测试环境虚拟机内存较少，防止执行过程进行被意外杀死，做如下配置\n\nvi /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n\x3c!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --\x3e\n<property>\n     <name>yarn.nodemanager.pmem-check-enabled</name>\n     <value>false</value>\n</property>\n\n\x3c!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --\x3e\n<property>\n     <name>yarn.nodemanager.vmem-check-enabled</name>\n     <value>false</value>\n</property>\n\x3c!-- Spark2中jersey版本是2.22，但是yarn中还需要依赖1.9，版本不兼容 --\x3e\n<property>\n\t    <name>yarn.timeline-service.enabled</name>\n\t\t<value>false</value>\n</property>\t\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n分发\n\nxsync /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n修改/opt/module/spark/conf/spark-env.sh，添加YARN_CONF_DIR配置，保证后续运行任务的路径都变成集群路径\n\ncd /opt/module/spark-yarn/conf/\nmv spark-env.sh.template spark-env.sh\nvi spark-env.sh\n\nYARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop\n\n\n1\n2\n3\n4\n5\n\n\n分发spark-yarn\n\nxsync /opt/module/spark-yarn/\n\n\n1\n\n\n启动HDFS以及YARN集群\n\nstart-dfs.sh #102\nstart-yarn.sh #103\n\n\n1\n2\n\n\n执行求PI案例\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n--master yarn，表示Yarn方式运行\n\n如果遇到 WARN Utils: Service \'SparkUI\' could not bind on port 4040. Attempting port 4041. java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig 报错 上面配置yarn-site.xml已经解决此报错问题\n\n1.找到yarn下面相关包\n\nfind /usr/hdp/ |grep jersey\n\n\n1\n\n\n2.拷贝jar到spark\n\n所缺的类在 jersey-core-1.9.jar 和 jersey-client-1.9.jar 两个jar包中 将 jersey-core-1.9.jar 和 jersey-client-1.9.jar 这两个包拷贝到$SPARK_HOME/jars目录下\n\n\n# 配置历史服务\n\n由于是重新解压的Spark压缩文件，所以需要针对Yarn模式，再次配置一下历史服务器。\n\n修改spark-default.conf.template\n\ncd /opt/module/spark-yarn/conf/\nmv spark-defaults.conf.template spark-defaults.conf\nvi spark-defaults.conf\n\n#配置spark历史服务\nspark.eventLog.enabled          true\nspark.eventLog.dir               hdfs://hadoop102:8020/directory\n\nxsync spark-defaults.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n修改 spark-env.sh配置\n\nvi spark-env.sh\n\nexport SPARK_HISTORY_OPTS="\n-Dspark.history.ui.port=18080 \n-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory \n-Dspark.history.retainedApplications=30"\n\nxsync spark-env.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 配置查看历史日志\n\n为了从Yarn上关联到Spark历史服务器，需要配置关联路径\n\n修改配置文件/opt/module/spark/conf/spark-defaults.conf\n\nvim /opt/module/spark-yarn/conf/spark-defaults.conf\n\nspark.yarn.historyServer.address=hadoop102:18080\nspark.history.ui.port=18080\n\nxsync /opt/module/spark-yarn/conf/spark-defaults.conf\n\n\n1\n2\n3\n4\n5\n6\n\n\n启动spark历史服务\n\ncd /opt/module/spark-yarn/\nsbin/start-history-server.sh \n\n\n1\n2\n\n\n重新提交任务\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n查询http://hadoop103:8088/cluster\n\n\n\n\n# 运行流程\n\nSpark有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。\n\nyarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。\n\nyarn-cluster：Driver程序运行在由ResourceManager启动的APPMaster适用于生产环境。\n\n客户端模式（默认）\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode client \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n集群模式\n\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\n默认无法访问需要在yarn-site.xml添加配置并启动yarn历史服务器\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n<property>\n    <name>yarn.log.server.url</name>\n    <value>http://hadoop104:19888/jobhistory/logs</value>\n</property>\n\n\n1\n2\n3\n4\n\n\n启动历史服务器\n\nmapred --daemon start historyserver\n\n\n1\n\n\nhttp://hadoop102:19888/jobhistory/logs/hadoop103:44236/container_1639655468064_0005_01_000001/container_1639655468064_0005_01_000001/atguigu/stdout?start=-4096\n\n\n\n\n\n\n# Mesos模式\n\nSpark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用Yarn调度。\n\n\n# 几种模式对比\n\n模式           SPARK安装机器数   需启动的进程          所属者\nLocal        1            无               Spark\nStandalone   3            Master及Worker   Spark\nYarn         1            Yarn及HDFS       Hadoop\n\n\n# 端口号总结\n\n 1. Spark历史服务器端口号：18080 （类比于Hadoop历史服务器端口号：19888）\n 2. Spark Master Web端口号：8080（类比于Hadoop的NameNode Web端口号：9870(50070)）\n 3. Spark Master内部通信服务端口号：7077 （类比于Hadoop的8020(9000)端口）\n 4. Spark查看当前Spark-shell运行任务情况端口号：4040\n 5. Hadoop YARN任务运行情况查看端口号：8088\n\n\n# WordCount案例\n\nSpark Shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成Jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理Jar包的依赖。\n\n新建maven项目 并添加scala框架支持 导入pom文件\n\n<dependencies>\n    <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-core_2.11</artifactId>\n        <version>2.1.1</version>\n    </dependency>\n</dependencies>\n<build>\n    <finalName>WordCount</finalName>\n    <plugins>\n        <plugin>\n            <groupId>net.alchim31.maven</groupId>\n            <artifactId>scala-maven-plugin</artifactId>\n            <version>4.5.3</version>\n            <executions>\n                <execution>\n                   <goals>\n                      <goal>compile</goal>\n                      <goal>testCompile</goal>\n                   </goals>\n                </execution>\n             </executions>\n        </plugin>\n    </plugins>\n</build>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n如果maven版本为3.2.x，插件下载报错，那么修改插件版本为3.3.2\n\n创建伴生对象WordCount，编写代码\n\npackage com.atguigu.spark.day01\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject WordCount {\n  def main(args: Array[String]): Unit = {\n    //创建SparckConfig配置文件\n    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("WordCount")\n    //创建sparkContext对象\n    val sc: SparkContext = new SparkContext(conf)\n    //读取外部数据\n    val textRDD: RDD[String] = sc.textFile(args(0))\n    //对读取到的内容进行切割并进行扁平化操作\n    val flatMapRDD: RDD[String] = textRDD.flatMap(_.split(" "))\n    //对数据集中的内容进行结构的转换 -- 计数\n    val mapRDD: RDD[(String, Int)] = flatMapRDD.map((_, 1))\n    //对相同单词 出现次数进行汇总\n    val reduceRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_ + _)\n    //存储为文件\n    reduceRDD.saveAsTextFile(args(1))\n//    val res: Array[(String, Int)] = reduceRDD.collect()\n    \n    //释放资源\n    sc.stop()\n  }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n本地运行\n\n\n\n添加打包插件\n\n     <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <version>3.0.0</version>\n                <configuration>\n                    <archive>\n                        <manifest>\n                            <mainClass>com.atguigu.spark.day01.WordCount</mainClass>\n                        </manifest>\n                    </archive>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nmaven点击package打包，将WordCount.jar(不带依赖)上传到/opt/module/spark-yarn目录\n\n在HDFS上创建，存储输入数据文件的路径/input\n\nhadoop fs -mkdir /input\nhadoop fs -put /opt/module/spark-local-standalone/input/1.txt /input\n\n\n1\n2\n\n\n执行任务\n\ncd /opt/module/spark-yarn\nbin/spark-submit \\\n--class com.atguigu.spark.day01.WordCount \\\n--master yarn \\\nWordCount.jar \\\n/input \\\n/output\n\n\n1\n2\n3\n4\n5\n6\n7\n',normalizedContent:'# spark 入门\n\n部署spark集群大体上分为两种模式：单机模式与集群模式 大多数分布式框架都支持单机模式，方便开发者调试框架的运行环境。但是在生产环境中，并不会使用单机模式。因此，后续直接按照集群模式部署spark集群。 下面详细列举了spark目前支持的部署模式。\n\n 1. local模式：在本地部署单个spark服务\n 2. standalone模式：spark自带的任务调度模式。（国内常用）\n 3. yarn模式：spark使用hadoop的yarn组件进行资源与任务调度。（国内常用）\n 4. mesos模式：spark使用mesos平台进行资源与任务的调度。\n\n1）官网地址：http://spark.apache.org/\n\n2）文档查看地址：https://spark.apache.org/docs/2.1.1/\n\n3）下载地址：https://spark.apache.org/downloads.html\n\n\n# local模式\n\n上传并解压安装包\n\ncd /opt/software/\ntar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -c /opt/module/\ncd /opt/module/\nmv spark-2.1.1-bin-hadoop2.7/ spark-local\ncd spark-local\n\n\n1\n2\n3\n4\n5\n\n\n\n# 官方求pi案例\n\n利用蒙特·卡罗算法求pi\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master local[2] \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n * --class：表示要执行程序的主类；\n\n * --master local[2]\n   \n   * local: 没有指定线程数，则所有计算都运行在一个线程当中，没有任何并行计算\n   \n   * local[k]:指定使用k个core来运行计算，比如local[2]就是运行2个core来执行\n   \n   * local[*]: 自动帮你按照cpu最多核来设置线程数。比如cpu有4核，spark帮你自动设置4个线程计算\n\n * spark-examples_2.11-2.1.1.jar：要运行的程序；\n\n * 10：要运行程序的输入参数（计算圆周率π的次数，计算次数越多，准确率越高）；\n\n\n\n\n# 官方wordcount案例\n\n读取多个输入文件，统计每个单词出现的总次数\n\n\n\n准备数据\n\nmkdir input\necho hello world > input/1.txt\necho hello spark > input/2.txt\n\n\n1\n2\n3\n\n\n启动spark-shell\n\nbin/spark-shell\n\n\n1\n\n\n\n\nsc是sparkcore程序的入口；spark是sparksql程序入口；master = local[*]表示本地模式运行。\n\nsc.textfile("/opt/module/spark-local/input").flatmap(_.split(" ")).map((_,1)).reducebykey(_+_).collect\n\n\n1\n\n\n\n\n查看web页面\n\n\n\nspark-shell窗口关闭掉，则hadoop102:4040页面关闭\n\n本地模式下，默认的调度器为fifo。\n\n\n\n\n# 集群角色\n\n\n# master 和 worker\n\n\n\n\n# driver 和 executor\n\n\n\nmaster和worker是spark的守护进程，即spark在特定模式下正常运行所必须的进程。driver和executor是临时程序，当有具体任务提交到spark集群才会开启的程序\n\n\n# standalone模式\n\nstandalone模式是spark自带的资源调动引擎，构建一个由master + slave构成的spark集群，spark运行在集群中。\n\n这个要和hadoop中的standalone区别开来。这里的standalone是指只用spark来搭建一个集群，不需要借助其他的框架。是相对于yarn和mesos来说的。\n\n集群规划\n\n        hadoop102       hadoop103   hadoop104\nspark   master worker   worker      worker\n\n解压安装\n\ncd /opt/software/\ntar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -c /opt/module/\ncd /opt/module/\nmv spark-2.1.1-bin-hadoop2.7/ spark-standalone\ncd spark-standalone\n\n\n1\n2\n3\n4\n5\n\n\n配置spark\n\ncd conf\nmv slaves.template slaves\nvim slaves\n\nhadoop102\nhadoop103\nhadoop104\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n修改spark-env.sh文件，添加master节点\n\nmv spark-env.sh.template spark-env.sh\nvim spark-env.sh\n\nspark_master_host=hadoop102\nspark_master_port=7077\n\n\n1\n2\n3\n4\n5\n\n\n分发\n\nxsync /opt/module/spark-standalone/\n\n\n1\n\n\n启动集群\n\ncd ..\nsbin/start-all.sh\n\n\n1\n2\n\n\n查看进程\n\njps\n\n================atguigu@hadoop102================\n3330 jps\n3238 worker\n3163 master\n================atguigu@hadoop103================\n2966 jps\n2908 worker\n================atguigu@hadoop104================\n2978 worker\n3036 jps\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n如果遇到 “java_home not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置\n\nexport java_home=/opt/module/jdk1.8.0_212\n\n\n1\n\n\n网页查看 访问hadoop102:8080\n\n官方求pi案例\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master spark://hadoop102:7077 \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n查看web页面 hadoop102:8080\n\n\n\n一共12个cores 12核 每个核1024内存\n\n\n\n我们提交任务时也可以通过属性来控制 核数和内存\n\n配置executor可用内存为2g，使用cpu核数为2个\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master spark://hadoop102:7077 \\\n--executor-memory 2g \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n参数                         解释                                                          可选值举例\n--class                    spark程序中包含主函数的类                                             \n--master                   spark程序运行的模式                                                本地模式：local[*]、spark://hadoop102:7077、 yarn\n--executor-memory 1g       指定每个executor可用内存为1g                                         符合集群内存配置即可，具体情况具体分析。\n--total-executor-cores 2   指定所有executor使用的cpu核数为2个                                     \napplication-jar            打包好的应用jar，包含依赖。这个url在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file://   \n                           path，那么所有的节点的path都包含同样的jar\napplication-arguments      传给main()方法的参数                                               \n\n\n# 配置历史服务\n\n由于spark-shell停止掉后，hadoop102:4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况。\n\n先停止spark\n\nsbin/stop-all.sh\n\n\n1\n\n\n修改spark-default.conf文件\n\ncd /opt/module/spark-standalone/conf/\nmv spark-defaults.conf.template spark-defaults.conf\nvi spark-defaults.conf\n\n\n1\n2\n3\n\n\nspark.eventlog.enabled          true\nspark.eventlog.dir               hdfs://hadoop102:8020/directory\n\n\n1\n2\n\n\n分发\n\nxsync spark-defaults.conf\n\n\n1\n\n\n启动hadoop集群并且保证logdir的目录提前存在\n\nstart-dfs.sh\nhadoop fs -mkdir /directory\n\n\n1\n2\n\n\n修改spark-env.sh文件\n\nvi spark-env.sh\n\nexport spark_history_opts="\n-dspark.history.ui.port=18080 \n-dspark.history.fs.logdirectory=hdfs://hadoop102:8020/directory \n-dspark.history.retainedapplications=30"\n\n\n1\n2\n3\n4\n5\n6\n\n\n参数1含义：webui访问的端口号为18080\n\n参数2含义：指定历史服务器日志存储路径\n\n参数3含义：指定保存application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。\n\n分发配置文件\n\nxsync spark-env.sh\n\n\n1\n\n\n启动spark\n\nsbin/start-all.sh\n\n\n1\n\n\n启动历史服务\n\nsbin/start-history-server.sh\n\n\n1\n\n\n\n\n再执行任务\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master spark://hadoop102:7077 \\\n--executor-memory 1g \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n访问历史服务器\n\nhttp://hadoop102:18080/\n\n\n\n\n# 配置高可用（ha）\n\n\n\n停止spark集群\n\nsbin/stop-all.sh\n\n\n1\n\n\n启动zookeeper\n\nzk.sh start\n\n\n1\n\n\n修改spark-env.sh文件添加如下配置\n\nvim conf/spark-env.sh\n\n#注释掉如下内容：\n#spark_master_host=hadoop102\n#spark_master_port=7077\n\n#添加上如下内容。配置由zookeeper管理master，在zookeeper节点中自动创建/spark目录，用于管理：\nexport spark_daemon_java_opts="\n-dspark.deploy.recoverymode=zookeeper \n-dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 \n-dspark.deploy.zookeeper.dir=/spark"\n\n#zookeeper3.5的adminserver默认端口是8080，和spark的webui冲突 所以要把spark默认8080端口改为8989\nexport spark_master_webui_port=8989\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n分发配置\n\nxsync conf/spark-env.sh\n\n\n1\n\n\n启动\n\nsbin/start-all.sh\n\n\n1\n\n\n在103上单独启动master节点 一共有两个master节点\n\nsbin/start-master.sh\n\n\n1\n\n\n\n\nhadoop103上的master处于待命状态\n\n在102将sprak-local/input 数据上传到hadoop集群的/input目录\n\nhadoop fs -put /opt/module/spark-local/input/ /input\n\n\n1\n\n\nspark ha集群访问 注意master 为两个spark master\n\nbin/spark-shell \\\n--master spark://hadoop102:7077,hadoop103:7077 \\\n--executor-memory 2g \\\n--total-executor-cores 2\n\n\n1\n2\n3\n4\n\n\n执行wordcount程序\n\nsc.textfile("hdfs://hadoop102:8020/input").flatmap(_.split(" ")).map((_,1)).reducebykey(_+_).collect\n\n\n1\n\n\n测试高可用 在102上kill掉master进程\n\n查看hadoop103:8989 是否从standby 变为alive\n\n\n# 运行流程\n\nspark有standalone-client和standalone-cluster两种模式，主要区别在于：driver程序的运行节点。\n\n客户端模式\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master spark://hadoop102:7077,hadoop103:7077 \\\n--executor-memory 2g \\\n--total-executor-cores 2 \\\n--deploy-mode client \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n--deploy-mode client，表示driver程序运行在本地客户端 默认为client\n\n\n\n集群模式模式\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master spark://hadoop102:7077,hadoop103:7077 \\\n--executor-memory 2g \\\n--total-executor-cores 2 \\\n--deploy-mode cluster \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n查看http://hadoop102:8989/页面，点击completed drivers里面的worker的结果\n\n\n\n\n\n\n\n\n# yarn模式\n\nspark客户端直接连接yarn，不需要额外构建spark集群。\n\n停止standalone模式下的spark集群\n\nsbin/stop-all.sh\nsbin/stop-master.sh  #103的master\nzk.sh stop\n\n\n1\n2\n3\n\n\n解压安装\n\ncd /opt/software\ntar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -c /opt/module/\ncd /opt/module/\nmv spark-2.1.1-bin-hadoop2.7/ spark-yarn\n\n\n1\n2\n3\n4\n\n\n修改hadoop配置文件/opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml 添加如下内容\n\n因为测试环境虚拟机内存较少，防止执行过程进行被意外杀死，做如下配置\n\nvi /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n\x3c!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --\x3e\n<property>\n     <name>yarn.nodemanager.pmem-check-enabled</name>\n     <value>false</value>\n</property>\n\n\x3c!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --\x3e\n<property>\n     <name>yarn.nodemanager.vmem-check-enabled</name>\n     <value>false</value>\n</property>\n\x3c!-- spark2中jersey版本是2.22，但是yarn中还需要依赖1.9，版本不兼容 --\x3e\n<property>\n\t    <name>yarn.timeline-service.enabled</name>\n\t\t<value>false</value>\n</property>\t\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n分发\n\nxsync /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n修改/opt/module/spark/conf/spark-env.sh，添加yarn_conf_dir配置，保证后续运行任务的路径都变成集群路径\n\ncd /opt/module/spark-yarn/conf/\nmv spark-env.sh.template spark-env.sh\nvi spark-env.sh\n\nyarn_conf_dir=/opt/module/hadoop-3.1.3/etc/hadoop\n\n\n1\n2\n3\n4\n5\n\n\n分发spark-yarn\n\nxsync /opt/module/spark-yarn/\n\n\n1\n\n\n启动hdfs以及yarn集群\n\nstart-dfs.sh #102\nstart-yarn.sh #103\n\n\n1\n2\n\n\n执行求pi案例\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master yarn \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n--master yarn，表示yarn方式运行\n\n如果遇到 warn utils: service \'sparkui\' could not bind on port 4040. attempting port 4041. java.lang.noclassdeffounderror: com/sun/jersey/api/client/config/clientconfig 报错 上面配置yarn-site.xml已经解决此报错问题\n\n1.找到yarn下面相关包\n\nfind /usr/hdp/ |grep jersey\n\n\n1\n\n\n2.拷贝jar到spark\n\n所缺的类在 jersey-core-1.9.jar 和 jersey-client-1.9.jar 两个jar包中 将 jersey-core-1.9.jar 和 jersey-client-1.9.jar 这两个包拷贝到$spark_home/jars目录下\n\n\n# 配置历史服务\n\n由于是重新解压的spark压缩文件，所以需要针对yarn模式，再次配置一下历史服务器。\n\n修改spark-default.conf.template\n\ncd /opt/module/spark-yarn/conf/\nmv spark-defaults.conf.template spark-defaults.conf\nvi spark-defaults.conf\n\n#配置spark历史服务\nspark.eventlog.enabled          true\nspark.eventlog.dir               hdfs://hadoop102:8020/directory\n\nxsync spark-defaults.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n修改 spark-env.sh配置\n\nvi spark-env.sh\n\nexport spark_history_opts="\n-dspark.history.ui.port=18080 \n-dspark.history.fs.logdirectory=hdfs://hadoop102:8020/directory \n-dspark.history.retainedapplications=30"\n\nxsync spark-env.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 配置查看历史日志\n\n为了从yarn上关联到spark历史服务器，需要配置关联路径\n\n修改配置文件/opt/module/spark/conf/spark-defaults.conf\n\nvim /opt/module/spark-yarn/conf/spark-defaults.conf\n\nspark.yarn.historyserver.address=hadoop102:18080\nspark.history.ui.port=18080\n\nxsync /opt/module/spark-yarn/conf/spark-defaults.conf\n\n\n1\n2\n3\n4\n5\n6\n\n\n启动spark历史服务\n\ncd /opt/module/spark-yarn/\nsbin/start-history-server.sh \n\n\n1\n2\n\n\n重新提交任务\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master yarn \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n\n\n查询http://hadoop103:8088/cluster\n\n\n\n\n# 运行流程\n\nspark有yarn-client和yarn-cluster两种模式，主要区别在于：driver程序的运行节点。\n\nyarn-client：driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。\n\nyarn-cluster：driver程序运行在由resourcemanager启动的appmaster适用于生产环境。\n\n客户端模式（默认）\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master yarn \\\n--deploy-mode client \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n集群模式\n\nbin/spark-submit \\\n--class org.apache.spark.examples.sparkpi \\\n--master yarn \\\n--deploy-mode cluster \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n10\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\n默认无法访问需要在yarn-site.xml添加配置并启动yarn历史服务器\n\nvim /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml\n\n\n1\n\n\n<property>\n    <name>yarn.log.server.url</name>\n    <value>http://hadoop104:19888/jobhistory/logs</value>\n</property>\n\n\n1\n2\n3\n4\n\n\n启动历史服务器\n\nmapred --daemon start historyserver\n\n\n1\n\n\nhttp://hadoop102:19888/jobhistory/logs/hadoop103:44236/container_1639655468064_0005_01_000001/container_1639655468064_0005_01_000001/atguigu/stdout?start=-4096\n\n\n\n\n\n\n# mesos模式\n\nspark客户端直接连接mesos；不需要额外构建spark集群。国内应用比较少，更多的是运用yarn调度。\n\n\n# 几种模式对比\n\n模式           spark安装机器数   需启动的进程          所属者\nlocal        1            无               spark\nstandalone   3            master及worker   spark\nyarn         1            yarn及hdfs       hadoop\n\n\n# 端口号总结\n\n 1. spark历史服务器端口号：18080 （类比于hadoop历史服务器端口号：19888）\n 2. spark master web端口号：8080（类比于hadoop的namenode web端口号：9870(50070)）\n 3. spark master内部通信服务端口号：7077 （类比于hadoop的8020(9000)端口）\n 4. spark查看当前spark-shell运行任务情况端口号：4040\n 5. hadoop yarn任务运行情况查看端口号：8088\n\n\n# wordcount案例\n\nspark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在ide中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个maven项目，利用maven来管理jar包的依赖。\n\n新建maven项目 并添加scala框架支持 导入pom文件\n\n<dependencies>\n    <dependency>\n        <groupid>org.apache.spark</groupid>\n        <artifactid>spark-core_2.11</artifactid>\n        <version>2.1.1</version>\n    </dependency>\n</dependencies>\n<build>\n    <finalname>wordcount</finalname>\n    <plugins>\n        <plugin>\n            <groupid>net.alchim31.maven</groupid>\n            <artifactid>scala-maven-plugin</artifactid>\n            <version>4.5.3</version>\n            <executions>\n                <execution>\n                   <goals>\n                      <goal>compile</goal>\n                      <goal>testcompile</goal>\n                   </goals>\n                </execution>\n             </executions>\n        </plugin>\n    </plugins>\n</build>\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n如果maven版本为3.2.x，插件下载报错，那么修改插件版本为3.3.2\n\n创建伴生对象wordcount，编写代码\n\npackage com.atguigu.spark.day01\n\nimport org.apache.spark.rdd.rdd\nimport org.apache.spark.{sparkconf, sparkcontext}\n\nobject wordcount {\n  def main(args: array[string]): unit = {\n    //创建sparckconfig配置文件\n    val conf: sparkconf = new sparkconf().setmaster("local[*]").setappname("wordcount")\n    //创建sparkcontext对象\n    val sc: sparkcontext = new sparkcontext(conf)\n    //读取外部数据\n    val textrdd: rdd[string] = sc.textfile(args(0))\n    //对读取到的内容进行切割并进行扁平化操作\n    val flatmaprdd: rdd[string] = textrdd.flatmap(_.split(" "))\n    //对数据集中的内容进行结构的转换 -- 计数\n    val maprdd: rdd[(string, int)] = flatmaprdd.map((_, 1))\n    //对相同单词 出现次数进行汇总\n    val reducerdd: rdd[(string, int)] = maprdd.reducebykey(_ + _)\n    //存储为文件\n    reducerdd.saveastextfile(args(1))\n//    val res: array[(string, int)] = reducerdd.collect()\n    \n    //释放资源\n    sc.stop()\n  }\n\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n本地运行\n\n\n\n添加打包插件\n\n     <plugin>\n                <groupid>org.apache.maven.plugins</groupid>\n                <artifactid>maven-assembly-plugin</artifactid>\n                <version>3.0.0</version>\n                <configuration>\n                    <archive>\n                        <manifest>\n                            <mainclass>com.atguigu.spark.day01.wordcount</mainclass>\n                        </manifest>\n                    </archive>\n                    <descriptorrefs>\n                        <descriptorref>jar-with-dependencies</descriptorref>\n                    </descriptorrefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nmaven点击package打包，将wordcount.jar(不带依赖)上传到/opt/module/spark-yarn目录\n\n在hdfs上创建，存储输入数据文件的路径/input\n\nhadoop fs -mkdir /input\nhadoop fs -put /opt/module/spark-local-standalone/input/1.txt /input\n\n\n1\n2\n\n\n执行任务\n\ncd /opt/module/spark-yarn\nbin/spark-submit \\\n--class com.atguigu.spark.day01.wordcount \\\n--master yarn \\\nwordcount.jar \\\n/input \\\n/output\n\n\n1\n2\n3\n4\n5\n6\n7\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"函数式编程",frontmatter:{title:"函数式编程",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/b7cb84/",categories:["大数据","Scala"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/08.Scala/04.%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B.html",relativePath:"大数据/08.Scala/04.函数式编程.md",key:"v-fc416092",path:"/pages/b7cb84/",headers:[{level:2,title:"函数和方法的区别",slug:"函数和方法的区别",normalizedTitle:"函数和方法的区别",charIndex:115},{level:2,title:"函数定义",slug:"函数定义",normalizedTitle:"函数定义",charIndex:950},{level:2,title:"函数参数",slug:"函数参数",normalizedTitle:"函数参数",charIndex:1724},{level:2,title:"函数至简原则（重点）",slug:"函数至简原则-重点",normalizedTitle:"函数至简原则（重点）",charIndex:2800},{level:2,title:"函数高阶用法",slug:"函数高阶用法",normalizedTitle:"函数高阶用法",charIndex:4825},{level:2,title:"匿名函数",slug:"匿名函数",normalizedTitle:"匿名函数",charIndex:5549},{level:2,title:"函数柯里化和闭包",slug:"函数柯里化和闭包",normalizedTitle:"函数柯里化和闭包",charIndex:7369},{level:2,title:"递归",slug:"递归",normalizedTitle:"递归",charIndex:8068},{level:2,title:"控制抽象",slug:"控制抽象",normalizedTitle:"控制抽象",charIndex:8546}],headersStr:"函数和方法的区别 函数定义 函数参数 函数至简原则（重点） 函数高阶用法 匿名函数 函数柯里化和闭包 递归 控制抽象",content:'# 函数式编程\n\n解决问题时，将问题分解成一个一个的步骤，将每个步骤进行封装（函数），通过调用这些封装好的步骤，解决问题。\n\nScala语言是一个完全函数式编程语言。万物皆函数。\n\n函数的本质：函数可以当做一个值进行传递\n\n\n# 函数和方法的区别\n\n 1. 核心概念\n    1. 为完成某一功能的程序语句的集合，称为函数。\n    2. 类中的函数称之方法。\n    3. 定义在方法或者函数内部 称为函数\n\n（1）Scala语言可以在任何 的语法结构中声明任何的语法\n\n（2）函数没有重载和重写的概念；方法可以进行重载和重写\n\n（3）Scala中函数可以嵌套定义\n\nobject TestFunction {\n\n    // (2)方法可以进行重载和重写，程序可以执行\n    def main(): Unit = {\n\n    }\n\n    def main(args: Array[String]): Unit = {\n        // （1）Scala语言可以在任何的语法结构中声明任何的语法\n        import java.util.Date\n        new Date()\n\n        // (2)函数没有重载和重写的概念，程序报错\n        def test(): Unit ={\n            println("无参，无返回值")\n        }\n        test()\n\n        def test(name:String):Unit={\n            println()\n        }\n\n        //（3）Scala中函数可以嵌套定义\n        def test2(): Unit ={\n\n            def test3(name:String):Unit={\n                println("函数可以嵌套定义")\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# 函数定义\n\n def main(args: Array[String]): Unit = {\n\n        // 函数1：无参，无返回值\n        def test1(): Unit ={\n            println("无参，无返回值")\n        }\n        test1()\n\n        // 函数2：无参，有返回值\n        def test2():String={\n            return "无参，有返回值"\n        }\n        println(test2())\n\n        // 函数3：有参，无返回值\n        def test3(s:String):Unit={\n            println(s)\n        }\n        test3("jinlian")\n\n        // 函数4：有参，有返回值\n        def test4(s:String):String={\n            return s+"有参，有返回值"\n        }\n        println(test4("hello "))\n\n\n        // 函数5：多参，无返回值\n        def test5(name:String, age:Int):Unit={\n            println(s"$name, $age")\n        }\n        test5("dalang",40)\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 函数参数\n\n 1. 可变参数\n 2. 如果参数列表中存在多个参数，那么可变参数一般放置在最后\n 3. 参数默认值，一般将有默认值的参数放置在参数列表的后面\n 4. 带名参数\n\n // （1）可变参数\n        def test( s : String* ): Unit = {\n            println(s)\n        }\n\n        // 有输入参数：输出 Array\n        test("Hello", "Scala")\n\n        // 无输入参数：输出List()\n        test()\n\n        // (2)如果参数列表中存在多个参数，那么可变参数一般放置在最后\n        def test2( name : String, s: String* ): Unit = {\n            println(name + "," + s)\n        }\n\n        test2("jinlian", "dalang")\n\n        // (3)参数默认值\n        def test3( name : String, age : Int = 30 ): Unit = {\n            println(s"$name, $age")\n        }\n\n        // 如果参数传递了值，那么会覆盖默认值\n        test3("jinlian", 20)\n\n        // 如果参数有默认值，在调用的时候，可以省略这个参数\n        test3("dalang")\n\n        // 一般情况下，将有默认值的参数放置在参数列表的后面\n        def test4( sex : String = "男", name : String ): Unit =      {\n            println(s"$name, $sex")\n        }\n\t\t// Scala函数中参数传递是，从左到右\n        //test4("wusong") \n\n        //（4）带名参数\n        test4(name="ximenqing")\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 函数至简原则（重点）\n\n函数至简原则：能省则省\n\n 1. return可以省略，Scala会使用函数体的最后一行代码作为返回值\n 2. 如果函数体只有一行代码，可以省略花括号\n 3. 返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）\n 4. 如果有return，则不能省略返回值类型，必须指定\n 5. 如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用\n 6. Scala如果期望是无返回值类型，可以省略等号\n 7. 如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加\n 8. 如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略\n 9. 如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略\n\nobject TestFunction {\n\n    def main(args: Array[String]): Unit = {\n        \n       \n\n        // （0）函数标准写法\n        def f( s : String ): String = {\n            return s + " jinlian"\n        }\n        println(f("Hello"))\n\n        // 至简原则:能省则省\n\n        //（1） return可以省略,Scala会使用函数体的最后一行代码作为返回值\n        def f1( s : String ): String =  {\n            s + " jinlian"\n        }\n        println(f1("Hello"))\n\n        //（2）如果函数体只有一行代码，可以省略花括号\n        def f2(s:String):String = s + " jinlian"\n\n\n        //（3）返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）\n        def f3( s : String ) = s + " jinlian"\n        println(f3("Hello3"))\n\n        //（4）如果有return，则不能省略返回值类型，必须指定。\n        def f4() :String = {\n            return "ximenqing4"\n        }\n        println(f4())\n\n        //（5）如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用\n        def f5(): Unit = {\n            return "dalang5"\n        }\n        println(f5())\n\n        //（6）Scala如果期望是无返回值类型,可以省略等号\n        // 将无返回值的函数称之为过程\n        def f6() {\n            "dalang6"\n        }\n        println(f6())\n\n        //（7）如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加\n        def f7() = "dalang7"\n        println(f7())\n        println(f7)\n\n        //（8）如果函数没有参数列表，那么小括号可以省略,调用时小括号必须省略\n        def f8 = "dalang"\n        //println(f8())\n        println(f8)\n\n        //（9）如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略\n        def f9 = (x:String)=>{println("wusong")}\n\n        def f10(f:String=>Unit) = {\n            f("")\n        }\n\n        f10(f9)\n        println(f10((x:String)=>{println("wusong")}))\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n\n# 函数高阶用法\n\n函数可以作为值进行传递\n\n//（1）调用foo函数，把返回值给变量f\n\n        //val f = foo()\n        val f = foo\n        println(f)\n\n        //（2）在被调用函数foo后面加上 _，相当于把函数foo当成一个整体，传递给变量f1\n        val f1 = foo _\n\n        foo()\n        f1()\n\t\t//（3）如果明确变量类型，那么不使用下划线也可以将函数作为整体传递给变量\n\t\tvar f2:()=>Int = foo \n    }\n\n    def foo():Int = {\n        println("foo...")\n        1\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n函数可以作为参数进行传递\n\n // （1）定义一个函数，函数参数还是一个函数签名；f表示函数名称;(Int,Int)表示输入两个Int参数；Int表示函数返回值\n    def f1(f: (Int, Int) => Int): Int = {\n        f(2, 4)\n    }\n    \n    // （2）定义一个函数，参数和返回值类型和f1的输入参数一致\n    def add(a: Int, b: Int): Int = a + b\n    \n    // （3）将add函数作为参数传递给f1函数，如果能够推断出来不是调用，_可以省略\n    println(f1(add))\nprintln(f1(add _))\n//可以传递匿名函数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n函数可以作为函数返回值返回\n\ndef main(args: Array[String]): Unit = {\n    def f1() = {\n        def f2() = {\n\n}\nf2 _\n}\n\nval f = f1()\n// 因为f1函数的返回值依然为函数，所以可以变量f可以作为函数继续调用\nf()\n// 上面的代码可以简化为\nf1()()\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 匿名函数\n\n（1）参数的类型可以省略，会根据形参进行自动的推导\n\n（2）类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参数超过1的永远不能省略圆括号。\n\n（3）匿名函数如果只有一行，则大括号也可以省略\n\n（4）如果参数只出现一次，则参数省略且后面参数可以用_代替\n\n（5）如果可以推断出，println是一个函数体，而不是调用语句，那么(_)可以省略\n\ndef main(args: Array[String]): Unit = {\n\n        // （1）定义一个函数：参数包含数据和逻辑函数\n        def operation(arr: Array[Int], op: Int => Int) = {\n            for (elem <- arr) yield op(elem)\n        }\n\n        // （2）定义逻辑函数\n        def op(ele: Int): Int = {\n            ele + 1\n        }\n\n        // （3）标准函数调用\n        val arr = operation(Array(1, 2, 3, 4), op)\n        println(arr.mkString(","))\n\n        // （4）采用匿名函数\n        val arr1 = operation(Array(1, 2, 3, 4), (ele: Int) => {\n            ele + 1\n        })\n        println(arr1.mkString(","))\n\n        // （4.1）参数的类型可以省略，会根据形参进行自动的推导;\n        val arr2 = operation(Array(1, 2, 3, 4), (ele) => {\n            ele + 1\n        })\n        println(arr2.mkString(","))\n\n        // （4.2）类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参数超过1的永远不能省略圆括号。\n        val arr3 = operation(Array(1, 2, 3, 4), ele => {\n            ele + 1\n        })\n        println(arr3.mkString(","))\n\n        // (4.3) 匿名函数如果只有一行，则大括号也可以省略\n        val arr4 = operation(Array(1, 2, 3, 4), ele => ele + 1)\n        println(arr4.mkString(","))\n\n        //（4.4）如果参数只出现一次，则参数省略且后面参数可以用_代替\n        val arr5 = operation(Array(1, 2, 3, 4), _ + 1)\n        println(arr5.mkString(","))\n    \t\n    \t//（5）如果可以推断出，println是一个函数体，而不是调用语句，那么(_)可以省略\n    \t\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# 函数柯里化和闭包\n\n闭包：如果一个函数，访问到了它的外部（局部）变量的值，那么这个函数和他所处的环境，称为闭包\n\n函数柯里化：把一个参数列表的多个参数，变成多个参数列表。\n\nobject TestFunction {\n\n    def main(args: Array[String]): Unit = {\n        def f1()={\n\t\t\tvar a:Int = 10\n            def f2(b:Int)={\n                a + b\n            }\n            f2 _\n        }\n\n        // 在调用时，f1函数执行完毕后，局部变量a应该随着栈空间释放掉\n        val f = f1() \n\n        // 但是在此处，变量a其实并没有释放，而是包含在了f2函数的内部，形成了闭合的效果\n        println(f(3))\n\n        \n        println(f1()(3))\n\n        // 函数柯里化，其实就是将复杂的参数逻辑变得简单化,函数柯里化一定存在闭包\n        def f3()(b:Int)={\n             a + b\n        }\n\n        println(f3()(3))\n        \n        \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 递归\n\n一个函数/方法在函数/方法体内又调用了本身，我们称之为递归调用\n\nobject TestFunction {\n\n    def main(args: Array[String]): Unit = {\n\n        // 阶乘\n        // 递归算法\n        // 1) 方法调用自身\n        // 2) 方法必须要有跳出的逻辑\n        // 3) 方法调用自身时，传递的参数应该有规律\n        // 4) scala中的递归必须声明函数返回值类型\n\n        println(test(5))\n    }\n\n    def test(i : Int) : Int = {\n        if (i == 1) {\n            1\n        } else {\n            i * test(I - 1)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 控制抽象\n\n值调用：把函数运行后的值传递过去\n\nobject TestControl {\n\n    def main(args: Array[String]): Unit = {\n\n        def f = ()=>{\n            println("f...")\n            10\n        }\n\n        foo(f())\n    }\n\n    def foo(a: Int):Unit = {\n        println(a)\n        println(a)\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n名调用：把代码块传递过去\n\nobject TestControl {\n\n    def main(args: Array[String]): Unit = {\n\n        def f = ()=>{\n            println("f...")\n            10\n        }\n\n        foo(f())\n    }\n\n\t//def foo(a: Int):Unit = {\n    //代码块写法为 变量命名 => 返回值类型   a: =>Int\n    def foo(a: =>Int):Unit = {//注意这里变量a没有小括号了 变成 a: =>Int\n        println(a)\n        println(a)\n    }\n}\n//输出结果：\n//f...\n//10\n//f...\n//10 \n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\nJava只有值调用；Scala既有值调用，又有名调用\n\n自定义一个while循环\n\nobject TestFunction {\n\n    def main(args: Array[String]): Unit = {\n\n        var i:Int = 1\n        myWhile(i <= 10){\n            println(i)\n            i +=1\n        }\n    }\n\n    def myWhile(condition: =>Boolean)(op: =>Unit):Unit={\n\n        if (condition){\n            op\n            myWhile(condition)(op)\n        }\n    }\n}\t\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',normalizedContent:'# 函数式编程\n\n解决问题时，将问题分解成一个一个的步骤，将每个步骤进行封装（函数），通过调用这些封装好的步骤，解决问题。\n\nscala语言是一个完全函数式编程语言。万物皆函数。\n\n函数的本质：函数可以当做一个值进行传递\n\n\n# 函数和方法的区别\n\n 1. 核心概念\n    1. 为完成某一功能的程序语句的集合，称为函数。\n    2. 类中的函数称之方法。\n    3. 定义在方法或者函数内部 称为函数\n\n（1）scala语言可以在任何 的语法结构中声明任何的语法\n\n（2）函数没有重载和重写的概念；方法可以进行重载和重写\n\n（3）scala中函数可以嵌套定义\n\nobject testfunction {\n\n    // (2)方法可以进行重载和重写，程序可以执行\n    def main(): unit = {\n\n    }\n\n    def main(args: array[string]): unit = {\n        // （1）scala语言可以在任何的语法结构中声明任何的语法\n        import java.util.date\n        new date()\n\n        // (2)函数没有重载和重写的概念，程序报错\n        def test(): unit ={\n            println("无参，无返回值")\n        }\n        test()\n\n        def test(name:string):unit={\n            println()\n        }\n\n        //（3）scala中函数可以嵌套定义\n        def test2(): unit ={\n\n            def test3(name:string):unit={\n                println("函数可以嵌套定义")\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# 函数定义\n\n def main(args: array[string]): unit = {\n\n        // 函数1：无参，无返回值\n        def test1(): unit ={\n            println("无参，无返回值")\n        }\n        test1()\n\n        // 函数2：无参，有返回值\n        def test2():string={\n            return "无参，有返回值"\n        }\n        println(test2())\n\n        // 函数3：有参，无返回值\n        def test3(s:string):unit={\n            println(s)\n        }\n        test3("jinlian")\n\n        // 函数4：有参，有返回值\n        def test4(s:string):string={\n            return s+"有参，有返回值"\n        }\n        println(test4("hello "))\n\n\n        // 函数5：多参，无返回值\n        def test5(name:string, age:int):unit={\n            println(s"$name, $age")\n        }\n        test5("dalang",40)\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 函数参数\n\n 1. 可变参数\n 2. 如果参数列表中存在多个参数，那么可变参数一般放置在最后\n 3. 参数默认值，一般将有默认值的参数放置在参数列表的后面\n 4. 带名参数\n\n // （1）可变参数\n        def test( s : string* ): unit = {\n            println(s)\n        }\n\n        // 有输入参数：输出 array\n        test("hello", "scala")\n\n        // 无输入参数：输出list()\n        test()\n\n        // (2)如果参数列表中存在多个参数，那么可变参数一般放置在最后\n        def test2( name : string, s: string* ): unit = {\n            println(name + "," + s)\n        }\n\n        test2("jinlian", "dalang")\n\n        // (3)参数默认值\n        def test3( name : string, age : int = 30 ): unit = {\n            println(s"$name, $age")\n        }\n\n        // 如果参数传递了值，那么会覆盖默认值\n        test3("jinlian", 20)\n\n        // 如果参数有默认值，在调用的时候，可以省略这个参数\n        test3("dalang")\n\n        // 一般情况下，将有默认值的参数放置在参数列表的后面\n        def test4( sex : string = "男", name : string ): unit =      {\n            println(s"$name, $sex")\n        }\n\t\t// scala函数中参数传递是，从左到右\n        //test4("wusong") \n\n        //（4）带名参数\n        test4(name="ximenqing")\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 函数至简原则（重点）\n\n函数至简原则：能省则省\n\n 1. return可以省略，scala会使用函数体的最后一行代码作为返回值\n 2. 如果函数体只有一行代码，可以省略花括号\n 3. 返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）\n 4. 如果有return，则不能省略返回值类型，必须指定\n 5. 如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用\n 6. scala如果期望是无返回值类型，可以省略等号\n 7. 如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加\n 8. 如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略\n 9. 如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略\n\nobject testfunction {\n\n    def main(args: array[string]): unit = {\n        \n       \n\n        // （0）函数标准写法\n        def f( s : string ): string = {\n            return s + " jinlian"\n        }\n        println(f("hello"))\n\n        // 至简原则:能省则省\n\n        //（1） return可以省略,scala会使用函数体的最后一行代码作为返回值\n        def f1( s : string ): string =  {\n            s + " jinlian"\n        }\n        println(f1("hello"))\n\n        //（2）如果函数体只有一行代码，可以省略花括号\n        def f2(s:string):string = s + " jinlian"\n\n\n        //（3）返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）\n        def f3( s : string ) = s + " jinlian"\n        println(f3("hello3"))\n\n        //（4）如果有return，则不能省略返回值类型，必须指定。\n        def f4() :string = {\n            return "ximenqing4"\n        }\n        println(f4())\n\n        //（5）如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用\n        def f5(): unit = {\n            return "dalang5"\n        }\n        println(f5())\n\n        //（6）scala如果期望是无返回值类型,可以省略等号\n        // 将无返回值的函数称之为过程\n        def f6() {\n            "dalang6"\n        }\n        println(f6())\n\n        //（7）如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加\n        def f7() = "dalang7"\n        println(f7())\n        println(f7)\n\n        //（8）如果函数没有参数列表，那么小括号可以省略,调用时小括号必须省略\n        def f8 = "dalang"\n        //println(f8())\n        println(f8)\n\n        //（9）如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略\n        def f9 = (x:string)=>{println("wusong")}\n\n        def f10(f:string=>unit) = {\n            f("")\n        }\n\n        f10(f9)\n        println(f10((x:string)=>{println("wusong")}))\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n\n# 函数高阶用法\n\n函数可以作为值进行传递\n\n//（1）调用foo函数，把返回值给变量f\n\n        //val f = foo()\n        val f = foo\n        println(f)\n\n        //（2）在被调用函数foo后面加上 _，相当于把函数foo当成一个整体，传递给变量f1\n        val f1 = foo _\n\n        foo()\n        f1()\n\t\t//（3）如果明确变量类型，那么不使用下划线也可以将函数作为整体传递给变量\n\t\tvar f2:()=>int = foo \n    }\n\n    def foo():int = {\n        println("foo...")\n        1\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n函数可以作为参数进行传递\n\n // （1）定义一个函数，函数参数还是一个函数签名；f表示函数名称;(int,int)表示输入两个int参数；int表示函数返回值\n    def f1(f: (int, int) => int): int = {\n        f(2, 4)\n    }\n    \n    // （2）定义一个函数，参数和返回值类型和f1的输入参数一致\n    def add(a: int, b: int): int = a + b\n    \n    // （3）将add函数作为参数传递给f1函数，如果能够推断出来不是调用，_可以省略\n    println(f1(add))\nprintln(f1(add _))\n//可以传递匿名函数\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n函数可以作为函数返回值返回\n\ndef main(args: array[string]): unit = {\n    def f1() = {\n        def f2() = {\n\n}\nf2 _\n}\n\nval f = f1()\n// 因为f1函数的返回值依然为函数，所以可以变量f可以作为函数继续调用\nf()\n// 上面的代码可以简化为\nf1()()\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 匿名函数\n\n（1）参数的类型可以省略，会根据形参进行自动的推导\n\n（2）类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参数超过1的永远不能省略圆括号。\n\n（3）匿名函数如果只有一行，则大括号也可以省略\n\n（4）如果参数只出现一次，则参数省略且后面参数可以用_代替\n\n（5）如果可以推断出，println是一个函数体，而不是调用语句，那么(_)可以省略\n\ndef main(args: array[string]): unit = {\n\n        // （1）定义一个函数：参数包含数据和逻辑函数\n        def operation(arr: array[int], op: int => int) = {\n            for (elem <- arr) yield op(elem)\n        }\n\n        // （2）定义逻辑函数\n        def op(ele: int): int = {\n            ele + 1\n        }\n\n        // （3）标准函数调用\n        val arr = operation(array(1, 2, 3, 4), op)\n        println(arr.mkstring(","))\n\n        // （4）采用匿名函数\n        val arr1 = operation(array(1, 2, 3, 4), (ele: int) => {\n            ele + 1\n        })\n        println(arr1.mkstring(","))\n\n        // （4.1）参数的类型可以省略，会根据形参进行自动的推导;\n        val arr2 = operation(array(1, 2, 3, 4), (ele) => {\n            ele + 1\n        })\n        println(arr2.mkstring(","))\n\n        // （4.2）类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参数超过1的永远不能省略圆括号。\n        val arr3 = operation(array(1, 2, 3, 4), ele => {\n            ele + 1\n        })\n        println(arr3.mkstring(","))\n\n        // (4.3) 匿名函数如果只有一行，则大括号也可以省略\n        val arr4 = operation(array(1, 2, 3, 4), ele => ele + 1)\n        println(arr4.mkstring(","))\n\n        //（4.4）如果参数只出现一次，则参数省略且后面参数可以用_代替\n        val arr5 = operation(array(1, 2, 3, 4), _ + 1)\n        println(arr5.mkstring(","))\n    \t\n    \t//（5）如果可以推断出，println是一个函数体，而不是调用语句，那么(_)可以省略\n    \t\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# 函数柯里化和闭包\n\n闭包：如果一个函数，访问到了它的外部（局部）变量的值，那么这个函数和他所处的环境，称为闭包\n\n函数柯里化：把一个参数列表的多个参数，变成多个参数列表。\n\nobject testfunction {\n\n    def main(args: array[string]): unit = {\n        def f1()={\n\t\t\tvar a:int = 10\n            def f2(b:int)={\n                a + b\n            }\n            f2 _\n        }\n\n        // 在调用时，f1函数执行完毕后，局部变量a应该随着栈空间释放掉\n        val f = f1() \n\n        // 但是在此处，变量a其实并没有释放，而是包含在了f2函数的内部，形成了闭合的效果\n        println(f(3))\n\n        \n        println(f1()(3))\n\n        // 函数柯里化，其实就是将复杂的参数逻辑变得简单化,函数柯里化一定存在闭包\n        def f3()(b:int)={\n             a + b\n        }\n\n        println(f3()(3))\n        \n        \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 递归\n\n一个函数/方法在函数/方法体内又调用了本身，我们称之为递归调用\n\nobject testfunction {\n\n    def main(args: array[string]): unit = {\n\n        // 阶乘\n        // 递归算法\n        // 1) 方法调用自身\n        // 2) 方法必须要有跳出的逻辑\n        // 3) 方法调用自身时，传递的参数应该有规律\n        // 4) scala中的递归必须声明函数返回值类型\n\n        println(test(5))\n    }\n\n    def test(i : int) : int = {\n        if (i == 1) {\n            1\n        } else {\n            i * test(i - 1)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 控制抽象\n\n值调用：把函数运行后的值传递过去\n\nobject testcontrol {\n\n    def main(args: array[string]): unit = {\n\n        def f = ()=>{\n            println("f...")\n            10\n        }\n\n        foo(f())\n    }\n\n    def foo(a: int):unit = {\n        println(a)\n        println(a)\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n名调用：把代码块传递过去\n\nobject testcontrol {\n\n    def main(args: array[string]): unit = {\n\n        def f = ()=>{\n            println("f...")\n            10\n        }\n\n        foo(f())\n    }\n\n\t//def foo(a: int):unit = {\n    //代码块写法为 变量命名 => 返回值类型   a: =>int\n    def foo(a: =>int):unit = {//注意这里变量a没有小括号了 变成 a: =>int\n        println(a)\n        println(a)\n    }\n}\n//输出结果：\n//f...\n//10\n//f...\n//10 \n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\njava只有值调用；scala既有值调用，又有名调用\n\n自定义一个while循环\n\nobject testfunction {\n\n    def main(args: array[string]): unit = {\n\n        var i:int = 1\n        mywhile(i <= 10){\n            println(i)\n            i +=1\n        }\n    }\n\n    def mywhile(condition: =>boolean)(op: =>unit):unit={\n\n        if (condition){\n            op\n            mywhile(condition)(op)\n        }\n    }\n}\t\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"Scala介绍",frontmatter:{title:"Scala介绍",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/d2afbe/",categories:["大数据","Scala"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/08.Scala/01.Scala%E4%BB%8B%E7%BB%8D.html",relativePath:"大数据/08.Scala/01.Scala介绍.md",key:"v-dbe517a8",path:"/pages/d2afbe/",headers:[{level:2,title:"Scala 和 Java 关系",slug:"scala-和-java-关系",normalizedTitle:"scala 和 java 关系",charIndex:91}],headersStr:"Scala 和 Java 关系",content:"# Scala介绍\n\n 1. Spark 新一代内存级大数据计算框架\n 2. Spark 是使用Scala编写的\n 3. Spark 的兴起 带动了Scala语言的发展\n\n\n\n\n# Scala 和 Java 关系\n\n\n\n * Java运行原理\n   \n   * 先编译 再解释\n   * .java源文件---\x3e编译器(javac)---\x3e.class字节码文件---\x3eJVM(不同平台)---\x3e机器指令\n\n * Scala运行原理\n   \n   * 先编译 再解释\n   * .scala源文件---\x3e编译器(scalac)---\x3e.class字节码文件---\x3eJVM(不同平台)---\x3e机器指令\n   \n   由上图看到scala在编译运行和java一样 并且可以使用java中的类库和语法\n\n",normalizedContent:"# scala介绍\n\n 1. spark 新一代内存级大数据计算框架\n 2. spark 是使用scala编写的\n 3. spark 的兴起 带动了scala语言的发展\n\n\n\n\n# scala 和 java 关系\n\n\n\n * java运行原理\n   \n   * 先编译 再解释\n   * .java源文件---\x3e编译器(javac)---\x3e.class字节码文件---\x3ejvm(不同平台)---\x3e机器指令\n\n * scala运行原理\n   \n   * 先编译 再解释\n   * .scala源文件---\x3e编译器(scalac)---\x3e.class字节码文件---\x3ejvm(不同平台)---\x3e机器指令\n   \n   由上图看到scala在编译运行和java一样 并且可以使用java中的类库和语法\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"SprakCore",frontmatter:{title:"SprakCore",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/a0eb57/",categories:["大数据","Spark"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/09.Spark/03.SprakCore.html",relativePath:"大数据/09.Spark/03.SprakCore.md",key:"v-52219ca2",path:"/pages/a0eb57/",headers:[{level:2,title:"RDD之装饰者模式",slug:"rdd之装饰者模式",normalizedTitle:"rdd之装饰者模式",charIndex:16},{level:2,title:"RDD 弹性分布式数据集",slug:"rdd-弹性分布式数据集",normalizedTitle:"rdd 弹性分布式数据集",charIndex:212},{level:2,title:"RDD 特性",slug:"rdd-特性",normalizedTitle:"rdd 特性",charIndex:231},{level:2,title:"RDD 编程",slug:"rdd-编程",normalizedTitle:"rdd 编程",charIndex:244},{level:3,title:"RDD创建",slug:"rdd创建",normalizedTitle:"rdd创建",charIndex:317},{level:3,title:"分区规则",slug:"分区规则",normalizedTitle:"分区规则",charIndex:1918}],headersStr:"RDD之装饰者模式 RDD 弹性分布式数据集 RDD 特性 RDD 编程 RDD创建 分区规则",content:'# SprakCore\n\n\n# RDD之装饰者模式\n\n我们的bufferreadReader就是典型的装饰者模式,通过原有类不断增强功能,并保持原有类的功能\n\n装饰者模式：动态地将责任附加到对象上。若要扩展功能，装饰者提供了比继承更有弹性的替代方案。\n\n\n\n并且拥有一个特殊的特征 只有最外层的装饰者调用时 才会调用内部原有的装饰对象 称为惰性加载\n\n\n\n而我们RDD也是通过装饰者马上来包装不断装饰原有的功能\n\n\n\n\n# RDD 弹性分布式数据集\n\n\n\n\n# RDD 特性\n\n\n\n\n# RDD 编程\n\n\n\n算子：从认知心理学角度来讲，解决问题其实是将问题的初始状态，通过一系列的转换操作（operator），变成解决状态。\n\n\n# RDD创建\n\n在Spark中创建RDD的创建方式可以分为三种：从集合中创建RDD、从外部存储创建RDD、从其他RDD创建。\n\n# 从集合中创建RDD\n\n * sc.parallelize(list)\n * sc.makeRDD(list)\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//通过读取内存集合中的数据 创建RDD\nobject Spark01_CreateRDD_mem {\n  def main(args: Array[String]): Unit = {\n    //Spark配置文件对象\n    val conf: SparkConf = new SparkConf().setAppName(" Spark01_CreateRDD_mem").setMaster("local[*]")\n    //创建SparkContext对象\n    val sc: SparkContext = new SparkContext(conf)\n    //创建一个集合\n    val list: List[Int] = List(1, 2, 3, 4)\n\n    //根据集合创建RDD 方式一\n    //    val rdd: RDD[Int] = sc.parallelize(list)\n    //根据集合创建RDD 方式二 底层调用是parallelize方法\n    val rdd: RDD[Int] = sc.makeRDD(list)\n\n    rdd.collect().foreach(println)\n\n\n    sc.stop()\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 从外部存储系统的数据集创建\n\n * sc.textFile(path)\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\n\n//通过读取外部文件 创建RDD\nobject Spark02_CreateRDD_file {\n  def main(args: Array[String]): Unit = {\n    val conf: SparkConf = new SparkConf().setAppName("Spark02_CreateRDD_file").setMaster("local[*]")\n    val sc: SparkContext = new SparkContext(conf)\n\n    //读取本地文件数据\n    val rdd: RDD[String] = sc.textFile("D:\\\\code\\\\spark\\\\input\\\\1.txt")\n    rdd.collect().foreach(println)\n\n    //从HDFS读取数据\n    val hdfsRdd: RDD[String] = sc.textFile("hdfs://hadoop102:8020/input")\n    hdfsRdd.collect().foreach(println)\n\n    sc.stop()\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 分区规则\n\n# 默认分区规则\n\n * rdd.partitions 查看分区\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{Partition, SparkConf, SparkContext}\n\n/**\n * 默认分区\n * - 从集合中创建RDD\n *    取决于分配给应用的CPU的核数   如为* 则为cpu全部核数\n * - 读取外部文件创建RDD\n *    math.min(取决于分配给应用的CPU的核数,2)\n */\nobject Spark03_Partition_default {\n  def main(args: Array[String]): Unit = {\n    val conf: SparkConf = new SparkConf().setAppName("Spark03_Partition_default").setMaster("local[*]")\n    val sc: SparkContext = new SparkContext(conf)\n\n    //通过集合创建RDD\n//    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))\n    val rdd: RDD[String] = sc.textFile("D:\\\\code\\\\spark\\\\input\\\\1.txt")\n    //查看分区效果\n    val partitions: Array[Partition] = rdd.partitions\n    println(partitions.size) //分区数\n    rdd.saveAsTextFile("D:\\\\code\\\\spark\\\\output")\n\n\n    sc.stop()\n\n  }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n\n# 指定分区\n\n * sc.makeRDD(data , partition)\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{Partition, SparkConf, SparkContext}\n\n/**\n * 指定分区分区\n *  -根据下标与分区数进行运算 求出 [x,y) 开始到结束下标元素的具体分区分配\n *    - start (i * arr.lent) / partition\n *    - end ((i +1) * arr.lent ) / partition\n */\nobject Spark04_Partition_mem {\n  def main(args: Array[String]): Unit = {\n    val conf: SparkConf = new SparkConf().setAppName("Spark04_Partition_mem").setMaster("local[*]")\n    val sc: SparkContext = new SparkContext(conf)\n\n    //通过集合创建RDD\n//    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4),4) //默认分区数为cpu核数\n    //1）4个数据，设置4个分区，输出：0分区->1，1分区->2，2分区->3，3分区->4\n    //val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 4)\n\n    //2）4个数据，设置3个分区，输出：0分区->1，1分区->2，2分区->3,4\n    //val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 3)\n\n    //3）5个数据，设置3个分区，输出：0分区->1，1分区->2、3，2分区->4、5\n    val rdd: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4, 5), 3)\n\n    //查看分区效果\n    val partitions: Array[Partition] = rdd.partitions\n    println(partitions.size) //分区数\n    rdd.saveAsTextFile("D:\\\\code\\\\spark\\\\output")\n\n\n    sc.stop()\n\n  }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n\n# 读取文件指定分区\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n * 读取外部文件 创建RDD\n *  - 默认分区规则\n *    math.min(分配的核数,2)\n *   - 指定分区 minPartitions 最小分区数 并不是实际分区个数\n *   - 在实际计算分区个数的时候 会根据文件的总大小和 最小分区数进行相除运算\n *      -  如果余数为0  最小分区数为实际分区数\n *      - 如果余数不为0 则实际分区数要看实际切片\n *\n */\nobject Spark05_Partition_file {\n  def main(args: Array[String]): Unit = {\n    val conf: SparkConf = new SparkConf().setAppName("Spark05_Partition_file").setMaster("local[*]")\n    val sc: SparkContext = new SparkContext(conf)\n\n    //    val rdd: RDD[String] = sc.textFile("D:\\\\code\\\\spark\\\\input\\\\1.txt") //默认分区为2个\n    //2）输入数据1-4，每行一个数字；输出：0=>{1、2} 1=>{3} 2=>{4} 3=>{空}\n    //val rdd: RDD[String] = sc.textFile("input/3.txt",3)\n\n    //3）输入数据1-4，一共一行；输出：0=>{1234} 1=>{空} 2=>{空} 3=>{空}\n    val rdd: RDD[String] = sc.textFile("input/4.txt", 3)\n\n    rdd.saveAsTextFile("D:\\\\code\\\\spark\\\\output")\n\n    sc.stop()\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n\ngetSplits文件返回的是切片规划，真正读取是在compute方法中创建LineRecordReader读取的，有两个关键变量\n\nstart=split.getStart()\n\nend = start + split.getLength\n\n假设读取text.txt 指定最小分区为5\n\nabc\nef\ng\nhj\nklm\n\n\n1\n2\n3\n4\n5\n\n\n此文件总大小为19字节 19%5=3 每个分区每次追加个数要求为 19/6=3字节 19字节总大小/3每次字节=6个分区 余 1 所有实际分区数为 6+1 =7\n\n如果 目前总大小/(总大小 / 最小分区数) < 1.1 则不切片 则以指定分区数 详情源码看上图的while循环\n\n分区0的数据为 每个分区每次读取3字节 因为读取是读取一行如果这一行数据超过了每次读取则整行读取 并读取下个分区\n\n0~3索引的数据\n\nabc\\r\\n\n\n\n1\n\n\n分区1的数据为 3~6索引的数据\n\nef\\r\\n\n\n\n1\n\n\n分区2的数据为 6~9索引的数据\n\ng\\r\\n\n\n\n1\n\n\n分区3数据为 9~12索引的数据\n\nhj\\r\\n\n\n\n1\n\n\n分区4数据为 12~15索引的数据\n\n\n\n分区5数据为 15~18索引的数据\n\nklm\\r\n\n\n1\n\n\n分区6数据为 18~1索引的数据\n\n',normalizedContent:'# sprakcore\n\n\n# rdd之装饰者模式\n\n我们的bufferreadreader就是典型的装饰者模式,通过原有类不断增强功能,并保持原有类的功能\n\n装饰者模式：动态地将责任附加到对象上。若要扩展功能，装饰者提供了比继承更有弹性的替代方案。\n\n\n\n并且拥有一个特殊的特征 只有最外层的装饰者调用时 才会调用内部原有的装饰对象 称为惰性加载\n\n\n\n而我们rdd也是通过装饰者马上来包装不断装饰原有的功能\n\n\n\n\n# rdd 弹性分布式数据集\n\n\n\n\n# rdd 特性\n\n\n\n\n# rdd 编程\n\n\n\n算子：从认知心理学角度来讲，解决问题其实是将问题的初始状态，通过一系列的转换操作（operator），变成解决状态。\n\n\n# rdd创建\n\n在spark中创建rdd的创建方式可以分为三种：从集合中创建rdd、从外部存储创建rdd、从其他rdd创建。\n\n# 从集合中创建rdd\n\n * sc.parallelize(list)\n * sc.makerdd(list)\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.rdd\nimport org.apache.spark.{sparkconf, sparkcontext}\n\n//通过读取内存集合中的数据 创建rdd\nobject spark01_createrdd_mem {\n  def main(args: array[string]): unit = {\n    //spark配置文件对象\n    val conf: sparkconf = new sparkconf().setappname(" spark01_createrdd_mem").setmaster("local[*]")\n    //创建sparkcontext对象\n    val sc: sparkcontext = new sparkcontext(conf)\n    //创建一个集合\n    val list: list[int] = list(1, 2, 3, 4)\n\n    //根据集合创建rdd 方式一\n    //    val rdd: rdd[int] = sc.parallelize(list)\n    //根据集合创建rdd 方式二 底层调用是parallelize方法\n    val rdd: rdd[int] = sc.makerdd(list)\n\n    rdd.collect().foreach(println)\n\n\n    sc.stop()\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 从外部存储系统的数据集创建\n\n * sc.textfile(path)\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.rdd\nimport org.apache.spark.{sparkconf, sparkcontext}\n\n\n//通过读取外部文件 创建rdd\nobject spark02_createrdd_file {\n  def main(args: array[string]): unit = {\n    val conf: sparkconf = new sparkconf().setappname("spark02_createrdd_file").setmaster("local[*]")\n    val sc: sparkcontext = new sparkcontext(conf)\n\n    //读取本地文件数据\n    val rdd: rdd[string] = sc.textfile("d:\\\\code\\\\spark\\\\input\\\\1.txt")\n    rdd.collect().foreach(println)\n\n    //从hdfs读取数据\n    val hdfsrdd: rdd[string] = sc.textfile("hdfs://hadoop102:8020/input")\n    hdfsrdd.collect().foreach(println)\n\n    sc.stop()\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 分区规则\n\n# 默认分区规则\n\n * rdd.partitions 查看分区\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.rdd\nimport org.apache.spark.{partition, sparkconf, sparkcontext}\n\n/**\n * 默认分区\n * - 从集合中创建rdd\n *    取决于分配给应用的cpu的核数   如为* 则为cpu全部核数\n * - 读取外部文件创建rdd\n *    math.min(取决于分配给应用的cpu的核数,2)\n */\nobject spark03_partition_default {\n  def main(args: array[string]): unit = {\n    val conf: sparkconf = new sparkconf().setappname("spark03_partition_default").setmaster("local[*]")\n    val sc: sparkcontext = new sparkcontext(conf)\n\n    //通过集合创建rdd\n//    val rdd: rdd[int] = sc.makerdd(list(1, 2, 3, 4))\n    val rdd: rdd[string] = sc.textfile("d:\\\\code\\\\spark\\\\input\\\\1.txt")\n    //查看分区效果\n    val partitions: array[partition] = rdd.partitions\n    println(partitions.size) //分区数\n    rdd.saveastextfile("d:\\\\code\\\\spark\\\\output")\n\n\n    sc.stop()\n\n  }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n\n# 指定分区\n\n * sc.makerdd(data , partition)\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.rdd\nimport org.apache.spark.{partition, sparkconf, sparkcontext}\n\n/**\n * 指定分区分区\n *  -根据下标与分区数进行运算 求出 [x,y) 开始到结束下标元素的具体分区分配\n *    - start (i * arr.lent) / partition\n *    - end ((i +1) * arr.lent ) / partition\n */\nobject spark04_partition_mem {\n  def main(args: array[string]): unit = {\n    val conf: sparkconf = new sparkconf().setappname("spark04_partition_mem").setmaster("local[*]")\n    val sc: sparkcontext = new sparkcontext(conf)\n\n    //通过集合创建rdd\n//    val rdd: rdd[int] = sc.makerdd(list(1, 2, 3, 4),4) //默认分区数为cpu核数\n    //1）4个数据，设置4个分区，输出：0分区->1，1分区->2，2分区->3，3分区->4\n    //val rdd: rdd[int] = sc.makerdd(array(1, 2, 3, 4), 4)\n\n    //2）4个数据，设置3个分区，输出：0分区->1，1分区->2，2分区->3,4\n    //val rdd: rdd[int] = sc.makerdd(array(1, 2, 3, 4), 3)\n\n    //3）5个数据，设置3个分区，输出：0分区->1，1分区->2、3，2分区->4、5\n    val rdd: rdd[int] = sc.makerdd(array(1, 2, 3, 4, 5), 3)\n\n    //查看分区效果\n    val partitions: array[partition] = rdd.partitions\n    println(partitions.size) //分区数\n    rdd.saveastextfile("d:\\\\code\\\\spark\\\\output")\n\n\n    sc.stop()\n\n  }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n\n# 读取文件指定分区\n\npackage com.atguigu.spark.day02\n\nimport org.apache.spark.rdd.rdd\nimport org.apache.spark.{sparkconf, sparkcontext}\n\n/**\n * 读取外部文件 创建rdd\n *  - 默认分区规则\n *    math.min(分配的核数,2)\n *   - 指定分区 minpartitions 最小分区数 并不是实际分区个数\n *   - 在实际计算分区个数的时候 会根据文件的总大小和 最小分区数进行相除运算\n *      -  如果余数为0  最小分区数为实际分区数\n *      - 如果余数不为0 则实际分区数要看实际切片\n *\n */\nobject spark05_partition_file {\n  def main(args: array[string]): unit = {\n    val conf: sparkconf = new sparkconf().setappname("spark05_partition_file").setmaster("local[*]")\n    val sc: sparkcontext = new sparkcontext(conf)\n\n    //    val rdd: rdd[string] = sc.textfile("d:\\\\code\\\\spark\\\\input\\\\1.txt") //默认分区为2个\n    //2）输入数据1-4，每行一个数字；输出：0=>{1、2} 1=>{3} 2=>{4} 3=>{空}\n    //val rdd: rdd[string] = sc.textfile("input/3.txt",3)\n\n    //3）输入数据1-4，一共一行；输出：0=>{1234} 1=>{空} 2=>{空} 3=>{空}\n    val rdd: rdd[string] = sc.textfile("input/4.txt", 3)\n\n    rdd.saveastextfile("d:\\\\code\\\\spark\\\\output")\n\n    sc.stop()\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n\ngetsplits文件返回的是切片规划，真正读取是在compute方法中创建linerecordreader读取的，有两个关键变量\n\nstart=split.getstart()\n\nend = start + split.getlength\n\n假设读取text.txt 指定最小分区为5\n\nabc\nef\ng\nhj\nklm\n\n\n1\n2\n3\n4\n5\n\n\n此文件总大小为19字节 19%5=3 每个分区每次追加个数要求为 19/6=3字节 19字节总大小/3每次字节=6个分区 余 1 所有实际分区数为 6+1 =7\n\n如果 目前总大小/(总大小 / 最小分区数) < 1.1 则不切片 则以指定分区数 详情源码看上图的while循环\n\n分区0的数据为 每个分区每次读取3字节 因为读取是读取一行如果这一行数据超过了每次读取则整行读取 并读取下个分区\n\n0~3索引的数据\n\nabc\\r\\n\n\n\n1\n\n\n分区1的数据为 3~6索引的数据\n\nef\\r\\n\n\n\n1\n\n\n分区2的数据为 6~9索引的数据\n\ng\\r\\n\n\n\n1\n\n\n分区3数据为 9~12索引的数据\n\nhj\\r\\n\n\n\n1\n\n\n分区4数据为 12~15索引的数据\n\n\n\n分区5数据为 15~18索引的数据\n\nklm\\r\n\n\n1\n\n\n分区6数据为 18~1索引的数据\n\n',charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3},{title:"SparkSQL",frontmatter:{title:"SparkSQL",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/0de4e9/",categories:["大数据","Spark"],tags:[null]},regularPath:"/%E5%A4%A7%E6%95%B0%E6%8D%AE/09.Spark/04.SparkSQL.html",relativePath:"大数据/09.Spark/04.SparkSQL.md",key:"v-12efaf51",path:"/pages/0de4e9/",headers:[{level:2,title:"DataFrame",slug:"dataframe",normalizedTitle:"dataframe",charIndex:326},{level:2,title:"DataSet",slug:"dataset",normalizedTitle:"dataset",charIndex:340}],headersStr:"DataFrame DataSet",content:"# SparkSQL\n\nSpark SQL是Spark用于结构化数据(structured data)处理的Spark模块。\n\n与基本的Spark RDD API不同，Spark SQL的抽象数据类型为Spark提供了关于数据结构和正在执行的计算的更多信息。\n\n我们已经学习了Hive，它是将Hive SQL转换成 MapReduce然后提交到集群上执行，大大简化了编写 MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所以Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！\n\nSpark SQL它提供了2个编程抽象，类似Spark Core中的RDD\n\n * DataFrame\n\n * DataSet\n\n特点:\n\n 1. 使用相同的方式连接不同的数据源\n 2. 在已有的仓库上直接运行 SQL 或者 HiveQL\n 3. 通过 JDBC 或者 ODBC 来连接\n\n\n# DataFrame\n\n在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。\n\nDataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。\n\n同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从 API 易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API 要更加友好，门槛更低。\n\n左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。\n\nDataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待\n\nDataFrame也是懒执行的，但性能上比RDD要高，主要原因：优化的执行计划，即查询计划通过Spark catalyst optimiser进行优化。\n\n\n\n\n# DataSet\n\nDataSet是分布式数据集合。DataSet是Spark 1.6中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。DataSet也可以使用功能性的转换（操作map，flatMap，filter等等）。\n\n",normalizedContent:"# sparksql\n\nspark sql是spark用于结构化数据(structured data)处理的spark模块。\n\n与基本的spark rdd api不同，spark sql的抽象数据类型为spark提供了关于数据结构和正在执行的计算的更多信息。\n\n我们已经学习了hive，它是将hive sql转换成 mapreduce然后提交到集群上执行，大大简化了编写 mapreduce的程序的复杂性，由于mapreduce这种计算模型执行效率比较慢。所以spark sql的应运而生，它是将spark sql转换成rdd，然后提交到集群执行，执行效率非常快！\n\nspark sql它提供了2个编程抽象，类似spark core中的rdd\n\n * dataframe\n\n * dataset\n\n特点:\n\n 1. 使用相同的方式连接不同的数据源\n 2. 在已有的仓库上直接运行 sql 或者 hiveql\n 3. 通过 jdbc 或者 odbc 来连接\n\n\n# dataframe\n\n在spark中，dataframe是一种以rdd为基础的分布式数据集，类似于传统数据库中的二维表格。\n\ndataframe与rdd的主要区别在于，前者带有schema元信息，即dataframe所表示的二维表数据集的每一列都带有名称和类型。这使得spark sql得以洞察更多的结构信息，从而对藏于dataframe背后的数据源以及作用于dataframe之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观rdd，由于无从得知所存数据元素的具体内部结构，spark core只能在stage层面进行简单、通用的流水线优化。\n\n同时，与hive类似，dataframe也支持嵌套数据类型（struct、array和map）。从 api 易用性的角度上看，dataframe api提供的是一套高层的关系操作，比函数式的rdd api 要更加友好，门槛更低。\n\n左侧的rdd[person]虽然以person为类型参数，但spark框架本身不了解person类的内部结构。而右侧的dataframe却提供了详细的结构信息，使得 spark sql 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。\n\ndataframe是为数据提供了schema的视图。可以把它当做数据库中的一张表来对待\n\ndataframe也是懒执行的，但性能上比rdd要高，主要原因：优化的执行计划，即查询计划通过spark catalyst optimiser进行优化。\n\n\n\n\n# dataset\n\ndataset是分布式数据集合。dataset是spark 1.6中添加的一个新抽象，是dataframe的一个扩展。它提供了rdd的优势（强类型，使用强大的lambda函数的能力）以及spark sql优化执行引擎的优点。dataset也可以使用功能性的转换（操作map，flatmap，filter等等）。\n\n",charsets:{cjk:!0},lastUpdated:"2022/03/18, 07:01:11",lastUpdatedTimestamp:1647558071e3}],themeConfig:{bodyBgImg:"/img/aqua.jpg",bodyBgImgOpacity:1,nav:[{text:"首页",link:"/"},{text:"后端",link:"/Java/",items:[{text:"Java",items:[{text:"JavaSE",link:"/pages/df8281/"},{text:"JavaEE",link:"/pages/544a3f/"}]},{text:"Python",items:[{text:"Python",link:"/pages/35c9ab/"},{text:"Python模块",link:"/pages/cf7131/"},{text:"机器学习",link:"/pages/ddf9fb/"}]},{text:"服务器",items:[{text:"Linux",link:"/pages/124a07/"},{text:"SQL",link:"/pages/e051f6/"}]}]},{text:"大数据",link:"/hadoop/",items:[{text:"Hadoop",link:"/pages/8e0c98/"},{text:"Zookeeper",link:"/pages/f38fc8/"},{text:"Hive",link:"/pages/2315d6/"},{text:"Flume",link:"/pages/b084db/"},{text:"Kafka",link:"/pages/872cdc/"},{text:"Azkaban",link:"/pages/bac882/"},{text:"Hbase",link:"/pages/310c39/"},{text:"Scala",link:"/pages/d2afbe/"},{text:"Spark",link:"/pages/cc783e/"}]},{text:"前端",link:"/web/",items:[{text:"html",link:"/pages/23186a/"},{text:"Vue2",link:"/pages/6af871/"}]},{text:"408",link:"/408/",items:[{text:"数据结构与算法",link:"/pages/8e5251/"}]}],sidebarDepth:2,logo:"/img/logo.png",repo:"Iekrwh/blog-VuePress-theme-vdoing",searchMaxSuggestions:10,lastUpdated:"上次更新",docsDir:"docs",editLinks:!0,editLinkText:"编辑",sidebar:{"/408/":[{title:"数据结构",collapsable:!0,children:[["01.数据结构/01.位运算.md","位运算","/pages/8e5251/"],["01.数据结构/02.最基本的数据结构.md","最基本的数据结构","/pages/38d7c8/"],["01.数据结构/03.前缀和数组.md","前缀和数组","/pages/3ad1d4/"],["01.数据结构/04.random的随机行为.md","random的随机行为","/pages/64e136/"],["01.数据结构/05.对数器.md","对数器","/pages/9c9f24/"],["01.数据结构/06.二分法查找.md","二分法查找","/pages/06b58c/"],["01.数据结构/07.方法参数传递是值还是引用.md","方法参数传递是值还是引用","/pages/7e9265/"],["01.数据结构/08.链表.md","链表","/pages/d3b8b4/"],["01.数据结构/09.位图.md","位图","/pages/d2cd3f/"],["01.数据结构/10.位运算实现四则运算.md","位运算实现四则运算","/pages/51bf81/"],["01.数据结构/11.二叉树.md","二叉树","/pages/3e25e1/"],["01.数据结构/12.排序.md","排序","/pages/0dcdc5/"],["01.数据结构/13.时间复杂度.md","时间复杂度","/pages/ef4d8b/"],["01.数据结构/14.队列和栈.md","队列和栈","/pages/3bda9f/"],["01.数据结构/15.递归.md","递归","/pages/fe2eaf/"],["01.数据结构/16.堆(优先级队列).md","堆(优先级队列)","/pages/88e08c/"]]},{title:"计算机操作系统",collapsable:!0,children:[]},{title:"计算机网络",collapsable:!0,children:[]},{title:"计算机组成原理",collapsable:!0,children:[]}],catalogue:{},"/前端/":[{title:"html",collapsable:!0,children:[["01.html/01.BS架构.md","BS架构","/pages/23186a/"],["01.html/02.HTML.md","HTML","/pages/3ba523/"],["01.html/03.HTML5.md","HTML5","/pages/8600fc/"],["01.html/04.表单标签.md","表单标签","/pages/c2f00e/"],["01.html/05.CSS.md","CSS","/pages/b3f43c/"],["01.html/06.html+css网页.md","html+css网页","/pages/5bc756/"],["01.html/07.CSS3.md","CSS3","/pages/54a5fa/"],["01.html/08.移动布局.md","移动布局","/pages/dc74bd/"],["01.html/09.JavaScript.md","JavaScript","/pages/e5e5b8/"]]},{title:"Vue2",collapsable:!0,children:[["02.Vue2/00.vue 补充.md","vue 补充","/pages/6af871/"],["02.Vue2/01.安装Vue CLI.md","安装Vue CLI","/pages/758bea/"],["02.Vue2/02.在Vue CLI中导入 Elment.md","在Vue CLI中导入 Elment","/pages/3b8d98/"],["02.Vue2/03.Elment.md","Elment","/pages/cb74f3/"],["02.Vue2/04.Axios.md","Axios","/pages/e17f8f/"],["02.Vue2/05.配置Vue路由.md","配置Vue路由","/pages/efa853/"],["02.Vue2/06.github 推送.md","github 推送","/pages/56d54a/"],["02.Vue2/07.安装 elment-tiptap.md","安装 elment-tiptap","/pages/93608e/"],["02.Vue2/08.安装 cropperjs 图片裁切工具.md","安装 cropperjs 图片裁切工具","/pages/f0d787/"],["02.Vue2/09.让两个组件之间通讯.md","让两个组件之间通讯","/pages/e5e482/"],["02.Vue2/10.安装echart.md","安装echart","/pages/812e04/"],["02.Vue2/11.文件对象.md","文件对象","/pages/2bf2c3/"],["02.Vue2/12.打包发布.md","打包发布","/pages/fc097d/"],["02.Vue2/13.webpack.md","webpack","/pages/b60752/"],["02.Vue2/14.vue 版本.md","vue 版本","/pages/0a8d91/"],["02.Vue2/15.优化打包.md","优化打包","/pages/b8f860/"],["02.Vue2/16.vue 图形界面.md","vue 图形界面","/pages/c64d65/"],["02.Vue2/17.路由懒加载.md","路由懒加载","/pages/27ac6a/"],["02.Vue2/18.element 按需引用.md","element 按需引用","/pages/1bc0dd/"],["02.Vue2/19.缓存和并行处理.md","缓存和并行处理","/pages/14f647/"],["02.Vue2/20.JavaScript 异步编程.md","JavaScript 异步编程","/pages/195534/"]]}],"/后端/":[{title:"JavaSE",collapsable:!0,children:[["01.JavaSE/01.java跨平台原理.md","java跨平台原理","/pages/df8281/"],["01.JavaSE/02.JRE(Java Runtime Enviroment).md","JRE(Java Runtime Enviroment)","/pages/0c8879/"],["01.JavaSE/03.JDK(Java Development Kit).md","JDK(Java Development Kit)","/pages/d217f4/"],["01.JavaSE/04.常用dos命令.md","常用dos命令","/pages/47da21/"],["01.JavaSE/05.配置系统path环境变量.md","配置系统path环境变量","/pages/3ee21a/"],["01.JavaSE/06.开发运行流程.md","开发运行流程","/pages/417c2d/"],["01.JavaSE/07.编写规范问题.md","编写规范问题","/pages/977c49/"],["01.JavaSE/08.基础语法.md","基础语法","/pages/97d4f2/"],["01.JavaSE/09.关键字.md","关键字","/pages/d2d164/"],["01.JavaSE/10.常量.md","常量","/pages/10baa9/"],["01.JavaSE/11.数据类型.md","数据类型","/pages/ec81f9/"],["01.JavaSE/12.数值型内容占用和取整范围.md","数值型内容占用和取整范围","/pages/3f6e03/"],["01.JavaSE/13.变量.md","变量","/pages/558212/"],["01.JavaSE/14.标识符.md","标识符","/pages/fcb871/"],["01.JavaSE/15.类型转换.md","类型转换","/pages/b2c2df/"],["01.JavaSE/16.算术运算符.md","算术运算符","/pages/a3cd43/"],["01.JavaSE/17.自增自减运算符.md","自增自减运算符","/pages/a4bf30/"],["01.JavaSE/18.关系运算符.md","关系运算符","/pages/a5f46a/"],["01.JavaSE/19.逻辑运算符.md","逻辑运算符","/pages/ae5f03/"],["01.JavaSE/20.短路逻辑运算符.md","短路逻辑运算符","/pages/5cf2b5/"],["01.JavaSE/21.三元运算符.md","三元运算符","/pages/ad2097/"],["01.JavaSE/22.数据输入.md","数据输入","/pages/9ca53c/"],["01.JavaSE/23.流程控制.md","流程控制","/pages/fc3701/"],["01.JavaSE/24.循环语句.md","循环语句","/pages/db81b2/"],["01.JavaSE/25.idea中的辅助键.md","idea中的辅助键","/pages/3455ab/"],["01.JavaSE/26.数组.md","数组","/pages/add390/"],["01.JavaSE/27.方法.md","方法","/pages/8361e2/"],["01.JavaSE/28.面向对象基础.md","面向对象基础","/pages/0a58cf/"],["01.JavaSE/29.标准类.md","标准类","/pages/ab994e/"],["01.JavaSE/30.字符串.md","字符串","/pages/8a171b/"],["01.JavaSE/31.ArrayList集合.md","ArrayList集合","/pages/ab65fb/"],["01.JavaSE/32.继承.md","继承","/pages/c7fa91/"],["01.JavaSE/33.修饰符.md","修饰符","/pages/f3b824/"],["01.JavaSE/34.权限修饰符.md","权限修饰符","/pages/047301/"],["01.JavaSE/35.状态修饰符.md","状态修饰符","/pages/392033/"],["01.JavaSE/36.多态.md","多态","/pages/6407a8/"],["01.JavaSE/37.抽象.md","抽象","/pages/a12790/"],["01.JavaSE/38.接口.md","接口","/pages/db4705/"],["01.JavaSE/39.类和接口的关系.md","类和接口的关系","/pages/b47784/"],["01.JavaSE/40.抽象类与接口的区别.md","抽象类与接口的区别","/pages/102423/"],["01.JavaSE/41.类名作为形参和返回值.md","类名作为形参和返回值","/pages/384827/"],["01.JavaSE/42.内部类.md","内部类","/pages/86e52c/"],["01.JavaSE/43.Api.md","Api","/pages/a4cd8f/"],["01.JavaSE/44.异常.md","异常","/pages/236d5b/"],["01.JavaSE/45.集合.md","集合","/pages/f58543/"],["01.JavaSE/46.泛型.md","泛型","/pages/d6e1c4/"],["01.JavaSE/47.Set集合.md","Set集合","/pages/5137f9/"],["01.JavaSE/48.树.md","树","/pages/6e8f69/"],["01.JavaSE/49.TreeSet遍历.md","TreeSet遍历","/pages/8b6f0f/"],["01.JavaSE/50.哈希值.md","哈希值","/pages/1793cd/"],["01.JavaSE/51.哈希表.md","哈希表","/pages/7525b1/"],["01.JavaSE/52.HashSet集合.md","HashSet集合","/pages/642d0a/"],["01.JavaSE/53.Map 集合.md","Map 集合","/pages/b8041f/"],["01.JavaSE/54.可变参数.md","可变参数","/pages/4fb557/"],["01.JavaSE/55.创建不可变的集合.md","创建不可变的集合","/pages/7db438/"],["01.JavaSE/56.Stream流.md","Stream流","/pages/46103a/"],["01.JavaSE/57.方法引用.md","方法引用","/pages/566611/"],["01.JavaSE/58.File.md","File","/pages/48b908/"],["01.JavaSE/59.多线程.md","多线程","/pages/606294/"],["01.JavaSE/60.多线程高级.md","多线程高级","/pages/c73ee4/"],["01.JavaSE/61.网络编程.md","网络编程","/pages/fc7fd4/"],["01.JavaSE/62.类加载器.md","类加载器","/pages/a33e90/"],["01.JavaSE/63.反射.md","反射","/pages/b3e612/"],["01.JavaSE/64.XML.md","XML","/pages/6bad64/"],["01.JavaSE/65.枚举.md","枚举","/pages/19426f/"],["01.JavaSE/66.注解.md","注解","/pages/1bf317/"],["01.JavaSE/67.单元测试.md","单元测试","/pages/7ba344/"],["01.JavaSE/68.日志.md","日志","/pages/344d91/"],["01.JavaSE/69.HTTP协议.md","HTTP协议","/pages/a7a164/"],["01.JavaSE/70.Servlet.md","Servlet","/pages/9c9dd6/"],["01.JavaSE/71.请求对象.md","请求对象","/pages/83bc98/"],["01.JavaSE/72.响应对象.md","响应对象","/pages/d35ebd/"],["01.JavaSE/73.Cookie.md","Cookie","/pages/133dfe/"],["01.JavaSE/74.Session.md","Session","/pages/83bc22/"],["01.JavaSE/75.JSP.md","JSP","/pages/427528/"],["01.JavaSE/76.Listener.md","Listener","/pages/70e34e/"]]},{title:"JavaEE",collapsable:!0,children:[["02.JavaEE/01.JDBC.md","JDBC","/pages/544a3f/"],["02.JavaEE/02.MyBatis.md","MyBatis","/pages/c1aa2b/"],["02.JavaEE/03.Jackson.md","Jackson","/pages/df2f58/"],["02.JavaEE/04.Jedis.md","Jedis","/pages/40c5ff/"],["02.JavaEE/05.Maven.md","Maven","/pages/682f06/"],["02.JavaEE/06.POI.md","POI","/pages/528ce7/"],["02.JavaEE/07.Spring.md","Spring","/pages/17e650/"],["02.JavaEE/08.Spring MVC.md","Spring MVC","/pages/8cd1ce/"],["02.JavaEE/09.Maven 高级.md","Maven 高级","/pages/127f3b/"],["02.JavaEE/10.Dubbo.md","Dubbo","/pages/dc966e/"],["02.JavaEE/11.Zookeeper.md","Zookeeper","/pages/7bc195/"],["02.JavaEE/12.Spring Security.md","Spring Security","/pages/ca6c88/"],["02.JavaEE/13.Spring Boot.md","Spring Boot","/pages/4f5fb3/"],["02.JavaEE/14.Spring Boot 高级.md","Spring Boot 高级","/pages/207023/"],["02.JavaEE/15.RabbitMQ.md","RabbitMQ","/pages/3a5e24/"],["02.JavaEE/16.RabbitMQ 高级.md","RabbitMQ 高级","/pages/da3871/"],["02.JavaEE/17.Spring Cloud.md","Spring Cloud","/pages/e886f8/"],["02.JavaEE/18.Docker.md","Docker","/pages/c588b5/"],["02.JavaEE/19.ElasticSearch.md","ElasticSearch","/pages/95da1a/"],["02.JavaEE/20.ElasticSearch 高级.md","ElasticSearch 高级","/pages/cf92bb/"]]},{title:"Linux",collapsable:!0,children:[["03.Linux/01.Linux.md","Linux","/pages/124a07/"],["03.Linux/02.shell.md","shell","/pages/f9e4a1/"],["03.Linux/03.Nginx.md","Nginx","/pages/0aeef9/"],["03.Linux/04.java.md","java","/pages/a70c4f/"],["03.Linux/05.Tomcat.md","Tomcat","/pages/f323f1/"]]},{title:"SQL",collapsable:!0,children:[["04.SQL/01.常见的数据库产品.md","常见的数据库产品","/pages/e051f6/"],["04.SQL/02.数据库相关概念.md","数据库相关概念","/pages/4b2b60/"],["04.SQL/03.数据库存储数据的特点.md","数据库存储数据的特点","/pages/d2b3d3/"],["04.SQL/04.MySQL服务端.md","MySQL服务端","/pages/641ffa/"],["04.SQL/05.环境变量.md","环境变量","/pages/a4dffe/"],["04.SQL/06.命令符指令.md","命令符指令","/pages/8c64bc/"],["04.SQL/07.SQL语句.md","SQL语句","/pages/4c9417/"],["04.SQL/08.数据库.md","数据库","/pages/04fedb/"],["04.SQL/09.表.md","表","/pages/5a2265/"],["04.SQL/10.约束.md","约束","/pages/de8702/"],["04.SQL/11.多表操作.md","多表操作","/pages/89e8d8/"],["04.SQL/12.视图.md","视图","/pages/3293cf/"],["04.SQL/13.备份.md","备份","/pages/f561ab/"],["04.SQL/14.MySQL 存储过程和函数.md","MySQL 存储过程和函数","/pages/336822/"],["04.SQL/15.触发器.md","触发器","/pages/108b35/"],["04.SQL/16.事务.md","事务","/pages/4c8706/"],["04.SQL/17.存储引擎.md","存储引擎","/pages/7d59ad/"],["04.SQL/18.索引.md","索引","/pages/acc2dd/"],["04.SQL/19.锁.md","锁","/pages/3c034d/"],["04.SQL/20.MyCat 中间件.md","MyCat 中间件","/pages/3ca264/"],["04.SQL/21.Nosql.md","Nosql","/pages/4a12e8/"],["04.SQL/22.Redis.md","Redis","/pages/a002c8/"],["04.SQL/23.Redis高级.md","Redis高级","/pages/4e4f7f/"],["04.SQL/24.MongoDB.md","MongoDB","/pages/e5cb41/"]]},{title:"Python",collapsable:!0,children:[["05.Python/01.IDE.md","IDE","/pages/35c9ab/"],["05.Python/02.close project.md","close project","/pages/0c7ca5/"],["05.Python/03.交互性编程.md","交互性编程","/pages/ec9ba3/"],["05.Python/04.注释.md","注释","/pages/d37106/"],["05.Python/05.变量以及数据类型.md","变量以及数据类型","/pages/f3c5b0/"],["05.Python/06.列表类型.md","列表类型","/pages/d7e80b/"],["05.Python/07.字典类型.md","字典类型","/pages/bbada7/"],["05.Python/08.集合类型.md","集合类型","/pages/11c241/"],["05.Python/09.元组类型.md","元组类型","/pages/684f07/"],["05.Python/10.查看数据类型.md","查看数据类型","/pages/1f8618/"],["05.Python/11.标识符.md","标识符","/pages/d9c568/"],["05.Python/12.输出.md","输出","/pages/1d58f1/"],["05.Python/13.输入.md","输入","/pages/c9ca02/"],["05.Python/14.二进制 八进制 十进制 十六进制.md","二进制 八进制 十进制 十六进制","/pages/6019e1/"],["05.Python/15.进制转换.md","进制转换","/pages/e533d0/"],["05.Python/16.bin、oct、hex内置函数.md","bin、oct、hex内置函数","/pages/1e0ec6/"],["05.Python/17.数据类型的转换.md","数据类型的转换","/pages/ebc10c/"],["05.Python/18.算数运算符.md","算数运算符","/pages/acb0af/"],["05.Python/19.分割.md","分割","/pages/687726/"],["05.Python/20.交换变量的值.md","交换变量的值","/pages/c13570/"],["05.Python/21.提取4位数的各位数.md","提取4位数的各位数","/pages/a15165/"],["05.Python/22.鸡兔同笼.md","鸡兔同笼","/pages/4d8423/"],["05.Python/23.圆的公式.md","圆的公式","/pages/bc04ac/"],["05.Python/24.比较运算符.md","比较运算符","/pages/661f38/"],["05.Python/25.逻辑运算符.md","逻辑运算符","/pages/70c3fd/"],["05.Python/26.位运算符.md","位运算符","/pages/8f779d/"],["05.Python/27.运算符的优先级.md","运算符的优先级","/pages/689d0c/"],["05.Python/28.if else  分支 条件判断语句.md","if else  分支 条件判断语句","/pages/0f98fb/"],["05.Python/29.if …elif…elif的使用.md","if …elif…elif的使用","/pages/9cbf41/"],["05.Python/30.if中的隐性转化.md","if中的隐性转化","/pages/a08b12/"],["05.Python/31.三元表达式.md","三元表达式","/pages/6be9fd/"],["05.Python/32.调试代码(Debug).md","调试代码(Debug)","/pages/921d6f/"],["05.Python/33.pass语句.md","pass语句","/pages/1f3eec/"],["05.Python/34.猜拳游戏.md","猜拳游戏","/pages/5f7ab5/"],["05.Python/35.随机数.md","随机数","/pages/559847/"],["05.Python/36.循环.md","循环","/pages/aed4d8/"],["05.Python/37.range 的使用.md","range 的使用","/pages/6bc87f/"],["05.Python/38.快捷键.md","快捷键","/pages/d3bc75/"],["05.Python/39.更改某个变量全部代码.md","更改某个变量全部代码","/pages/2d9020/"]]},{title:"Python模块",collapsable:!0,children:[["07.Python模块/01.request.md","request","/pages/cf7131/"],["07.Python模块/02.Beautifulsoup4.md","Beautifulsoup4","/pages/f14379/"],["07.Python模块/03.re 正则表达式.md","re 正则表达式","/pages/a3d900/"],["07.Python模块/04.jieba.md","jieba","/pages/4ab7a0/"],["07.Python模块/05.pymysql  数据库调用.md","pymysql  数据库调用","/pages/f8134b/"],["07.Python模块/06.selenium.md","selenium","/pages/34405e/"],["07.Python模块/07.time.md","time","/pages/fd5355/"],["07.Python模块/08.Falsk.md","Falsk","/pages/a6ad03/"]]},{title:"机器学习",collapsable:!0,children:[["08.机器学习/01.机器学习.md","机器学习","/pages/ddf9fb/"],["08.机器学习/02.matplotlib.md","matplotlib","/pages/9e1ba1/"],["08.机器学习/03.Numpy.md","Numpy","/pages/a58615/"],["08.机器学习/04.Pandas.md","Pandas","/pages/3f7274/"]]}],"/大数据/":[{title:"Hadoop",collapsable:!0,children:[["01.Hadoop/01.Hadoop.md","Hadoop","/pages/8e0c98/"],["01.Hadoop/02.环境安装.md","环境安装","/pages/083fca/"],["01.Hadoop/03.HDFS.md","HDFS","/pages/7afcbc/"],["01.Hadoop/04.winutils.md","winutils","/pages/981612/"],["01.Hadoop/05.IDEA中创建hadoop项目.md","IDEA中创建hadoop项目","/pages/7e6b01/"],["01.Hadoop/06.java操作.md","java操作","/pages/9ce9a0/"],["01.Hadoop/07.HDFS的数据流.md","HDFS的数据流","/pages/bbfc59/"],["01.Hadoop/08.NameNode 工作机制.md","NameNode 工作机制","/pages/785074/"],["01.Hadoop/09.DataNode.md","DataNode","/pages/49d0d4/"],["01.Hadoop/10.MapReduce.md","MapReduce","/pages/070d96/"],["01.Hadoop/11.MapReduce原理.md","MapReduce原理","/pages/65db7d/"],["01.Hadoop/12.Yarn.md","Yarn","/pages/89b57e/"],["01.Hadoop/13.Hadoop企业优化.md","Hadoop企业优化","/pages/ce529c/"],["01.Hadoop/14.Hadoop 新特性.md","Hadoop 新特性","/pages/8dd6c9/"],["01.Hadoop/15.日志.md","日志","/pages/1cc354/"],["01.Hadoop/16.Hadoop HA高可用.md","Hadoop HA高可用","/pages/0dc6f9/"]]},{title:"Zookeeper",collapsable:!0,children:[["02.Zookeeper/01.概述.md","概述","/pages/f38fc8/"],["02.Zookeeper/02.集群搭建.md","集群搭建","/pages/1597a2/"],["02.Zookeeper/03.客户端命令行操作.md","客户端命令行操作","/pages/be648b/"],["02.Zookeeper/04.内部原理.md","内部原理","/pages/75493d/"],["02.Zookeeper/05.Api.md","Api","/pages/4a9c91/"]]},{title:"Hive",collapsable:!0,children:[["03.Hive/01.介绍.md","介绍","/pages/2315d6/"],["03.Hive/02.环境.md","环境","/pages/66f25c/"],["03.Hive/03.DBeaver.md","DBeaver","/pages/5a826b/"],["03.Hive/04.Hive 类型.md","Hive 类型","/pages/f55408/"],["03.Hive/05.Hive 客户端命令.md","Hive 客户端命令","/pages/6652e3/"],["03.Hive/06.DDL数据定义.md","DDL数据定义","/pages/675adb/"],["03.Hive/07.DML.md","DML","/pages/ff1fce/"],["03.Hive/08.查询.md","查询","/pages/73e227/"],["03.Hive/09.函数.md","函数","/pages/8f0a49/"],["03.Hive/10.自定义函数.md","自定义函数","/pages/b1e5c6/"],["03.Hive/11.压缩和存储.md","压缩和存储","/pages/9c39ac/"],["03.Hive/12.企业优化.md","企业优化","/pages/447a54/"],["03.Hive/13.Hive实战.md","Hive实战","/pages/1aa014/"]]},{title:"Flume",collapsable:!0,children:[["04.Flume/01.Flume.md","Flume","/pages/b084db/"],["04.Flume/02.Flime基础架构.md","Flime基础架构","/pages/f8bd8e/"],["04.Flume/03.Flume安装.md","Flume安装","/pages/e462b7/"],["04.Flume/04.入门案例.md","入门案例","/pages/c12064/"],["04.Flume/05.Flume 进阶.md","Flume 进阶","/pages/6a0c06/"],["04.Flume/06.自定义组件.md","自定义组件","/pages/8e2fbd/"],["04.Flume/07.面试题.md","面试题","/pages/800f9f/"]]},{title:"Kafka",collapsable:!0,children:[["05.Kafka/01.Kafka.md","Kafka","/pages/872cdc/"],["05.Kafka/02.架构.md","架构","/pages/3c04a9/"],["05.Kafka/03.Kafka 安装.md","Kafka 安装","/pages/6bd0d7/"],["05.Kafka/04.命令操作.md","命令操作","/pages/ab6220/"],["05.Kafka/05.Kafka原理.md","Kafka原理","/pages/6c830a/"],["05.Kafka/06.Kafka API.md","Kafka API","/pages/939509/"],["05.Kafka/07.Flume 对接 Kafka.md","Flume 对接 Kafka","/pages/cf0e70/"],["05.Kafka/08.Kafka监控.md","Kafka监控","/pages/f5b30c/"],["05.Kafka/09.Kafka面试题.md","Kafka面试题","/pages/cf96cc/"]]},{title:"Azkaban",collapsable:!0,children:[["06.Azkaban/01.Azkaban.md","Azkaban","/pages/bac882/"],["06.Azkaban/02.任务调度.md","任务调度","/pages/c7cfc3/"],["06.Azkaban/03.安装.md","安装","/pages/394c5d/"],["06.Azkaban/04.Azkaban实战.md","Azkaban实战","/pages/4ff79d/"]]},{title:"Hbase",collapsable:!0,children:[["07.Hbase/01.Hbase.md","Hbase","/pages/310c39/"],["07.Hbase/02.Hbase数据模型.md","Hbase数据模型","/pages/817c32/"],["07.Hbase/03.Hbase 安装.md","Hbase 安装","/pages/a8d091/"],["07.Hbase/04.Hbase shell.md","Hbase shell","/pages/73c948/"],["07.Hbase/05.Hbase原理.md","Hbase原理","/pages/ec28f2/"],["07.Hbase/06.Phoenix.md","Phoenix","/pages/6dd88b/"],["07.Hbase/07.Hbase与Hive的集成.md","Hbase与Hive的集成","/pages/e654ab/"],["07.Hbase/08.HBase优化.md","HBase优化","/pages/ce507c/"]]},{title:"Scala",collapsable:!0,children:[["08.Scala/01.Scala介绍.md","Scala介绍","/pages/d2afbe/"],["08.Scala/02.Scala入门.md","Scala入门","/pages/a93b77/"],["08.Scala/03.流程控制.md","流程控制","/pages/8b0acc/"],["08.Scala/04.函数式编程.md","函数式编程","/pages/b7cb84/"]]},{title:"Spark",collapsable:!0,children:[["09.Spark/01.Spark.md","Spark","/pages/cc783e/"],["09.Spark/02.Spark 入门.md","Spark 入门","/pages/3223e3/"],["09.Spark/03.SprakCore.md","SprakCore","/pages/a0eb57/"],["09.Spark/04.SparkSQL.md","SparkSQL","/pages/0de4e9/"]]}]},author:{name:"Iekr",link:"https://github.com/Iekrwh"},blogger:{avatar:"https://gitee.com/Iekrwh/md-images/raw/master/images/$7ZY44WI036RW5{EPWQXCX6.jpg",name:"Iekr",slogan:"苦逼后端开发"},social:{icons:[{iconClass:"icon-youjian",title:"发邮件",link:"mailto:iekr_wh@qq.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/Iekrwh"},{iconClass:"icon-erji",title:"听音乐",link:"http://music.163.com/album?id=73927024"}]},footer:{createYear:2022,copyrightInfo:'Evan Xu | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a>'},htmlModules:{homeSidebarB:'<div style="padding: 0.95rem">\n    <p style="\n      color: var(--textColor);\n      opacity: 0.9;\n      font-size: 20px;\n      font-weight: bold;\n      margin: 0 0 8px 0;\n    ">放点图\n    <img src="https://gitee.com/Iekrwh/md-images/raw/master/images/2$B8JFSXYL_RLH8_GYY7AS1.gif"  style="width:100%;" />\n    </div>'},blogInfo:{blogCreate:"2022-03-18",indexView:!0,pageView:!0,readingTime:!0,eachFileWords:[{name:"位运算",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\01.位运算.md",wordsCount:509,readingTime:"2.1m",title:"位运算",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/8e5251/",categories:[408,"数据结构"],tags:[null]},{name:"最基本的数据结构",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\02.最基本的数据结构.md",wordsCount:62,readingTime:"1",title:"最基本的数据结构",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/38d7c8/",categories:[408,"数据结构"],tags:[null]},{name:"前缀和数组",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\03.前缀和数组.md",wordsCount:183,readingTime:"1",title:"前缀和数组",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/3ad1d4/",categories:[408,"数据结构"],tags:[null]},{name:"random的随机行为",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\04.random的随机行为.md",wordsCount:668,readingTime:"2.7m",title:"random的随机行为",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/64e136/",categories:[408,"数据结构"],tags:[null]},{name:"对数器",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\05.对数器.md",wordsCount:156,readingTime:"1",title:"对数器",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/9c9f24/",categories:[408,"数据结构"],tags:[null]},{name:"二分法查找",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\06.二分法查找.md",wordsCount:178,readingTime:"1",title:"二分法查找",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/06b58c/",categories:[408,"数据结构"],tags:[null]},{name:"方法参数传递是值还是引用",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\07.方法参数传递是值还是引用.md",wordsCount:397,readingTime:"1.3m",title:"方法参数传递是值还是引用",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/7e9265/",categories:[408,"数据结构"],tags:[null]},{name:"链表",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\08.链表.md",wordsCount:"2.3k",readingTime:"10.8m",title:"链表",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/d3b8b4/",categories:[408,"数据结构"],tags:[null]},{name:"位图",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\09.位图.md",wordsCount:206,readingTime:"1",title:"位图",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/d2cd3f/",categories:[408,"数据结构"],tags:[null]},{name:"位运算实现四则运算",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\10.位运算实现四则运算.md",wordsCount:854,readingTime:"4.1m",title:"位运算实现四则运算",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/51bf81/",categories:[408,"数据结构"],tags:[null]},{name:"二叉树",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\11.二叉树.md",wordsCount:"2.1k",readingTime:"10.7m",title:"二叉树",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/3e25e1/",categories:[408,"数据结构"],tags:[null]},{name:"排序",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\12.排序.md",wordsCount:"2.1k",readingTime:"10.6m",title:"排序",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/0dcdc5/",categories:[408,"数据结构"],tags:[null]},{name:"时间复杂度",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\13.时间复杂度.md",wordsCount:395,readingTime:"1.4m",title:"时间复杂度",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/ef4d8b/",categories:[408,"数据结构"],tags:[null]},{name:"队列和栈",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\14.队列和栈.md",wordsCount:477,readingTime:"2.5m",title:"队列和栈",date:"2022-03-18T16:03:55.000Z",permalink:"/pages/3bda9f/",categories:[408,"数据结构"],tags:[null]},{name:"递归",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\15.递归.md",wordsCount:93,readingTime:"1",title:"递归",date:"2022-03-18T16:03:56.000Z",permalink:"/pages/fe2eaf/",categories:[408,"数据结构"],tags:[null]},{name:"堆(优先级队列)",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\408\\01.数据结构\\16.堆(优先级队列).md",wordsCount:605,readingTime:"2.7m",title:"堆(优先级队列)",date:"2022-03-18T16:03:56.000Z",permalink:"/pages/88e08c/",categories:[408,"数据结构"],tags:[null]},{name:"我做了一个手写春联小网页，祝大家虎年暴富",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\_posts\\随笔\\我做了一个手写春联小网页，祝大家虎年暴富.md",wordsCount:"2.9k",readingTime:"14.6m",title:"我做了一个手写春联小网页，祝大家虎年暴富",date:"2022-01-28T14:59:51.000Z",permalink:"/pages/829589/",titleTag:"原创",sidebar:"auto",categories:["随笔"],tags:[null]},{name:"BS架构",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\01.BS架构.md",wordsCount:179,readingTime:"1",title:"BS架构",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/23186a/",categories:["前端","Html"],tags:[null]},{name:"HTML",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\02.HTML.md",wordsCount:"1.3k",readingTime:"5.1m",title:"HTML",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/3ba523/",categories:["前端","Html"],tags:[null]},{name:"HTML5",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\03.HTML5.md",wordsCount:574,readingTime:"2.2m",title:"HTML5",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/8600fc/",categories:["前端","Html"],tags:[null]},{name:"表单标签",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\04.表单标签.md",wordsCount:755,readingTime:"2.8m",title:"表单标签",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/c2f00e/",categories:["前端","Html"],tags:[null]},{name:"CSS",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\05.CSS.md",wordsCount:"5.3k",readingTime:"20m",title:"CSS",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/b3f43c/",categories:["前端","Html"],tags:[null]},{name:"html+css网页",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\06.html+css网页.md",wordsCount:756,readingTime:"3m",title:"html+css网页",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/5bc756/",categories:["前端","Html"],tags:[null]},{name:"CSS3",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\07.CSS3.md",wordsCount:"1.5k",readingTime:"6m",title:"CSS3",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/54a5fa/",categories:["前端","Html"],tags:[null]},{name:"移动布局",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\08.移动布局.md",wordsCount:"2.2k",readingTime:"8.7m",title:"移动布局",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/dc74bd/",categories:["前端","Html"],tags:[null]},{name:"JavaScript",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\01.html\\09.JavaScript.md",wordsCount:"3.4k",readingTime:"13.5m",title:"JavaScript",date:"2022-03-18T00:54:47.000Z",permalink:"/pages/e5e5b8/",categories:["前端","Html"],tags:[null]},{name:"vue 补充",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\00.vue 补充.md",wordsCount:"1.2k",readingTime:"4.5m",title:"vue 补充",date:"2022-03-18T00:55:26.000Z",permalink:"/pages/6af871/",categories:["前端","Vue2"],tags:[null]},{name:"安装Vue CLI",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\01.安装Vue CLI.md",wordsCount:99,readingTime:"1",title:"安装Vue CLI",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/758bea/",categories:["前端","Vue2"],tags:[null]},{name:"在Vue CLI中导入 Elment",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\02.在Vue CLI中导入 Elment.md",wordsCount:57,readingTime:"1",title:"在Vue CLI中导入 Elment",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/3b8d98/",categories:["前端","Vue2"],tags:[null]},{name:"Elment",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\03.Elment.md",wordsCount:549,readingTime:"2.2m",title:"Elment",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/cb74f3/",categories:["前端","Vue2"],tags:[null]},{name:"Axios",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\04.Axios.md",wordsCount:586,readingTime:"2.3m",title:"Axios",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/e17f8f/",categories:["前端","Vue2"],tags:[null]},{name:"配置Vue路由",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\05.配置Vue路由.md",wordsCount:140,readingTime:"1",title:"配置Vue路由",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/efa853/",categories:["前端","Vue2"],tags:[null]},{name:"github 推送",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\06.github 推送.md",wordsCount:127,readingTime:"1",title:"github 推送",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/56d54a/",categories:["前端","Vue2"],tags:[null]},{name:"安装 elment-tiptap",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\07.安装 elment-tiptap.md",wordsCount:398,readingTime:"1.9m",title:"安装 elment-tiptap",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/93608e/",categories:["前端","Vue2"],tags:[null]},{name:"安装 cropperjs 图片裁切工具",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\08.安装 cropperjs 图片裁切工具.md",wordsCount:38,readingTime:"1",title:"安装 cropperjs 图片裁切工具",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/f0d787/",categories:["前端","Vue2"],tags:[null]},{name:"让两个组件之间通讯",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\09.让两个组件之间通讯.md",wordsCount:130,readingTime:"1",title:"让两个组件之间通讯",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/e5e482/",categories:["前端","Vue2"],tags:[null]},{name:"安装echart",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\10.安装echart.md",wordsCount:49,readingTime:"1",title:"安装echart",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/812e04/",categories:["前端","Vue2"],tags:[null]},{name:"文件对象",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\11.文件对象.md",wordsCount:215,readingTime:"1",title:"文件对象",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/2bf2c3/",categories:["前端","Vue2"],tags:[null]},{name:"打包发布",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\12.打包发布.md",wordsCount:749,readingTime:"3.3m",title:"打包发布",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/fc097d/",categories:["前端","Vue2"],tags:[null]},{name:"webpack",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\13.webpack.md",wordsCount:"2.1k",readingTime:"9m",title:"webpack",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/b60752/",categories:["前端","Vue2"],tags:[null]},{name:"vue 版本",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\14.vue 版本.md",wordsCount:58,readingTime:"1",title:"vue 版本",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/0a8d91/",categories:["前端","Vue2"],tags:[null]},{name:"优化打包",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\15.优化打包.md",wordsCount:519,readingTime:"2.1m",title:"优化打包",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/b8f860/",categories:["前端","Vue2"],tags:[null]},{name:"vue 图形界面",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\16.vue 图形界面.md",wordsCount:58,readingTime:"1",title:"vue 图形界面",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/c64d65/",categories:["前端","Vue2"],tags:[null]},{name:"路由懒加载",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\17.路由懒加载.md",wordsCount:243,readingTime:"1",title:"路由懒加载",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/27ac6a/",categories:["前端","Vue2"],tags:[null]},{name:"element 按需引用",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\18.element 按需引用.md",wordsCount:77,readingTime:"1",title:"element 按需引用",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/1bc0dd/",categories:["前端","Vue2"],tags:[null]},{name:"缓存和并行处理",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\19.缓存和并行处理.md",wordsCount:90,readingTime:"1",title:"缓存和并行处理",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/14f647/",categories:["前端","Vue2"],tags:[null]},{name:"JavaScript 异步编程",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\前端\\02.Vue2\\20.JavaScript 异步编程.md",wordsCount:668,readingTime:"2.8m",title:"JavaScript 异步编程",date:"2022-03-18T00:55:10.000Z",permalink:"/pages/195534/",categories:["前端","Vue2"],tags:[null]},{name:"java跨平台原理",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\01.java跨平台原理.md",wordsCount:54,readingTime:"1",title:"java跨平台原理",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/df8281/",categories:["后端","Java"],tags:[null]},{name:"JRE(Java Runtime Enviroment)",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\02.JRE(Java Runtime Enviroment).md",wordsCount:49,readingTime:"1",title:"JRE(Java Runtime Enviroment)",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/0c8879/",categories:["后端","Java"],tags:[null]},{name:"JDK(Java Development Kit)",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\03.JDK(Java Development Kit).md",wordsCount:99,readingTime:"1",title:"JDK(Java Development Kit)",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/d217f4/",categories:["后端","Java"],tags:[null]},{name:"常用dos命令",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\04.常用dos命令.md",wordsCount:103,readingTime:"1",title:"常用dos命令",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/47da21/",categories:["后端","Java"],tags:[null]},{name:"配置系统path环境变量",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\05.配置系统path环境变量.md",wordsCount:64,readingTime:"1",title:"配置系统path环境变量",date:"2022-03-18T00:48:41.000Z",permalink:"/pages/3ee21a/",categories:["后端","Java"],tags:[null]},{name:"开发运行流程",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\06.开发运行流程.md",wordsCount:79,readingTime:"1",title:"开发运行流程",date:"2022-03-18T00:48:41.000Z",permalink:"/pages/417c2d/",categories:["后端","Java"],tags:[null]},{name:"编写规范问题",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\07.编写规范问题.md",wordsCount:71,readingTime:"1",title:"编写规范问题",date:"2022-03-18T00:48:41.000Z",permalink:"/pages/977c49/",categories:["后端","Java"],tags:[null]},{name:"基础语法",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\08.基础语法.md",wordsCount:100,readingTime:"1",title:"基础语法",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/97d4f2/",categories:["后端","Java"],tags:[null]},{name:"关键字",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\09.关键字.md",wordsCount:32,readingTime:"1",title:"关键字",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/d2d164/",categories:["后端","Java"],tags:[null]},{name:"常量",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\10.常量.md",wordsCount:204,readingTime:"1",title:"常量",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/10baa9/",categories:["后端","Java"],tags:[null]},{name:"数据类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\11.数据类型.md",wordsCount:74,readingTime:"1",title:"数据类型",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/ec81f9/",categories:["后端","Java"],tags:[null]},{name:"数值型内容占用和取整范围",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\12.数值型内容占用和取整范围.md",wordsCount:134,readingTime:"1",title:"数值型内容占用和取整范围",date:"2022-03-18T00:48:36.000Z",permalink:"/pages/3f6e03/",categories:["后端","Java"],tags:[null]},{name:"变量",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\13.变量.md",wordsCount:70,readingTime:"1",title:"变量",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/558212/",categories:["后端","Java"],tags:[null]},{name:"标识符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\14.标识符.md",wordsCount:153,readingTime:"1",title:"标识符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/fcb871/",categories:["后端","Java"],tags:[null]},{name:"类型转换",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\15.类型转换.md",wordsCount:136,readingTime:"1",title:"类型转换",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/b2c2df/",categories:["后端","Java"],tags:[null]},{name:"算术运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\16.算术运算符.md",wordsCount:90,readingTime:"1",title:"算术运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/a3cd43/",categories:["后端","Java"],tags:[null]},{name:"自增自减运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\17.自增自减运算符.md",wordsCount:59,readingTime:"1",title:"自增自减运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/a4bf30/",categories:["后端","Java"],tags:[null]},{name:"关系运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\18.关系运算符.md",wordsCount:53,readingTime:"1",title:"关系运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/a5f46a/",categories:["后端","Java"],tags:[null]},{name:"逻辑运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\19.逻辑运算符.md",wordsCount:73,readingTime:"1",title:"逻辑运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ae5f03/",categories:["后端","Java"],tags:[null]},{name:"短路逻辑运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\20.短路逻辑运算符.md",wordsCount:106,readingTime:"1",title:"短路逻辑运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/5cf2b5/",categories:["后端","Java"],tags:[null]},{name:"三元运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\21.三元运算符.md",wordsCount:77,readingTime:"1",title:"三元运算符",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ad2097/",categories:["后端","Java"],tags:[null]},{name:"数据输入",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\22.数据输入.md",wordsCount:103,readingTime:"1",title:"数据输入",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/9ca53c/",categories:["后端","Java"],tags:[null]},{name:"流程控制",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\23.流程控制.md",wordsCount:449,readingTime:"1.8m",title:"流程控制",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/fc3701/",categories:["后端","Java"],tags:[null]},{name:"循环语句",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\24.循环语句.md",wordsCount:347,readingTime:"1.3m",title:"循环语句",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/db81b2/",categories:["后端","Java"],tags:[null]},{name:"idea中的辅助键",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\25.idea中的辅助键.md",wordsCount:127,readingTime:"1",title:"idea中的辅助键",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/3455ab/",categories:["后端","Java"],tags:[null]},{name:"数组",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\26.数组.md",wordsCount:699,readingTime:"2.5m",title:"数组",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/add390/",categories:["后端","Java"],tags:[null]},{name:"方法",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\27.方法.md",wordsCount:675,readingTime:"2.4m",title:"方法",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/8361e2/",categories:["后端","Java"],tags:[null]},{name:"面向对象基础",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\28.面向对象基础.md",wordsCount:"1.1k",readingTime:"4m",title:"面向对象基础",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/0a58cf/",categories:["后端","Java"],tags:[null]},{name:"标准类",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\29.标准类.md",wordsCount:147,readingTime:"1",title:"标准类",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ab994e/",categories:["后端","Java"],tags:[null]},{name:"字符串",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\30.字符串.md",wordsCount:885,readingTime:"3.2m",title:"字符串",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/8a171b/",categories:["后端","Java"],tags:[null]},{name:"ArrayList集合",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\31.ArrayList集合.md",wordsCount:291,readingTime:"1.1m",title:"ArrayList集合",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/ab65fb/",categories:["后端","Java"],tags:[null]},{name:"继承",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\32.继承.md",wordsCount:919,readingTime:"3.2m",title:"继承",date:"2022-03-18T00:48:37.000Z",permalink:"/pages/c7fa91/",categories:["后端","Java"],tags:[null]},{name:"修饰符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\33.修饰符.md",wordsCount:66,readingTime:"1",title:"修饰符",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/f3b824/",categories:["后端","Java"],tags:[null]},{name:"权限修饰符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\34.权限修饰符.md",wordsCount:31,readingTime:"1",title:"权限修饰符",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/047301/",categories:["后端","Java"],tags:[null]},{name:"状态修饰符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\35.状态修饰符.md",wordsCount:248,readingTime:"1",title:"状态修饰符",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/392033/",categories:["后端","Java"],tags:[null]},{name:"多态",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\36.多态.md",wordsCount:295,readingTime:"1m",title:"多态",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/6407a8/",categories:["后端","Java"],tags:[null]},{name:"抽象",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\37.抽象.md",wordsCount:224,readingTime:"1",title:"抽象",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/a12790/",categories:["后端","Java"],tags:[null]},{name:"接口",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\38.接口.md",wordsCount:278,readingTime:"1m",title:"接口",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/db4705/",categories:["后端","Java"],tags:[null]},{name:"类和接口的关系",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\39.类和接口的关系.md",wordsCount:115,readingTime:"1",title:"类和接口的关系",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/b47784/",categories:["后端","Java"],tags:[null]},{name:"抽象类与接口的区别",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\40.抽象类与接口的区别.md",wordsCount:161,readingTime:"1",title:"抽象类与接口的区别",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/102423/",categories:["后端","Java"],tags:[null]},{name:"类名作为形参和返回值",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\41.类名作为形参和返回值.md",wordsCount:138,readingTime:"1",title:"类名作为形参和返回值",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/384827/",categories:["后端","Java"],tags:[null]},{name:"内部类",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\42.内部类.md",wordsCount:878,readingTime:"3.6m",title:"内部类",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/86e52c/",categories:["后端","Java"],tags:[null]},{name:"Api",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\43.Api.md",wordsCount:706,readingTime:"3m",title:"Api",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/a4cd8f/",categories:["后端","Java"],tags:[null]},{name:"异常",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\44.异常.md",wordsCount:611,readingTime:"2.5m",title:"异常",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/236d5b/",categories:["后端","Java"],tags:[null]},{name:"集合",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\45.集合.md",wordsCount:744,readingTime:"2.7m",title:"集合",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/f58543/",categories:["后端","Java"],tags:[null]},{name:"泛型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\46.泛型.md",wordsCount:527,readingTime:"2.2m",title:"泛型",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/d6e1c4/",categories:["后端","Java"],tags:[null]},{name:"Set集合",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\47.Set集合.md",wordsCount:357,readingTime:"1.4m",title:"Set集合",date:"2022-03-18T00:48:38.000Z",permalink:"/pages/5137f9/",categories:["后端","Java"],tags:[null]},{name:"树",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\48.树.md",wordsCount:"1.5k",readingTime:"5.6m",title:"树",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/6e8f69/",categories:["后端","Java"],tags:[null]},{name:"TreeSet遍历",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\49.TreeSet遍历.md",wordsCount:37,readingTime:"1",title:"TreeSet遍历",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/8b6f0f/",categories:["后端","Java"],tags:[null]},{name:"哈希值",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\50.哈希值.md",wordsCount:140,readingTime:"1",title:"哈希值",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/1793cd/",categories:["后端","Java"],tags:[null]},{name:"哈希表",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\51.哈希表.md",wordsCount:54,readingTime:"1",title:"哈希表",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/7525b1/",categories:["后端","Java"],tags:[null]},{name:"HashSet集合",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\52.HashSet集合.md",wordsCount:382,readingTime:"1.5m",title:"HashSet集合",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/642d0a/",categories:["后端","Java"],tags:[null]},{name:"Map 集合",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\53.Map 集合.md",wordsCount:177,readingTime:"1",title:"Map 集合",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/b8041f/",categories:["后端","Java"],tags:[null]},{name:"可变参数",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\54.可变参数.md",wordsCount:103,readingTime:"1",title:"可变参数",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/4fb557/",categories:["后端","Java"],tags:[null]},{name:"创建不可变的集合",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\55.创建不可变的集合.md",wordsCount:169,readingTime:"1",title:"创建不可变的集合",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/7db438/",categories:["后端","Java"],tags:[null]},{name:"Stream流",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\56.Stream流.md",wordsCount:558,readingTime:"2.6m",title:"Stream流",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/46103a/",categories:["后端","Java"],tags:[null]},{name:"方法引用",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\57.方法引用.md",wordsCount:221,readingTime:"1",title:"方法引用",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/566611/",categories:["后端","Java"],tags:[null]},{name:"File",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\58.File.md",wordsCount:328,readingTime:"1.4m",title:"File",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/48b908/",categories:["后端","Java"],tags:[null]},{name:"多线程",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\59.多线程.md",wordsCount:"1.4k",readingTime:"5.4m",title:"多线程",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/606294/",categories:["后端","Java"],tags:[null]},{name:"多线程高级",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\60.多线程高级.md",wordsCount:"2.2k",readingTime:"7.9m",title:"多线程高级",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/c73ee4/",categories:["后端","Java"],tags:[null]},{name:"网络编程",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\61.网络编程.md",wordsCount:"1.3k",readingTime:"5.3m",title:"网络编程",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/fc7fd4/",categories:["后端","Java"],tags:[null]},{name:"类加载器",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\62.类加载器.md",wordsCount:368,readingTime:"1.5m",title:"类加载器",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/a33e90/",categories:["后端","Java"],tags:[null]},{name:"反射",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\63.反射.md",wordsCount:979,readingTime:"4m",title:"反射",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/b3e612/",categories:["后端","Java"],tags:[null]},{name:"XML",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\64.XML.md",wordsCount:641,readingTime:"2.9m",title:"XML",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/6bad64/",categories:["后端","Java"],tags:[null]},{name:"枚举",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\65.枚举.md",wordsCount:186,readingTime:"1",title:"枚举",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/19426f/",categories:["后端","Java"],tags:[null]},{name:"注解",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\66.注解.md",wordsCount:348,readingTime:"1.2m",title:"注解",date:"2022-03-18T00:48:39.000Z",permalink:"/pages/1bf317/",categories:["后端","Java"],tags:[null]},{name:"单元测试",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\67.单元测试.md",wordsCount:120,readingTime:"1",title:"单元测试",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/7ba344/",categories:["后端","Java"],tags:[null]},{name:"日志",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\68.日志.md",wordsCount:419,readingTime:"1.8m",title:"日志",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/344d91/",categories:["后端","Java"],tags:[null]},{name:"HTTP协议",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\69.HTTP协议.md",wordsCount:201,readingTime:"1",title:"HTTP协议",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/a7a164/",categories:["后端","Java"],tags:[null]},{name:"Servlet",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\70.Servlet.md",wordsCount:907,readingTime:"3.8m",title:"Servlet",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/9c9dd6/",categories:["后端","Java"],tags:[null]},{name:"请求对象",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\71.请求对象.md",wordsCount:539,readingTime:"2m",title:"请求对象",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/83bc98/",categories:["后端","Java"],tags:[null]},{name:"响应对象",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\72.响应对象.md",wordsCount:524,readingTime:"2m",title:"响应对象",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/d35ebd/",categories:["后端","Java"],tags:[null]},{name:"Cookie",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\73.Cookie.md",wordsCount:313,readingTime:"1.2m",title:"Cookie",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/133dfe/",categories:["后端","Java"],tags:[null]},{name:"Session",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\74.Session.md",wordsCount:168,readingTime:"1",title:"Session",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/83bc22/",categories:["后端","Java"],tags:[null]},{name:"JSP",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\75.JSP.md",wordsCount:"1.1k",readingTime:"4.7m",title:"JSP",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/427528/",categories:["后端","Java"],tags:[null]},{name:"Listener",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\01.JavaSE\\76.Listener.md",wordsCount:554,readingTime:"2.1m",title:"Listener",date:"2022-03-18T00:48:40.000Z",permalink:"/pages/70e34e/",categories:["后端","Java"],tags:[null]},{name:"JDBC",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\01.JDBC.md",wordsCount:"2.1k",readingTime:"8.9m",title:"JDBC",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/544a3f/",categories:["后端","JavaEE"],tags:[null]},{name:"MyBatis",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\02.MyBatis.md",wordsCount:"3.7k",readingTime:"16.2m",title:"MyBatis",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/c1aa2b/",categories:["后端","JavaEE"],tags:[null]},{name:"Jackson",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\03.Jackson.md",wordsCount:402,readingTime:"2.1m",title:"Jackson",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/df2f58/",categories:["后端","JavaEE"],tags:[null]},{name:"Jedis",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\04.Jedis.md",wordsCount:573,readingTime:"2.7m",title:"Jedis",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/40c5ff/",categories:["后端","JavaEE"],tags:[null]},{name:"Maven",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\05.Maven.md",wordsCount:767,readingTime:"3.5m",title:"Maven",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/682f06/",categories:["后端","JavaEE"],tags:[null]},{name:"POI",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\06.POI.md",wordsCount:469,readingTime:"2.2m",title:"POI",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/528ce7/",categories:["后端","JavaEE"],tags:[null]},{name:"Spring",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\07.Spring.md",wordsCount:"6.4k",readingTime:"30.4m",title:"Spring",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/17e650/",categories:["后端","JavaEE"],tags:[null]},{name:"Spring MVC",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\08.Spring MVC.md",wordsCount:"4.6k",readingTime:"21.3m",title:"Spring MVC",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/8cd1ce/",categories:["后端","JavaEE"],tags:[null]},{name:"Maven 高级",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\09.Maven 高级.md",wordsCount:727,readingTime:"3.2m",title:"Maven 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/127f3b/",categories:["后端","JavaEE"],tags:[null]},{name:"Dubbo",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\10.Dubbo.md",wordsCount:"1.6k",readingTime:"6.4m",title:"Dubbo",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/dc966e/",categories:["后端","JavaEE"],tags:[null]},{name:"Zookeeper",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\11.Zookeeper.md",wordsCount:"2.4k",readingTime:"10.3m",title:"Zookeeper",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/7bc195/",categories:["后端","JavaEE"],tags:[null]},{name:"Spring Security",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\12.Spring Security.md",wordsCount:"2.6k",readingTime:"11.9m",title:"Spring Security",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/ca6c88/",categories:["后端","JavaEE"],tags:[null]},{name:"Spring Boot",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\13.Spring Boot.md",wordsCount:"1.7k",readingTime:"7.5m",title:"Spring Boot",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/4f5fb3/",categories:["后端","JavaEE"],tags:[null]},{name:"Spring Boot 高级",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\14.Spring Boot 高级.md",wordsCount:"2.3k",readingTime:"10.9m",title:"Spring Boot 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/207023/",categories:["后端","JavaEE"],tags:[null]},{name:"RabbitMQ",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\15.RabbitMQ.md",wordsCount:"4.2k",readingTime:"21.2m",title:"RabbitMQ",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/3a5e24/",categories:["后端","JavaEE"],tags:[null]},{name:"RabbitMQ 高级",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\16.RabbitMQ 高级.md",wordsCount:"3.1k",readingTime:"14.5m",title:"RabbitMQ 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/da3871/",categories:["后端","JavaEE"],tags:[null]},{name:"Spring Cloud",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\17.Spring Cloud.md",wordsCount:"8.7k",readingTime:"40.1m",title:"Spring Cloud",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/e886f8/",categories:["后端","JavaEE"],tags:[null]},{name:"Docker",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\18.Docker.md",wordsCount:"2.1k",readingTime:"9.4m",title:"Docker",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/c588b5/",categories:["后端","JavaEE"],tags:[null]},{name:"ElasticSearch",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\19.ElasticSearch.md",wordsCount:"2.8k",readingTime:"12.8m",title:"ElasticSearch",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/95da1a/",categories:["后端","JavaEE"],tags:[null]},{name:"ElasticSearch 高级",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\02.JavaEE\\20.ElasticSearch 高级.md",wordsCount:"4.5k",readingTime:"20.6m",title:"ElasticSearch 高级",date:"2022-03-18T00:51:47.000Z",permalink:"/pages/cf92bb/",categories:["后端","JavaEE"],tags:[null]},{name:"Linux",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\03.Linux\\01.Linux.md",wordsCount:"4.2k",readingTime:"17.2m",title:"Linux",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/124a07/",categories:["后端","Linux"],tags:[null]},{name:"shell",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\03.Linux\\02.shell.md",wordsCount:558,readingTime:"2.3m",title:"shell",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/f9e4a1/",categories:["后端","Linux"],tags:[null]},{name:"Nginx",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\03.Linux\\03.Nginx.md",wordsCount:223,readingTime:"1.1m",title:"Nginx",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/0aeef9/",categories:["后端","Linux"],tags:[null]},{name:"java",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\03.Linux\\04.java.md",wordsCount:123,readingTime:"1",title:"java",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/a70c4f/",categories:["后端","Linux"],tags:[null]},{name:"Tomcat",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\03.Linux\\05.Tomcat.md",wordsCount:79,readingTime:"1",title:"Tomcat",date:"2022-03-18T00:52:16.000Z",permalink:"/pages/f323f1/",categories:["后端","Linux"],tags:[null]},{name:"常见的数据库产品",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\01.常见的数据库产品.md",wordsCount:59,readingTime:"1",title:"常见的数据库产品",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/e051f6/",categories:["后端","SQL"],tags:[null]},{name:"数据库相关概念",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\02.数据库相关概念.md",wordsCount:104,readingTime:"1",title:"数据库相关概念",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4b2b60/",categories:["后端","SQL"],tags:[null]},{name:"数据库存储数据的特点",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\03.数据库存储数据的特点.md",wordsCount:82,readingTime:"1",title:"数据库存储数据的特点",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/d2b3d3/",categories:["后端","SQL"],tags:[null]},{name:"MySQL服务端",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\04.MySQL服务端.md",wordsCount:68,readingTime:"1",title:"MySQL服务端",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/641ffa/",categories:["后端","SQL"],tags:[null]},{name:"环境变量",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\05.环境变量.md",wordsCount:87,readingTime:"1",title:"环境变量",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/a4dffe/",categories:["后端","SQL"],tags:[null]},{name:"命令符指令",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\06.命令符指令.md",wordsCount:358,readingTime:"1.3m",title:"命令符指令",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/8c64bc/",categories:["后端","SQL"],tags:[null]},{name:"SQL语句",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\07.SQL语句.md",wordsCount:148,readingTime:"1",title:"SQL语句",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4c9417/",categories:["后端","SQL"],tags:[null]},{name:"数据库",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\08.数据库.md",wordsCount:146,readingTime:"1",title:"数据库",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/04fedb/",categories:["后端","SQL"],tags:[null]},{name:"表",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\09.表.md",wordsCount:507,readingTime:"2.1m",title:"表",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/5a2265/",categories:["后端","SQL"],tags:[null]},{name:"约束",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\10.约束.md",wordsCount:515,readingTime:"2m",title:"约束",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/de8702/",categories:["后端","SQL"],tags:[null]},{name:"多表操作",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\11.多表操作.md",wordsCount:392,readingTime:"1.4m",title:"多表操作",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/89e8d8/",categories:["后端","SQL"],tags:[null]},{name:"视图",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\12.视图.md",wordsCount:132,readingTime:"1",title:"视图",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/3293cf/",categories:["后端","SQL"],tags:[null]},{name:"备份",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\13.备份.md",wordsCount:58,readingTime:"1",title:"备份",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/f561ab/",categories:["后端","SQL"],tags:[null]},{name:"MySQL 存储过程和函数",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\14.MySQL 存储过程和函数.md",wordsCount:444,readingTime:"1.7m",title:"MySQL 存储过程和函数",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/336822/",categories:["后端","SQL"],tags:[null]},{name:"触发器",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\15.触发器.md",wordsCount:324,readingTime:"1.3m",title:"触发器",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/108b35/",categories:["后端","SQL"],tags:[null]},{name:"事务",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\16.事务.md",wordsCount:726,readingTime:"2.6m",title:"事务",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/4c8706/",categories:["后端","SQL"],tags:[null]},{name:"存储引擎",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\17.存储引擎.md",wordsCount:220,readingTime:"1",title:"存储引擎",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/7d59ad/",categories:["后端","SQL"],tags:[null]},{name:"索引",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\18.索引.md",wordsCount:587,readingTime:"2.3m",title:"索引",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/acc2dd/",categories:["后端","SQL"],tags:[null]},{name:"锁",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\19.锁.md",wordsCount:605,readingTime:"2.1m",title:"锁",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/3c034d/",categories:["后端","SQL"],tags:[null]},{name:"MyCat 中间件",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\20.MyCat 中间件.md",wordsCount:"1.5k",readingTime:"7.1m",title:"MyCat 中间件",date:"2022-03-18T00:52:35.000Z",permalink:"/pages/3ca264/",categories:["后端","SQL"],tags:[null]},{name:"Nosql",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\21.Nosql.md",wordsCount:579,readingTime:"2.1m",title:"Nosql",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4a12e8/",categories:["后端","SQL"],tags:[null]},{name:"Redis",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\22.Redis.md",wordsCount:"2.1k",readingTime:"8.5m",title:"Redis",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/a002c8/",categories:["后端","SQL"],tags:[null]},{name:"Redis高级",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\23.Redis高级.md",wordsCount:"6.4k",readingTime:"24.3m",title:"Redis高级",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/4e4f7f/",categories:["后端","SQL"],tags:[null]},{name:"MongoDB",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\04.SQL\\24.MongoDB.md",wordsCount:"15k",readingTime:"1h5m",title:"MongoDB",date:"2022-03-18T00:52:36.000Z",permalink:"/pages/e5cb41/",categories:["后端","SQL"],tags:[null]},{name:"IDE",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\01.IDE.md",wordsCount:71,readingTime:"1",title:"IDE",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/35c9ab/",categories:["后端","Python"],tags:[null]},{name:"close project",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\02.close project.md",wordsCount:30,readingTime:"1",title:"close project",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/0c7ca5/",categories:["后端","Python"],tags:[null]},{name:"交互性编程",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\03.交互性编程.md",wordsCount:48,readingTime:"1",title:"交互性编程",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/ec9ba3/",categories:["后端","Python"],tags:[null]},{name:"注释",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\04.注释.md",wordsCount:54,readingTime:"1",title:"注释",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/d37106/",categories:["后端","Python"],tags:[null]},{name:"变量以及数据类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\05.变量以及数据类型.md",wordsCount:198,readingTime:"1",title:"变量以及数据类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/f3c5b0/",categories:["后端","Python"],tags:[null]},{name:"列表类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\06.列表类型.md",wordsCount:29,readingTime:"1",title:"列表类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/d7e80b/",categories:["后端","Python"],tags:[null]},{name:"字典类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\07.字典类型.md",wordsCount:60,readingTime:"1",title:"字典类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/bbada7/",categories:["后端","Python"],tags:[null]},{name:"集合类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\08.集合类型.md",wordsCount:35,readingTime:"1",title:"集合类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/11c241/",categories:["后端","Python"],tags:[null]},{name:"元组类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\09.元组类型.md",wordsCount:32,readingTime:"1",title:"元组类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/684f07/",categories:["后端","Python"],tags:[null]},{name:"查看数据类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\10.查看数据类型.md",wordsCount:88,readingTime:"1",title:"查看数据类型",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/1f8618/",categories:["后端","Python"],tags:[null]},{name:"标识符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\11.标识符.md",wordsCount:219,readingTime:"1",title:"标识符",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/d9c568/",categories:["后端","Python"],tags:[null]},{name:"输出",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\12.输出.md",wordsCount:115,readingTime:"1",title:"输出",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/1d58f1/",categories:["后端","Python"],tags:[null]},{name:"输入",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\13.输入.md",wordsCount:88,readingTime:"1",title:"输入",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/c9ca02/",categories:["后端","Python"],tags:[null]},{name:"二进制 八进制 十进制 十六进制",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\14.二进制 八进制 十进制 十六进制.md",wordsCount:136,readingTime:"1",title:"二进制 八进制 十进制 十六进制",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/6019e1/",categories:["后端","Python"],tags:[null]},{name:"进制转换",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\15.进制转换.md",wordsCount:377,readingTime:"1.6m",title:"进制转换",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/e533d0/",categories:["后端","Python"],tags:[null]},{name:"bin、oct、hex内置函数",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\16.bin、oct、hex内置函数.md",wordsCount:88,readingTime:"1",title:"bin、oct、hex内置函数",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/1e0ec6/",categories:["后端","Python"],tags:[null]},{name:"数据类型的转换",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\17.数据类型的转换.md",wordsCount:443,readingTime:"1.6m",title:"数据类型的转换",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/ebc10c/",categories:["后端","Python"],tags:[null]},{name:"算数运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\18.算数运算符.md",wordsCount:630,readingTime:"2.4m",title:"算数运算符",date:"2022-03-18T00:52:51.000Z",permalink:"/pages/acb0af/",categories:["后端","Python"],tags:[null]},{name:"分割",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\19.分割.md",wordsCount:104,readingTime:"1",title:"分割",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/687726/",categories:["后端","Python"],tags:[null]},{name:"交换变量的值",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\20.交换变量的值.md",wordsCount:82,readingTime:"1",title:"交换变量的值",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/c13570/",categories:["后端","Python"],tags:[null]},{name:"提取4位数的各位数",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\21.提取4位数的各位数.md",wordsCount:90,readingTime:"1",title:"提取4位数的各位数",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/a15165/",categories:["后端","Python"],tags:[null]},{name:"鸡兔同笼",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\22.鸡兔同笼.md",wordsCount:161,readingTime:"1",title:"鸡兔同笼",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/4d8423/",categories:["后端","Python"],tags:[null]},{name:"圆的公式",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\23.圆的公式.md",wordsCount:64,readingTime:"1",title:"圆的公式",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/bc04ac/",categories:["后端","Python"],tags:[null]},{name:"比较运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\24.比较运算符.md",wordsCount:204,readingTime:"1",title:"比较运算符",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/661f38/",categories:["后端","Python"],tags:[null]},{name:"逻辑运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\25.逻辑运算符.md",wordsCount:405,readingTime:"1.7m",title:"逻辑运算符",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/70c3fd/",categories:["后端","Python"],tags:[null]},{name:"位运算符",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\26.位运算符.md",wordsCount:370,readingTime:"1.6m",title:"位运算符",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/8f779d/",categories:["后端","Python"],tags:[null]},{name:"运算符的优先级",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\27.运算符的优先级.md",wordsCount:82,readingTime:"1",title:"运算符的优先级",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/689d0c/",categories:["后端","Python"],tags:[null]},{name:"if else  分支 条件判断语句",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\28.if else  分支 条件判断语句.md",wordsCount:375,readingTime:"1.5m",title:"if else  分支 条件判断语句",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/0f98fb/",categories:["后端","Python"],tags:[null]},{name:"if …elif…elif的使用",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\29.if …elif…elif的使用.md",wordsCount:235,readingTime:"1m",title:"if …elif…elif的使用",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/9cbf41/",categories:["后端","Python"],tags:[null]},{name:"if中的隐性转化",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\30.if中的隐性转化.md",wordsCount:78,readingTime:"1",title:"if中的隐性转化",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/a08b12/",categories:["后端","Python"],tags:[null]},{name:"三元表达式",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\31.三元表达式.md",wordsCount:97,readingTime:"1",title:"三元表达式",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/6be9fd/",categories:["后端","Python"],tags:[null]},{name:"调试代码(Debug)",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\32.调试代码(Debug).md",wordsCount:102,readingTime:"1",title:"调试代码(Debug)",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/921d6f/",categories:["后端","Python"],tags:[null]},{name:"pass语句",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\33.pass语句.md",wordsCount:95,readingTime:"1",title:"pass语句",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/1f3eec/",categories:["后端","Python"],tags:[null]},{name:"猜拳游戏",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\34.猜拳游戏.md",wordsCount:153,readingTime:"1",title:"猜拳游戏",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/5f7ab5/",categories:["后端","Python"],tags:[null]},{name:"随机数",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\35.随机数.md",wordsCount:61,readingTime:"1",title:"随机数",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/559847/",categories:["后端","Python"],tags:[null]},{name:"循环",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\36.循环.md",wordsCount:664,readingTime:"2.8m",title:"循环",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/aed4d8/",categories:["后端","Python"],tags:[null]},{name:"range 的使用",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\37.range 的使用.md",wordsCount:111,readingTime:"1",title:"range 的使用",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/6bc87f/",categories:["后端","Python"],tags:[null]},{name:"快捷键",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\38.快捷键.md",wordsCount:80,readingTime:"1",title:"快捷键",date:"2022-03-18T00:52:52.000Z",permalink:"/pages/d3bc75/",categories:["后端","Python"],tags:[null]},{name:"更改某个变量全部代码",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\05.Python\\39.更改某个变量全部代码.md",wordsCount:47,readingTime:"1",title:"更改某个变量全部代码",date:"2022-03-18T00:52:50.000Z",permalink:"/pages/2d9020/",categories:["后端","Python"],tags:[null]},{name:"request",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\01.request.md",wordsCount:690,readingTime:"3.1m",title:"request",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/cf7131/",categories:["后端","Python模块"],tags:[null]},{name:"Beautifulsoup4",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\02.Beautifulsoup4.md",wordsCount:"2.3k",readingTime:"11.1m",title:"Beautifulsoup4",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/f14379/",categories:["后端","Python模块"],tags:[null]},{name:"re 正则表达式",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\03.re 正则表达式.md",wordsCount:"1.5k",readingTime:"5.6m",title:"re 正则表达式",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/a3d900/",categories:["后端","Python模块"],tags:[null]},{name:"jieba",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\04.jieba.md",wordsCount:646,readingTime:"2.7m",title:"jieba",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/4ab7a0/",categories:["后端","Python模块"],tags:[null]},{name:"pymysql  数据库调用",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\05.pymysql  数据库调用.md",wordsCount:251,readingTime:"1.2m",title:"pymysql  数据库调用",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/f8134b/",categories:["后端","Python模块"],tags:[null]},{name:"selenium",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\06.selenium.md",wordsCount:202,readingTime:"1",title:"selenium",date:"2022-03-18T00:53:25.000Z",permalink:"/pages/34405e/",categories:["后端","Python模块"],tags:[null]},{name:"time",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\07.time.md",wordsCount:203,readingTime:"1m",title:"time",date:"2022-03-18T00:53:26.000Z",permalink:"/pages/fd5355/",categories:["后端","Python模块"],tags:[null]},{name:"Falsk",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\07.Python模块\\08.Falsk.md",wordsCount:65,readingTime:"1",title:"Falsk",date:"2022-03-18T00:53:26.000Z",permalink:"/pages/a6ad03/",categories:["后端","Python模块"],tags:[null]},{name:"机器学习",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\08.机器学习\\01.机器学习.md",wordsCount:696,readingTime:"2.9m",title:"机器学习",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/ddf9fb/",categories:["后端","机器学习"],tags:[null]},{name:"matplotlib",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\08.机器学习\\02.matplotlib.md",wordsCount:"1.4k",readingTime:"7.2m",title:"matplotlib",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/9e1ba1/",categories:["后端","机器学习"],tags:[null]},{name:"Numpy",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\08.机器学习\\03.Numpy.md",wordsCount:"2.7k",readingTime:"11.1m",title:"Numpy",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/a58615/",categories:["后端","机器学习"],tags:[null]},{name:"Pandas",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\后端\\08.机器学习\\04.Pandas.md",wordsCount:"7k",readingTime:"31.8m",title:"Pandas",date:"2022-03-18T00:53:49.000Z",permalink:"/pages/3f7274/",categories:["后端","机器学习"],tags:[null]},{name:"Hadoop",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\01.Hadoop.md",wordsCount:374,readingTime:"1.5m",title:"Hadoop",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/8e0c98/",categories:["Hadoop"],tags:["Hadoop"]},{name:"环境安装",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\02.环境安装.md",wordsCount:"2.5k",readingTime:"12.4m",title:"环境安装",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/083fca/",categories:["Hadoop"],tags:[null]},{name:"HDFS",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\03.HDFS.md",wordsCount:545,readingTime:"2.3m",title:"HDFS",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/7afcbc/",categories:["Hadoop"],tags:[null]},{name:"winutils",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\04.winutils.md",wordsCount:86,readingTime:"1",title:"winutils",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/981612/",categories:["Hadoop"],tags:[null]},{name:"IDEA中创建hadoop项目",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\05.IDEA中创建hadoop项目.md",wordsCount:205,readingTime:"1.1m",title:"IDEA中创建hadoop项目",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/7e6b01/",categories:["Hadoop"],tags:[null]},{name:"java操作",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\06.java操作.md",wordsCount:365,readingTime:"1.7m",title:"java操作",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/9ce9a0/",categories:["Hadoop"],tags:[null]},{name:"HDFS的数据流",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\07.HDFS的数据流.md",wordsCount:294,readingTime:"1.2m",title:"HDFS的数据流",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/bbfc59/",categories:["Hadoop"],tags:[null]},{name:"NameNode 工作机制",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\08.NameNode 工作机制.md",wordsCount:301,readingTime:"1.4m",title:"NameNode 工作机制",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/785074/",categories:["Hadoop"],tags:[null]},{name:"DataNode",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\09.DataNode.md",wordsCount:555,readingTime:"2.6m",title:"DataNode",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/49d0d4/",categories:["Hadoop"],tags:[null]},{name:"MapReduce",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\10.MapReduce.md",wordsCount:"1.5k",readingTime:"7.7m",title:"MapReduce",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/070d96/",categories:["Hadoop"],tags:[null]},{name:"MapReduce原理",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\11.MapReduce原理.md",wordsCount:"7k",readingTime:"34.9m",title:"MapReduce原理",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/65db7d/",categories:["Hadoop"],tags:[null]},{name:"Yarn",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\12.Yarn.md",wordsCount:"2k",readingTime:"9.6m",title:"Yarn",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/89b57e/",categories:["Hadoop"],tags:[null]},{name:"Hadoop企业优化",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\13.Hadoop企业优化.md",wordsCount:"1.1k",readingTime:"4.4m",title:"Hadoop企业优化",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/ce529c/",categories:["Hadoop"],tags:[null]},{name:"Hadoop 新特性",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\14.Hadoop 新特性.md",wordsCount:504,readingTime:"2.3m",title:"Hadoop 新特性",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/8dd6c9/",categories:["Hadoop"],tags:[null]},{name:"日志",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\15.日志.md",wordsCount:170,readingTime:"1",title:"日志",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/1cc354/",categories:["Hadoop"],tags:[null]},{name:"Hadoop HA高可用",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\01.Hadoop\\16.Hadoop HA高可用.md",wordsCount:"1.9k",readingTime:"8.5m",title:"Hadoop HA高可用",date:"2022-03-17T21:41:51.000Z",permalink:"/pages/0dc6f9/",categories:["Hadoop"],tags:[null]},{name:"概述",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\02.Zookeeper\\01.概述.md",wordsCount:90,readingTime:"1",title:"概述",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/f38fc8/",categories:["大数据","Zookeeper"],tags:[null]},{name:"集群搭建",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\02.Zookeeper\\02.集群搭建.md",wordsCount:272,readingTime:"1.3m",title:"集群搭建",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/1597a2/",categories:["大数据","Zookeeper"],tags:[null]},{name:"客户端命令行操作",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\02.Zookeeper\\03.客户端命令行操作.md",wordsCount:219,readingTime:"1",title:"客户端命令行操作",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/be648b/",categories:["大数据","Zookeeper"],tags:[null]},{name:"内部原理",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\02.Zookeeper\\04.内部原理.md",wordsCount:909,readingTime:"3.5m",title:"内部原理",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/75493d/",categories:["大数据","Zookeeper"],tags:[null]},{name:"Api",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\02.Zookeeper\\05.Api.md",wordsCount:506,readingTime:"2.8m",title:"Api",date:"2022-03-17T22:07:07.000Z",permalink:"/pages/4a9c91/",categories:["大数据","Zookeeper"],tags:[null]},{name:"介绍",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\01.介绍.md",wordsCount:97,readingTime:"1",title:"介绍",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/2315d6/",categories:["大数据","Hive"],tags:[null]},{name:"环境",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\02.环境.md",wordsCount:"1.9k",readingTime:"10.3m",title:"环境",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/66f25c/",categories:["大数据","Hive"],tags:[null]},{name:"DBeaver",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\03.DBeaver.md",wordsCount:81,readingTime:"1",title:"DBeaver",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/5a826b/",categories:["大数据","Hive"],tags:[null]},{name:"Hive 类型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\04.Hive 类型.md",wordsCount:872,readingTime:"3.7m",title:"Hive 类型",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/f55408/",categories:["大数据","Hive"],tags:[null]},{name:"Hive 客户端命令",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\05.Hive 客户端命令.md",wordsCount:522,readingTime:"2.2m",title:"Hive 客户端命令",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/6652e3/",categories:["大数据","Hive"],tags:[null]},{name:"DDL数据定义",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\06.DDL数据定义.md",wordsCount:"1.8k",readingTime:"7.5m",title:"DDL数据定义",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/675adb/",categories:["大数据","Hive"],tags:[null]},{name:"DML",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\07.DML.md",wordsCount:566,readingTime:"2.5m",title:"DML",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/ff1fce/",categories:["大数据","Hive"],tags:[null]},{name:"查询",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\08.查询.md",wordsCount:"2.3k",readingTime:"9.8m",title:"查询",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/73e227/",categories:["大数据","Hive"],tags:[null]},{name:"函数",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\09.函数.md",wordsCount:"2k",readingTime:"8.9m",title:"函数",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/8f0a49/",categories:["大数据","Hive"],tags:[null]},{name:"自定义函数",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\10.自定义函数.md",wordsCount:528,readingTime:"2.6m",title:"自定义函数",date:"2022-03-17T22:07:28.000Z",permalink:"/pages/b1e5c6/",categories:["大数据","Hive"],tags:[null]},{name:"压缩和存储",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\11.压缩和存储.md",wordsCount:"1.6k",readingTime:"7.1m",title:"压缩和存储",date:"2022-03-17T22:07:28.000Z",permalink:"/pages/9c39ac/",categories:["大数据","Hive"],tags:[null]},{name:"企业优化",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\12.企业优化.md",wordsCount:"1.8k",readingTime:"7.8m",title:"企业优化",date:"2022-03-17T22:07:28.000Z",permalink:"/pages/447a54/",categories:["大数据","Hive"],tags:[null]},{name:"Hive实战",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\03.Hive\\13.Hive实战.md",wordsCount:"2.3k",readingTime:"11.3m",title:"Hive实战",date:"2022-03-17T22:07:27.000Z",permalink:"/pages/1aa014/",categories:["大数据","Hive"],tags:[null]},{name:"Flume",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\04.Flume\\01.Flume.md",wordsCount:77,readingTime:"1",title:"Flume",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/b084db/",categories:["大数据","Flume"],tags:[null]},{name:"Flime基础架构",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\04.Flume\\02.Flime基础架构.md",wordsCount:448,readingTime:"1.8m",title:"Flime基础架构",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/f8bd8e/",categories:["大数据","Flume"],tags:[null]},{name:"Flume安装",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\04.Flume\\03.Flume安装.md",wordsCount:251,readingTime:"1.2m",title:"Flume安装",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/e462b7/",categories:["大数据","Flume"],tags:[null]},{name:"入门案例",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\04.Flume\\04.入门案例.md",wordsCount:"2.2k",readingTime:"10.1m",title:"入门案例",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/c12064/",categories:["大数据","Flume"],tags:[null]},{name:"Flume 进阶",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\04.Flume\\05.Flume 进阶.md",wordsCount:"2.5k",readingTime:"11.9m",title:"Flume 进阶",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/6a0c06/",categories:["大数据","Flume"],tags:[null]},{name:"自定义组件",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\04.Flume\\06.自定义组件.md",wordsCount:"2.3k",readingTime:"11.2m",title:"自定义组件",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/8e2fbd/",categories:["大数据","Flume"],tags:[null]},{name:"面试题",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\04.Flume\\07.面试题.md",wordsCount:907,readingTime:"3.4m",title:"面试题",date:"2022-03-17T22:07:46.000Z",permalink:"/pages/800f9f/",categories:["大数据","Flume"],tags:[null]},{name:"Kafka",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\01.Kafka.md",wordsCount:267,readingTime:"1",title:"Kafka",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/872cdc/",categories:["大数据","Kafka"],tags:[null]},{name:"架构",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\02.架构.md",wordsCount:724,readingTime:"2.7m",title:"架构",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/3c04a9/",categories:["大数据","Kafka"],tags:[null]},{name:"Kafka 安装",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\03.Kafka 安装.md",wordsCount:473,readingTime:"2.3m",title:"Kafka 安装",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/6bd0d7/",categories:["大数据","Kafka"],tags:[null]},{name:"命令操作",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\04.命令操作.md",wordsCount:300,readingTime:"1.4m",title:"命令操作",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/ab6220/",categories:["大数据","Kafka"],tags:[null]},{name:"Kafka原理",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\05.Kafka原理.md",wordsCount:"3.4k",readingTime:"12.9m",title:"Kafka原理",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/6c830a/",categories:["大数据","Kafka"],tags:[null]},{name:"Kafka API",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\06.Kafka API.md",wordsCount:"2.8k",readingTime:"13.7m",title:"Kafka API",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/939509/",categories:["大数据","Kafka"],tags:[null]},{name:"Flume 对接 Kafka",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\07.Flume 对接 Kafka.md",wordsCount:202,readingTime:"1.1m",title:"Flume 对接 Kafka",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/cf0e70/",categories:["大数据","Kafka"],tags:[null]},{name:"Kafka监控",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\08.Kafka监控.md",wordsCount:596,readingTime:"3.3m",title:"Kafka监控",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/f5b30c/",categories:["大数据","Kafka"],tags:[null]},{name:"Kafka面试题",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\05.Kafka\\09.Kafka面试题.md",wordsCount:826,readingTime:"3.2m",title:"Kafka面试题",date:"2022-03-17T22:08:02.000Z",permalink:"/pages/cf96cc/",categories:["大数据","Kafka"],tags:[null]},{name:"Azkaban",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\06.Azkaban\\01.Azkaban.md",wordsCount:80,readingTime:"1",title:"Azkaban",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/bac882/",categories:["大数据","Azkaban"],tags:[null]},{name:"任务调度",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\06.Azkaban\\02.任务调度.md",wordsCount:194,readingTime:"1",title:"任务调度",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/c7cfc3/",categories:["大数据","Azkaban"],tags:[null]},{name:"安装",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\06.Azkaban\\03.安装.md",wordsCount:875,readingTime:"4.2m",title:"安装",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/394c5d/",categories:["大数据","Azkaban"],tags:[null]},{name:"Azkaban实战",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\06.Azkaban\\04.Azkaban实战.md",wordsCount:347,readingTime:"1.8m",title:"Azkaban实战",date:"2022-03-17T22:08:50.000Z",permalink:"/pages/4ff79d/",categories:["大数据","Azkaban"],tags:[null]},{name:"Hbase",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\01.Hbase.md",wordsCount:42,readingTime:"1",title:"Hbase",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/310c39/",categories:["大数据","Hbase"],tags:[null]},{name:"Hbase数据模型",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\02.Hbase数据模型.md",wordsCount:621,readingTime:"2.4m",title:"Hbase数据模型",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/817c32/",categories:["大数据","Hbase"],tags:[null]},{name:"Hbase 安装",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\03.Hbase 安装.md",wordsCount:494,readingTime:"2.5m",title:"Hbase 安装",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/a8d091/",categories:["大数据","Hbase"],tags:[null]},{name:"Hbase shell",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\04.Hbase shell.md",wordsCount:352,readingTime:"1.5m",title:"Hbase shell",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/73c948/",categories:["大数据","Hbase"],tags:[null]},{name:"Hbase原理",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\05.Hbase原理.md",wordsCount:"1.5k",readingTime:"6m",title:"Hbase原理",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/ec28f2/",categories:["大数据","Hbase"],tags:[null]},{name:"Phoenix",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\06.Phoenix.md",wordsCount:"2.5k",readingTime:"11.2m",title:"Phoenix",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/6dd88b/",categories:["大数据","Hbase"],tags:[null]},{name:"Hbase与Hive的集成",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\07.Hbase与Hive的集成.md",wordsCount:732,readingTime:"3.3m",title:"Hbase与Hive的集成",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/e654ab/",categories:["大数据","Hbase"],tags:[null]},{name:"HBase优化",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\07.Hbase\\08.HBase优化.md",wordsCount:"1.5k",readingTime:"5.7m",title:"HBase优化",date:"2022-03-17T22:09:08.000Z",permalink:"/pages/ce507c/",categories:["大数据","Hbase"],tags:[null]},{name:"Scala介绍",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\08.Scala\\01.Scala介绍.md",wordsCount:204,readingTime:"1",title:"Scala介绍",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/d2afbe/",categories:["大数据","Scala"],tags:[null]},{name:"Scala入门",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\08.Scala\\02.Scala入门.md",wordsCount:534,readingTime:"2.3m",title:"Scala入门",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/a93b77/",categories:["大数据","Scala"],tags:[null]},{name:"流程控制",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\08.Scala\\03.流程控制.md",wordsCount:785,readingTime:"3.5m",title:"流程控制",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/8b0acc/",categories:["大数据","Scala"],tags:[null]},{name:"函数式编程",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\08.Scala\\04.函数式编程.md",wordsCount:"2.6k",readingTime:"10.8m",title:"函数式编程",date:"2022-03-17T22:09:38.000Z",permalink:"/pages/b7cb84/",categories:["大数据","Scala"],tags:[null]},{name:"Spark",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\09.Spark\\01.Spark.md",wordsCount:549,readingTime:"2.1m",title:"Spark",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/cc783e/",categories:["大数据","Spark"],tags:[null]},{name:"Spark 入门",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\09.Spark\\02.Spark 入门.md",wordsCount:"4k",readingTime:"18.9m",title:"Spark 入门",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/3223e3/",categories:["大数据","Spark"],tags:[null]},{name:"SprakCore",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\09.Spark\\03.SprakCore.md",wordsCount:"1.7k",readingTime:"7.7m",title:"SprakCore",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/a0eb57/",categories:["大数据","Spark"],tags:[null]},{name:"SparkSQL",filePath:"D:\\code\\vuepress-theme-vdoing\\docs\\大数据\\09.Spark\\04.SparkSQL.md",wordsCount:743,readingTime:"2.8m",title:"SparkSQL",date:"2022-03-17T22:09:51.000Z",permalink:"/pages/0de4e9/",categories:["大数据","Spark"],tags:[null]}],mdFileCountType:"archives",totalWords:"archives",moutedEvent:".tags-wrapper",indexIteration:2500,pageIteration:2500},indexImg:{navColor:2,switchNavColor:!0,bgTimeColorArray:["transparent","transparent","transparent","transparent"],bgTimeColor:!0,descFade:!0,desc:["Iekr个人博客，积跬步以至千里，致敬每个爱学习的你 —— 来自 Evan Xu","故事由我书写，旅程由你见证，传奇由她聆听 —— 来自 Young Kbt","这一生波澜壮阔或是不惊都没问题 —— 来自 Weibo"],descFontSize:"1.4rem",descFadeInTime:200,descFadeOutTime:100,descNextTime:800,bubble:!0,bubblePosition:0,bubbleNum:200}},locales:{"/":{lang:"zh-CN",title:"Chiriri's blog",description:"后端技术博客,专注后端学习与总结。Java,Spring,Scala,Hadoop,Spark,Flink,Python,Linux,Docker等技术文章。",path:"/"}}},ll=(t(105),t(141)),cl=(t(159),t(143),t(224)),dl=t(225),pl=(t(237),t(239),t(45));var ul={computed:{$filterPosts:function(){return this.$site.pages.filter((function(n){var e=n.frontmatter,t=e.pageComponent,a=e.article,r=e.home;return!(t||!1===a||!0===r)}))},$sortPosts:function(){return(n=this.$filterPosts).sort((function(n,e){var t=n.frontmatter.sticky,a=e.frontmatter.sticky;return t&&a?t==a?Object(pl.a)(n,e):t-a:t&&!a?-1:!t&&a?1:Object(pl.a)(n,e)})),n;var n},$sortPostsByDate:function(){return(n=this.$filterPosts).sort((function(n,e){return Object(pl.a)(n,e)})),n;var n},$groupPosts:function(){return function(n){for(var e={},t={},a=function(a,r){var o=n[a].frontmatter,i=o.categories,s=o.tags;"array"===Object(pl.n)(i)&&i.forEach((function(t){t&&(e[t]||(e[t]=[]),e[t].push(n[a]))})),"array"===Object(pl.n)(s)&&s.forEach((function(e){e&&(t[e]||(t[e]=[]),t[e].push(n[a]))}))},r=0,o=n.length;r<o;r++)a(r);return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags:function(){return function(n){var e=[],t=[];for(var a in n.categories)e.push({key:a,length:n.categories[a].length});for(var r in n.tags)t.push({key:r,length:n.tags[r].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Dr.component(cl.default),Dr.component(dl.default);function ml(n){return n.toString().padStart(2,"0")}t(381);Dr.component("Fantasy",(function(){return Promise.all([t.e(0),t.e(6)]).then(t.bind(null,505))})),Dr.component("BlockToggle",(function(){return Promise.all([t.e(0),t.e(5)]).then(t.bind(null,506))})),Dr.component("IndexBigImg",(function(){return Promise.all([t.e(0),t.e(4)]).then(t.bind(null,507))})),Dr.component("LastReadingPopup",(function(){return Promise.resolve().then(t.bind(null,141))})),Dr.component("CodeBlock",(function(){return Promise.resolve().then(t.bind(null,224))})),Dr.component("Badge",(function(){return Promise.all([t.e(0),t.e(7)]).then(t.bind(null,805))})),Dr.component("CodeGroup",(function(){return Promise.resolve().then(t.bind(null,225))}));t(382);var gl,hl,fl=t(51),vl=(t(385),t(139),t(223)),bl=t.n(vl),yl=t(103);"valine"===(hl="gitalk")?t.e(308).then(t.t.bind(null,500,7)).then((function(n){return n.default})):"gitalk"===hl&&Promise.all([t.e(0),t.e(307)]).then(t.t.bind(null,501,7)).then((function(){return t.e(306).then(t.t.bind(null,502,7))})).then((function(n){return gl=n.default}));function xl(n,e){var t={};return Reflect.ownKeys(n).forEach((function(a){if("string"==typeof n[a])try{t[a]=bl.a.render(n[a],e)}catch(e){console.warn('Comment config option error at key named "'.concat(a,'"')),console.warn("More info: ".concat(e.message)),t[a]=n[a]}else t[a]=n[a]})),t}console.log('How to use "'.concat("gitalk",'" in ').concat(yl.name,"@v").concat(yl.version,":"),yl.homepage);var kl={render:function(n,e){var t=document.createElement("div");t.id=e,document.querySelector("main.page").appendChild(t),new gl(xl({clientID:"a6e1355287947096b88b",clientSecret:"f0e77d070fabfcd5af95bebb82b2d574d7248d71",repo:"blog-gitalk-comment",owner:"iekr",admin:["iekr"],pagerDirection:"last",id:"<%- (frontmatter.permalink || frontmatter.to.path).slice(-16) %>",title:"「评论」<%- frontmatter.title %>",labels:["Gitalk","Comment"],body:"页面：<%- window.location.origin + (frontmatter.to.path || window.location.pathname) %>"},{frontmatter:n})).render(e)},clear:function(n){var e=document.querySelector("#".concat(n));return e&&e.remove(),!0}},wl=null;function Sl(n){return kl.clear("vuepress-plugin-comment")}function El(n){return!1!==n.comment&&!1!==n.comments}function Tl(n){if(clearTimeout(wl),document.querySelector("main.page"))return kl.render(n,"vuepress-plugin-comment");wl=setTimeout((function(){return Tl(n)}),200)}var _l={mounted:function(){var n=this;wl=setTimeout((function(){var e=Object(fl.a)({to:{},from:{}},n.$frontmatter);Sl()&&El(e)&&Tl(e)}),1e3),this.$router.afterEach((function(e,t){if(!e||!t||e.path!==t.path){var a=Object(fl.a)({to:e,from:t},n.$frontmatter);Sl()&&El(a)&&Tl(a)}}))}},Il=Object(rl.a)(_l,(function(){var n=this.$createElement;return(this._self._c||n)("div")}),[],!1,null,null,null).exports,jl=[function(n){var e=n.Vue,t=(n.options,n.router,n.siteData,n.isServer,!1);e.component(ll.default.name,ll.default),e.mixin({mounted:function(){t||(window.addEventListener("unload",this.saveLastReading),t=!0)},methods:{saveLastReading:function(){localStorage.setItem("lastReading",JSON.stringify({path:this.$route.path,scrollTop:document.documentElement.scrollTop,timestamp:(new Date).getTime()}))}}})},function(n){var e=n.Vue,t=(n.options,n.router,n.siteData);t.pages.map((function(n){var e=n.frontmatter,a=e.date,r=e.author;"string"==typeof a&&"Z"===a.charAt(a.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return"".concat(n.getUTCFullYear(),"-").concat(ml(n.getUTCMonth()+1),"-").concat(ml(n.getUTCDate())," ").concat(ml(n.getUTCHours()),":").concat(ml(n.getUTCMinutes()),":").concat(ml(n.getUTCSeconds()))}(a)),r?n.author=r:t.themeConfig.author&&(n.author=t.themeConfig.author)})),e.mixin(ul)},{},function(n){n.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(n){n.router;"undefined"!=typeof window&&function(){var n=document.createElement("script"),e=window.location.protocol.split(":")[0];n.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(n,t)}()},function(n){var e=n.router;"undefined"!=typeof window&&(window._hmt=window._hmt||[],function(){var n=document.createElement("script");n.src="https://hm.baidu.com/hm.js?503f098e7e5b3a5b5d8c5fc2938af002";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(n,e)}(),e.afterEach((function(n){_hmt.push(["_trackPageview",n.fullPath])})))},function(n){n.Vue.component("Comment",Il)}],zl=["Comment","BlockToggle"];t(217);function Cl(n,e){return(Cl=Object.setPrototypeOf||function(n,e){return n.__proto__=e,n})(n,e)}t(218);function Al(n){return(Al=Object.setPrototypeOf?Object.getPrototypeOf:function(n){return n.__proto__||Object.getPrototypeOf(n)})(n)}function Pl(n,e){if(e&&("object"===_i(e)||"function"==typeof e))return e;if(void 0!==e)throw new TypeError("Derived constructors may only return object or undefined");return function(n){if(void 0===n)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return n}(n)}function Dl(n){var e=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(n){return!1}}();return function(){var t,a=Al(n);if(e){var r=Al(this).constructor;t=Reflect.construct(a,arguments,r)}else t=a.apply(this,arguments);return Pl(this,t)}}var ql=function(n){!function(n,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");n.prototype=Object.create(e&&e.prototype,{constructor:{value:n,writable:!0,configurable:!0}}),Object.defineProperty(n,"prototype",{writable:!1}),e&&Cl(n,e)}(t,n);var e=Dl(t);function t(){return cs(this,t),e.apply(this,arguments)}return ps(t)}(function(){function n(){cs(this,n),this.store=new Dr({data:{state:{}}})}return ps(n,[{key:"$get",value:function(n){return this.store.state[n]}},{key:"$set",value:function(n,e){Dr.set(this.store.state,n,e)}},{key:"$emit",value:function(){var n;(n=this.store).$emit.apply(n,arguments)}},{key:"$on",value:function(){var n;(n=this.store).$on.apply(n,arguments)}}]),n}());Object.assign(ql.prototype,{getPageAsyncComponent:Ni,getLayoutAsyncComponent:Ui,getAsyncComponent:Ji,getVueComponent:$i});var Ll={install:function(n){var e=new ql;n.$vuepress=e,n.prototype.$vuepress=e}};function Bl(n){n.beforeEach((function(e,t,a){if(Ol(n,e.path))a();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){var r=e.path.replace(/\/$/,"")+".html";Ol(n,r)?a(r):a()}else a();else{var o=e.path+"/",i=e.path+".html";Ol(n,i)?a(i):Ol(n,o)?a(o):a()}}))}function Ol(n,e){var t=e.toLowerCase();return n.options.routes.some((function(n){return n.path.toLowerCase()===t}))}var Fl={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(n){var e=this.pageKey||this.$parent.$page.key;return Vi("pageKey",e),Dr.component(e)||Dr.component(e,Ni(e)),Dr.component(e)?n(e):n("")}},Rl={functional:!0,props:{slotKey:String,required:!0},render:function(n,e){var t=e.props,a=e.slots;return n("div",{class:["content__".concat(t.slotKey)]},a()[t.slotKey])}},Ml={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Hl=(t(390),t(391),Object(rl.a)(Ml,(function(){var n=this.$createElement,e=this._self._c||n;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Nl(){return(Nl=Object(a.a)(regeneratorRuntime.mark((function n(e){var t,a,r,o;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:sl.routerBase||sl.base,Bl(a=new Ei({base:t,mode:"history",fallback:!1,routes:il,scrollBehavior:function(n,e,t){return t||(n.hash?!Dr.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})}})),r={},n.prev=4,n.next=7,Promise.all(jl.filter((function(n){return"function"==typeof n})).map((function(n){return n({Vue:Dr,options:r,router:a,siteData:sl,isServer:e})})));case 7:n.next=12;break;case 9:n.prev=9,n.t0=n.catch(4),console.error(n.t0);case 12:return o=new Dr(Object.assign(r,{router:a,render:function(n){return n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},zl.map((function(e){return n(e)})))])}})),n.abrupt("return",{app:o,router:a});case 14:case"end":return n.stop()}}),n,null,[[4,9]])})))).apply(this,arguments)}Dr.config.productionTip=!1,Dr.use(Ei),Dr.use(Ll),Dr.mixin(function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Dr;Ti(e),t.$vuepress.$set("siteData",e);var a=n(t.$vuepress.$get("siteData")),r=new a,o=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(r)),i={};return Object.keys(o).reduce((function(n,e){return e.startsWith("$")&&(n[e]=o[e].get),n}),i),{computed:i}}((function(n){return function(){function e(){cs(this,e)}return ps(e,[{key:"setPage",value:function(n){this.__page=n}},{key:"$site",get:function(){return n}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var n,e,t=this.$site.locales,a=void 0===t?{}:t;for(var r in a)"/"===r?e=a[r]:0===this.$page.path.indexOf(r)&&(n=a[r]);return n||e||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var n=this.$page.frontmatter.canonicalUrl;return"string"==typeof n&&n}},{key:"$title",get:function(){var n=this.$page,e=this.$page.frontmatter.metaTitle;if("string"==typeof e)return e;var t=this.$siteTitle,a=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?a?a+" | "+t:t:a||"VuePress"}},{key:"$description",get:function(){var n=function(n){if(n){var e=n.filter((function(n){return"description"===n.name}))[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(n,e){for(var t=0;t<n.length;t++){var a=n[t];if(a.path.toLowerCase()===e.toLowerCase())return a}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),e}()}),sl)),Dr.component("Content",Fl),Dr.component("ContentSlotsDistributor",Rl),Dr.component("OutboundLink",Hl),Dr.component("ClientOnly",{functional:!0,render:function(n,e){var t=e.parent,a=e.children;if(t._isMounted)return a;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),Dr.component("Layout",Ui("Layout")),Dr.component("NotFound",Ui("NotFound")),Dr.prototype.$withBase=function(n){var e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.5",hash:"b9dff75"},function(n){return Nl.apply(this,arguments)}(!1).then((function(n){var e=n.app;n.router.onReady((function(){e.$mount("#app")}))}))}]);