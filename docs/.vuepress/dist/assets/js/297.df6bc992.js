(window.webpackJsonp=window.webpackJsonp||[]).push([[297],{795:function(s,a,t){"use strict";t.r(a);var e=t(15),r=Object(e.a)({},(function(){var s=this,a=s.$createElement,t=s._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h1",{attrs:{id:"hbase优化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hbase优化"}},[s._v("#")]),s._v(" HBase优化")]),s._v(" "),t("h2",{attrs:{id:"预分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#预分区"}},[s._v("#")]),s._v(" 预分区")]),s._v(" "),t("p",[s._v("每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。")]),s._v(" "),t("h3",{attrs:{id:"手动设定预分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#手动设定预分区"}},[s._v("#")]),s._v(" 手动设定预分区")]),s._v(" "),t("div",{staticClass:"language-sql line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("create")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'staff1'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'info'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("SPLITS "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'1000'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2000'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'3000'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'4000'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[t("img",{attrs:{src:"https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211125213840477.png",alt:"image-20211125213840477"}})]),s._v(" "),t("p",[s._v("分为5个区 0-1000 1000-2000 2000-3000 3000-4000")]),s._v(" "),t("h3",{attrs:{id:"生成16进制序列预分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#生成16进制序列预分区"}},[s._v("#")]),s._v(" 生成16进制序列预分区")]),s._v(" "),t("div",{staticClass:"language-sql line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("create")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'staff2'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'info'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'partition2'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("{NUMREGIONS "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" SPLITALGO "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'HexStringSplit'")]),s._v("}\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("会分为15个区")]),s._v(" "),t("p",[t("img",{attrs:{src:"https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211125214007240.png",alt:"image-20211125214007240"}})]),s._v(" "),t("h3",{attrs:{id:"按文件设置的规则分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#按文件设置的规则分区"}},[s._v("#")]),s._v(" 按文件设置的规则分区")]),s._v(" "),t("p",[s._v("创建splits.txt文件内容如下：")]),s._v(" "),t("div",{staticClass:"language-sh line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-sh"}},[t("code",[s._v("aaaa\nbbbb\ncccc\ndddd\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("div",{staticClass:"language-sql line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("create")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'staff3'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'partition3'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("SPLITS_FILE "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'/home/atguigu/splits.txt'")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[t("img",{attrs:{src:"https://gitee.com/Iekrwh/md-images/raw/master/images/image-20211125214154240.png",alt:"image-20211125214154240"}})]),s._v(" "),t("h3",{attrs:{id:"使用javaapi创建预分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#使用javaapi创建预分区"}},[s._v("#")]),s._v(" 使用JavaAPI创建预分区")]),s._v(" "),t("div",{staticClass:"language-java line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//自定义算法，产生一系列hash散列值存储在二维数组中")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("byte")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" splitKeys "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" 某个散列值函数\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//创建HbaseAdmin实例")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("HBaseAdmin")]),s._v(" hAdmin "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("HBaseAdmin")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("HbaseConfiguration")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("create")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//创建HTableDescriptor实例")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("HTableDescriptor")]),s._v(" tableDesc "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("HTableDescriptor")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tableName"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表")]),s._v("\nhAdmin"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("createTable")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tableDesc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" splitKeys"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("h2",{attrs:{id:"rowkey设计"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rowkey设计"}},[s._v("#")]),s._v(" RowKey设计")]),s._v(" "),t("p",[s._v("一条数据的"),t("strong",[s._v("唯一标识")]),s._v("就是RowKey，那么"),t("strong",[s._v("这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内")]),s._v("，设计RowKey的主要目的 ，就是"),t("strong",[s._v("让数据均匀的分布于所有的region中")]),s._v("，在一定程度上"),t("strong",[s._v("防止数据倾斜")]),s._v("。")]),s._v(" "),t("h3",{attrs:{id:"生成随机数、hash、散列值"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#生成随机数、hash、散列值"}},[s._v("#")]),s._v(" 生成随机数、hash、散列值")]),s._v(" "),t("p",[s._v("比如：\n原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7\n原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd\n原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913\n在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。")]),s._v(" "),t("h3",{attrs:{id:"字符串反转"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#字符串反转"}},[s._v("#")]),s._v(" 字符串反转")]),s._v(" "),t("p",[s._v("20170524000001转成10000042507102\n20170524000002转成20000042507102")]),s._v(" "),t("h3",{attrs:{id:"字符串拼接"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#字符串拼接"}},[s._v("#")]),s._v(" 字符串拼接")]),s._v(" "),t("p",[s._v("20170524000001_a12e\n20170524000001_93i7")]),s._v(" "),t("h2",{attrs:{id:"内存优化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#内存优化"}},[s._v("#")]),s._v(" 内存优化")]),s._v(" "),t("p",[s._v("HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，"),t("strong",[s._v("一般会分配整个可用内存的70%给HBase的Java堆")]),s._v("。但是"),t("strong",[s._v("不建议分配非常大的堆内存")]),s._v("，因为"),t("strong",[s._v("GC")]),s._v("过程持续太久会导致RegionServer处于"),t("strong",[s._v("长期不可用状态")]),s._v("，"),t("strong",[s._v("一般16~48G内存就可以了")]),s._v("，如果因为框架"),t("strong",[s._v("占用内存过高导致系统内存不足")]),s._v("，框架一样会被系统服务拖死。")]),s._v(" "),t("h2",{attrs:{id:"基础优化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基础优化"}},[s._v("#")]),s._v(" 基础优化")]),s._v(" "),t("h3",{attrs:{id:"允许在hdfs的文件中追加内容"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#允许在hdfs的文件中追加内容"}},[s._v("#")]),s._v(" 允许在HDFS的文件中追加内容")]),s._v(" "),t("p",[s._v("在hdfs-site.xml、hbase-site.xml中添加")]),s._v(" "),t("p",[s._v("属性：dfs.support.append")]),s._v(" "),t("p",[s._v("解释："),t("strong",[s._v("开启HDFS追加同步")]),s._v("，可以"),t("strong",[s._v("优秀的配合HBase的数据同步和持久化")]),s._v("。默认值为"),t("strong",[s._v("true")]),s._v("。")]),s._v(" "),t("h3",{attrs:{id:"优化datanode允许的最大文件打开数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#优化datanode允许的最大文件打开数"}},[s._v("#")]),s._v(" 优化DataNode允许的最大文件打开数")]),s._v(" "),t("p",[s._v("hdfs-site.xml")]),s._v(" "),t("p",[s._v("属性：dfs.datanode.max.transfer.threads")]),s._v(" "),t("p",[s._v("解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096")]),s._v(" "),t("h3",{attrs:{id:"优化延迟高的数据操作的等待时间"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#优化延迟高的数据操作的等待时间"}},[s._v("#")]),s._v(" 优化延迟高的数据操作的等待时间")]),s._v(" "),t("p",[s._v("hdfs-site.xml")]),s._v(" "),t("p",[s._v("属性：dfs.image.transfer.timeout")]),s._v(" "),t("p",[s._v("解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。")]),s._v(" "),t("h3",{attrs:{id:"优化数据的写入效率"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#优化数据的写入效率"}},[s._v("#")]),s._v(" 优化数据的写入效率")]),s._v(" "),t("p",[s._v("mapred-site.xml")]),s._v(" "),t("p",[s._v("属性：  mapreduce.map.output.compress  mapreduce.map.output.compress.codec")]),s._v(" "),t("p",[s._v("解释："),t("strong",[s._v("开启这两个数据可以大大提高文件的写入效率")]),s._v("，减少写入时间。")]),s._v(" "),t("p",[s._v("第一个属性值修改为true")]),s._v(" "),t("p",[s._v("第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。")]),s._v(" "),t("h3",{attrs:{id:"设置rpc监听数量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#设置rpc监听数量"}},[s._v("#")]),s._v(" 设置RPC监听数量")]),s._v(" "),t("p",[s._v("hbase-site.xml")]),s._v(" "),t("p",[s._v("属性：Hbase.regionserver.handler.count")]),s._v(" "),t("p",[s._v("解释：默认值为30，"),t("strong",[s._v("用于指定RPC监听的数量")]),s._v("，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。")]),s._v(" "),t("h3",{attrs:{id:"优化hstore文件大小"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#优化hstore文件大小"}},[s._v("#")]),s._v(" 优化HStore文件大小")]),s._v(" "),t("p",[s._v("hbase-site.xml")]),s._v(" "),t("p",[s._v("属性：hbase.hregion.max.filesize")]),s._v(" "),t("p",[s._v("解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为"),t("strong",[s._v("一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长")]),s._v("。该值的意思就是，"),t("strong",[s._v("如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile")]),s._v("。")]),s._v(" "),t("h3",{attrs:{id:"优化hbase客户端缓存"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#优化hbase客户端缓存"}},[s._v("#")]),s._v(" 优化HBase客户端缓存")]),s._v(" "),t("p",[s._v("hbase-site.xml")]),s._v(" "),t("p",[s._v("属性：hbase.client.write.buffer")]),s._v(" "),t("p",[s._v("解释：用于指定"),t("strong",[s._v("Hbase客户端缓存")]),s._v("，增大该值可以减少RPC调用次数，但是会"),t("strong",[s._v("消耗更多内存")]),s._v("，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。")]),s._v(" "),t("h3",{attrs:{id:"指定scan-next扫描hbase所获取的行数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#指定scan-next扫描hbase所获取的行数"}},[s._v("#")]),s._v(" 指定scan.next扫描HBase所获取的行数")]),s._v(" "),t("p",[s._v("hbase-site.xml")]),s._v(" "),t("p",[s._v("属性：hbase.client.scanner.caching")]),s._v(" "),t("p",[s._v("解释：用于"),t("strong",[s._v("指定scan.next方法获取的默认行数")]),s._v("，值越大，消耗内存越大。")]),s._v(" "),t("h3",{attrs:{id:"flush、compact、split机制"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flush、compact、split机制"}},[s._v("#")]),s._v(" flush、compact、split机制")]),s._v(" "),t("p",[s._v("当MemStore达到阈值，"),t("strong",[s._v("将Memstore中的数据Flush进Storefile")]),s._v("；")]),s._v(" "),t("p",[s._v("compact机制则是把flush出来的"),t("strong",[s._v("小文件合并成大的Storefile文件")]),s._v("。")]),s._v(" "),t("p",[s._v("split则是当Region达到阈值，会把过大的"),t("strong",[s._v("Region一分为二")]),s._v("。")]),s._v(" "),t("p",[s._v("涉及属性：")]),s._v(" "),t("p",[s._v("即："),t("strong",[s._v("128M就是Memstore的默认阈值")])]),s._v(" "),t("p",[t("strong",[s._v("hbase.hregion.memstore.flush.size：134217728")])]),s._v(" "),t("p",[s._v("即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。")]),s._v(" "),t("p",[s._v("**hbase.regionserver.global.memstore.upperLimit：0.4  **")]),s._v(" "),t("p",[t("strong",[s._v("hbase.regionserver.global.memstore.lowerLimit：0.38")])]),s._v(" "),t("p",[s._v("即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit")])])}),[],!1,null,null,null);a.default=r.exports}}]);